---
format: pdf
title: "k-nearest-neighbors Handout"
execute:
  fig-height: 3
  warning: false
---

# Introduction to Classification and Training Data

__k-nearest neighbors__ (or __knn__) is an introductory supervised machine learning algorithm, most commonly used as a classification algorithm.

__Classification__ - _def_ - 

&nbsp;

&nbsp;

&nbsp;

<!-- Classification refers to prediction of a categorical response variable with two or more categories.  -->

For example, for a data set with SLU students, we might be interested in predicting whether or not each student graduates in four years (so the response has two categories: graduates in 4 years or doesn't). We might want to classify this response based on various student characteristics like anticipated major, GPA, standardized test scores, etc. knn can also be used to predict a quantitative response, but we'll focus on categorical responses throughout this section.

If you've had STAT 213, you might try to draw some parallels to knn and classification using logistic regression. Note, however, that logistic regression required the response to have __two__ levels while knn can classify a response variable that has __two or more__ levels.

To introduce this, we will be using `pokemon_full.csv` data. Pokemon have different Types: we will use `Type` as a categorical response that we are interested in predicting. For simplicity, we will only use Pokemon's primary type and we will only use __4__ different types: 

```{r, message = FALSE, warning = FALSE, echo = TRUE}
set.seed(1119)
library(tidyverse)
library(here)
pokemon <- read_csv(here("other", "pokemon_full.csv")) |>
  filter(Type %in% c("Steel", "Dark", "Fire", "Ice"))
```

__Overall Goal__: 

<!-- Our goal is to develop a k-nearest-neighbors model that is able to classify/predict Pokemon `Type` from a set of predictors, like Pokemon `HP`, `Attack`, `Defense`, etc. -->

\pagebreak

### Training and Test Data

In order to develop our knn model (note that we still haven't discussed what knn actually is yet!), we first need to discuss terms that applies to almost __all__ predictive/classification modeling: training and test data.

__training data__ - _def_ - 

<!-- A __training__ data set is a subset of the full data set used to fit various models. For the example below, the training data set will be just 15 observations for pedagogical purposes. More commonly, the training data set will contain 50%-80% of the observations in the full data set. -->

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

__test data__ - _def_ - 

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

<!-- data set consists of the remaining 20%-50% of the observations not in the training data set. A test data set is used to assess different the performances of various models that were fit using the training data set. -->

Why do we need to do this division? Using the full data set for both training a model and testing that model is "cheating:" the model will perform better than it should because we are using each observation twice: once for fitting and once for testing. Having a separate test data set that wasn't used to fit the model gives the model a more "fair" test, as these observations are supposed to be new data that the model hasn't yet seen.

The following code uses that `sample_n()` function to randomly select 15 observations to be in the training data set. `anti_join()` then makes a test data set without the 15 pokemon in the training data set. (Note that our training data set would almost always be larger than 15, but we are using a small number here so that we can more clearly see how knn works in the next section).

```{r, message = FALSE}
train_sample <- pokemon |>
  sample_n(15)
test_sample <- anti_join(pokemon, train_sample)
```

The ideas of a training data set and test data set are pervasive in predictive and classification models, including models not related to knn. Note that we are going to do this method because it's the simplest: if you wanted to take this a step further, you'd repeat the training and test process 5 or 10 times, using what's known as __k-fold cross-validation.__

__Exercises__

1. Explain what `anti_join()` joins on when by isnâ€™t specified and why not specifying a by argument works for this example. You may need to use `?anti_join` to answer this question

&nbsp;

\pagebreak

# Introduction to K-Nearest-Neighbors

The purpose of this section is to introduce __k-nearest neighbors__ (or __knn__), an introductory supervised machine learning algorithm, most commonly used to predict a categorical response variable with two or more categories. 

To introduce this, we will be using Pokemon data (the last time many of you will see Pokemon data!). Pokemon have different Types: we will use `Type` as a categorical response that we are interested in predicting. For simplicity, we will only use Pokemon's primary type and we will only use __4__ different types: 

```{r}
#| echo: false
set.seed(1119)
library(tidyverse)
library(pander)
library(here) 
pokemon <- read_csv(here("other", "pokemon_full.csv")) |> filter(Type %in% c("Steel", "Dark", "Fire", "Ice"))
train_sample <- pokemon |>
  sample_n(15)

test_sample <- anti_join(pokemon, train_sample)

train_sample |> head() |> pander()
```

\pagebreak

#### knn with k = 1 and 1 Predictor

Suppose that we have just __15__ pokemon in our data set. We want to predict `Type` from just one predictor, Defense. Below is a plot that shows the defenses of the 15 pokemon in our data set, and has points coloured by Type and with different shapes for Type.

```{r}
#| echo: false
ggplot(data = train_sample, aes(x = Defense, y = 1, colour = Type, shape = Type)) +
  geom_point(size = 4) +  theme(axis.title.y=element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank())
```

We see from the plot that `Steel` type Pokemon tend to have pretty high defense values. Now suppose that we want to predict the `Type` for a new Pokemon not in the original data set, Dialga. We know that Dialga has a `Defense` stat of 120: the plot below shows Dialga marked with a large black X.

&nbsp;

&nbsp;

```{r}
#| echo: false
dialga <- test_sample |> slice(63)
ggplot(data = train_sample, aes(x = Defense, y = 1, colour = Type, shape = Type)) +
  geom_point(size = 4) +  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank()) +
  geom_point(data = dialga, colour = "black", shape = 4, size = 7)
```

What would the prediction for Dialga be? Why?

##### knn with k > 1 and One Predictor

But, we might not necessarily want to predict the response value based on the single nearest neighbor. Dialga is also near many purple plus signs: should those factor in at all? We can extend knn to different values for k. For example, $k = 3$ looks at the 3 nearest neighbors, and assigns a prediction as the category that appears the __most__ among those 3 nearest neighbors.

What would the prediction for Dialga be? Why?

&nbsp;

&nbsp;

&nbsp;

&nbsp;

##### knn with k > 1 and More Than One Predictor

We can increase the number of predictors in our knn model as well. We can generally include as many predictors as we would like, but visualizing becomes challenging with more than 2 predictors and nearly impossible with more than 3 predictors. For the case of two predictors, suppose that we want to use Defense and Speed as our predictors for Type. Dialga, the Pokemon we want to predict for, is again marked with a large black X.

```{r}
#| echo: false
ggplot(data = train_sample, aes(x = Defense, y = Speed, colour = Type, shape = Type)) +
  geom_point(size = 3) +
  geom_point(data = dialga, colour = "black", shape = 4, size = 5)
```

For $k = 1$, what `Type` would you predict for Dialga?

&nbsp;

&nbsp;

&nbsp;

For $k = 3$, what `Type` would you predict for Dialga?

&nbsp;

&nbsp;

&nbsp;

##### Scaling Predictor Variables before Using knn

In general, we want to scale any quantitative predictors when using knn because it relies on distances between points in its predictions. This is easiest to see with an example. Suppose, in our Pokemon example, we just have 2 observations in our data set: a Dark Type pokemon with a height of 15 centimeters and a weight of 505 pounds, and a Fire Type Pokemon with a height of 9 centimeters and a weight of 250 pounds. 

```{r}
#| echo: false
train_tiny <- train_sample |> slice(1:2)
newobs <- tibble(height = 15, weight = 350, Type = "Unknown")
ggplot(data = train_tiny, aes(x = height, y = weight, shape = Type)) +
  geom_point(size = 5, aes(colour = Type)) + xlim(c(7, 17)) + ylim(c(200, 550)) +
  geom_point(data = newobs, shape = 4, size = 10)
```

On the plot is also given a Pokemon that we wish to predict the Type of, marked with a black X. Upon visual inspection, with k = 1, it looks like we would classify this pokemon as Dark. However, because the units of weight and height are on very different scales, if we actually find the distances between the X and the two pokemon in our data set....

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

To get around this issue, it is customary to scale all quantitative predictors before applying knn. One method of doing this is applying

$$
scaled_x = \frac{x - min(x)}{max(x) - min(x)}
$$

For example, scaling `weight` for the 15 original pokemon:

```{r, echo = TRUE}
library(pander)
train_sample |> slice(1:3) |> select(weight) |>
  pander()
```

puts all weights between 0 and 1: 

```{r, echo = TRUE}
train_sample |> mutate(weight_s = (weight - min(weight)) / 
                          (max(weight) - min(weight))) |>
  select(weight_s) |>
  slice(1:3) |>
  pander()
```

If we do the same with `height`, then the variables will contribute more equally to the distance metric used in knn. 

The code below scales all numeric variables in a data set, using the `across()` function. `across()` applies a tranformation to every column in a data set that satisfies the condition given in the `where` argument. You would probably be able to take a data science with `R` class every semester of your college career and still come "across" new super-useful functions!

```{r, echo = TRUE}
library(pander)
train_sample |>
  mutate(across(where(is.numeric), ~ (.x - min(.x)) /
                                 (max(.x) - min(.x)))) |>
  slice(1:3) |> select(2, 3, 4, 5) |>
  pander()
```

#### Exercises

1. In this tiny example, the actual (height, weight) coordinates of the Fire pokemon are (9, 250), the actual coordinates of the Dark pokemon are (15, 505), and the actual coordinates of the test pokemon are (15, 350). We mentioned that, visually, the pokemon looks "closer" to the Dark type pokemon. Verify that this is __not__ actually the case by computing the actual distances numerically.

&nbsp;

&nbsp;

&nbsp;

&nbsp;

2. After scaling according to the formula in this section, the coordinates (height, weight) of the Fire pokemon are (0, 0) and the coordinates of the Dark pokemon are (1, 1). (Since there are only two observations, the formula doesn't give any output between 0 and 1 for this tiny example). The scaled coordinates for the test pokemon are (1, 0.39). Verify that, after scaling, the test pokemon is "closer" to the Dark type pokemon by numerically computing distances.

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

3. Consider again the example with 15 pokemon in the training data set and a single predictor, `Defense`.

```{r, fig.height = 2}
#| echo: false
ggplot(data = train_sample, aes(x = Defense, y = 1, colour = Type, shape = Type)) +
  geom_point(size = 4) +  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank()) +
  geom_point(data = dialga, colour = "black", shape = 4, size = 7)
```

With k = 2, there is a tie between Fire and Steel. Come up with a way in which you might break ties in a knn algorithm.

&nbsp;

&nbsp;

&nbsp;

&nbsp;

4. Explain what knn would use as a prediction for all test observations if k equals the number of observations in the training data set minus 1.

&nbsp;

&nbsp;

&nbsp;

\pagebreak

# Evaluating knn Models

Some things we haven't yet discussed include:

* how should we choose which predictors to explore for knn models?

* how will we evaluate what makes a model "good"?

* how will we choose a value of $k$?

We will tackle each of these questions, to varying degrees, in this section.

### Choosing Predictors

We will continue to use the scaled version of the pokemon data set for this handout. This time, we will have __75__ pokemon in our training data set and we are only looking at the `Steel`, `Dark`, `Fire`, and `Ice` types.

```{r}
set.seed(1119)
library(tidyverse)
library(pander)
library(here) 
pokemon <- read_csv(here("other", "pokemon_full.csv")) |>
  filter(Type %in% c("Steel", "Dark", "Fire", "Ice")) |>
  mutate(across(where(is.numeric), ~ (.x - min(.x)) /
                                 (max(.x) - min(.x)))) 
train_sample <- pokemon |>
  sample_n(75)

test_sample <- anti_join(pokemon, train_sample)

train_sample |> head() |> slice(1:3) |> pander()
```

There are many candidate predictors in this data set: `HP`, `Attack`, `Defense`, ..., all the way up to `base_experience`. How should we determine which predictors to include in our model? 

\pagebreak

Much of this will be trial and error by evaluating different models with a criterion that we will talk about in the next section. However, it is always helpful to explore the data set with graphics to get us to a good starting point. A __scatterplot matrix__ is a useful exploratory tool. The following is a scatterplot matrix with the response variable, `Type`, and just three candidate predictors, `HP`, `Attack`, and `Defense`, created with the `GGally` ("g-g-ally") package.

```{r}
## install.packages("GGally")
library(GGally)
ggpairs(data = train_sample, columns = c(4, 5, 6, 3), 
        lower = list(combo = wrap(ggally_facethist, bins = 15)))
```

The `lower` argument changes the number of bins in the faceted histograms in the bottom row. You can mostly ignore this.

The `columns` argument __is__ important: it allows you to specify which columns you want to look at. I prefer putting the response, `Type` (column `3`) in the last slot.

We can examine this to see which variables seem to have a relationship with `Type`. Where would we want to look for this?

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

What's given on the diagonal of the scatterplot matrix?

&nbsp;

&nbsp;

&nbsp;

&nbsp;

Which variables might we want to include as predictors in a knn model? 

\pagebreak

### Evaluating a knn Model with the Test Data

One definition of "good" in the classification context is a model that has a high proportion of correct predictions in the test data set. This should make some intuitive sense, as we would hope that a "good" model correctly classifies most `Dark` pokemon as `Dark`, most `Fire` pokemon as `Fire`, etc. 

In order to examine the performance of a particular model, we'll create a __confusion matrix__ that shows the results of the model's classification on observations in the test data set. Note that in STAT 213, we didn't call this a confusion matrix; we instead called this a classification table. 

__confusion matrix__ - _def_ - 

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

Let's fit a knn model and see the resulting confusion matrix with a training sample of __75__ pokemon. Don't worry about the code to fit the model yet, but note that we've already scaled the predictor variables to be between 0 and 1 in a previous `R` chunk.

```{r}
library(tidyverse)
set.seed(11232021) ## run this line so that you get the same 
## results as I do!

train_sample_2 <- pokemon |>
  sample_n(75)
test_sample_2 <- anti_join(pokemon, train_sample_2)
```

```{r}
library(class)

## create a data frame that only has the predictors
## that we will use
train_small <- train_sample_2 |> select(HP, Attack, Defense)
test_small <- test_sample_2 |> select(HP, Attack, Defense)

## put our response variable into a vector
train_cat <- train_sample_2$Type
test_cat <- test_sample_2$Type
```

```{r}
## fit the knn model with 9 nearest neighbors
knn_mod <- knn(train = train_small, test = test_small,
               cl = train_cat, k = 9)
knn_mod
```

The output of `knn_mod` gives the predicted categories for the __test__ sample. We can compare the predictions from the knn model with the actual pokemon `Type`s in the test sample with `table()`, which makes a confusion matrix:

```{r}
table(knn_mod, test_cat) |> pander() 
```

The columns of the confusion matrix give the actual Pokemon types in the test data `test_cat` while the rows give the predicted types from our knn model.

Interpret the value in the top-left of the confusion matrix.

&nbsp;

&nbsp;

&nbsp;

Interpret the value in the row with __Dark__ and the column with __Ice__.

&nbsp;

&nbsp;

&nbsp;

One common metric used to assess overall model performance is the model's __classification rate__, which is computed as the number of correct classifications divided by the total number of observations in the test data set.

Calculate our classification rate:

&nbsp;

&nbsp;

&nbsp;

The following is code that can automatically calculate the classification rate:

```{r, eval = FALSE}
tab <- table(knn_mod, test_cat) 
sum(diag(tab)) / sum(tab)
```

A baseline classification rate to compare to is a model that just classifies everything in the test data set as the most common `Type` in the training data set. In this case, the most common `Type` in the training data set is `Fire`, and there were 17 Fire pokemon in the test data set (out of 45 total Pokemon). So, a baseline model with no predictors has a classification rate of:

&nbsp;

&nbsp;

\pagebreak

### Choosing k

We will choose $k$, the number of neighbors considered, using a bit of trial and error as well. However, we will discuss the relative advantages of smaller and larger k values. Which value is "best" is entirely dependent on the data at hand!

What are some advantages for making k smaller?

&nbsp;

&nbsp;

&nbsp;

&nbsp;

What are some advantages for making k larger?

&nbsp;

&nbsp;

&nbsp;

&nbsp;

We will now use `R` to practice fitting these knn models on this Pokemon data set.




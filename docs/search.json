[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DATA / STAT 234",
    "section": "",
    "text": "Syllabus and Course Information\nWelcome! This is the course materials website for DATA / STAT 234. Though we will use other sources at times, we will use materials on this site most heavily."
  },
  {
    "objectID": "index.html#general-information",
    "href": "index.html#general-information",
    "title": "DATA / STAT 234",
    "section": "General Information",
    "text": "General Information\nInstructor Information\n\nProfessor: Matt Higham\nOffice: Bewkes 123\nEmail: mhigham@stlawu.edu\nSemester: Spring 2024\nSections:\n\nMWF 10:30 - 11:30\nMWF 11:40 - 12:40\n\nOffice Hours: 15 minute slots bookable at my calendly page.\n\nNote that you must book a time for office hours at least 12 hours in advance to guarantee that I am present and available at that time.\n\n\nCourse Materials\n\nDATA_STAT 234 Materials Bundle. This will be our primary source of materials.\nTextbooks (only used as references):\n\nR for Data Science by Grolemund and Wickham, found here in a free online version.\nModern Data Science with R by Baumer, Kaplan, and Horton, found here in a free online version.\n\nComputer with Internet access."
  },
  {
    "objectID": "index.html#course-information",
    "href": "index.html#course-information",
    "title": "DATA / STAT 234",
    "section": "Course Information",
    "text": "Course Information\nWelcome to DATA/STAT 234! The overall purpose of this course is learn the data science skills necessary to complete large-scale data analysis projects. The tool that we will be using to achieve this goal is the statistical software language R. We will work with a wide variety of interesting data sets throughout the semester to build our R skills. In particular, we will focus on the Data Analysis Life Cycle (Grolemund and Wickham 2020):\n\nWe will put more emphasis on the Import, Tidy, Transform, Visualize, and Communicate parts of the cycle, as an introduction to Modeling part is covered in STAT 213.\n\nUse of R and RStudio\nWe will use the statistical software R to construct graphs and analyze data. A few notes:\n\nR and RStudio are both free to use.\nAdditionally, we will be using Quarto for data analysis reports. Note: It’s always nice to start assignments and projects as early as possible, but this is particularly important to do for assignments and projects involving R. It’s no fun to try and figure out why code is not working at the last minute. If you start early enough though, you will have plenty of time to seek help and therefore won’t waste a lot of time on a coding error."
  },
  {
    "objectID": "index.html#general-course-outcomes",
    "href": "index.html#general-course-outcomes",
    "title": "DATA / STAT 234",
    "section": "General Course Outcomes",
    "text": "General Course Outcomes\n\nImport data of a few different types into R for analysis.\nTidy data into a form that can be more easily visualized, summarised, and modeled.\nTransform, Wrangle, and Visualize variables in a data set to assess patterns in the data.\nCommunicate the results of your analysis to a target audience with a written report, or, possibly an oral presentation.\nPractice reproducible statistical practices through the use of Quarto for data analysis projects.\nExplain why it is ethically important to consider the context that a data set comes in.\nDevelop the necessary skills to be able to ask and answer future data analysis questions on your own, either using R or another program, such as Python.\n\nTo paraphrase the R for Data Science textbook, about 80% of the skills necessary to do a complete data analysis project can be learned through coursework in classes like this one. But, 20% of any particular project will involve learning new things that are specific to that project. Achieving Goal # 7 will allow you to learn this extra 20% on your own."
  },
  {
    "objectID": "index.html#how-you-will-be-assessed",
    "href": "index.html#how-you-will-be-assessed",
    "title": "DATA / STAT 234",
    "section": "How You Will Be Assessed",
    "text": "How You Will Be Assessed\nThe components to your grade are described below:\n\nModules\n\nAside from the first module (which has a unique structure), the modules in the course will be composed of either:\n\nAn exercise set due on Monday (worth 5 points), a take-home quiz due on Wednesday (worth 10 points), and a handwritten in-class quiz on Wednesday (worth 60 points). We should have 4 total modules follow this structure.\nAn exercise set due on Monday (worth 5 points), a take-home quiz due on Wednesday (worth 10 points), and an in-class coding quiz taken on a computer on Wednesday (worth 60 points). We should have 4 total modules follow this structure.\nAn exercise set due on Monday (worth 5 points), a participation assessment (worth 15 points), and a Project due Wednesday (worth 55 points). We should have 3 total modules follow this structure.\n\nThe lowest module will be dropped from your grade so the total number of points available from all modules is 11 * 75 = 825 points.\nAdditionally, for one module, you are permitted to complete whatever is due for that module late and turn it in by the following class period. If there is an in-class quiz as part of the module, you should contact me via email to schedule a make-up time to take the quiz (again, this time should be scheduled before the following class).\nFinally, if you choose to take the (optional) in-person final exam (described below), the score that you earn on that final will replace your second lowest module score.\n\nFinal Project\n\nThere is one final project, worth 100 points. The primary purpose of the final project is to give you an opportunity to assemble topics throughout the course into one coherent data analysis. You will be able to choose the data set you use for your final project, so you might begin thinking about a particular topic or data set you are interested in exploring. The final project will be presented in a format to be decided later in the semester.\n\nFinal Exam\n\nThere is an optional Final Exam worth 75 points. You must be on campus for our final exam time if you would like to take the optional final exam. If you do not take the optional final, then the average of all of your module scores will be used for the 75 points for this exam.\nIf you take the final exam and if your final exam score percentage is better than your 2nd lowest module grade, then that grade will be replaced with your final exam score. For example, suppose your 13 module scores are: 75, 70, 70, 70, 65, 65, 59, 58, 40, 34, 15, 0. You take the final exam and score a 60 / 75. Then, the 60 is used for your final exam score and your new module scores would be: 75, 70, 70, 70, 65, 65, 59, 58, 40, 34, 60, 0. The 0 would still be dropped as your lowest module score.\n\nBreakdown\n\n825 points for Modules\n75 points for the (optional) Final Exam\n100 points for the Final Project\n\nPoints add up to 1000 so your grade at the end of the semester will be the number of points you’ve earned across all categories divided by 1000.\n\n\nGrading Scale\nThe following is a rough grading scale. I reserve the right to make any changes to the scale if necessary.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrade\n4.0\n3.75\n3.5\n3.25\n3.0\n2.75\n2.5\n2.25\n2.0\n1.75\n1.5\n1.25\n1.0\n0.0\n\n\n\n\nPoints\n950-1000\n920-949\n890-919\n860-889\n830-859\n810-829\n770-809\n750-769\n720-749\n700-719\n670-699\n640-669\n600-639\n0-599"
  },
  {
    "objectID": "index.html#collaboration-diversity-accessibility-and-academic-integrity",
    "href": "index.html#collaboration-diversity-accessibility-and-academic-integrity",
    "title": "DATA / STAT 234",
    "section": "Collaboration, Diversity, Accessibility, and Academic Integrity",
    "text": "Collaboration, Diversity, Accessibility, and Academic Integrity\n\nRules for Collaboration\nCollaboration with your classmates on exercises, take-home quizzes, and projects is encouraged, but you must follow these guidelines:\n\nyou must state the name(s) of who you collaborated with at the top of each assessment.\nall work must be your own. This means that you should never send someone your code via email or let someone directly type code off of your screen. Instead, you can talk about strategies for solving problems and help or ask someone about a coding error.\nyou may use the Internet and StackExchange, but you also should not copy paste code directly from the website, without citing that you did so. Policies about AI will be clearly stated on each individual assignment: for most assignments, you are permitted to use AI as long as you clearly state all queries that you make and clearly state what you used from the AI responses to the queries.\n\n\n\n\n\nDiversity Statement\nDiversity encompasses differences in age, colour, ethnicity, national origin, gender, physical or mental ability, religion, socioeconomic background, veteran status, sexual orientation, and marginalized groups. The interaction of different human characteristics brings about a positive learning environment. Diversity is both respected and valued in this classroom.\n\n\n\n\nAccessibility Statement\nThe message below is copied from the Student Accessibility Services Office:\nYour experience in this class is important to me. It is the policy and practice of St. Lawrence University to create inclusive and accessible learning environments consistent with federal and state law. If you have established accommodations with the Student Accessibility Services Office in the past, please activate your accommodations so we can discuss how they will be implemented in this course.\nIf you have not yet established services through the Student Accessibility Services Office but have a temporary health condition or permanent disability that requires accommodations (conditions include but not limited to; mental health, attention-related, learning, vision, hearing, physical or health impacts), please contact the Student Accessibility Services Office directly to set up a meeting. The Student Accessibility Services Office will work with you on the interactive process that establishes reasonable accommodations.\nColor Vision Deficiency: The Student Accessibility Services office can loan glasses for students who are color vision deficient. Please contact the office to make an appointment.\nFor more specific information about setting up an appointment with Student Accessibility Services please see the options listed below:\nTelephone: 315.229.5537\nEmail: studentaccessibility@stlawu.edu\nWebsite: https://www.stlawu.edu/offices/student-accessibility-services\n\n\n\n\nAcademic Dishonesty\nAcademic dishonesty will not be tolerated. Any specific policies for this course are supplementary to the\nHonor Code. According to the St. Lawrence University Academic Honor Policy,\n\nIt is assumed that all work is done by the student unless the instructor/mentor/employer gives specific permission for collaboration.\nCheating on examinations and tests consists of knowingly giving or using or attempting to use unauthorized assistance during examinations or tests.\nDishonesty in work outside of examinations and tests consists of handing in or presenting as original work which is not original, where originality is required.\n\nClaims of ignorance and academic or personal pressure are unacceptable as excuses for academic dishonesty. Students must learn what constitutes one’s own work and how the work of others must be acknowledged.\nFor more information, refer to www.stlawu.edu/acadaffairs/academic_honor_policy.pdf.\nTo avoid academic dishonesty, it is important that you follow all directions and collaboration rules and ask for clarification if you have any questions about what is acceptable for a particular assignment or exam. If I suspect academic dishonesty, a score of zero will be given for the entire assignment in which the academic dishonesty occurred for all individuals involved and Academic Honor Council will be notified. If a pattern of academic dishonesty is found to have occurred, a grade of 0.0 for the entire course can be given.\nIt is important to work in a way that maximizes your learning. Be aware that students who rely too much on others for the homework and projects tend to do poorly on the quizzes and exams.\nPlease note that in addition the above, any assignments in which your score is reduced due to academic dishonesty will not be dropped according to the quiz policy e.g., if you receive a zero on a quiz because of academic dishonesty, it will not be dropped from your grade."
  },
  {
    "objectID": "index.html#tentative-schedule",
    "href": "index.html#tentative-schedule",
    "title": "DATA / STAT 234",
    "section": "Tentative Schedule",
    "text": "Tentative Schedule\n\n\n\n\n\n\n\n\nWeek\nDate\nTopics\n\n\n\n\n0\n1/17\nIntroduction to R, R Studio\n\n\n1\n1/22\nGraphics with ggplot2\n\n\n2\n1/29\nData Wrangling with dplyr\n\n\n\n\n\n\n\n3\n2/5\nCommunication with Quarto and ggplot2\n\n\n4\n2/12\nSoft Skills and Workflow\n\n\n5\n2/19\nData Tidying with tidyr\n\n\n6\n2/26\nBase R\n\n\n7\n3/4\nFactors with forcats and Data Ethics\n\n\n8\n3/11\nData Import with readr, jsonlite, rvest, and tibble\n\n\n9\n3/25\nData Merging with dplyr\n\n\n10\n4/1\nIntro to Statistical/Machine Learning with knn\n\n\n11\n4/8\nText Data with tidytext and stringr\n\n\n12\n4/15\nDates and Times with lubridate\n\n\n13\n4/22\nConnections to STAT and CS\n\n\n14\n4/29\nDatabases and SQL with dbplyr"
  },
  {
    "objectID": "01-intro.html#installation",
    "href": "01-intro.html#installation",
    "title": "1  Getting Started",
    "section": "\n1.1 Installation",
    "text": "1.1 Installation\nIn this section, we will work on installing R and R Studio to your personal laptop. The following videos provide instructions on how to install R and R Studio to your laptop computer. It will be easiest if you complete all of these steps consecutively in one sitting.\n\nWatch and follow along with a video on installing R.\n\nstarting link: https://www.r-project.org/\n\n\n\nWatch and follow along with a video on installing R Studio.\n\nstarting link: https://posit.co/download/rstudio-desktop/\n\n\n\nWatch and follow along with a video on installing R packages and making an R project ."
  },
  {
    "objectID": "01-intro.html#intro-to-r-and-r-studio",
    "href": "01-intro.html#intro-to-r-and-r-studio",
    "title": "1  Getting Started",
    "section": "\n1.2 Intro to R and R Studio\n",
    "text": "1.2 Intro to R and R Studio\n\nR is a statistical computing software used by many statisticians as well as professionals in other fields, such as biology, ecology, business, and psychology. The goal of Week 0 is to provide basic familiarity with R and Quarto, which we will be using for the entire semester.\n\n1.2.1 Installing R and R Studio\n\nThe R Studio server is a computer that is set-up to carry out any R-based analyses for students with remote access to the computer (in our case, through SLU Login credentials). It might be helpful to think about the server as a large machine with no keyboard and no screen: it’s only purpose is to execute the code. You may have used the R Studio server in a different stats course. The server does have some benefits, such as\n\nusing the server ensures that we are all using the same version of R. In theory, if one person gets an error, then everyone should get that same error.\ninstalling R and R Studio on your personal device is much easier after you’ve had some experience using it through the server.\nyou don’t need a computer that is capable of running R to use the server (you can use a tablet or a Chromebook since the server does all of the actual computation).\n\nWe, however, will move away from the server and install R and R Studio on our own devices. Though the server does have some advantages, there are also some disadvantages:\n\nyou won’t have your SLU login forever, so, if you wanted to use R post graduation, you’d need to know how to install it.\nyou haven’t had experience installing R packages. This is quite easy to do, but I’ve installed all necessary R packages on the server for us so you haven’t had to worry about this step.\nthe server requires good Internet access and also has the potential to crash.\n\n1.2.2 Creating an R Project\nAfter you have both R and R Studio installed, open R Studio in your Applications. Create a new folder on your Desktop (or some other place that is easy for you to access and remember). Make sure the folder name has no spaces in it.\nThen, in R Studio, create an R Project by Clicking File -&gt; New Project -&gt; Existing Directory. Navigate to the DATA234 folder you made, and click Create Project. You should see a new window of R Studio open up.\n\nFinally, we want to create a new Quarto file by clicking File -&gt; New File -&gt; Quarto Document. You can give your new Quarto file a title if you want, and then click okay.\nWe are also going to change one option routinely in our Quarto files. Change the first few lines of the file to be something like:\n---\ntitle: \"Your Title\"\nauthor: \"Your Name\"\nformat: \n  html:\n    embed-resources: true\n---\n\n\n\n\n\n\nNote\n\n\n\nThe embed-resources: true option ensures that all figures, images, tables, etc. are contained in the one .html file, which is important because, for quizzes and exercises, you will typically only turn in the .html file.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBefore moving on, click the Render button in the top-left window at the top of the menu bar. Make sure that the file renders to a pretty-looking .html file. The newly rendered .html file can now be found in your folder with your R project."
  },
  {
    "objectID": "01-intro.html#what-are-r-r-studio-and-quarto",
    "href": "01-intro.html#what-are-r-r-studio-and-quarto",
    "title": "1  Getting Started",
    "section": "\n1.3 What are R, R Studio, and Quarto?",
    "text": "1.3 What are R, R Studio, and Quarto?\nThe distinction between the 3 will become more clear later on. For now,\n\nR is a statistical coding software used heavily for data analysis and statistical procedures.\nR Studio is a nice IDE (Integrated Development Environment) for R that has a lot of convenient features. Think of this as just a convenient User Interface.\nQuarto allows users to mix regular Microsoft-Word-style text with code. The .qmd file ending denotes an Quarto file. Quarto has many options that we will use heavily throughout the semester, but there’s no need to worry about these now.\n\n\n1.3.1 R Packages and the tidyverse\n\nYou can think of R packages as add-ons to R that let you do things that R on its own would not be able to do. If you’re in to video games, you can think of R packages as extra Downloadable Content (DLC). But, unlike most gaming DLC, R packages are always free and we will make very heavy use of R packages.\nThe tidyverse is a series of R packages that are useful for data science. In the order that we will encounter them in this class, the core tidyverse packages are:\n\n\nggplot2 for plotting data\n\ndplyr for data wrangling and summarizing\n\ntidyr for data tidying and reshaping\n\nreadr for data import\n\ntibble for how data is stored\n\nstringr for text data\n\nforcats for factor (categorical) data\n\npurrr, for functional programming, the only one of these core 8 that we won’t get to use\n\nWe will use packages outside of the core tidyverse as well, but the tidyverse is the main focus.\n\n\n1.3.2 Installing R Packages\nOn the R Studio server, either myself or one of the other statistics faculty members have installed all packages that we’ve needed to use on the server globally. However, if you want to use a package that isn’t installed on the server, or, you want to use a package using R Studio on your own personal computer, you need to install it first.\n\n\n\n\n\n\nImportant\n\n\n\nInstallation only needs to happen once (or until you upgrade R, which usually doesn’t happen too often), whereas the package needs to be loaded with library() every time you open R.\n\n\nThe analogy of a lightbulb might be helpful. You only need to screw in the lightbulb into a socket once, but, every time you want the lightbulb to provide light, you need to flip the light switch.\nIn the lightbulb analogy, what does putting the lightbulb into the socket correspond to? What does flipping the light switch correspond to?\nNow that you have R on your own computer, you’ll need to install all packages that you want to use (but, remember that you just need to install each package once). Try installing the tidyverse package, a collection of many useful data science packages, with:\n\ninstall.packages(\"tidyverse\")"
  },
  {
    "objectID": "01-intro.html#putting-code-in-a-.qmd-file",
    "href": "01-intro.html#putting-code-in-a-.qmd-file",
    "title": "1  Getting Started",
    "section": "\n1.4 Putting Code in a .qmd File",
    "text": "1.4 Putting Code in a .qmd File\nThe first thing that we will do that involves code is to load a package into R with the library() function. A package is just an R add-on that lets you do more than you could with just R on its own. Load the tidyverse package into R by typing and running the library(tidyverse) line. To create a code chunk, click Insert -&gt; R. Within this code chunk, type in library(tidyverse) and run the code by either\n\nClicking the “Run” button in the menu bar of the top-left window of R Studio or\n(Recommended) Clicking “Command + Enter” on a Mac or “Control + Enter” on a PC.\n\nNote that all code appears in grey boxes surrounded by three backticks while normal text has a different colour background with no backticks.\n\nlibrary(tidyverse)\n\nWhen you run the previous line, some text will appear in the bottom-left window. We won’t worry too much about what this text means now, but we also won’t ignore it completely. You should be able to spot the 8 core tidyverse packages listed above as well as some numbers that follow each package. The numbers correspond to the package version. There’s some other things too, but as long as this text does not start with “Error:”, you’re good to go!\nCongrats on running your first line of code for this class! This particular code isn’t particularly exciting because it doesn’t really do anything that we can see.\nIn your R chunk, on a new line, try typing in a basic calculation, like 71 + 9 or 4 / 3, them run the line and observe the result.\nSo, that still wasn’t super exciting. R can perform basic calculations, but you could just use a calculator or Excel for that. In order to look at things that are a bit more interesting, we need some data."
  },
  {
    "objectID": "01-intro.html#election-data-example",
    "href": "01-intro.html#election-data-example",
    "title": "1  Getting Started",
    "section": "\n1.5 Election Data Example",
    "text": "1.5 Election Data Example\nWe will first use a data set on the 2000 United States Presidential election between former President George Bush and Al Gore obtained from http://www.econometrics.com/intro/votes.htm. For those unfamiliar with U.S. political elections, it is enough to know that each state is allocated a certain number of “electoral votes” for the president: states award all of their electoral votes to the candidate that receives the most ballots in that state. You can read more about this strange system on Wikipedia.\nFlorida is typically a highly-contentious “battleground” state. The data set that we have has the following variables, recorded for each of the 67 counties in Florida:\n\n\nGore, the number of people who voted for Al Gore in 2000\n\nBush, the number of people who voted for George Bush in 2000\n\nBuchanan, the number of people who voted for the third-party candidate Buchanan\n\nNader, the number of people who voted for the third-party candidate Nader\n\nOther, the number of people who voted for a candidate other than the previous 4 listed\n\nCounty, the name of the county in Florida\n\nRead in the data set, name the data set pres_df, and take a look at the data set by running the head(pres_df) line, which shows the first few observations of the data set:\n\nlibrary(tidyverse)\n\npres_df &lt;- read_csv(\"https://raw.githubusercontent.com/highamm/ds234_quarto/main/data_online/pres2000.csv\") \nhead(pres_df)\n#&gt; # A tibble: 6 × 6\n#&gt;     Gore   Bush Buchanan Nader Other County  \n#&gt;    &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   \n#&gt; 1  47365  34124      263  3226   751 ALACHUA \n#&gt; 2   2392   5610       73    53    26 BAKER   \n#&gt; 3  18850  38637      248   828   242 BAY     \n#&gt; 4   3075   5414       65    84    35 BRADFORD\n#&gt; 5  97318 115185      570  4470   852 BREVARD \n#&gt; 6 386561 177323      788  7101  1623 BROWAR\n\n\n\n\n\n\n\nNote\n\n\n\nPay special attention to the variable names given at the top of the printed output: we’ll need to use these names when we make all of our plots. And, R is case-sensitive, meaning that we will, for example, need to use Gore, not gore.\n\n\nLet’s go ahead and begin our exploration of the data by making a histogram of the number of people who voted for Gore in each county. Recall that a histogram is useful if we would like a graph of a single quantitative variable. Copy the following code to an R chunk and run the code:\n\nggplot(data = pres_df, aes(x = Gore)) +\n  geom_histogram(colour = \"black\", fill = \"white\") +\n  labs(x = \"Votes for Gore in Florida\")\n\n\n\n\nWe can also change some things in the code above. The following brief questions will ask you to add or change small things about the historam we created for the number of votes.\nExercise 1. What do the 1e+05, 2e+05, etc. labels on the x-axis mean?\nExercise 2. R gives us a message to “Pick a better value with binwidth” instead of the default bins = 30. Add , bins = 15 inside the parentheses of geom_histogram() to change the number of bins in the histogram.\nExercise 3. Change the colour of the inside of the bins to \"darkred\". Do you think that the colour of the inside of the bins maps to colour or fill? Try both!\n\n\n\n\n\n\nHint (only if you get stuck)\n\n\n\n\n\nFor the histogram, colour controls the outside lines of the bins while fill controls what gets filled in the bins.\n\n\n\nExercise 4. There are a couple of observations with very high vote values. What could explain these large outliers?\nExercise 5. We can also change the variable that is being plotted on the histogram. Try changing x = Gore, to x = Bush to plot a histogram of the number of votes Bush received in Florida counties.\n\nThus far, we’ve figured out that there a couple of counties with very large numbers of votes for Gore and very large number of votes for Bush. We don’t know the reason for this (if some counties are very democratic, very republican, or if some counties are just more populous). Do the counties that have a large number of votes for Bush also tend to have a large number of votes for Gore? And what about the other candidates: do they have any interesting patterns?\nLet’s start by making a scatterplot of the number of votes for Gore and the number of votes for Bush.\n\n\n\n\n\n\nNote\n\n\n\nThe geom_ for making a scatterplot is called geom_point() because we are adding a layer of points to the plot.\n\n\n\nggplot(data = pres_df, aes(x = Gore, y = Bush)) +\n  geom_point()\n\n\n\n\nWhat patterns do you see in the scatterplot?\nNow, let’s change the x variable from Gore to Buchanan. You should notice something strange in this scatterplot. Try to come up with one explanation for why the outlying point has so many votes for Buchanan.\n\nggplot(data = pres_df, aes(x = Buchanan, y = Bush)) +\n  geom_point()\n\n\n\n\n\nIn trying to come up with an explanation, it would be nice to figure out which Florida county has that outlying point and it would be nice if we knew something about Florida counties. To remedy the first issue, recall that we can type View(pres_df) in the lower-left window of R Studio to pull up a spreadsheet of the data set. Once you have the new window open, click on the column heading Buchanan to sort the votes for Buchanan from high to low to figure out which county is the outlier.\nUse some Google sleuthing skills to find an explanation: try to search for “2000 united states presidential election [name of outlier county]”. Write a sentence about what you find.\n\n\n\n\n\n\nHint\n\n\n\n\n\nIf nothing useful pops up in an initial search, try adding the term “butterfly ballot” to your search.\n\n\n\nWe have used the 2000 Presidential data set to find out something really interesting! In particular, we have used exploratory data analysis to examine a data set, without having a specific question of interest that we want to answer. This type of exploring is often really useful, but does have some drawbacks, which we will discuss later in the semester."
  },
  {
    "objectID": "01-intro.html#alcohol-data-example",
    "href": "01-intro.html#alcohol-data-example",
    "title": "1  Getting Started",
    "section": "\n1.6 Alcohol Data Example",
    "text": "1.6 Alcohol Data Example\nWe will be looking at two data sets just to get a little bit of a preview of things we will be working on for the rest of the semester.\n\n\n\n\n\n\nImportant\n\n\n\nDo not worry about understanding what the following code is doing at this point. There will be plenty of time to understand this in the weeks ahead. The purpose of this section is just to get used to using R: there will be more detailed explanations and exercises about the functions used and various options in the coming weeks. In particular, the following code uses the ggplot2, dplyr, and tidyr packages, which we will cover in detail throughout the first ~ 3-4 weeks of this course.\n\n\nData for this first part was obtained from fivethirtyeight at Five Thirty Eight GitHub page.\nThe first step is to read the data set into R. Though you have already downloaded alcohol.csv in the data zip, we still need to load it into R. Check to make sure the alcohol.csv is in the data folder in your bottom-right hand window. The following code can be copied to an R code chunk to read in the data:\n\nread_csv(\"https://raw.githubusercontent.com/highamm/ds234_quarto/main/data_online/alcohol.csv\")\n\nNote that we do not need the full file extension if we have the data set in an R project.\nDid something show up in your console window? If so, great! If not, make sure that the data set is in the data folder and that you have an R project set up.\nWe would like to name our data set something so that we could easily reference it later, so name your data set using the &lt;- operator, as in\n\nalcohol_data &lt;- read_csv(\"https://raw.githubusercontent.com/highamm/ds234_quarto/main/data_online/alcohol.csv\")\n\n\n\n\n\n\n\nNote\n\n\n\nYou can name your data set whatever you want to (with a few restrictions), but it’s generally helpful to give it a name that you will remember and can easily be typed moving forward. So, assigning the name alcohol_data makes more sense than assigning the data to a name like fjrewrjweaipjfadpgfj.\n\n\nNow, if you run the line of code above where you name the data set, and run alcohol_data, you should see the data set appear:\n\nalcohol_data\n#&gt; # A tibble: 193 × 5\n#&gt;   country  beer_servings spirit_servings wine_servings total_litres_of_pure…¹\n#&gt;   &lt;chr&gt;            &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt;                  &lt;dbl&gt;\n#&gt; 1 Afghani…             0               0             0                    0  \n#&gt; 2 Albania             89             132            54                    4.9\n#&gt; 3 Algeria             25               0            14                    0.7\n#&gt; 4 Andorra            245             138           312                   12.4\n#&gt; 5 Angola             217              57            45                    5.9\n#&gt; 6 Antigua…           102             128            45                    4.9\n#&gt; # ℹ 187 more rows\n#&gt; # ℹ abbreviated name: ¹​total_litres_of_pure_alcohol\n\nWhat’s in this data set? We see a few variables on the columns:\n\n\ncountry: the name of the country\n\nbeer_servings: the average number of beer servings per person per year\n\nspirit_servings: the average number of spirit (hard alcohol) servings per person per year\n\nwine_servings: the average number of wine servings per person per year\n\ntotal_litres_of_pure_alcohol: the average total litres of pure alcohol consumed per person per year.\n\nOne goal of this class is for you to be able to pose questions about a data set and then use the tools we will learn to answer those questions. For example, we might want to know what the distribution of total litres of alcohol consumed per person looks like across countries. To do this, we can make a plot with the ggplot2 package, one of the packages that automatically loads with tidyverse. We might start by constructing the following plot. Reminder: the goal of this is not for everyone to understand the code in this plot, so don’t worry too much about that.\n\nggplot(data = alcohol_data,\n       mapping = aes(total_litres_of_pure_alcohol)) +\n  geom_histogram(colour = \"black\", fill = \"white\", bins = 15)\n\n\n\n\nI now want to see where the United States (USA) falls on this distribution by drawing a red vertical line for the total litres of alcohol consumed in the United States. To do so, I’ll first use the filter() function in the dplyr package (again, we will learn about that function in detail later). Copy and paste the following lines of code into a new R chunk. Then, run the lines.\n\nsmall_df &lt;- alcohol_data |&gt; filter(country == \"USA\")\nggplot(data = alcohol_data,\n       mapping = aes(total_litres_of_pure_alcohol)) +\n  geom_histogram(colour = \"black\", fill = \"white\", bins = 15) +\n  geom_vline(data = small_df,\n             aes(xintercept = total_litres_of_pure_alcohol),\n             colour = \"red\")\n\nIt looks like there are some countries that consume little to no alcohol. We might want to know what these countries are:\n\nalcohol_data |&gt; filter(total_litres_of_pure_alcohol == 0)\n\nIt looks like there are 13 countries in the data set that consume no alcohol. Note that, in the chunk above, we have to use in total_litres_of_pure_alcohol as the variable name because this is the name of the variable in the data set. Even something like spelling litres in the American English liters (total_liters_of_pure_alcohol) would throw an error because this isn’t the exact name of the variable in the data set. This is something that can be very aggravating when you are first learning any coding language.\nNow suppose that we want to know the 3 countries that consume the most beer, the 3 countries that consume the most spirits, and the 3 countries that consume the most wine per person. If you’re a trivia person, you can form some guesses. Without cheating, I am going to guess (Germany, USA, and UK) for beer, (Spain, Italy, and USA) for wine, and (Russia, Poland, and Lithuania) for spirits. Let’s do beer first!\n\nalcohol_data |&gt; mutate(rankbeer = rank(desc(beer_servings))) |&gt;\n  arrange(rankbeer) |&gt; \n  filter(rankbeer &lt;= 3)\n\nLet’s do the same thing for Wine and Spirits:\n\nalcohol_data |&gt; mutate(rankwine = rank(desc(wine_servings))) |&gt;\n  arrange(rankwine) |&gt; \n  filter(rankwine &lt;= 3)\n\nalcohol_data |&gt; mutate(rankspirits = rank(desc(spirit_servings))) |&gt;\n  arrange(rankspirits) |&gt; \n  filter(rankspirits &lt;= 3)\n\nFinally, suppose that I want to know which country consumes the most wine relative to their beer consumption? Let’s first look at this question graphically. I need to tidy the data first with the pivot_longer() function from the tidyr package:\n\nonecountry_df &lt;- alcohol_data |&gt; \n  filter(country == \"Denmark\")\n\nggplot(data = alcohol_data,\n       mapping = aes(x = beer_servings, y = wine_servings)) + \n  geom_point(alpha = 0.5) +\n  geom_text(data = onecountry_df, aes(label = country),\n    colour = \"purple\", nudge_y = 20) +\n  geom_point(data = onecountry_df, colour = \"purple\",\n             size = 2.5, shape = 1) +\n  geom_abline(aes(slope = 1, intercept = 0), alpha = 0.3)\n\n\n\n\nThe x-axis corresponds to beer servings while the y-axis corresponds to wine servings. A reference line is given so with countries above the line consuming more wine than beer. We will get into how to make a plot like this later: for now, copy the code chunk and change the labeled point so that it corresponds to a country that interests you (other than Denmark). We might be able to better answer the original question numerically by computing the wine to beer ratio for each country and then ordering from the largest ratio to the smallest ratio:\n\nalcohol_data |&gt;\n  mutate(wbratio = wine_servings / beer_servings) |&gt;\n  arrange(desc(wbratio)) |&gt;\n  select(country, beer_servings, wine_servings, wbratio)\n\nWhy is one of the ratios Inf?\n\n1.6.1 Exercises\n\nWhat is the shape of the distribution of total alcohol consumption? Left-skewed, right-skewed, or approximately symmetric? Unimodal or multimodal?\nIn the histogram of total alcohol consumption, pick a country other than the USA that interests you. See if you can change the code in the chunk that made the histogram so that the red vertical line is drawn for the country that interests you.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the View() function to look at the alcohol data set by typing View(alcohol_data) in your bottom-left window to help you see which countries are in the data set.\n\nView(alcohol_data)\n\nCareful about capitalization: R is case sensitive so USA is different than usa.\n\n\n\n\nIn the histogram of total alcohol consumption, change the fill colour of the bins in the histogram above: what should be changed in the code chunk?\nIn the spirit rankings, why do you think only 2 countries showed up instead of 3? Can you do any investigation as to why this is the case?\nIn the rankings code, what if you wanted to look at the top 5 countries instead of the top 3? See if you could change the code.\nChange the wine to beer ratio code example to find the countries with the highest beer to wine consumption (instead of wine to beer consumption)."
  },
  {
    "objectID": "01-intro.html#athlete-data-example",
    "href": "01-intro.html#athlete-data-example",
    "title": "1  Getting Started",
    "section": "\n1.7 Athlete Data Example",
    "text": "1.7 Athlete Data Example\nAs another example, we will look at a data set on the top 100 highest paid athletes in 2014. The athletesdata was obtained from https://github.com/ali-ce/datasets data set has information on the following variables from the 100 highest paid athletes of 2014, according to Forbes (pay includes both salary and endorsements):\n\n\nName (name of the athlete)\n\nRank (where the athlete ranks, with 1 being the highest paid)\n\nSport (the sport the athlete plays)\n\nendorsements (money from sponsorships from companies)\n\ntotalpay (in millions in the year of 2014, salary + endorsements)\n\nsalary (money from tournaments or contract salary)\n\nage of athlete in 2014\n\nGender (Male or Female)\n\nWe will first read in the data set below and name it athletes. We can then use the head() function to look at the first few rows of the data set.\n\nathletes &lt;- read_csv(\"https://raw.githubusercontent.com/highamm/ds234_quarto/main/data_online/athletesdata.csv\")\nhead(athletes)\n#&gt; # A tibble: 6 × 9\n#&gt;    ...1 Name             Rank Sport endorsements totalpay salary   age Gender\n#&gt;   &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; \n#&gt; 1     1 Aaron Rodgers      55 Foot…      7500000 22000000 1.45e7    31 Male  \n#&gt; 2     2 Adam Scott         95 Golf       9000000 17700000 8.70e6    34 Male  \n#&gt; 3     3 Adrian Gonzalez    60 Base…       400000 21500000 2.11e7    32 Male  \n#&gt; 4     4 Alex Rodriguez     48 Base…       300000 22900000 2.26e7    39 Male  \n#&gt; 5     5 Alfonso Soriano    93 Base…        50000 18050000 1.80e7    38 Male  \n#&gt; 6     6 Amar'e Stoudem…    27 Bask…      5000000 26700000 2.17e7    32 Male\n\nThere are many different interesting questions to answer with this data set. First, we might be interested in the relationship between athlete age and salary for the top 100 athletes. Recall from an earlier stat course that one appropriate graphic to examine this relationship is a scatterplot:\n\nggplot(data = athletes, mapping = aes(x = age, y = salary)) + \n  geom_point() +\n  geom_smooth(se = FALSE)\n\n\n\n\nDo you see anything strange with the scatterplot? What do you think the y-axis tick labels of 2.5e+07, 5.0e+07, etc. mean?\nNow let’s see if we can count the number of athletes in the Top 100 that are in my personal favourite sport, Tennis:\n\nathletes |&gt; group_by(Sport) |&gt;\n  summarise(counts = n()) |&gt;\n  filter(Sport == \"Tennis\")\n#&gt; # A tibble: 1 × 2\n#&gt;   Sport  counts\n#&gt;   &lt;chr&gt;   &lt;int&gt;\n#&gt; 1 Tennis      6\n\nIt looks like there are 6 athletes: we can see who they are and sort them by their Rank with:\n\nathletes |&gt;\n  filter(Sport == \"Tennis\") |&gt;\n  arrange(Rank)\n#&gt; # A tibble: 6 × 9\n#&gt;    ...1 Name             Rank Sport endorsements totalpay salary   age Gender\n#&gt;   &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; \n#&gt; 1    82 Roger Federer       7 Tenn…     52000000 56200000 4.20e6    33 Male  \n#&gt; 2    78 Rafael Nadal        9 Tenn…     30000000 44500000 1.45e7    28 Male  \n#&gt; 3    72 Novak Djokovic     17 Tenn…     21000000 33100000 1.21e7    27 Male  \n#&gt; 4    64 Maria Sharapova    34 Tenn…     22000000 24400000 2.40e6    27 Female\n#&gt; 5    60 Li Na              41 Tenn…     18000000 23600000 5.6 e6    32 Female\n#&gt; 6    89 Serena Williams    55 Tenn…     11000000 22000000 1.10e7    33 Female\n\nFinally, let’s see if we can compare the ratio of endorsements (from commercials and products) to salary of professional athletes in the Top 100 in 2 sports: Football (referring to American Football) and Basketball. Recall from an earlier Stat class that we might want to use side-by-side boxplots to make this comparison since we have one categorical variable (Sport Type) and one quantitative variable (Ratio of Endorsements to Salary).\n\nfootball_basketball &lt;- athletes |&gt;\n  filter(Sport == \"Football\" | Sport == \"Basketball\")\n\nggplot(data = football_basketball,\n       aes(x = Sport, y = endorsements / salary)) + \n  geom_boxplot() +\n  labs(y = \"Endorsements / Salary\")\n\n\n\n\nIn the graph, an endorsements / salary ratio of 1 indicates that the person makes half of their overall pay from endorsements and half of their overall pay from salary.\nWhich sport looks like it tends to receive a larger proportion of their overall pay from endorsements for athletes in the top 100?\n\n\n1.7.1 Exercises\n\nInstead of looking at the relationship between age and salary in the top 100 athletes of 2014, change the plot to look at the relationship between age and endorsements. What would you change in the code above? Try it!\nPick a Sport other than Tennis and see if you can count the number of athletes in the top 100 in that sport as well as sort them by Rank.\n\n\n\n\n\n\n\nCaution\n\n\n\nNot all sports will have athletes in the Top 100.\n\n\nHow many athletes are in the top 100 in the sport that you chose?\n\nIn the endorsements / salary example, change one of the sports to the sport of your choice and make a comparison. Which sport tends to receive a larger proportion of their overall pay from endorsements.\nWhat qualification might you want to make about your statement in the previous exercise? (Is this a random sample of athletes from each sport? Why does that matter?).\nIn the side-by-side boxplots comparing the endorsements to salary ratio of two different sports, I’ve changed the y-axis label above to be Endorsements / Salary using the labs(y = \"Endorsements / Salary\") statement. Try changing the x-axis label to something else. What do you think you would need to add to the plot?"
  },
  {
    "objectID": "01-intro.html#finishing-up-common-errors-in-r",
    "href": "01-intro.html#finishing-up-common-errors-in-r",
    "title": "1  Getting Started",
    "section": "\n1.8 Finishing Up: Common Errors in R\n",
    "text": "1.8 Finishing Up: Common Errors in R\n\nWe will now talk a little bit about getting errors in R and what can be done to correct some common errors.\nYou may have encountered some errors by this point in the document. Let’s go over a few common errors as well as discuss how to comment your code.\n\nA missing parenthesis: any open parenthesis ( needs to close ). Try running the following code chunk without fixing anything.\n\n\nggplot(data = athletes, aes(x = Sport, y = salary) + \n  geom_boxplot()\n\nNotice in your bottom-left window that the &gt; symbol that starts a line changes to a +. This is generally bad!! It means that you forgot to close a parenthesis ) or a quote (' or \"). No code will run since R thinks you are still trying to type something into a function. To fix this issue, click your cursor into the bottom-left window and press Esc. Then, try to find the error in the code chunk.\n\nCan you find the missing closing parenthesis above?\n\n\nMissing Comma. Try running the following code chunk without fixing anything.\n\n\nggplot(data = athletes aes(x = Sport, y = salary)) + \n  geom_boxplot()\n\nR gives you an “Error: unexpected symbol in ….”. Oftentimes, this means that there is a missing comma or that you spelled a variable name incorrectly.\n\nCan you find the missed comma above?\n\n\nCapitalization Issues\n\n\nathletes |&gt; filter(sport == \"Tennis\")\n\nIn the original data set, the variable Sport is capitalized. Not capitalizing it means that R won’t be able to find it and proclaims that “object sport not found”.\n\nForgetting Quotes. Character strings need to have quotation marks around them. We will discuss more of this later, but graph labels and titles need to have quotes around them since they don’t directly refer to columns or rows in our data set:\n\n\nggplot(data = athletes, aes(x = Sport, y = endorsements)) + \n  geom_boxplot() + xlab(Popularity Measure)\n\nThe error for forgetting quotes is typically an “Unexpected Symbol” though this error is also given for other issues.\n\nWhere are the quotes missing in the code chunk above?\n\n\nFinally, we can add a comment to a code chunk with the # symbol (I always use double ## for some reason though). This allows you to type a comment into a code chunk that isn’t code:\n\n## this is a comment\n## this calculation might be useful later\n7 * 42\n#&gt; [1] 294\n\n\n\n\n\n\n\nImportant\n\n\n\nComments are most useful for longer code chunks, as they allow you to remember why you did something. They also tell someone whom you’ve shared your code with why you did something.\n\n\nSave this file by clicking File -&gt; Save or by using the keyboard shortcut Command + s (or Control + s on a PC). Render this file by clicking the Render button in the top-left window. You should see a .html file pop up, if there are no errors in your code!"
  },
  {
    "objectID": "01-intro.html#chapexercise-1",
    "href": "01-intro.html#chapexercise-1",
    "title": "1  Getting Started",
    "section": "\n1.9 Your Turn",
    "text": "1.9 Your Turn\nOpen a new .qmd file (File -&gt; New File -&gt; Quarto Document -&gt; OK) and delete the text explaining what Quarto is. Make sure that your Quarto document is embed-resources by using something like the following in the first few lines of the file:\n---\ntitle: \"Your Title\"\nauthor: \"Your Name\"\nformat: \n  html:\n    embed-resources: true\n---\nThen, complete the following exercise.\nYour Turn 1. For each question, type your answer on a new line, with a line space between your answers. All of these questions should be answered outside of code chunks since your answers will all be text, not code.\n\nWhat is your name and what is your class year (first-year, sophomore, junior, senior)?\nWhat is/are your major(s) and minor(s), either actual or intended?\nWhy are you taking this course? (Major requirement?, Minor requirement?, recommended by advisor or student?, exploring the field?, etc.). If you are taking it for a major or minor requirement, why did you decide to major or minor in statistics or data science?\nIn what semester and year did you take STAT 113 and who was your professor?\nHave you taken STAT 213? Have you taken CS 140?\nWhat is your hometown: city, state, country?\nDo you play a sport on campus? If so, what sport? If not, what is an activity that you do on or off-campus?\nWhat is your favorite TV show or movie or band/musical artist?\nTell me something about yourself.\nTake a look at the learning outcomes listed on the syllabus. Which are you most excited for and why?\nWhat are your expectations for this class and/or what do you hope to gain from this class?\nTake a moment to scroll through the advice from students who took this course in the Fall semester of 2021 or the Fall semester of 2022. What is one piece of advice that you hope to apply to our course this semester?\n\n\nRender your .qmd file into an .html file and submit your rendered .html file to Canvas in the “Practice Submission” Assignment. If your file won’t render, then submit the .qmd file instead.\nNice work: we will dive into ggplot() in the ggplot2 package next!"
  },
  {
    "objectID": "02-ggplot2.html#introduction-and-basic-terminology",
    "href": "02-ggplot2.html#introduction-and-basic-terminology",
    "title": "2  Plotting with ggplot2",
    "section": "\n2.1 Introduction and Basic Terminology",
    "text": "2.1 Introduction and Basic Terminology\nWe will begin our data science journey with plotting in the ggplot2 package. We are starting with plotting for a couple of reasons:\n\nPlotting is cool! We get to see an immediate result of our coding efforts in the form of a nice-to-look-at plot.\nIn an exploratory data analysis, you would typically start by making plots of your data.\n\nThroughout the first couple of sections, we will try to go very light on the technical code terminology to start out with (but we will come back to some things later in the semester). The terminology will make a lot more sense once you’ve actually worked with data. But, there are three terms that will be thrown around quite a bit in the next few weeks: function, argument, and object.\n\na function in R is always* (*always for this class) followed by an open ( and ended with a closed ). In non-technical terms, a function does something to its inputs and is often analogous to an English verb. For example, the mean() function calculates the mean, the rank() functions ranks a variable from lowest to highest, and the labs() is used to add labels to a plot. Every function has a help file that can be accessed by typing in ?name_of_function. Try typing ?mean in your lower left window.\nan argument is something that goes inside the parentheses in a function. Arguments could include objects, or they might not. In the bottom-left window, type ?mean to view the Help file on this R function. We see that mean() has 3 arguments: x, which is an R object, trim, and na.rm. trim = 0 is the default, which means that, by default, R will not trim any of the numbers when computing the mean.\nan object is something created in R, usually with &lt;-. So, in the code below where we read in the STAT 113 survey data set, stat113_df is an R object.\n\n\n\n\n\n\n\nNote\n\n\n\nThis terminology will make more sense as we go through these first couple of weeks and see some examples."
  },
  {
    "objectID": "02-ggplot2.html#basic-plot-structure",
    "href": "02-ggplot2.html#basic-plot-structure",
    "title": "2  Plotting with ggplot2",
    "section": "\n2.2 Basic Plot Structure",
    "text": "2.2 Basic Plot Structure\nWe will use the ggplot() function in the ggplot2 package to construct visualizations of data. the ggplot() function has 3 basic components:\n\na data argument, specifying the name of your data set (pres_df above)\na mapping argument, specifying that specifies the aesthetics of your plot (aes()). Common aesthetics are x position, y position, colour, size, shape, group, and fill.\na geom_    () component, specifying the geometric shape used to display the data.\n\nThe components are combined in the following form:\n\n## DO NOT COPY this code into your .qmd file: it will not run.\nggplot(data = name_of_data, aes(x = name_of_x_var,\n                                y = name_of_y_var,\n                                colour = name_of_colour_var,\n                                etc.)) +\n  geom_nameofgeom() +\n  .....&lt;other stuff&gt;\n\nThe structure of ggplot() plots is based on the Grammar of Graphics https://www.springer.com/gp/book/9780387245447. As with most new things, the components above will be easier to think about with some examples.\nThroughout this section, we will be using survey data from STAT 113 many years ago for the exercises in this section. For those who may not have taken STAT 113 from having AP credit or another reason, the STAT 113 survey is given to all students in STAT 113 across all sections. Some analyses in Intro Stat are then carried out using the survey.\nCreate a new Quarto file. Then, read in the data set by copying and pasting the R chunk below into an R chunk in your Quarto file.\n\nlibrary(tidyverse)\nstat113_df &lt;- read_csv(\"https://raw.githubusercontent.com/highamm/ds234_quarto/main/data_online/stat113.csv\")\n\nThe data set contains the following variables:\n\n\nYear, FirstYear, Sophomore, Junior, or Senior\n\nSex, M or F (for this data set, Sex is considered binary).\n\nHgt, height, in inches.\n\nWgt, weight, in pounds.\n\nHaircut, how much is paid for a haircut, typically.\nGPA\n\nExercise, amount of hours of exercise in a typical week.\n\nSport, whether or not the student plays a varsity sport.\n\nTV, amount of hours spent watching TV in a typical week.\n\nAward, Award preferred: choices are Olympic Medal, Nobel Prize, or Academy Award.\n\nPulse, pulse rate, in beats per minute.\n\nSocialMedia, most used social media platform (Instagram, SnapChat, FaceBook, Twitter, Other, or None)."
  },
  {
    "objectID": "02-ggplot2.html#introduction-to-plotting",
    "href": "02-ggplot2.html#introduction-to-plotting",
    "title": "2  Plotting with ggplot2",
    "section": "\n2.3 Introduction to Plotting",
    "text": "2.3 Introduction to Plotting\nWe will now move into making many of the graphics that you saw in STAT 113. We will also introduce some new types of graphs that you might not have seen before. Throughout the section, we will also introduce some of the syntax used in making these plots so that, by the end, you should be able to construct an entire plot on your own.\n\n2.3.1 Single Quantitative Variable\nIn STAT 113, you probably used a few graphs to explore a single quantitative variable. We will begin with what is probably the most common graph to use in this situation: the histogram. The following code creates a histogram for the GPA variable in the data set named stat113_df.\n\nggplot(data = stat113_df, aes(x = GPA)) +\n  geom_histogram()\n\n\n\n\nWe can add some function arguments to geom_histogram() to make the graph a little more pleasing to the eye. For the following histogram, we are changing the colour, fill, and the number of bins of the histogram of GPA:\n\nggplot(data = stat113_df, aes(x = GPA)) +\n  geom_histogram(colour = \"black\", fill = \"white\", bins = 17)\n\n\n\n\nLet’s note some of the components of the plot:\n\n\ndata = stat113_df tells R which data set to use. In this case, we want to use the data set that we read in earlier and named stat113_df.\n\naes(x = GPA) tells R to map the GPA variable in stat113_df to the x axis on the plot. We will come back to aes() (aesthetics) in a bit.\n\ngeom_histogram() tells R that we want to use the histogram GEOM to bin the GPA variable and construct the histogram. The extra options colour = \"black\", fill = \"white\", bins = 17 are options for the plot that change the colour, fill, and number of bins in the histogram.\n\nThere are other graphs besides a histogram that we can use to visualize a quantitative variable. Another useful graph is a frequency plot, made with the geom geom_freqpoly(). The code to make a frequency plot is given below.\n\n\n\n\n\n\nNote\n\n\n\nThe only thing that changes in the code is the GEOM: the data and the fact that we are mapping GPA to the x-axis remain consistent with the histogram.\n\n\n\nggplot(data = stat113_df, aes(x = GPA)) +\n  geom_freqpoly(bins = 15)\n\n\n\n\nThe frequency plot is just like a histogram but the counts are connected by a line instead of represented with bins. You can see how they relate by including both a geom_freqpoly() and a geom_histogram() in your plot\n\n\n\n\n\n\nImportant\n\n\n\nThe code below doesn’t make the prettiest graph but it does show how more complex graphs are built with ggplot(): by continually adding layers to the plot.\n\n\n\nggplot(data = stat113_df, aes(x = GPA)) +\n    geom_histogram(colour = \"black\", fill = \"white\", bins = 17) +\n    geom_freqpoly(bins = 15)\n\n\n\n\nExercise 1. Change the variable being plotted in the earlier histogram from GPA to another variable in the data set. Note that the variable must be quantitative. The variable names were given earlier in this section, but you can also see the variable names if you type in stat113_df in your bottom left window. Also, you can run names(stat113_df) in your bottom left window of R Studio to obtain all of the variable names in the stat113_df data frame.\nExercise 2. In the histogram you just created, change the colour of the histogram to be \"dodgerblue4\" and the fill of the histogram to be \"dodgerblue1\".\nExercise 3. There are a lot of named colours that you can use in R. After looking through some at http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf, change the colour and fill of the histogram again to be colours of your choice.\n\n2.3.2 Single Categorical Variable\nThe most common graph for visualizing a single categorical variable is a bar plot. The following code makes a bar plot with the SocialMedia variable from the STAT 113 survey. Keep in mind that the data is from 2018 - 2019, before the explosion of Tik Tok!\n\nggplot(data = stat113_df, aes(x = SocialMedia)) +\n  geom_bar()\n\n\n\n\nNote the syntax used to create the plot: we still are specifying data, the name of the data set used (stat113_df) and we still are mapping a variable (SocialMedia) to the x axis of the plot. geom_bar() is the GEOM used to create a bar plot.\nFor a single categorical variable, bar plots are by far the most common plot to make so we won’t discuss any other plots here.\nExercise 4. Choose another categorical variable to plot, and change x = SocialMedia to x = name_of_your_variable.\nExercise 5. geom_bar() also has colour and fill options just like geom_histogram() did. Add these arguments into geom_bar() in the plot you just created. You can choose whichever colours you want for colour and fill!\n\n2.3.3 Two Variables: Both Categorical\nTo explore the relationship between two categorical variables, we can construct a stacked barplot. Let’s look at the relationship between Year and SocialMedia first using two versions of a stacked bar plot: one stacked bar plot of the raw counts and one stacked bar plot of proportions.\nTo make a stacked bar plot of raw counts, we map the SocialMedia variable to the fill aesthetic:\n\nggplot(data = stat113_df, aes(x = Year, fill = SocialMedia)) +\n  geom_bar() \n\n\n\n\nBecause the number of students from each class are drastically different, it is difficult to read the distribution of SocialMedia preference for the levels of class Year with fewer students (Juniors and Seniors). Changing the y-axis from a count to a proportion of students who prefer that particular SocialMedia within each class year can help mitigate this problem:\n\nggplot(data = stat113_df, aes(x = Year, fill = SocialMedia)) +\n  geom_bar(position = \"fill\") +\n  labs(y = \"Proportion\")\n\n\n\n\nWe will not worry too much about the position = \"fill\" argument needed to make this change at this point. But, do note that, (1) with the proportion graph, we no longer see the sample size and (2) we’ve added labs(y = \"Proportion\") to change the y-axis label of the graph.\nExercise 6. Change both the x and fill variables to be two other categorical variables from the STAT 113 data set. Your new plot can either be of the raw counts or of the proportion. Write a one sentence interpretation of any findings from your new plot.\n\n2.3.4 Two Variables: One Cat. and One Quant.\nIf we are interested in exploring the relationship between a categorical variable and a quantitative variable, side-by-side boxplots can be useful. Let’s stick with the STAT 113 survey data to examine the relationship between Exercise, the amount of exercise in hours per week and Award preference.\n\nggplot(data = stat113_df, aes(x = Award, y = Exercise)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAgain, examine the consistency in the syntax: we specify data to be the name of the data set, we map Award to the x axis, Exercise to the y axis, and we provide a relevant GEOM, geom_boxplot().\n\n\nAlternatively, we can make side-by-side violin plots.\n\nggplot(data = stat113_df, aes(x = Award, y = Exercise)) +\n  geom_violin()\n\n\n\n\nThe violin plots are similar to boxplots, but are wider in areas that generally have more data.\nA third common option is what is known as a strip plot, given below:\n\nggplot(data = stat113_df, aes(x = Award, y = Exercise)) +\n  geom_jitter(width = 0.10)\n\n\n\n\nThe geom_jitter() GEOM adds a little bit of “noise” to the points that are plotted. In this case, the noise is added to the width and how much noise is added depends on the value of the width argument.\n\n\n\n\n\n\nImportant\n\n\n\nOne big advantage of a strip plot over a side-by-side boxplots and violin plots is that we can get a rough idea about the sample size in each group in the strip plot.\n\n\nExercise 7. Change the side-by-side violin plot to be of a different quantitative variable and a different categorical variable from the stat113_df data frame. Write a one sentence interpretation about any findings from your new plot.\nExercise 8. To your violin plot in the previous exercise, change the default colour and fill arguments to be two different colours of your choice.\n\n2.3.5 Two Quantitative Variables\nFor this, we will work with some fitness data collected from my Apple Watch since November 2018. The higham_fitness_clean.csv contains information on the following variables:\n\n\nStart, the month, day, and year that the fitness data was recorded on\n\nmonth, the month\n\nweekday, the day of the week\n\ndayofyear, the day of the year (so that 304 corresponds to the 304th day of the year)\n\ndistance, distance walked in miles\n\nsteps, the number of steps taken\n\nflights, the number of flights of stairs climbed\n\nactive_cals, the number of calories burned from activity\n\nstepgoal, whether or not I reached 10,000 steps for the day\n\nweekend_ind, a variable for whether or not the day of the week was a weekend day (Saturday or Sunday) or a weekday (Monday - Friday).\n\nRead in the data set by putting the following code into an R chunk in your Quarto file:\n\nlibrary(tidyverse)\nfitness_full &lt;- read_csv(\"https://raw.githubusercontent.com/highamm/ds234_quarto/main/data_online/higham_fitness_clean.csv\",\n                         col_types = list(stepgoal = col_factor())) |&gt;\n  mutate(weekend_ind = case_when(weekday == \"Sat\" | weekday == \"Sun\" ~ \"weekend\",\n                                 .default = \"weekday\"))\n\nBy far, the most common plot to visualize the relationship between two quantitative variables is the scatterplot, which is made with the geom_point() GEOM. Again, in the following code, try to note the similarities in syntax with the previous plots: we still specify the data frame that we are using, and we still map variables in that data frame to aesthetics on the plot.\nIn the following scatterplot, the variable distance is the x-variable and active_cals is the y-variable.\n\nggplot(data = fitness_full, aes(x = distance, y = active_cals)) +\n  geom_point()\n\n\n\n\nExercise 9. Do you see anything “odd” about the plot? Make a note of any strange observations that you see.\n\n\n\n\n\n\nHint\n\n\n\n\n\nIn particular, what about having 0’s for either distance or active_cals could indicate a flaw in how the data was collected?\n\n\n\nExercise 10. Change the x and y variables plotted to two different quantitative variables in the fitness data set. Give a one sentence interpretation of any findings from your new plot.\n\n\n\n\n\n\nHint\n\n\n\n\n\nRecall from STAT 113 that three aspects you might think about are the strength of the relationship (weak, moderate, strong), whether or not the relationship is linear, and the direction of the relationship (positive or negative)\n\n\n\n\n2.3.6 More Than Two Variables\nWhen constructing a graph involving more than two variables, there are two commonly used strategies: (1) Use another plot aesthetic for the third variable (most commonly a colour aesthetic) and (2) Facet by the third variable.\nFirst, in addition to x and y aesthetics, we can also use aes() to map variables to things like colour, size, and shape. For example, using the STAT 113 data set again, we might make a scatterplot with Wgt on the x-axis and Hgt on the y-axis, and use the colour aesthetic for the Sex of the student. In this older version of the STAT 113 survey there are only two levels of Sex in the data: M and F.\n\nggplot(data = stat113_df, aes(x = Wgt, y = Hgt, colour = Sex)) +\n  geom_point()\n\n\n\n\nColour is the most popular aesthetic, but other aesthetics include shape (if the third variable is categorical) and size (if the third variable is quantitative). Neither of these are particularly useful for this example but examples of using each are given in the next code chunk.\n\nggplot(data = stat113_df, aes(x = Wgt, y = Hgt, shape = Sex)) +\n  geom_point()\n\n\n\nggplot(data = stat113_df, aes(x = Wgt, y = Hgt, size = Exercise)) +\n  geom_point()\n\n\n\n\nA second option for including a third variable in a plot is to facet by that variable using the facet_wrap() function. The syntax of facet_wrap() is facet_wrap(~ name_of_faceting_variable).\n\n\n\n\n\n\nImportant\n\n\n\nFaceting creates a subplot for each level of the variable that you choose to facet by.\n\n\nFor example, in the following plot, we make a scatterplot of Hgt vs. Wgt and facet by Year. This makes 4 subplots: one for each class Year:\n\nggplot(data = stat113_df, aes(x = Wgt, y = Hgt)) +\n  geom_point() +\n  facet_wrap(~ Year)\n\n\n\n\nIn general, the more levels of the variable there are, the more useful faceting is compared to using a colour aesthetic. We will explore this idea more in the Exercises.\nExercise 11. In the fitness_full data, construct a scatterplot of two of the quantitative variables and facet by one of the categorical variables."
  },
  {
    "objectID": "02-ggplot2.html#practice",
    "href": "02-ggplot2.html#practice",
    "title": "2  Plotting with ggplot2",
    "section": "\n2.4 Practice",
    "text": "2.4 Practice\nIn general, practice exercises will be split between exercises that will be done together as a class and exercises that you will do in groups or on your own. The purpose of the class exercises is to give some guidance on how we might think logically through some of the code and the results. The purpose of the group exercises and the on your own exercises is so that you have a chance to practice what you’ve learned with your table or on your own.\n\n2.4.1 Class Exercises\nClass Exercise 1. As we progress through the semester, we will pay close attention to the way that our data set is structured. In the earlier bar plot that we made of the SocialMedia variable from the stat113_df data frame, we had a column called SocialMedia where each row represented a student. geom_bar() then counts up the total number of times each level appears to make the bar plot.\nHowever, sometimes, data are in format such that one column contains the levels of the categorical variable while another column contains the counts directly. For example, we can create such a data set using code that we will learn next week and name it stat113_restructured:\n\nstat113_restructured &lt;- stat113_df |&gt; group_by(SocialMedia) |&gt;\n  summarise(n_social = n())\nstat113_restructured\n#&gt; # A tibble: 6 × 2\n#&gt;   SocialMedia n_social\n#&gt;   &lt;chr&gt;          &lt;int&gt;\n#&gt; 1 Facebook          13\n#&gt; 2 Instagram        173\n#&gt; 3 None               3\n#&gt; 4 Other             16\n#&gt; 5 Snapchat         171\n#&gt; 6 Twitter           21\n\n\nSuppose that you attempted to make a bar plot of SocialMedia with stat113_restructured using geom_bar(). Predict what the plot will look like.\nConstruct the plot described in (a) to test your prediction.\nWhen the data is structured such that there is a column that uniquely gives the levels of the categorical variable and a second column that gives the count of each level, we typically use the geom_col() GEOM instead of geom_bar(). geom_col() uses both an x and a y aesthetic mapping. Construct the bar plot using stat113_restructured with the geom_col() GEOM.\n\nClass Exercise 2. To the scatterplot of distance vs. active calories, add a smoother with the geom_smooth() GEOM. After adding the smoother, investigate the following arguments: (1) method, (2) se, and (3) span.\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou can investigate the arguments either by typing ?geom_smooth in your lower left window or by using Google or ChatGPT.\n\n\n\nClass Exercise 3. Another useful GEOM for some types of quantitative variables is geom_line(), which is often used if the x-axis variable is time. Construct a line plot with the fitness_full data to look at how steps changes through time.\nClass Exercise 4. Thus far, we have only faceted by a single variable. With the stat113_df data frame, figure out (with the help of Google) how to facet by two variables to make a plot that shows the relationship between GPA (y-axis) and Exercise (x-axis) with four facets: one for male students who play a sport, one for female students who play a sport, one for male students who do not play a sport, and one for female students who do not play a sport.\nClass Exercise 5. In STAT 113, boxplots are typically introduced using the * symbol to identify outliers. Using a combination of the help ?geom_boxplot and Googling “R point shapes”, figure out how to modify your side-by-side boxplots so that the outliers are shown using *, not the default dots.\nClass Exercise 6. We can change the y-axis of a histogram to be “density” instead of a raw count. This means that each bar shows a proportion of cases instead of a raw count. Google something like “ggplot2 geom_histogram with density” to figure out how to create a y aes() to show density instead of count.\n\n2.4.2 Your Turn\nYour Turn 1. Suppose that you ask each student in this class the following question: Is your major Statistics, Data Science, or Other. You now want to make a bar plot of your hypothetical data. In pairs, have one person complete part (a) and the second person complete part (b).\n\nSketch out (with pen and paper) how the data would be structured if you were to use geom_bar() to make a bar plot of the Major variable.\nSketch out (with pen and paper) how the data would be structured if you were to use geom_col() to make a bar plot of the Major variable.\n\nYour Turn 2. With a partner, decide on two quantitative variables and one categorical variable from the STAT 113 data set that you are interested in exploring. Have one person complete part (a) and the second person complete part (b).\n\nMake a scatterplot that colours the points by the categorical variable.\nMake a set of faceted scatterplots (that are faceted by the levels of the categorical variable).\n\nWith your partner, decide which of the two graphs is “better” for exploring the three variables and why.\nYour Turn 3. Repeat the previous exercise with your partner with different variables, but switch parts (so that you will work on (a) if you previously worked on (b) and vice versa).\nYour Turn 4. Besides strip plots, boxplots, and violin plots, we can also make coloured frequency plots or faceted histograms if we are interested in exploring the relationship between a quantitative variable and a categorical variable. With a partner, decide on one quantitative and one categorical variable that you wish to explore from the fitness_full data set. Have one person complete part (a) and the second person complete part (b).\n\nMake a coloured frequency plot of the variables you selected (colouring by the levels of the categorical variable).\nMake a set of faceted histograms (faceting by the categorical variable that you selected).\n\nYour Turn 5. In your group, discuss why it would not make sense to construct a line plot of Exercise vs. GPA from the stat113_df data set.\n\n\n\n\n\n\nHint\n\n\n\n\n\nWhat about the GPA variable in particular would make construction of a line plot impossible?\n\n\n\nYour Turn 6. In your group, discuss why you think R gives us a warning message that it “Removed 70 rows containing non-finite values” whenever we make a plot of the GPA variable from stat113_df.\nYour Turn 7. On your own, construct a plot of your choosing, with any variables from either of the data sets we have used so far! Give a one sentence interpretation of your findings from your plot.\nYour Turn 8. A common theme that we’ll see throughout the course is that it’s advantageous to know as much background information as possible about the data set we are analyzing. Data sets will be easier to analyze and pose questions about if you’re familiar with the subject matter.\nIn your group, give an example of something that you know about STAT 113 and the survey data set that helped you answer or pose a question that someone from another university (and therefore unfamiliar with our intro stat course) wouldn’t know.\nIn your group, give an example of something that you don’t know about the fitness data set that the person who owns the fitness data would know. Why does that give an advantage to the person who is more familiar with the fitness data?"
  },
  {
    "objectID": "02-ggplot2.html#aesthetic-mapping-in-ggplot",
    "href": "02-ggplot2.html#aesthetic-mapping-in-ggplot",
    "title": "2  Plotting with ggplot2",
    "section": "\n2.5 Aesthetic Mapping in ggplot\n",
    "text": "2.5 Aesthetic Mapping in ggplot\n\nGoals:\n\nfurther use ggplot2 to construct plots of data\nexplain how aesthetics are used in ggplot() and describe the difference between local and global aesthetics.\n\nThe purpose of this second section on ggplot2 is to discuss more about plot aesthetics. While the previous section focused more on basic plot construction, this section will focus more on how the plots are constructed by what is specified in the aes() mapping argument.\n\n\n\n\n\n\nImportant\n\n\n\nThe aesthetic mappings are what map variables in the data frame we are using to characteristics of the plot.\n\n\nWhich aesthetic mappings are available depend on the GEOM used. Common mappings include x, y, colour, fill, size, shape, and linewidth.\nHowever, colour, fill, size, shape, and linewidth are also things that we commonly change outside of aesthetic mappings. Details of when to put an argument inside of aes() or outside of aes() are discussed next.\n\n2.5.1 Inside vs. Outside aes()\n\nExamine the following graph of some of the variables in the fitness data set we used in the previous section.\n\nlibrary(tidyverse)\nfitness_full &lt;- read_csv(\"https://raw.githubusercontent.com/highamm/ds234_quarto/main/data_online/higham_fitness_clean.csv\",\n                         col_types = list(stepgoal = col_factor())) |&gt;\n  mutate(weekend_ind = case_when(weekday == \"Sat\" | weekday == \"Sun\" ~ \"weekend\",\n                                 .default = \"weekday\"))\n\nggplot(data = fitness_full,\n       aes(x = Start, y = active_cals, colour = stepgoal)) +\n  geom_point()\n\n\n\n\nHere, we have used a colour aesthetic to map the stepgoal variable to colour. But, what if we wanted to simply colour all of the points \"blue\". Try running:\n\nggplot(data = fitness_full,\n       aes(x = Start, y = active_cals, colour = \"blue\")) +\n  geom_point()\n\n\n\n\nWhat does the graph look like? Did it do what you expected? To actual colour the points \"blue\", we need to specify colour = \"blue\" outside of aes() because \"blue\" is not a variable from the fitness_full data frame. Also, when giving non-aesthetic characteristics to a plot, the argument must go with the associated GEOM; so, colour = \"blue\" must be an argument to geom_point():\n\nggplot(data = fitness_full,\n       aes(x = Start, y = active_cals)) +\n  geom_point(colour = \"blue\")\n\n\n\n\nTo sum up, putting colour = ____ inside aes() or outside aes() achieves different things. In general,\n\nwhen we want to map something in our data set (fitness_full) to something in our plot (x, y, colour, size, etc.), we put that inside the aes() as in aes(colour = weekend_ind).\nWhen we assign fixed characteristics (making all points a larger size, widening the width of a line, using a constant colour, etc.) that don’t come from the data, we put them outside of aes().\n\n\n\n\n\n\n\nNote\n\n\n\nWe actually have already used non-aesthetic options in ggplot(), though we did not frame them as non-aesthetic options at the time. In the previous section, we often changed the colour option and fill option to histograms, bar plots, and box plots. These are non-aesthetic options because, again, we are not mapping a variable in the data frame to these arguments.\n\n\nExercise 1. The default point size is 1.5 while the default point shape is 19. In the following plot, change the defaults so that the points are a little larger than the default and so that the shape is an X. Google image search “R point shapes” to figure out which integer corresponds to the X point shape.\n\nggplot(data = fitness_full,\n       aes(x = Start, y = active_cals)) +\n  geom_point(size = 1.5, shape = 19)\n\n\n\n\nExercise 2. Use the linewidth option in geom_smooth() to increase the width of the smoother. Note that the default linewidth is 1.\n\nggplot(data = fitness_full,\n       aes(x = Start, y = active_cals)) +\n  geom_point() +\n  geom_smooth()\n\n\n\n\n\n2.5.2 Local vs. Global Aesthetics\nUp until this point, we have always supplied aesthetics globally in the ggplot() function. However, there are times when we must override the global aesthetics and change the locally within an individual GEOM.\nIn the following plot, the global aesthetics x, y, and colour are passed to all further GEOMs (geom_point() and geom_smooth() in this case) unless specifically overridden. So, the plot with global aesthetics\n\nggplot(data = fitness_full,\n       aes(x = Start, y = active_cals, colour = stepgoal)) +\n  geom_point() +\n  geom_smooth()\n\n\n\n\nis equivalent to the following plot with local aesthetics:\n\nggplot(data = fitness_full) +\n  geom_point(aes(x = Start, y = active_cals, colour = stepgoal)) +\n  geom_smooth(aes(x = Start, y = active_cals, colour = stepgoal))\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen we put the aes() in ggplot() as global aesthetics, R perpetuates these aes() aesthetics in all further GEOMs in your plotting command.\n\n\nAs our plots become more complex, we will need to think carefully about whether we want to supply an aesthetic globally or locally. As an example, we can change which aesthetics are supplied globally and which are supplied locally to create a graph that has the points coloured by stepgoal but a single smoother and a graph that has smoothers for each level of stepgoal but a single point colour:\n\nggplot(data = fitness_full,\n       aes(x = Start, y = active_cals)) +\n  geom_point(aes(colour = stepgoal)) +\n  geom_smooth()\n\n\n\n\nggplot(data = fitness_full,\n       aes(x = Start, y = active_cals)) +\n  geom_point() +\n  geom_smooth(aes(colour = stepgoal))\n\n\n\n\nExercise 3. Modify the following plot so that the aesthetics are all explicitly given locally to each GEOM instead of globally.\n\nggplot(data = fitness_full,\n       aes(x = Start, y = active_cals)) +\n  geom_point() +\n  geom_line()\n\n\n\n\n\n2.5.3 R Code Style\nWe want our code to be as readable as possible. This not only benefits other people who may read your code (like me), but it also benefits you, particularly if you read your own code in the future. I try to follow the Style Guide in the Advanced R book: http://adv-r.had.co.nz/Style.html. Feel free to skim through that, but you don’t need to worry about it too much: you should be able to pick up on some important elements just from going through this course. You might actually end up having better code style if you haven’t had any previous coding experience.\nAs a quick example of why code style can be important, consider the following two code chunks, both of which produce the same graph.\n\nggplot(data=fitness_full,aes(x=Start,y=steps))+geom_point(colour=\n     \"darkgreen\")+geom_smooth(se=FALSE,colour=\"black\",linewidth=1.4)\n\n\nggplot(data = fitness_full,\n       aes(x = Start, y = steps)) +\n  geom_point(colour = \"darkgreen\") +\n  geom_smooth(se = FALSE, colour = \"black\", linewidth = 1.4)\n\nWhich code chunk would you want to read two years from now? Which code chunk would you want your classmate/friend/coworker to read? (assuming you like your classmate/friend/coworker….)\nExercise 4. Fix the following code so that the code is more readable.\n\nggplot(data =fitness_full,\n       aes(x =weekday,y= steps)) +geom_boxplot(fill=\"coral1\",colour =\"black\")\n\n\n\n\nExercise 5. Read the very short paper at https://joss.theoj.org/papers/10.21105/joss.01686 on an Introduction to the tidyverse, and answer the questions below in your Quarto file. Answer the following questions by typing answers in your .qmd document. Because these answers do not involve code, you should be able to type them into your .qmd document outside of a code chunk.\n\nWhat are the two major areas that the tidyverse doesn’t provide tools for?\nHow do the authors define “tidy”?\nWhat does it mean for the tidyverse to be “human-centred”?\nIn about 2 sentences, describe the data science “cycle” given in the diagram at the top of page 3."
  },
  {
    "objectID": "02-ggplot2.html#practice-1",
    "href": "02-ggplot2.html#practice-1",
    "title": "2  Plotting with ggplot2",
    "section": "\n2.6 Practice",
    "text": "2.6 Practice\nIn general, practice exercises will be split between exercises that will be done together as a class and exercises that you will do in groups or on your own. The purpose of the class exercises is to give some guidance on how we might think logically through some of the code and the results. The purpose of the group exercises and the on your own exercises is so that you have a chance to practice what you’ve learned with your table or on your own.\nFor some of these exercises we will again use the stat113_df data, described in the previous section.\n\nstat113_df &lt;- read_csv(\"https://raw.githubusercontent.com/highamm/ds234_quarto/main/data_online/stat113.csv\")\nhead(stat113_df)\n#&gt; # A tibble: 6 × 12\n#&gt;   Year      Sex     Hgt   Wgt Haircut   GPA Exercise Sport    TV Award  Pulse\n#&gt;   &lt;chr&gt;     &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n#&gt; 1 Sophomore M        66   155       0  2.9        15 Yes       8 Olymp…    72\n#&gt; 2 FirstYear F        69   170      17  3.87       14 Yes      12 Olymp…    51\n#&gt; 3 FirstYear F        64   130      40  3.3         5 No        5 Olymp…    68\n#&gt; 4 FirstYear M        68   157      35  3.21       10 Yes      15 Olymp…    54\n#&gt; 5 FirstYear M        72   175      20  3.1         2 No        5 Nobel     NA\n#&gt; 6 Junior    F        62   150      50  3.3         8 Yes       5 Olymp…    86\n#&gt; # ℹ 1 more variable: SocialMedia &lt;chr&gt;\n\n\n2.6.1 Class Exercises\nClass Exercise 1. With the stat113_df data, make a scatterplot of Hgt on the y-axis and Wgt on the x-axis, colouring by Sport. Add a smooth fitted curve to your scatterplot. Then, move colour = Sport from an aes() in the ggplot() function to an aes() in the geom_point() function. What changes in the plot? Can you give an explanation as to why that change occurs?\nClass Exercise 2. Fix the following plot so that the fill colour of the histogram is \"white\" and the outline colour is \"black\".\n\nggplot(data = stat113_df, aes(x = TV)) +\n  geom_histogram(aes(colour = \"black\", fill = \"white\"))\n\n\n\n\n\n2.6.2 Your Turn\nYour Turn 1. Suppose you have a generic data frame named toy_df, with quantitative variables xvar and yvar, and categorical variable groupvar. In your group, select which of the following plots will result in a scatterplot of xvar and yvar where there are separate coloured fitted linear regression line for each level of groupvar but all of the points are coloured \"purple\". You may select one, more than one, or none at all.\n\n## a\nggplot(data = toy_df, aes(x = xvar, y = yvar, colour = groupvar)) +\n  geom_point(colour = \"purple\") +\n  geom_smooth(aes(method = \"lm\"))\n\n## b\nggplot(data = toy_df, aes(x = xvar, y = yvar, colour = groupvar)) +\n  geom_point(colour = \"purple\") +\n  geom_smooth(method = \"lm\")\n\n## c\nggplot(data = toy_df, aes(x = xvar, y = yvar)) +\n  geom_point(aes(colour = \"purple\")) +\n  geom_smooth(method = \"lm\", colour = groupvar)\n\n## d\nggplot(data = toy_df, aes(x = xvar, y = yvar)) +\n  geom_point(colour = \"purple\") +\n  geom_smooth(method = \"lm\", colour = groupvar)\n\n## e\nggplot(data = toy_df, aes(x = xvar, y = yvar)) +\n  geom_point(colour = \"purple\") +\n  geom_smooth(method = \"lm\", aes(colour = groupvar))\n\n## f\nggplot(data = toy_df, aes(x = xvar, y = yvar)) +\n  geom_point(aes(colour = \"purple\")) +\n  geom_smooth(method = \"lm\", aes(colour = groupvar))\n\nYour Turn 2. Choose one of the plots that you selected as “correct” in the previous exercise. In your group, discuss, how the syntax of the plot would change if all aesthetics were supplied locally instead of globally? Based on this discussion, when are global aesthetics more useful than local aesthetics?"
  },
  {
    "objectID": "03-dplyr.html#filterslice-select-arrange-and-piping",
    "href": "03-dplyr.html#filterslice-select-arrange-and-piping",
    "title": "3  Wrangling with dplyr",
    "section": "\n3.1 filter(),slice(), select(), arrange(), and Piping",
    "text": "3.1 filter(),slice(), select(), arrange(), and Piping\nThroughout this chapter, we will use the babynames data set in the babynames R package. To begin, install the babynames package by typing install.packages(\"babynames\") in your bottom-left console window, and then load the babynames package in with\n\nlibrary(babynames)\n\nRead about the data set by typing ?babynames in your bottom-left window of R Studio. We see that this data set contains baby name data provided by the SSA in the United States dating back to 1880:\n\nbabynames\n#&gt; # A tibble: 1,924,665 × 5\n#&gt;    year sex   name          n   prop\n#&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;  &lt;dbl&gt;\n#&gt; 1  1880 F     Mary       7065 0.0724\n#&gt; 2  1880 F     Anna       2604 0.0267\n#&gt; 3  1880 F     Emma       2003 0.0205\n#&gt; 4  1880 F     Elizabeth  1939 0.0199\n#&gt; 5  1880 F     Minnie     1746 0.0179\n#&gt; 6  1880 F     Margaret   1578 0.0162\n#&gt; # ℹ 1,924,659 more rows\n\nThere are many interesting and informative plots that we could make with this data set, but most require some data wrangling first. This chapter will provide the foundation for such wrangling skills.\nBefore we begin, we also need to load in the tidyverse package so that we can use the dplyr functions.\n\nlibrary(tidyverse)\n\n\n3.1.1 filter() and slice(): Choosing Rows\nThere are a lot of commonly used dplyr wrangling functions, but we need to start somewhere! We will begin with two functions used to choose rows to keep in the data frame: filter(), which keeps rows based on a condition that is provided, and slice(), which keeps rows based on row numbers that are provided.\n\n\n\n\n\n\nImportant\n\n\n\nfilter() is a way to keep rows by specifying a condition related to one or more of the variables in the data set.\n\n\nWe can keep rows based on a categorical variable or a quantitative variable or a combination of any number of categorical and quantitative variables. R uses the following symbols to make comparisons:\n\n\n&lt; and &lt;= for less than and less than or equal to, respectively\n\n&gt; and &gt;= for greater than and greater than or equal to, respectively\n\n== for equal to (careful: equal to is a double equal sign ==)\n\n!= for not equal to (in general, ! denotes “not”)\n\nThe easiest way to see how filter works is with some examples. The following line of code filter()s the babynames data set based on a condition. See if you can guess what the following statement does before running the code.\n\nbabynames |&gt; filter(name == \"Matthew\")\n\nWe’re seeing our first of many, many, many, many, many, many, many instances of using |&gt; to “pipe.” Piping is a really convenient, easy-to-read way to build a sequence of commands. How you can read the above code is:\n\nTake the babynames data frame object and with babynames, and then\nperform a filter() step to keep only the rows where name is Matthew.\n\n\n\n\n\n\n\nNote\n\n\n\nThe “and then” phrase is the most commonly used phrase to explain how to read the pipe |&gt; in a line of code.\n\n\nExamine the following four filter() statements and see if you can guess what each one is doing before running the code.\n\nbabynames |&gt; filter(year &gt;= 2000)\nbabynames |&gt; filter(sex != \"M\")\nbabynames |&gt; filter(prop &gt; 0.05)\nbabynames |&gt; filter(year == max(year))\n\nNote that some things put in quotes, like \"M\" while some things aren’t, like 2000. Generally, if we are filtering by a numeric variable, like prop, any value used in the filtering statement does not go in quotes, while if we are filtering by a categorical variable, like sex or name, the value used in the filtering statement does go in quotes.\nWe can also combine conditions on multiple variables in filter() using Boolean operators. Look at the Venn diagrams in R for Data Science to learn about the various Boolean operators you can use in R: https://r4ds.had.co.nz/transform.html#logical-operators. The Boolean operators can be used in other functions in R as well, as we will see in the next section with mutate().\nThe following gives some examples. See if you can figure out what each line of code is doing before running it.\n\nbabynames |&gt; filter(n &gt; 20000 | prop &gt; 0.05)\nbabynames |&gt; filter(sex == \"F\" & name == \"Mary\")\nbabynames |&gt; filter(sex == \"F\" & name == \"Mary\" & prop &gt; 0.05)\n\n\n\n\n\n\n\nImportant\n\n\n\nWhile filter() lets us choose which rows in a data set to keep based on a condition, slice() allows us to specify the row numbers corresponding to rows that we want to keep.\n\n\nFor example, suppose that we only want to keep the first 5 rows of the babynames data:\n\nbabynames |&gt; slice(c(1, 2, 3, 4, 5))\n#&gt; # A tibble: 5 × 5\n#&gt;    year sex   name          n   prop\n#&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;  &lt;dbl&gt;\n#&gt; 1  1880 F     Mary       7065 0.0724\n#&gt; 2  1880 F     Anna       2604 0.0267\n#&gt; 3  1880 F     Emma       2003 0.0205\n#&gt; 4  1880 F     Elizabeth  1939 0.0199\n#&gt; 5  1880 F     Minnie     1746 0.0179\n\nThe c() function is used here to bind the numbers 1, 2, 3, 4, 5 into a vector. c() actually stands for “concatenate.” We can alternatively use slice(1:5), which is shorthand for slice(c(1, 2, 3, 4, 5)). The : is useful for larger numbers of rows that we might want to keep. For example, slice(1:1000) is much easier than slice(c(1, 2, 3, 4, ...., 999, 1000)). While slice() is useful, it is relatively simple. We’ll come back to it again in a few weeks as well when we discuss subsetting in base R.\nExercise 1. Use filter() on the babynames data set to only keep rows with your name. (Note that, the data set has a large variety of names, but, if there were fewer than 5 people born in the United States with your name every year, then your name might not appear in the data set. In that case, just use a different name that interests you!).\nExercise 2. Use filter() to only keep rows in the babynames data set from the year 2000 onward.\nExercise 3. Use filter() to only keep rows in babynames with your name from Exercise 1 and that have a year from 2000 onward.\n\n\n\n\n\n\nHint (only if you get stuck)\n\n\n\n\n\nYou will need to use the & Boolean operator to combine your conditions from the previous two exercises.\n\n\n\nExercise 4. Use slice() to keep the first 20 rows of babynames.\n\n3.1.2 arrange(): Ordering Rows\nThe arrange() function allows us to order rows in the data set using one or more variables. The function is very straightforward: we only need to provide the name of the variable that we want to order by. For example,\n\nbabynames |&gt; arrange(n)\n#&gt; # A tibble: 1,924,665 × 5\n#&gt;    year sex   name          n      prop\n#&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;     &lt;dbl&gt;\n#&gt; 1  1880 F     Adelle        5 0.0000512\n#&gt; 2  1880 F     Adina         5 0.0000512\n#&gt; 3  1880 F     Adrienne      5 0.0000512\n#&gt; 4  1880 F     Albertine     5 0.0000512\n#&gt; 5  1880 F     Alys          5 0.0000512\n#&gt; 6  1880 F     Ana           5 0.0000512\n#&gt; # ℹ 1,924,659 more rows\n\norders the babynames data so that n, the number of babies, goes from smallest to largest. If we want to arrange() by descending order of a variable, we just wrap the name of the variable we are ordering by with desc() for “descending:”\n\nbabynames |&gt; arrange(desc(n))\n#&gt; # A tibble: 1,924,665 × 5\n#&gt;    year sex   name        n   prop\n#&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;int&gt;  &lt;dbl&gt;\n#&gt; 1  1947 F     Linda   99686 0.0548\n#&gt; 2  1948 F     Linda   96209 0.0552\n#&gt; 3  1947 M     James   94756 0.0510\n#&gt; 4  1957 M     Michael 92695 0.0424\n#&gt; 5  1947 M     Robert  91642 0.0493\n#&gt; 6  1949 F     Linda   91016 0.0518\n#&gt; # ℹ 1,924,659 more rows\n\nExercise 5. Use arrange() to sort the data frame so that it is in descending order by the prop variable. This allows us to examine the names with the highest proportion of babies given that name for that given year.\nExercise 6. Use arrange() on the categorical variable name. What does arrange() seem to do on a categorical variable?\n\n3.1.3 select(): Choosing Columns\nWe might also be interested in getting rid of some of the columns in a data set. One reason to do this is if there are an overwhelming (30+) columns in a data set, but we know that we just need a few of them. The easiest way to use select() is to just input the names of the columns that you want to keep. For example, if we were only interested in the name and n variables, we could make a data frame with only those variables with:\n\nbabynames |&gt; select(name, n)\n#&gt; # A tibble: 1,924,665 × 2\n#&gt;   name          n\n#&gt;   &lt;chr&gt;     &lt;int&gt;\n#&gt; 1 Mary       7065\n#&gt; 2 Anna       2604\n#&gt; 3 Emma       2003\n#&gt; 4 Elizabeth  1939\n#&gt; 5 Minnie     1746\n#&gt; 6 Margaret   1578\n#&gt; # ℹ 1,924,659 more rows\n\nWe might also want to use select() to get rid of one or more columns. If this is the case, we denote any column you want to get rid of with -. For example, we might want to get rid of the sex column and the prop column with\n\nbabynames |&gt; select(-sex, -prop)\n#&gt; # A tibble: 1,924,665 × 3\n#&gt;    year name          n\n#&gt;   &lt;dbl&gt; &lt;chr&gt;     &lt;int&gt;\n#&gt; 1  1880 Mary       7065\n#&gt; 2  1880 Anna       2604\n#&gt; 3  1880 Emma       2003\n#&gt; 4  1880 Elizabeth  1939\n#&gt; 5  1880 Minnie     1746\n#&gt; 6  1880 Margaret   1578\n#&gt; # ℹ 1,924,659 more rows\n\nselect() comes with many useful helper functions, but these are oftentimes not needed. One of the helper functions that is actually often useful is everything(), which selects all columns in a data frame. This can be used to help re-order columns if you have a particular column that you want to appear first:\n\nbabynames |&gt; select(name, everything())\n#&gt; # A tibble: 1,924,665 × 5\n#&gt;   name       year sex       n   prop\n#&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;int&gt;  &lt;dbl&gt;\n#&gt; 1 Mary       1880 F      7065 0.0724\n#&gt; 2 Anna       1880 F      2604 0.0267\n#&gt; 3 Emma       1880 F      2003 0.0205\n#&gt; 4 Elizabeth  1880 F      1939 0.0199\n#&gt; 5 Minnie     1880 F      1746 0.0179\n#&gt; 6 Margaret   1880 F      1578 0.0162\n#&gt; # ℹ 1,924,659 more rows\n\nThe previous line of code puts name as the first variable in the data frame.\n\n\n\n\n\n\nNote\n\n\n\nFor data frames with only a few columns, re-ordering the columns is generally not that useful, but, for data frames with hundreds or thousands of columns, moving variables of interest to the beginning of the data frame helps us view these variables more readily.\n\n\nExercise 7. Use select() to keep only the year, name and prop variables in two ways: (1) keep year, name, and prop directly by specifying these variable names in select() and (2) keep only year, name, and prop indirectly by specifying that sex, and n should be dropped from the data frame.\n\n3.1.4 Assigning a Data Frame a Name\nUp until now, we have not assigned any of the new data frames we are creating a name. For example, with\n\nbabynames |&gt; filter(name == \"Matthew\")\n#&gt; # A tibble: 212 × 5\n#&gt;    year sex   name        n     prop\n#&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;int&gt;    &lt;dbl&gt;\n#&gt; 1  1880 M     Matthew   113 0.000954\n#&gt; 2  1881 M     Matthew    80 0.000739\n#&gt; 3  1882 M     Matthew   109 0.000893\n#&gt; 4  1883 M     Matthew    86 0.000765\n#&gt; 5  1884 M     Matthew   117 0.000953\n#&gt; 6  1885 M     Matthew   111 0.000957\n#&gt; # ℹ 206 more rows\n\nwe get a printout of the data with only the name Matthew, but, the data frame object is not assigned a name at all. So, we cannot use ggplot() to make a plot of n only for the name Matthew because we do not have a named data set that only has the name Matthew. In other words, R performs the filter(), but the filtered data frame doesn’t get saved to any particular named object. If we want to “save” the new data set for future use (like in a plot command), then we can use the assignment operator, &lt;-:\n\nbabynames_matthew &lt;- babynames |&gt; filter(name == \"Matthew\")\n\nIn the previous chunk, we assign the data frame with only Matthew names to the name babynames_matthew:\n\nbabynames_matthew\n#&gt; # A tibble: 212 × 5\n#&gt;    year sex   name        n     prop\n#&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;int&gt;    &lt;dbl&gt;\n#&gt; 1  1880 M     Matthew   113 0.000954\n#&gt; 2  1881 M     Matthew    80 0.000739\n#&gt; 3  1882 M     Matthew   109 0.000893\n#&gt; 4  1883 M     Matthew    86 0.000765\n#&gt; 5  1884 M     Matthew   117 0.000953\n#&gt; 6  1885 M     Matthew   111 0.000957\n#&gt; # ℹ 206 more rows\n\n\n\n\n\n\n\nNote\n\n\n\nWe can really use almost any name we would like but it’s more helpful to use a name that makes sense (babynames_matthew) than a name that is meaningless.\n\n\nAn example of giving a data frame a meaningless name would be:\n\ntake_it_kronk_feel_the_power &lt;- babynames |&gt; filter(name == \"Matthew\")\n\nWe can use the data frame take_it_kronk_feel_the_power in the same way as babynames_matthew, but take_it_kronk_feel_the_power is not as easy to remember and doesn’t make any sense in this context.\nExercise 8. Name your data frame that you created from Exercise 1. Then, type in the name you assigned to the data frame to verify that you can see some printed out data.\n\n3.1.5 More about the Pipe\nWe are jumping straight into using piping, but we do want to have an appreciation on how terrible life would be without it. What piping does is make whatever is given before the |&gt; pipe the first argument of whatever function follows the |&gt;. So, for a toy data frame named df with variable xvar,\n\ndf |&gt; filter(xvar &lt;= 3)\n\nis equivalent to\n\nfilter(df, xvar &lt;= 3)\n\nIt might also help to use an analogy when thinking about piping. Consider the Ke$ha’s morning routine in the opening of the song Tik Tok. If we were to write her morning routine in terms of piping,\n\nkesha |&gt; wake_up(time = \"morning\", feels_like = \"P-Diddy\") |&gt;\n  grab(glasses) |&gt;\n  brush(teeth, item = \"jack\", unit = \"bottle\") |&gt; ....\n\nKesha first wakes up in the morning, and then the Kesha that has woken up grabs her glasses, and then the Kesha who has woken up and has grabbed her glasses proceeds to brush her teeth, etc.\nThe pipe operator |&gt; is loaded automatically with R. We will heavily use the pipe throughout the entire semester so it’s worth it to delve a little deeper into what it is here. We will use the fitness data to further explore the pipe. Read in the data with\n\nfitness_df &lt;- read_csv(\"https://raw.githubusercontent.com/highamm/ds234_quarto/main/data_online/higham_fitness_clean.csv\",\n                       col_types = list(stepgoal = col_factor()))\nfitness_df\n#&gt; # A tibble: 1,722 × 9\n#&gt;   Start      active_cals distance flights  steps month weekday dayofyear\n#&gt;   &lt;date&gt;           &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;\n#&gt; 1 2018-11-28        57.8    0.930       0  1885. Nov   Wed           332\n#&gt; 2 2018-11-29       509.     4.64       18  8953. Nov   Thu           333\n#&gt; 3 2018-11-30       599.     6.05       12 11665  Nov   Fri           334\n#&gt; 4 2018-12-01       661.     6.80        6 12117  Dec   Sat           335\n#&gt; 5 2018-12-02       527.     4.61        1  8925. Dec   Sun           336\n#&gt; 6 2018-12-03       550.     3.96        2  7205  Dec   Mon           337\n#&gt; # ℹ 1,716 more rows\n#&gt; # ℹ 1 more variable: stepgoal &lt;fct&gt;\n\nThus far, we have used a single pipe |&gt; to do one task, but, we can string together multiple dplyr functions with multiple |&gt; statements. For example, suppose that we wanted to keep only the weekdays that were Sat and Sun, and then, order the data set so that it was in descending order of steps taken:\n\nfitness_df |&gt; filter(weekday == \"Sat\" | weekday == \"Sun\") |&gt;\n  arrange(desc(steps))\n#&gt; # A tibble: 492 × 9\n#&gt;   Start      active_cals distance flights  steps month weekday dayofyear\n#&gt;   &lt;date&gt;           &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;\n#&gt; 1 2023-07-15       2291.     18.0     193 32589. Jul   Sat           196\n#&gt; 2 2019-08-11       1385.     15.8     173 29480. Aug   Sun           223\n#&gt; 3 2019-04-14       1767.     17.9       1 25578  Apr   Sun           104\n#&gt; 4 2018-12-15       1091.     11.2      27 21224. Dec   Sat           349\n#&gt; 5 2022-05-29       1291.     11.8     103 20853  May   Sun           149\n#&gt; 6 2019-02-10       1392.     12.8       5 20700. Feb   Sun            41\n#&gt; # ℹ 486 more rows\n#&gt; # ℹ 1 more variable: stepgoal &lt;fct&gt;\n\nThis reads: “take fitness_df and then filter() it to keep only Saturdays and Sundays and then sort it in descending order of steps.”\n\n\n\n\n\n\nImportant\n\n\n\nConsecutive pipes build on each other: we can slowly build out what the pipe is doing step-by-step. Running code “pipe by pipe” can be a very helpful way to understand what each consecutive piping statement is doing.\n\n\nThe code\n\nfitness_df |&gt; filter(weekday == \"Sat\" | weekday == \"Sun\")\n#&gt; # A tibble: 492 × 9\n#&gt;   Start      active_cals distance flights  steps month weekday dayofyear\n#&gt;   &lt;date&gt;           &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;\n#&gt; 1 2018-12-01        661.     6.80       6 12117  Dec   Sat           335\n#&gt; 2 2018-12-02        527.     4.61       1  8925. Dec   Sun           336\n#&gt; 3 2018-12-08        775.     6.46       0  8834. Dec   Sat           342\n#&gt; 4 2018-12-09        341.     2.81       0  5265  Dec   Sun           343\n#&gt; 5 2018-12-15       1091.    11.2       27 21224. Dec   Sat           349\n#&gt; 6 2018-12-16       1029.     8.84      79 16376  Dec   Sun           350\n#&gt; # ℹ 486 more rows\n#&gt; # ℹ 1 more variable: stepgoal &lt;fct&gt;\n\nis equivalent to:\n\nfilter(fitness_df, weekday == \"Sat\" | weekday == \"Sun\")\n#&gt; # A tibble: 492 × 9\n#&gt;   Start      active_cals distance flights  steps month weekday dayofyear\n#&gt;   &lt;date&gt;           &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;\n#&gt; 1 2018-12-01        661.     6.80       6 12117  Dec   Sat           335\n#&gt; 2 2018-12-02        527.     4.61       1  8925. Dec   Sun           336\n#&gt; 3 2018-12-08        775.     6.46       0  8834. Dec   Sat           342\n#&gt; 4 2018-12-09        341.     2.81       0  5265  Dec   Sun           343\n#&gt; 5 2018-12-15       1091.    11.2       27 21224. Dec   Sat           349\n#&gt; 6 2018-12-16       1029.     8.84      79 16376  Dec   Sun           350\n#&gt; # ℹ 486 more rows\n#&gt; # ℹ 1 more variable: stepgoal &lt;fct&gt;\n\nThen,\n\nfilter(fitness_df, weekday == \"Sat\" | weekday == \"Sun\") |&gt;\n  arrange(desc(steps))\n#&gt; # A tibble: 492 × 9\n#&gt;   Start      active_cals distance flights  steps month weekday dayofyear\n#&gt;   &lt;date&gt;           &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;\n#&gt; 1 2023-07-15       2291.     18.0     193 32589. Jul   Sat           196\n#&gt; 2 2019-08-11       1385.     15.8     173 29480. Aug   Sun           223\n#&gt; 3 2019-04-14       1767.     17.9       1 25578  Apr   Sun           104\n#&gt; 4 2018-12-15       1091.     11.2      27 21224. Dec   Sat           349\n#&gt; 5 2022-05-29       1291.     11.8     103 20853  May   Sun           149\n#&gt; 6 2019-02-10       1392.     12.8       5 20700. Feb   Sun            41\n#&gt; # ℹ 486 more rows\n#&gt; # ℹ 1 more variable: stepgoal &lt;fct&gt;\n\nis equivalent to:\n\narrange(filter(fitness_df, weekday == \"Sat\" | weekday == \"Sun\"),\n        desc(steps))\n#&gt; # A tibble: 492 × 9\n#&gt;   Start      active_cals distance flights  steps month weekday dayofyear\n#&gt;   &lt;date&gt;           &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;\n#&gt; 1 2023-07-15       2291.     18.0     193 32589. Jul   Sat           196\n#&gt; 2 2019-08-11       1385.     15.8     173 29480. Aug   Sun           223\n#&gt; 3 2019-04-14       1767.     17.9       1 25578  Apr   Sun           104\n#&gt; 4 2018-12-15       1091.     11.2      27 21224. Dec   Sat           349\n#&gt; 5 2022-05-29       1291.     11.8     103 20853  May   Sun           149\n#&gt; 6 2019-02-10       1392.     12.8       5 20700. Feb   Sun            41\n#&gt; # ℹ 486 more rows\n#&gt; # ℹ 1 more variable: stepgoal &lt;fct&gt;\n\nSo, why use the pipe? Compare the code the uses the pipe operator to the code that doesn’t. Which is easier to read? Which do you think is easier to write? The example shows that, for our purposes, the pipe is most useful in aiding the readability of our code. It’s a lot easier to see what’s happening in the code chunk with the pipes than it is in the previous code chunk without the pipe because, with the pipe, we can read the code from left to right and top to bottom. Without the pipe, we need to read the code from the “inside to the outside”, which is much more challenging.\nThere are many situations in which using the |&gt; would not be appropriate. To use the pipe, what precedes the |&gt; must be the first argument in the function following the |&gt;. For tidyverse functions that we use throughout the course, this condition is almost always true because these functions were designed with piping in mind. But, for non-tidyverse functions, this may not be the case.\nFor example, if you have taken STAT 213, you’ve used lm() to fit many different types of linear models. If you haven’t taken STAT 213, lm(response ~ explanatory, data = name_of_data_set) stands for “linear model” and can be used to fit the simple linear regression model that you learned about in STAT 113. You might expect something like this to fit a linear model using fitness_df with active_cals as the response and steps as the predictor:\n\nfitness_df |&gt; lm(active_cals ~ steps)\n#&gt; Error in as.data.frame.default(data): cannot coerce class '\"formula\"' to a data.frame\n\nBut it throws us an error. Typing in ?lm reveals that its first argument is a formula to fit the model, not a data set. So the function is trying to run\n\nlm(fitness_df, active_cals ~ steps)\n#&gt; Error in as.data.frame.default(data): cannot coerce class '\"formula\"' to a data.frame\n\nwhich doesn’t work because the arguments to the function are mixed up (the formula should appear first and the data set should appear second).\n\n\n\n\n\n\nNote\n\n\n\nThe pipe operator |&gt; is relatively new. Previously, the primary pipe operator used was %&gt;% and came from the magrittr package. For almost all cases, the two operators are equivalent. However, when scanning the Internet for help with code, you will probably see %&gt;% used in many of people’s responses on sites like StackOverflow.\n\n\nExercise 9. This “More About the Pipe” section will make more sense as we proceed through the course. For now, we just want to be able to understand that something like df |&gt; slice(1:6) is equivalent to slice(df, 1:6). Practice by converting the following code to use the pipe:\n\nfilter(fitness_df, active_cals &gt; 50)\n#&gt; # A tibble: 1,690 × 9\n#&gt;   Start      active_cals distance flights  steps month weekday dayofyear\n#&gt;   &lt;date&gt;           &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;\n#&gt; 1 2018-11-28        57.8    0.930       0  1885. Nov   Wed           332\n#&gt; 2 2018-11-29       509.     4.64       18  8953. Nov   Thu           333\n#&gt; 3 2018-11-30       599.     6.05       12 11665  Nov   Fri           334\n#&gt; 4 2018-12-01       661.     6.80        6 12117  Dec   Sat           335\n#&gt; 5 2018-12-02       527.     4.61        1  8925. Dec   Sun           336\n#&gt; 6 2018-12-03       550.     3.96        2  7205  Dec   Mon           337\n#&gt; # ℹ 1,684 more rows\n#&gt; # ℹ 1 more variable: stepgoal &lt;fct&gt;"
  },
  {
    "objectID": "03-dplyr.html#practice",
    "href": "03-dplyr.html#practice",
    "title": "3  Wrangling with dplyr",
    "section": "\n3.2 Practice",
    "text": "3.2 Practice\nIn general, practice exercises will be split between exercises that will be done together as a class and exercises that you will do in groups or on your own. The purpose of the class exercises is to give some guidance on how we might think logically through some of the code and the results. The purpose of the group exercises and the on your own exercises is so that you have a chance to practice what you’ve learned with your table or on your own.\n\n3.2.1 Class Exercises\nFor many of these class exercises, we will start practicing stringing together multiple piping statements. We will also practice more with ggplot() by making plots of some of the data sets we are creating.\nClass Exercise 1. In the babynames data set, use filter(), arrange(), and slice() to print the 10 most popular Male babynames in 2017.\nClass Exercise 2. Name the data frame you created in the previous exercise and use the data frame to make a bar plot of the 10 most popular Male babynames in 2017, along with the number of babies with each name in that year.\nClass Exercise 3. In the babynames data set, keep only the rows with your name (or, another name that interests you) and one sex (either \"M\" or \"F\").\nClass Exercise 4. Name the new data set you made in the previous exercise and then construct a line plot that looks at either the n or prop of your chosen name through year.\nClass Exercise 5. Explain why the following code gives a warning message and returns NA. Use the order of Arguments in ?mean in your explanation.\n\nfitness_df |&gt; mean(distance)\n#&gt; [1] NA\n\n\n3.2.2 Your Turn\nYour Turn 1. Get a little more practice with using conditions in filter() by completing the following:\n\nCreate a data set with all years except the year 1945.\nCreate a data set that only has rows where n is between 400 and 5000.\nCreate a data set with rows that are either F (female) Monica names or F (female) Hilary names.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe syntax filter(x_var &gt; 20 & &lt; 30) is not correct. The variable that we are filtering with needs to go in each conditional statement: filter(x_var &gt; 20 & x_var &lt; 30)\n\n\n\nYour Turn 2. Name your data set is part (c) of the previous exercise. Then, use the data set to make a line plot comparing the popularity of the Monica and Hilary names through time.\nYour Turn 3. Choose 5 names paired with 5 sexes that interest you and create a new data set that only has data on those 5 name/sex combinations. Then, make a line plot showing the popularity of these 5 names over time.\nYour Turn 4. Choose a year and a sex that interests you and create a data set that only contain observations from that year and sex. Then, make a bar plot that shows the top 10 names from that year and sex, showing the prop of babies born with each of those 10 names in that year."
  },
  {
    "objectID": "03-dplyr.html#mutate-group_by-summarise-and-missing-values",
    "href": "03-dplyr.html#mutate-group_by-summarise-and-missing-values",
    "title": "3  Wrangling with dplyr",
    "section": "\n3.3 mutate(), group_by(), summarise(), and Missing Values",
    "text": "3.3 mutate(), group_by(), summarise(), and Missing Values\nGoals:\n\nUse the mutate(), if_else(), and case_when() functions to create new variables.\nUse group_by() and summarise() to create useful summaries of a data set. * Explain what dplyr functions do with missing values.\nCombine the above goals with plotting to explore a data set on SLU majors.\n\nThroughout this section, we will use a data set that contains observations for each of SLU’s majors, with three variables per major:\n\n\nMajor, the name of the major.\n\nnfemales, the number of female graduates in that major from 2017 - 2021.\n\nnmales, the number of male graduates in that major from 2017 - 2021.\n\nBegin by reading in the data set with\n\nlibrary(tidyverse)\nslumajors_df &lt;- read_csv(\"https://raw.githubusercontent.com/highamm/ds234_quarto/main/data_online/SLU_Majors_17_21.csv\")\nslumajors_df\n#&gt; # A tibble: 30 × 3\n#&gt;   Major                        nfemales nmales\n#&gt;   &lt;chr&gt;                           &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 Anthropology                       35     13\n#&gt; 2 Art & Art History                  62     11\n#&gt; 3 Biochemistry                       15      6\n#&gt; 4 Biology                           152     58\n#&gt; 5 Business in the Liberal Arts      192    301\n#&gt; 6 Chemistry                          28     20\n#&gt; # ℹ 24 more rows\n\n\n3.3.1 mutate(): Create Variables\nSometimes, we will want to create a new variable that’s not in the data set, oftentimes using if_else(), case_when(), or basic algebraic operations on one or more of the columns already present in the data set.\nR understands the following symbols:\n\n\n+ for addition, - for subtraction\n\n* for multiplication, / for division\n\n^ for raising something to a power (3 ^ 2 is equal to 9)\n\nR also does the same order of operations as usual: parentheses, then exponents, then multiplication and division, then addition and subtraction.\nFor example, suppose that we want to create a variable in slumajors_df that has the total number of students graduating in each major. We can do this with mutate():\n\nslumajors_df |&gt; mutate(ntotal = nfemales + nmales)\n#&gt; # A tibble: 30 × 4\n#&gt;   Major                        nfemales nmales ntotal\n#&gt;   &lt;chr&gt;                           &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 Anthropology                       35     13     48\n#&gt; 2 Art & Art History                  62     11     73\n#&gt; 3 Biochemistry                       15      6     21\n#&gt; 4 Biology                           152     58    210\n#&gt; 5 Business in the Liberal Arts      192    301    493\n#&gt; 6 Chemistry                          28     20     48\n#&gt; # ℹ 24 more rows\n\nThe |&gt; operator reads as “and then”, as in “take slumajors_df and then mutate() it.”\nPiping is a really convenient, easy-to-read way to build a sequence of commands. How you can read the above code is:\n\nTake slumajors_df and with slumajors_df,\nperform a mutate() step to create the new variable called ntotal, which is nfemales plus nmales.\n\nSince this is our first time using mutate(), let’s also delve into what the function is doing. In general, mutate() reads:\nmutate(name_of_new_variable = operations_on_old_variables).\nR just automatically assumes that you want to do the operation for every single row in the data set, which is often quite convenient!\nWe might also want to create a variable that is the percentage of students identifying as female for each major:\n\nslumajors_df |&gt;\n  mutate(percfemale = 100 * nfemales / (nfemales + nmales))\n#&gt; # A tibble: 30 × 4\n#&gt;   Major                        nfemales nmales percfemale\n#&gt;   &lt;chr&gt;                           &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1 Anthropology                       35     13       72.9\n#&gt; 2 Art & Art History                  62     11       84.9\n#&gt; 3 Biochemistry                       15      6       71.4\n#&gt; 4 Biology                           152     58       72.4\n#&gt; 5 Business in the Liberal Arts      192    301       38.9\n#&gt; 6 Chemistry                          28     20       58.3\n#&gt; # ℹ 24 more rows\n\n\n\n\n\n\n\nImportant\n\n\n\nWhat happened to ntotal? Is it still in the printout? It’s not: when we created the variable ntotal, we didn’t actually save (or assign a name to) the new data set as anything. So R makes and prints the new variable, but it doesn’t get saved to any data set.\n\n\nIf we want to save the new data set for later use, then we can use the &lt;- operator. Here, we’re naming the new data set with the same name as the old data set: slumajors_df. Then, we’re doing the same thing for the percfemale variable. We won’t always want to give the new data set the same name as the old one: we’ll talk about this in more detail in the class exercises.\n\nslumajors_df &lt;- slumajors_df |&gt;\n  mutate(percfemale = 100 * nfemales / (nfemales + nmales))\n\n\nslumajors_df &lt;- slumajors_df |&gt; mutate(ntotal = nfemales + nmales)\n\nBut, you can pipe as many things together as you want to, so it’s probably easier to just create both variables in one go. The following chunk says to “Take slumajors_df and create a new variable ntotal. With that new data set, create a new variable called percfemale.” Finally, the slumajors_df &lt;- at the beginning says to “save this new data set as a data set with the same name, slumajors_df, thus overwriting the original slumajors_df.”\n\nslumajors_df &lt;- slumajors_df |&gt;\n  mutate(ntotal = nfemales + nmales) |&gt;\n  mutate(percfemale = 100 * nfemales / (nfemales + nmales))\n\nNow suppose that you want to make a new variable that is conditional on another variable (or more than one variable) in the data set. Then we would typically use mutate() coupled with\n\n\nif_else() if your new variable is created on only one condition\n\ncase_when() if your new variable is created on more than one condition\n\n\n\n\n\n\n\nImportant\n\n\n\nThe conditions that we supply use the same syntax as the conditions we used with filter(). For example, we still use the same Boolean operators (& for “and” and | for “or”, etc.).\n\n\nSuppose we want to create a new variable that tells us whether or not the Major has a majority of Women. That is, we want this new variable, morewomen to be \"Yes\" if the Major has more than 50% women and \"No\" if it has 50% or less.\n\nslumajors_df |&gt; mutate(morewomen = if_else(percfemale &gt; 50,\n                                            true = \"Yes\",\n                                            false = \"No\"))\n#&gt; # A tibble: 30 × 6\n#&gt;   Major                        nfemales nmales percfemale ntotal morewomen\n#&gt;   &lt;chr&gt;                           &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    \n#&gt; 1 Anthropology                       35     13       72.9     48 Yes      \n#&gt; 2 Art & Art History                  62     11       84.9     73 Yes      \n#&gt; 3 Biochemistry                       15      6       71.4     21 Yes      \n#&gt; 4 Biology                           152     58       72.4    210 Yes      \n#&gt; 5 Business in the Liberal Arts      192    301       38.9    493 No       \n#&gt; 6 Chemistry                          28     20       58.3     48 Yes      \n#&gt; # ℹ 24 more rows\n\nThe mutate() statement reads: create a new variable called morewomen that is equal to \"Yes\" if percfemale &gt; 50 is true and is equal to \"No\" if perfemale is not &gt; 0.5. The first argument is the condition, the second is what to name the new variable when the condition holds, and the third is what to name the variable if the condition does not hold.\nWe use conditions all of the time in every day life. For example, New York had a quarantine order stating that people coming from 22 states in July 2020 would need to quarantine. In terms of a condition, this would read “if you are traveling to New York from one of the 22 states, then you need to quarantine for 2 weeks. Else, if not, then you don’t need to quarantine.” The trick in using these conditions in R is getting used to the syntax of the code.\nWe can see from the above set up that if we had more than one condition, then we’d need to use a different function (or use nested if_else() statements, which can be a nightmare to read). If we have more than one condition for creating the new variable, we will use case_when().\nFor example, when looking at the output, we see that Biochemistry has 56% female graduates. That’s “about” a 50/50 split, so suppose we want a variable called large_majority that is \"female\" when the percent women is 70 or more, \"male\" when the percent women is 30 or less, and \"none\" when the percent female is between 30 and 70.\n\nslumajors_df |&gt; mutate(large_majority =\n                          case_when(percfemale &gt;= 70 ~ \"female\",\n                                    percfemale &lt;= 30 ~ \"male\",\n                                    percfemale &gt; 30 & percfemale &lt; 70 ~ \"none\")) \n#&gt; # A tibble: 30 × 6\n#&gt;   Major                      nfemales nmales percfemale ntotal large_majority\n#&gt;   &lt;chr&gt;                         &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;         \n#&gt; 1 Anthropology                     35     13       72.9     48 female        \n#&gt; 2 Art & Art History                62     11       84.9     73 female        \n#&gt; 3 Biochemistry                     15      6       71.4     21 female        \n#&gt; 4 Biology                         152     58       72.4    210 female        \n#&gt; 5 Business in the Liberal A…      192    301       38.9    493 none          \n#&gt; 6 Chemistry                        28     20       58.3     48 none          \n#&gt; # ℹ 24 more rows\n\nThe case_when() function reads “When the percent female is more than or equal to 70, assign the new variable large_majority the value of \"female\", when it’s less or equal to 30, assign the new variable large_majority the value of \"male\", and when it’s more than 30 and less than 70, assign the large_majority variable the value of \"none\".\nLet’s save these two new variables to the slumajors_df:\n\nslumajors_df &lt;- slumajors_df |&gt;\n  mutate(morewomen = if_else(percfemale &gt; 50,\n                             true = \"Yes\",\n                             false = \"No\")) |&gt;\n  mutate(large_majority =\n           case_when(percfemale &gt;= 70 ~ \"female\",\n                     percfemale &lt;= 30 ~ \"male\",\n                     percfemale &gt; 30 & percfemale &lt; 70 ~ \"none\")) \n\n\n\n\n\n\n\nNote\n\n\n\nNumeric values typically do not go in quotation marks (like 70 and 30) while strings like \"female\" and \"none\" do need quotes.\n\n\nExercise 1. Use mutate() with if_else() to create a new variable that is called major_size and is equal to \"large\" when the total number of majors is 100 or more and \"small\" when the total number of majors is less than 100.\nExercise 2. Use mutate() with case_when() to create a new variable that is called major_size2 and is \"large\" when the total number of majors is 150 or more, \"medium\" when the total number of majors is between 41 and 149, and \"small\" when the total number of majors is 40 or fewer.\nExercise 3. Create a new variable that is the nfemales to nmales ratio (so your new variable should be nfemales divided by nmales).\n\n3.3.2 summarise(): Create Summaries\nThe summarise() function is useful to get various summaries from the data. For example, suppose that we want to know the average major size at SLU across the five year span or the total number of majors across those five years. Then we can use summarise() and a summary function, like mean(), sum(), median(), max(), min(), n(), etc. You’ll notice that the format of summarise() is extremely similar to the format of mutate(). Using the slumajors_df data again just for one quick example,\n\nslumajors_df |&gt;\n  summarise(meantotalmajor = mean(ntotal),\n            totalgrad = sum(ntotal))\n#&gt; # A tibble: 1 × 2\n#&gt;   meantotalmajor totalgrad\n#&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1           117.      3502\n\nIn the code chunk above, we obtain two summaries (meantotalmajor and totalgrad), separated by a comma. In general, this syntax also works with mutate() to create more than one variable in the same mutate() function.\n\n\n\n\n\n\nNote\n\n\n\nMost summary functions are intuitive if you’ve had intro stat. But, if you’re not sure whether the summary for getting the maximum is maximum() or max(), just try both or do a quick google search.\n\n\nAnother useful summary function is n(), the counting function, which counts up the total number of rows.\n\nslumajors_df |&gt; summarise(totalobs = n())\n#&gt; # A tibble: 1 × 1\n#&gt;   totalobs\n#&gt;      &lt;int&gt;\n#&gt; 1       30\n\n\n\n\n\n\n\nNote\n\n\n\nThe counting function n() typically doesn’t have any arguments. It’s typically more useful when paired with group_by(), which is discussed next.\n\n\nExercise 4. Find the total number of nfemales in the data set and the total number of nmales in the data set.\n\n\n\n\n\n\nHint (only if you get stuck)\n\n\n\n\n\nThe summary function you will need to use within summarise() is sum(), which will add up the numbers in the nfemales column. You’ll then need to add a , and then sum() the number of nmales.\n\n\n\n\n3.3.3 group_by(): Groups\nsummarise() is often most useful when paired with a group_by() statement. Doing so allows us to get summaries across different groups. Conceptually, we can think of toy_df |&gt; group_by(categorical_var) as “grouping” toy_df into a bunch of different groups, with one group for each level of categorical_var. Then, operations from any other functions after the group_by() function are carried out for each group.\nFor example, suppose that you wanted the total number of registered births per year in the babynames data set that we used earlier. We can then use group_by() to “split” the babynames data frame into 138 separate data frames (one for each year in the data set). After we group_by(year), then the summarise() statement below adds up the n variable for each year:\n\nbabynames |&gt; group_by(year) |&gt;\n  summarise(totalbirths = sum(n))\n#&gt; # A tibble: 138 × 2\n#&gt;    year totalbirths\n#&gt;   &lt;dbl&gt;       &lt;int&gt;\n#&gt; 1  1880      201484\n#&gt; 2  1881      192696\n#&gt; 3  1882      221533\n#&gt; 4  1883      216946\n#&gt; 5  1884      243462\n#&gt; 6  1885      240854\n#&gt; # ℹ 132 more rows\n\nAs another example, we can also use the n() function to count up the number of rows for each year:\n\nbabynames |&gt; group_by(year) |&gt;\n  summarise(n_name = n())\n#&gt; # A tibble: 138 × 2\n#&gt;    year n_name\n#&gt;   &lt;dbl&gt;  &lt;int&gt;\n#&gt; 1  1880   2000\n#&gt; 2  1881   1935\n#&gt; 3  1882   2127\n#&gt; 4  1883   2084\n#&gt; 5  1884   2297\n#&gt; 6  1885   2294\n#&gt; # ℹ 132 more rows\n\n\n\n\n\n\n\nNote\n\n\n\ngroup_by() can be used in combination with the other dplyr functions we have learned about (mutate(), filter(), etc.). We will explore this functionality in class.\n\n\nExercise 5. With the babynames data set, use group_by() and summarise() to create a data set that has the total number of births for each name.\nExercise 6. With the babynames data set, use group_by() and summarise() to create a data set that has how many times each name appears in babynames.\n\n\n\n\n\n\nHint (only if you get stuck)\n\n\n\n\n\nFor one of the previous exercises, you will need to use the sum() function on n, the column with the total number of births while for the other exercise, you will need to use the n() function.\n\n\n\n\n3.3.4 Missing Values\nBoth of the data sets that we’ve worked with are nice in that they do not have any missing values. We’ll see plenty of examples of data sets with missing values later, so we should examine how the various functions that we’ve talked about so far tackle missing values.\nMissing values in R are denoted with NA for “Not Available.” Run the following code to create a toy data set with some missing values so that we can see how the various functions we’ve used so far deal with NA values.\n\ntoy_df &lt;- tibble(x = c(NA, 3, 4, 7),\n                 y = c(1, 4, 3, 2),\n                 z = c(\"A\", \"A\", \"B\", NA))\ntoy_df\n#&gt; # A tibble: 4 × 3\n#&gt;       x     y z    \n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n#&gt; 1    NA     1 A    \n#&gt; 2     3     4 A    \n#&gt; 3     4     3 B    \n#&gt; 4     7     2 &lt;NA&gt;\n\nWe will first discuss how to remove missing values when computing summaries for a variable that contains some NA values.\n\n\n\n\n\n\nImportant\n\n\n\nMissing values should not be removed without carefully examination and a note of what the consequences might be (i.e. why are these values missing?).\n\n\nWe have a toy data set that is meaningless, so we aren’t asking those questions now, but we will for any future data set that does have missing values!\nIf we have investigated the missing values and are comfortable with removing them, many functions that we would use in summarise() have an na.rm argument that we can set to TRUE to tell summarise() to remove any NAs before taking the mean(), median(), max(), etc.\n\ntoy_df |&gt; summarise(meanx = mean(x, na.rm = TRUE))\n#&gt; # A tibble: 1 × 1\n#&gt;   meanx\n#&gt;   &lt;dbl&gt;\n#&gt; 1  4.67\n\nIf we want to remove the missing values more directly, we can use the is.na() function in combination with filter(). If the variable is NA (Not Available) for an observation, is.na() evaluates to TRUE; if not, is.na() evaluates to FALSE. Test this out using mutate() to create a new variable for whether x is missing:\n\ntoy_df |&gt; mutate(missingx = is.na(x))\n#&gt; # A tibble: 4 × 4\n#&gt;       x     y z     missingx\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt;   \n#&gt; 1    NA     1 A     TRUE    \n#&gt; 2     3     4 A     FALSE   \n#&gt; 3     4     3 B     FALSE   \n#&gt; 4     7     2 &lt;NA&gt;  FALSE\n\nmissingx is TRUE only for the the first observation. We can use this to our advantage with filter() to filter it out of the data set, without going through the extra step of actually making a new variable missingx:\n\ntoy_df |&gt; filter(is.na(x) != TRUE)\n#&gt; # A tibble: 3 × 3\n#&gt;       x     y z    \n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n#&gt; 1     3     4 A    \n#&gt; 2     4     3 B    \n#&gt; 3     7     2 &lt;NA&gt;\n\nYou’ll commonly see this written as short-hand in people’s code you may come across as:\n\ntoy_df |&gt; filter(!is.na(x))\n#&gt; # A tibble: 3 × 3\n#&gt;       x     y z    \n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n#&gt; 1     3     4 A    \n#&gt; 2     4     3 B    \n#&gt; 3     7     2 &lt;NA&gt;\n\nwhich says to “keep anything that does not have a missing x value” (recall that the ! means “not” so that we are filtering to keep any value of x that is “not” missing).\nExercise 7. filter() toy_df to keep only the rows where z is not missing."
  },
  {
    "objectID": "03-dplyr.html#practice-1",
    "href": "03-dplyr.html#practice-1",
    "title": "3  Wrangling with dplyr",
    "section": "\n3.4 Practice",
    "text": "3.4 Practice\nIn general, practice exercises will be split between exercises that will be done together as a class and exercises that you will do in groups or on your own. The purpose of the class exercises is to give some guidance on how we might think logically through some of the code and the results. The purpose of the group exercises and the on your own exercises is so that you have a chance to practice what you’ve learned with your table or on your own.\n\n3.4.1 Class Exercises\nClass Exercise 1. About 55% of SLU students identify as female. So, in the definition of the morewomen variable, does it make more sense to use 55% as the cutoff or 50%?\nClass Exercise 2. Do you think it is ethical to exclude non-binary genders from analyses and graphs in the slumajors data set? What about in the stat113_df data that we looked at earlier? Why or why not? Try to construct arguments for both sides: you might also consider data privacy in your arguments.\nClass Exercise 3. Investigate what happens with case_when() when you give overlapping conditions and when you give conditions that don’t cover all observations. For overlapping conditions, create a variable testcase that is \"Yes\" when percfemale is greater than or equal to 40 and \"No\" when percfemale is greater than 60 For conditions that don’t cover all observations, create a variable testcase2 that is \"Yes\" when percfemale is greater than or equal to 55 and \"No\" when percfemale is less than 35. Put the new variables at the beginning of the data frame so that you can see what they contain more easily.\nClass Exercise 4. The rank() function can be used within mutate() to “rank” a variable from lowest values to highest values (so that the case with the lowest value receives a 1, the second lowest value receives a 2, etc.). Use group_by() and mutate() to rank the names from most to least popular in each year-sex combination. We will make some notes about what group_by() does with more than 1 variable.\nClass Exercise 5. From the data set in the previous exercise, filter() the data to keep only the most popular name in each year-sex combination and then construct a summary table showing how many times each name appears as the most popular name. With this exercise, we will make some notes about the ungroup() function, and, if we have time investigate a way to achieve the objective of this question without explicitly making a rank variable.\nClass Exercise 6. Run the following code. Intuitively, a slice(1, 2, 3, 4, 5) should grab the first five rows of the data set, but, when we try to run that, we get 1380 rows. What is the issue?\n\nbabynames_test &lt;- babynames |&gt;\n  group_by(year, sex) |&gt; mutate(ntest = n / prop)\nbabynames_test |&gt; slice(1, 2, 3, 4, 5)\n#&gt; # A tibble: 1,380 × 6\n#&gt; # Groups:   year, sex [276]\n#&gt;    year sex   name          n   prop   ntest\n#&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1  1880 F     Mary       7065 0.0724  97605.\n#&gt; 2  1880 F     Anna       2604 0.0267  97605.\n#&gt; 3  1880 F     Emma       2003 0.0205  97605.\n#&gt; 4  1880 F     Elizabeth  1939 0.0199  97605.\n#&gt; 5  1880 F     Minnie     1746 0.0179  97605.\n#&gt; 6  1880 M     John       9655 0.0815 118400.\n#&gt; # ℹ 1,374 more rows\n\n\n3.4.2 Your Turn\nYour Turn 1. Investigate how group_by() behaves with the five other main dplyr functions we have used. You can use toy_df to perform your investigation.\n\nmutate()\nslice()\nfilter()\narrange()\nselect()\n\nYour Turn 2. Use toy_df again to investigate how the dplyr functions treat missing values by default.\n\nmutate(). Try to create a new variable with mutate() involving x. What does R do with the missing value?\narrange(). Try arranging the data set by x. What does R do with the missing value?\nfilter(). Try filtering so that only observations where x is less than 5 are kept. What does R do with the missing value?\nsummarise(). Try using summarise() with a function involving x. What does R return?\ngroup_by() and summarise(). To your statement in 4, add a group_by(z) statement before your summarise(). What does R return now?\n\nYour Turn 3. In some cases throughout this chapter, we’ve renamed data sets using &lt;- with the same name like\n\ntoy_df &lt;- toy_df |&gt; mutate(newvar = x / y)\n\nIn other cases, we’ve given the data set a new name, like\n\ntoy_small &lt;- toy_df |&gt; filter(!is.na(x))\n\nFor which of the functions below is a generally “safe” to name the data set using the same name after using the function. Give a one sentence reason for each part.\n\nmutate()\narrange()\nfilter()\nsummarise()\nselect()\n\nYour Turn 4. Compare summarise() with mutate() using the following code. What’s the difference between the two functions?\n\nslumajors_df |&gt;\n  summarise(meantotalmajor = mean(ntotal),\n            totalgrad = sum(ntotal)) \nslumajors_df |&gt;\n  mutate(meantotalmajor = mean(ntotal),\n            totalgrad = sum(ntotal)) |&gt;\n  select(meantotalmajor, totalgrad, everything())\n\nYour Turn 5. With the babynames data, create a data set that has the number of unique names for each year. Then, make a line plot of the data.\nYour Turn 6. With the babynames data, create a data set that has the number of unique names for each year-sex combination. Then, make a line plot of the data.\nYour Turn 7. Create a data set that has a column for name so that each row shows the total number of births for each unique name across all years and both sexes."
  },
  {
    "objectID": "05-comm.html#reproducibility",
    "href": "05-comm.html#reproducibility",
    "title": "4  Communication with Quarto",
    "section": "\n4.1 Reproducibility",
    "text": "4.1 Reproducibility\nWe’ve been using Quarto for a while now, but have not yet talked about any of its features or how to do anything except insert a new code chunk. By the end of this section, we want to be able to use some of the Quarto options to make a nice-looking document (so that you can implement some of these options in your first mini-project).\nReproducibility is a concept that has recently gained popularity in the sciences for describing analyses that another researcher is able to repeat. That is, an analysis is reproducible if you provide enough information that the person sitting next to you can obtain identical results as long as they follow your procedures. An analysis is not reproducible if this isn’t the case.\nQuarto makes it easy for you to make your analysis reproducible for a couple of reasons:\n\na Quarto file will not render unless all of your code runs, meaning that you won’t accidentally give someone code that doesn’t work.\nQuarto combines the “coding” steps with the “write-up” steps into one coherent document that contains the code, all figures and tables, and any explanations.\n\nWe will “demo” a reproducible analysis in class.\n\n\n\n\n\n\nNote\n\n\n\nIf using Quarto for communication, you may want to utilize its spell-check feature. Go to Edit -&gt; Check Spelling, and you’ll be presented with a spell-checker that lets you change the spelling of any words you may have misspelled.\n\n\n\n4.1.1 R Scripts vs. Quarto\n\nWe’ve been using Quarto for the entirety of this course. But, you may have noticed that when you go to File -&gt; New File to open a new Quarto Document file, there are a ton of other options. The first option is R Script. Go ahead and open a new R Script file now.\nThe file you open should be completely blank.\n\n\n\n\n\n\nImportant\n\n\n\nAn R Script is a file that contains only R code.\n\n\nIt cannot have any text in it at all, unless that text is commented out with a #. For example, you could copy and paste all of the code that is inside a code chunk in a .qmd file to the .R file and run it line by line.\nSo, what are the advantages and disadvantages of using an R Script file compared to using an Quarto file? Let’s start with the advantages of Quarto. Quarto allows you to fully integrate text explanations of the code and results, the actual tables and figures themselves, and the code to make those tables and figures in one cohesive document. As we will see, if using R Scripts to write-up an analysis in Word, there is a lot of copy-pasting involved of results. For this reason, using Quarto often results in more reproducible analyses.\nThe advantage of an R Script would be in a situation where you really aren’t presenting results to anyone and you also don’t need any text explanations. This often occurs in two situations. (1) There are a lot of data preparation steps. In this case, you would typically complete all of these data prep steps in an R script and then write the resulting clean data to a .csv that you’d import in an Quarto file. (2) What you’re doing is complicated statistically. If this is the case, then the code is much more of a focus than the text or creating figures so you’d use an R Script.\nExercise 1. What’s the difference between R and Quarto?\nExercise 2. Why is a Quarto analysis more reproducible than a base R script analysis or a standard Excel analysis?"
  },
  {
    "objectID": "05-comm.html#quarto-files",
    "href": "05-comm.html#quarto-files",
    "title": "4  Communication with Quarto",
    "section": "\n4.2 Quarto Files",
    "text": "4.2 Quarto Files\nLet’s talk a bit more about the components of the Quarto file used to make the reproducible analysis shown in class.\nFirst, open a new Quarto file by clicking File -&gt; New File -&gt; Quarto Document and keep the new file so that it renders to HTML for now.\nThe first four to five lines at the top of the file make up the YAML (Yet Another Markup Language) header. We’ll come back to this at the end, as it’s the more frustrating part to learn.\nDelete the code below the YAML header and then paste the following code chunks to your clean .qmd file in two separate chunks:\n\nlibrary(tidyverse)\nhead(cars)\nggplot(data = cars, aes(x = speed, y = dist)) +\n  geom_point()\n\n\nsummary(cars)\n\n\n\n\n\n\n\nNote\n\n\n\nThe cars data set is built into R so there’s no need to do anything to read it in (it already exists in R itself).\n\n\n\n4.2.1 Code Chunk Options\nFirst, render your new file (and give it a name, when prompted). You should see some code, a couple of results tables, and a scatterplot.\nChunk options allow you to have some control over what gets printed to the file that you render. For example, you may or may not want: the code to be printed, the figure to be printed, the tables to be printed, the tidyverse message to be printed, etc. There are a ton of chunk options to give us control over the code and output that is shown! We are going to just focus on a few that are more commonly used.\nThe options below are common execute options in Quarto.\n\n\necho. This is set to either true to print the code or false to not print the code. In a blank line after ```{r}, insert the following, which tells Quarto not to print the code in that chunk: #| echo: false. Then, re-render your document to make sure that the code making the plot is actually hidden in the .html output.\n\nYou can keep adding other options on new lines. Some other options include:\n\nwarning. This is set to either true to print warnings and messages or false to not print warnings and messages. For example, when we load in the tidyverse, a message automatically prints out. In that same code chunk, add a new line with #| warning: false to get rid of the message. Re-render to make sure the message is actually gone.\noutput. By default, this is set to true and shows output of tables and figures. Change this to false to not print any output from running code. Practice adding a #| output: false to the code chunk in your Quarto file with summary(cars) and re-render to make sure the output from summary(cars) is gone.\neval. eval is set to true if the code is to be evaluated and false if not.\n\nBesides execute options, there are also options pertaining to the size of figures and figure captions. Some common examples include\n\nfig-height and fig-width control the height and width of figures. By default, these are both 7, but we can change the fig-height and fig-width to make figures take up less space in the rendered .html document (fig-height: 5, for example).\nfig-cap adds a figure caption to your figure. Try inserting #| fig-cap: \"Figure 1: caption text blah blah blah\" to your chunk options for the chunk with the plot.\n\n4.2.2 Global Options\nWhat we have discussed so far is how to change code and output options for individual chunks of code. But, it can be a pain to add a certain option to every single chunk of code that we want the option to apply to. What we can do instead is change the global option for the code and/or output options so that they apply to all code chunks, unless specifically overwritten in that chunk.\nWe can change the execute options (echo, warning, eval, output, and more) globally by adding a line to the YAML header at the top of your Quarto file. Try adding\nexecute: \n  echo: false\nas two new lines in between the title and format lines of the YAML header. This tells Quarto to not echo any code in any code chunk. However, note that you can change a local code chunk to have #| echo: true to override the global setting for that chunk.\nAdditional global execute options go on new lines:\nexecute: \n  echo: false\n  warning: false\nThe non-execute options pertaining to figure size can be changed globally by specifying the option after the html part of the YAML header. The following changes all figure heights to be 2, unless a chunk overrides the global setting:\n---\ntitle: \"Quarto Test\"\nexecute: \n  echo: false\nformat: \n  html:\n    fig-height: 2\n---\n\n\n\n\n\n\nImportant\n\n\n\nWe need to pay particular attention to the spacing of things in the YAML header. Notice, for example, that echo: false is indented by exactly two spaces. Try adding a space or deleting a space, and you’ll get an error!\n\n\n\n4.2.3 Figures and Tables\nWe’ve already seen that Figures will pop up automatically (unless we set output: false), which is quite convenient. Making tables that look nice requires one extra step.\nDelete the output: false option that you added earlier to the chunk with summary(cars). When you render your .qmd file now, results tables from head(cars) and summary(cars) look kind of ugly. We will focus on using the kable() function from the knitr package to make these tables much more aesthetically pleasing. Another option is to use the pander() function in the pander package. Both pander() and kable() are very simple functions to generate tables but will be more than sufficient for our purposes. To generate more complicated tables, see the xtable package or the kableExtra package.\nTo use these functions, simply add a |&gt; pipe with the name of the table function you want to use. head(cars) |&gt; kable() will make a nice-looking table with kable and head(cars) |&gt; pander() will use pander(). Before using kable() from the knitr package, you’ll need to install the knitr package with install.packages(\"knitr\") and load its library by adding the line library(knitr) above head(cars) |&gt; kable(). Before using pander(), you’ll need to install the pander package with install.packages(\"pander\") and then load its library by adding the line library(pander) above head(cars) |&gt; pander(). Try these out in your Quarto file.\nWhich table do you like better in this case?\nThere are plenty of options for making tables look presentable, which we will discuss in the Exercises. Keep in mind that youmay not use these when making tables for yourself. They’re much more useful when you’re writing a report that you want to share with others.\n\n4.2.4 Non-Code Options\nQuarto combines R (in the code chunks, which we’ve already discussed) with the Markdown syntax, which comprises the stuff outside the code chunks, like what you’re reading right now!\n\n\n\n\n\n\nNote\n\n\n\nThere are so many Markdown options, but most of the time, if you want to do something specific, you can just Google it. The purpose of what follows is just to get us familiar with the very basics and things you will probably use most often.\n\n\nBullet Points and Sub-bullet Points: Denoted with a * and -, respectively. The sub bullets should be indented by 4 spaces. Note that bullet points are not code and should not appear in a code chunk.\n* Bullet 1\n* Bullet 2\n    - Sub bullet 1\n    - Sub bullet 2\n    - Sub bullet 3\n\nEverything in Markdown is very particular with spacing. Things often have to be very precise. I personally just love it, but it can be frustrating sometimes. For example, indenting a sub-bullet by 3 spaces instead of 4 spaces will not make a sub-bullet.\n* Bullet 1\n   - Sub bullet 1\nNumbered Lists are the same as bulleted ones, except * is replaced with numbers 1., 2., etc.\nBold, Italics, Code. Surround text with __bold text__ to make text bold, _italic text_ to make text Italics, and backticks to make text look like Code.\nLinks: The simplest way to create a link to something on the web is to surround it with &lt; &gt; as in &lt;https://www.youtube.com/watch?v=gJf_DDAfDXs&gt;\nIf you want to name you link something other than the web address, use [name of link](https://www.youtube.com/watch?v=gJf_DDAfDXs), which should show up in your rendered document as “name of link” and, when clicked on, take you to the youtube video.\nHeaders: Headers are created with ## with fewer hashtags resulting in a bigger Header. Typing in #Big Header at the beginning of a line would make a big header, ### Medium Header would make a medium header, and ##### Small Header would make a small header. Headers are important because they get mapped to a table of contents.\nThere’s a lot of other stuff to explore: &lt;a href=“https://rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf” target=“blank&gt; https://rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf .\nBut, if you want to do something other than the basics, Google will definitely help.\n\n4.2.5 YAML\nWe have briefly discussed what’s given at the top of every .qmd file: the YAML header. The YAML header is the most frustrating part to change because it’s the most particular with spacing.\nIn addition to controlling global chunk options, we can also use the YAML header to specify a theme from  Bootswatch.\nThere are 25 themes in the Bootswatch project. The YAML header below uses the darkly theme. Try pasting in this YAML header and rendering the document to see the outputted theme.\n---\ntitle: \"Quarto Test\"\nexecute: \n  echo: false\n  warning: false\nformat: \n  html:\n    fig-height: 2\n    theme: darkly\n    embed-resources: true\n---\nWe can also add a table of contents, which will create a table of contents based on headers you have created with ##.\n---\ntitle: \"Quarto Test\"\nexecute: \n  echo: false\n  warning: false\nformat: \n  html:\n    fig-height: 2\n    theme: darkly\n    toc: true\n    embed-resources: true\n---\nThere are so many other options available for theming, and, if you know any css, you can provide your own .css file to customize the theme.\nFor the following exercises, we will use the built-in R data set mtcars, which has observations on makes and models of cars. The variables we will be using are:\n\n\ncyl, the number of cylinders a car has\n\nmpg, the mileage of the car, in miles per gallon\n\nBecause the data set is loaded every time R is started up, there is no need to have a line that reads in the data set. We can examine the first few observations with\n\nhead(mtcars)\n#&gt;                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n#&gt; Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n#&gt; Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n#&gt; Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n#&gt; Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n#&gt; Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n#&gt; Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nExercise 3. Create a table showing the mean mpg for each cyl group (cyl stands for cylinder and can be 4-cylinder, 6-cylinder, or 8-cylinder) with both kable() and pander().\n\n\n\n\n\n\nHint\n\n\n\n\n\nRemember to call the knitr library if using kable() and the pander library if using pander().\n\n\n\nExercise 4. Type ?kable into your console window and scroll through the Help file. Change the rounding of the mean so that it only displays one number after the decimal. Then, add a caption to the table that says “My First Table Caption!!”\n\n\n\n\n\n\nHint (only if you get stuck)\n\n\n\n\n\nThe arguments ou will need to change are digits and caption.\n\n\n\nExercise 5. Create a new R chunk and copy and paste the following into your new R chunk. Don’t worry about what factor() is doing: we will cover that soon!\n\nlibrary(tidyverse)\nhead(mtcars)\nggplot(data = mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_boxplot()\n\nModify the R chunk so that: (a) the figure height is 3, (b) the code from the R chunk shows in the .html file, (c) the table from running head(cars) is hidden in the .html file. Make (b) and (c) a local chunk option, but set (a) as a global option that applies to all of your R chunks. Hint: Instead of setting #| output: false, which would cause both the table and figure to not render in the .html output, add #| results: false as a local chunk option, which says to include figures in the .html output, but not tables or other printed output."
  },
  {
    "objectID": "05-comm.html#communication-options-for-ggplot2-plots",
    "href": "05-comm.html#communication-options-for-ggplot2-plots",
    "title": "4  Communication with Quarto",
    "section": "\n4.3 Communication Options for ggplot2 Plots",
    "text": "4.3 Communication Options for ggplot2 Plots\nWhen we first introduced plotting, we used histograms, boxplots, frequency plots, bar plots, scatterplots, line plots, and more to help us explore our data set. You will probably make many different plots in a single analysis, and, when exploring, it’s fine to keep these plots unlabeled and untitled with the default colour scheme and theme. They’re just for you, and you typically understand the data and what each variable means.\nHowever, when you’ve finished exploring and you’d like to communicate your results, both graphically and numerically, you’ll likely want to tweak your plots to look more aesthetically pleasing. You certainly wouldn’t be presenting every exploratory plot you made so this tweaking needs to be done on only a few plots. You might consider:\n\nchanging the x-axis and y-axis labels, changing the legend title, adding a title, adding a subtitle, and adding a caption with + labs()\nchanging the limits of the x-axis and y-axis with + xlim() and + ylim()\nchanging the colour scheme to be more visually appealing and easy to see for people with colour-vision-deficiency (CVD)\nlabeling certain points or lines with + geom_text() or + geom_label()\nchanging from the default theme with + theme_&lt;name_of_theme&gt;()\n\nThe bullet about labeling only certain points is the data set is one reason why we are doing this second ggplot2 section now, as opposed to immediately after the first ggplot2 section. As we will see, we’ll make use of combining what we’ve learned in dplyr to help us label interesting observations in our plots.\nThe Data\nThe Happy Planet Index (HPI) is a measure of how efficiently a country uses its ecological resources to give its citizens long “happy” lives. You can read more about this data here:  here.\nBut, the basic idea is that the HPI is a metric that computes how happy and healthy a country’s citizens are, but adjusts that by that country’s ecological footprint (how much “damage” the country does to planet Earth). The data set was obtained from  https://github.com/aepoetry/happy_planet_index_2016. Variables in the data set are:\n\n\nHPIRank, the rank of the country’s Happy Planet Index (lower is better)\n\nCountry, the name of the country\n\nLifeExpectancy, the average life expectancy of a citizen (in years)\n\nWellbeing, the average well being score (on a scale from 1 - 10). See the ladder question in the documentation for how this was calculated.\n\nHappyLifeYears, a combination of LifeExpectancy and Wellbeing\n\n\nFootprint, the ecological footprint per person (higher footprint means the average person in the country is less ecologically friendly)\n\nRead in the data set with\n\nlibrary(tidyverse)\nhpi_df &lt;- read_csv(\"https://raw.githubusercontent.com/highamm/ds234_quarto/main/data_online/hpi-tidy.csv\")\nhead(hpi_df)\n#&gt; # A tibble: 6 × 11\n#&gt;   HPIRank Country     LifeExpectancy Wellbeing HappyLifeYears Footprint\n#&gt;     &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1     109 Afghanistan           48.7      4.76           29.0     0.540\n#&gt; 2      18 Albania               76.9      5.27           48.8     1.81 \n#&gt; 3      26 Algeria               73.1      5.24           46.2     1.65 \n#&gt; 4     127 Angola                51.1      4.21           28.2     0.891\n#&gt; 5      17 Argentina             75.9      6.44           55.0     2.71 \n#&gt; 6      53 Armenia               74.2      4.37           41.9     1.73 \n#&gt; # ℹ 5 more variables: HappyPlanetIndex &lt;dbl&gt;, Population &lt;dbl&gt;,\n#&gt; #   GDPcapita &lt;dbl&gt;, GovernanceRank &lt;chr&gt;, Region &lt;chr&gt;\n\nLet’s look at the relationship between HappyLifeYears and Footprint for countries of different Regions of the world.\n\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point()\n\n\n\n\nWhich region seems to have the most variability in their Ecological Footprint?\n\n4.3.0.1 Change Labels and Titles\nWe can add + labs() to change various labels and titles throughout the plot:\n\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  labs(title = \"Countries with a Higher Ecological Footprint Tend to Have Citizens with Longer, Happier Lives\", \n       ## add title\n       subtitle = \"HappyLifeYears is a Combination of Life Expectancy and Citizen Well-Being\", \n       ## add subtitle (smaller text size than the title)\n       caption = \"Data Source: http://happyplanetindex.org/countries\", \n       ## add caption to the bottom of the figure\n       x = \"Ecological Footprint\", ## change x axis label\n       y = \"Happy Life Years\", ## change y axis label\n       colour = \"World Region\") ## change label of colour legend\n\n\n\n\nAny aes() that you use in your plot gets its own label and can be changed by name_of_aethetic = \"Your Label\". In the example above, we changed all three aes() labels: x, y, and colour.\nWhat is the only text on the plot that we aren’t able to change with labs()?\n\n4.3.0.2 Changing Axis Limits\nWe can also change the x-axis limits and the y-axis limits to, for example, start at 0 for the y-axis:\n\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  ylim(c(0, 70))\n\n\n\n\nIn this case, it makes the points on the plot a bit harder to see. You can also change where and how often tick marks appear on the x and y-axes. For special things like this, it is often best to just resort to Google (“ggplot how to change x-axis breaks tick marks” should help).\n\n4.3.0.3 Changing A Colour Scale\nWe want to use our graphics to communicate with others as clearly as possible. We also want to be as inclusive as possible in our communications. This means that, if we choose to use colour, our graphics should be made so that a colour-vision-deficient (CVD) person can read our graphs.\n\n\n\n\n\n\nNote\n\n\n\nAbout 4.5% of people are colour vision deficient, so it’s actually quite likely that a CVD person will view the graphics that you make (depending on how many people you share it with) More Information on CVD.\n\n\nThe colour scales from R Colour Brewer are readable for common types of CVD. A list of scales can be found here.\nWe would typically use the top scales if the variable you are colouring by is ordered sequentially (called seq for sequential, like grades in a course: A, B, C, D, F), the bottom scales if the variable is diverging (called div for diverging, like Republican / Democrat lean so that the middle is colourless), and the middle set of scales if the variable is not unordered and is categorical (called qual for qualitative like the names of different treatment drugs for a medical experiment).\nIn which of those 3 situations are we in for the World Region graph?\nIf we want to use one of these colour scales, we just need to add scale_colour_brewer() with the name of the scale we want to use.\n\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Accent\")\n\n\n\n\nTry changing the palette to something else besides \"Accent\". Do you like the new palette better or worse?\nOne more option to easily change the colour scale is to use the viridis package. The base viridis functions automatically load with ggplot2 so there’s no need to call the package with library(viridis). The viridis colour scales were made to be both aesthetically pleasing and CVD-friendly.\n\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_viridis_d(option = \"plasma\")\n\n\n\n\nA drawback of the viridis package is that the yellow can be really hard to see (at least for me).\nRead the examples section of the Help file for ?scale_colour_viridis_d. What’s the difference between scale_colour_viridis_d(), ?scale_colour_viridis_c(), and scale_colour_viridis_b()?\n\n4.3.0.4 Labeling Points or Lines of Interest\nOne goal we might have with communication is highlighting particular points in a data set that show something interesting. For example, we might want to label the points on the graph corresponding to the countries with the highest HPI in each region: these countries are doing the best in terms of using resources efficiently to maximize citizen happiness. Or, we might want to highlight some “bad” example of countries that are the least efficient in each region. Or, we might want to label the country that we are from on the graph.\nAll of this can be done with geom_text(). Let’s start by labeling all of the points. geom_text() needs one aesthetic called label which is the name of the column in the data set with the labels you want to use.\n\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_text(aes(label = Country))\n\n\n\n\nYikes! It’s quite uncommon to want to label all of the points. Let’s see if we can instead label each country with the best HPI in that country’s region. To do so, we first need to use our dplyr skills to create a new data set that has these 7 “best” countries. When we used group_by(), we typically used summarise() afterward. But, group_by() works with filter() as well!\n\nplot_df &lt;- hpi_df |&gt; group_by(Region) |&gt;\n  filter(HPIRank == min(HPIRank))\n\nplot_df is a data frame that only has the rows with the smallest HPIRank in each Region.\nNow that we have this new data set, we can use it within geom_text(). Recall that the data = argument in ggplot() carries on through all geoms unless we specify otherwise. Now is our chance to “specify otherwise” by including another data = argument within geom_text():\n\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_text(data = plot_df, aes(label = Country))\n\n\n\n\nThe show.legend = FALSE argument below tells geom_text() that we do not want its aesthetics to appear in the legend.\n\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point(aes(colour = Region)) +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_text(data = plot_df, aes(label = Country), show.legend = FALSE)\n\n\n\n\nA common issue, even with few labels, is that some of the labels could overlap. The ggrepel package solves this problem by including a geom_text_repel() geom that automatically repels any overlapping labels:\n\nlibrary(ggrepel)\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_text_repel(data = plot_df, aes(label = Country),\n                   show.legend = FALSE) \n\n\n\n\nAnd a final issue with the plot is that it’s not always very clear which point on the plot is being labeled. A trick used in the R for Data Science book is to surround the points that are being labeled with an open circle using an extra geom_point() function:\n\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears, colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_text_repel(data = plot_df, aes(label = Country), show.legend = FALSE) +\n  geom_point(data = plot_df, size = 3, shape = 1, show.legend = FALSE) \n\n\n\n\nIn the code above, shape = 1 says that the new point should be an open circle and size = 3 makes the point bigger, ensuring that it goes around the original point. show.legend = FALSE ensures that the larger open circles don’t become part of the legend.\nWe can use this same strategy to label specific countries. I’m interested in where the United States of America falls on this graph because I’m from the U.S. I’m also interested in where Denmark falls because that’s the country I’m most interested in visiting. Feel free to replace those countries with any that you’re interested in!\n\nplot_df_us &lt;- hpi_df |&gt;\n  filter(Country == \"United States of America\" | Country == \"Denmark\")\n\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_point(data = plot_df_us, size = 3, shape = 1,\n             show.legend = FALSE) +\n  geom_text_repel(data = plot_df_us, aes(label = Country),\n                   show.legend = FALSE)\n\n\n\n\n\n4.3.0.5 Plot Themes\nPlot themes are an easy way to change many aspects of your plot with an overall theme that someone developed. The default theme for ggplot2 graphs is theme_grey(), which is the graph with the grey background that we’ve been using in the entirety of this class. The 7 other themes are given in R for Data Science in Figure 28.3.\n\n\n\n\n\n\nNote\n\n\n\nMy favorite theme is definitely theme_minimal(). This theme gives a really clean look, so we will likely start using this theme in plots much more often now.\n\n\nHowever, there are many more choices in the ggthemes package. Load the package with library(ggthemes) and check out https://yutannihilation.github.io/allYourFigureAreBelongToUs/ggthemes/ for a few of the themes in the package. My personal favorites, all given below, are theme_solarized(), theme_fivethirtyeight(), and theme_economist(), but choosing a theme is mostly a matter of personal taste.\n\nlibrary(ggthemes)\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_point(data = plot_df_us, size = 3, shape = 1, show.legend = FALSE) +\n  geom_text_repel(data = plot_df_us, aes(label = Country), show.legend = FALSE) +\n  theme_solarized()\n\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_point(data = plot_df_us, size = 3, shape = 1, show.legend = FALSE) +\n  geom_text_repel(data = plot_df_us, aes(label = Country), show.legend = FALSE) +\n  theme_fivethirtyeight()\n\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_point(data = plot_df_us, size = 3, shape = 1, show.legend = FALSE) +\n  geom_text_repel(data = plot_df_us, aes(label = Country), show.legend = FALSE) +\n  theme_economist()\n\nThere’s still much more we can do with ggplot2. In fact, there are entire books on it. But, for most other specializations, we can usually use Google to help us!\nExercise 6. Change the following aspects of the plot given below.\n\nlibrary(palmerpenguins) ## will need to install.packages(\"palmerpenguins\")\n## if we have not used this package in class yet.\nggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm,\n                            colour = species)) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n\n\n\n\n\nChange the x axis label to be \"Bill Length (mm)\", the y axis label to be \"Bill Depth (mm)\", and the colour scale to be \"Species\".\nChange the default colour scale with scale_colour_viridis_d().\nChange the default theme to a theme of your choice.\nAdd a title to the plot."
  },
  {
    "objectID": "05-comm.html#practice",
    "href": "05-comm.html#practice",
    "title": "4  Communication with Quarto",
    "section": "\n4.4 Practice",
    "text": "4.4 Practice\nIn general, practice exercises will be split between exercises that will be done together as a class and exercises that you will do in groups or on your own. The purpose of the class exercises is to give some guidance on how we might think logically through some of the code and the results. The purpose of the group exercises and the on your own exercises is so that you have a chance to practice what you’ve learned with your table or on your own.\n\n4.4.1 Class Exercises\nSome data sets exist within specific R packages. For example, Jenny Bryan, who is quite famous in the stats/data science community, has put together the gapminder package so that users in R have access to a specific data set on countries throughout the world. https://github.com/jennybc/gapminder.\nTo load a data set within a specific R package, you first need to install the package with install.packages(\"gapminder\") and then load the package itself:\n\nlibrary(gapminder)\n\nThen, name the data set something. In this case, the name of the data set is gapminder, but it’s not always the same name as the package itself. We will name the data set country_df.\n\ncountry_df &lt;- gapminder\n\nExplore the data set with head(country_df) and ?gapminder before proceeding to the following exercises.\nClass Exercise 1. Make a line graph that shows the relationship between lifeExp and year for each of the countries in the data set, faceting the graph by continent and also colouring by continent (though this is redundant). Add an x-axis label, a y-axis label, a legend label, and a title to the graph.\nClass Exercise 2. Change the colour palette to be CVD-friendly using either scale_colour_brewer() or scale_colour_viridis_d().\nClass Exercise 3. We can see a couple of interesting trends in life expectancy. There is one country in Africa and one country in Asia that sees a sharp decline in life expectancy at one point. In Europe, there is one country that has a substantially lower life expectancy than the rest in the 1950s but catches up to other European countries by the 2000s. Use filter() to create a data set that only has these 3 countries. Then, use geom_text() to label all three countries on your plot.\nClass Exercise 4. Google the history of the countries in Africa and Asia that you just labeled. Add a very short description of why each country experienced a dip in life expectancy as a caption in your graph.\nClass Exercise 5. Suppose that we want the legend to appear on the bottom of the graph. Without using an entirely different theme, use Google to figure out how to move the legend from the right-hand side to the bottom.\nClass Exercise 6. If there are a lot of overlapping points or overlapping lines, we can use alpha to control the transparency of the lines. Google “change transparency of lines in ggplot” and change the alpha so that the lines are more transparent.\nClass Exercise 7. Change the theme of your plot to a theme in the ggthemes package. Then, change the order of your two commands to change the legend position and to change the overall theme. What happens?\nClass Exercise 8. Modify your .qmd file so that:\n\nonly the figure that you made in the previous exercise renders on your .html file. (Hint: use global options to help with this).\nnone of the code gets printed.\nthe warnings/messages that R prints by default are hidden in all code chunks.\nthe figure height is 4 instead of the default 7.\n\n4.4.2 Your Turn\nYour Turn 1. Your friend Chaz is doing a data analysis project in Excel to compare the average GPA of student athletes with the average GPA of non-student athletes. He has two variables: whether or not a student is a student athlete and GPA. He decides that a two-sample t-test is an appropriate procedure for this data (recall from Intro Stat that this procedure is appropriate for comparing a quantitative response (GPA) across two groups). Here are the steps of his analysis.\n\nHe writes the null and alternative hypotheses in words and in statistical notation.\nHe uses Excel to make a set of side-by-side boxplots. He changes the labels and the limits on the y-axis using Point-and-Click Excel operations.\nFrom his boxplots, he see that there are 3 outliers in the non-athlete group. These three students have GPAs of 0 because they were suspended for repeatedly refusing to wear masks indoors. Chaz decides that these 3 students should be removed from the analysis because, if they had stayed enrolled, their GPAs would have been different than 0. He deletes these 3 rows in Excel.\nChaz uses the t.test function in Excel to run the test. He writes down the degrees of freedom, the T-stat, and the p-value.\nChaz copies his graph to Word and writes a conclusion in context of the problem.\n\nWith your group, come up with two or more aspects of Chaz’s analysis that are not reproducible.\nYour Turn 2. Go back to the line plot that we made with the fitness data set of steps vs. Start date. Using geom_text(), add a label to the peak at April 14, 2019 that says \"half-marathon\" and a label to the peak at May 24, 2021 that say \"grand canyon\".\n\nYour Turn 3. Examine the mtcars data set again. The code chunk below adds the car_name variable to the data set (no need to worry about what this code is doing specifically at this point). With mtcars, create a scatterplot of mpg on the y-axis and wt on the x-axis. Then, label the point for the car in the data set that has the highest mpg with its car_name.\n\nmtcars &lt;- mtcars |&gt; rownames_to_column() |&gt; rename(car_name = rowname)\nmtcars\n#&gt;               car_name  mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n#&gt; 1            Mazda RX4 21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\n#&gt; 2        Mazda RX4 Wag 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\n#&gt; 3           Datsun 710 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\n#&gt; 4       Hornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\n#&gt; 5    Hornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\n#&gt; 6              Valiant 18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\n#&gt; 7           Duster 360 14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\n#&gt; 8            Merc 240D 24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\n#&gt; 9             Merc 230 22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\n#&gt; 10            Merc 280 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\n#&gt; 11           Merc 280C 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\n#&gt; 12          Merc 450SE 16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\n#&gt; 13          Merc 450SL 17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\n#&gt; 14         Merc 450SLC 15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\n#&gt; 15  Cadillac Fleetwood 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\n#&gt; 16 Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\n#&gt; 17   Chrysler Imperial 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\n#&gt; 18            Fiat 128 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\n#&gt; 19         Honda Civic 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\n#&gt; 20      Toyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\n#&gt; 21       Toyota Corona 21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\n#&gt; 22    Dodge Challenger 15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\n#&gt; 23         AMC Javelin 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\n#&gt; 24          Camaro Z28 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\n#&gt; 25    Pontiac Firebird 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\n#&gt; 26           Fiat X1-9 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\n#&gt; 27       Porsche 914-2 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n#&gt; 28        Lotus Europa 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n#&gt; 29      Ford Pantera L 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\n#&gt; 30        Ferrari Dino 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\n#&gt; 31       Maserati Bora 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\n#&gt; 32          Volvo 142E 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\nYour Turn 4. The theme() function is a way to really specialise your plot. We will explore some of these in this exercise. Using only options in theme() or options to change colours, shapes, sizes, etc., create the ugliest possible ggplot2 graph that you can make. You may not change the underlying data for this graph, but your goal is to investigate some of the options given in theme(). A list of theme options is given in this link.\n\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point()"
  },
  {
    "objectID": "14-workflow.html#r-and-file-organization",
    "href": "14-workflow.html#r-and-file-organization",
    "title": "5  Workflow and Other Skills",
    "section": "\n5.1 R and File Organization",
    "text": "5.1 R and File Organization\nR Projects are a convenient way to keep related code, data sets, and analyses together. Read this very short introduction in R for Data Science here: https://r4ds.had.co.nz/workflow-projects.html#paths-and-directories and https://r4ds.had.co.nz/workflow-projects.html#rstudio-projects.\nWhy should you rarely use an absolute directory?\nLook at the top of your bottom-left terminal window. If you’ve made an R project (which you should have!), you should see that a file path that is the current folder you’re working in. This is where R Studio will look for files by default.\nThe here package can help keep track of where files are and with reading in files. Install the here package with install.packages(\"here\"). Then, load the package and run the here() function with\n\nlibrary(here)\nhere()\n\nhere() prints the directory of where your current R project is.\nUp until now, we have been reading in data sets from a folder on the web where I have stored them. However, often we will have a local copy of the data set saved to our own computers. We will keep local data sets in a folder titled data in our current R Project.\n\n\n\n\n\n\nNote\n\n\n\nWe will download some data sets to our data folder from Canvas in class. Our primary way for reading in data from now on will be to read it in locally from our data folder.\n\n\nWhen reading in a data set locally with read_csv(), we would use\n\nlibrary(tidyverse)\nlibrary(here)\nathletes_df &lt;- read_csv(here(\"data/athletesdata.csv\"))\n\nhere() says to start looking for the file to read in from the root folder containing our current R project. From this folder, look for a folder called data, and, within data, look for a file called athletesdata.csv.\nSo, if you zipped up your project and sent it to someone else, they’d be able to open it and read that data file without needing to change any directory code!\nAgain, including here() specifies that we want R to start looking for our file at the Project root directory, which may or may not be the same as the directory where the .qmd file is.\n\n5.1.1 Exercises\nExercise 1. Take some time to modify your files in this course by creating a Quizzes folder. Move the relevant files to these folders and modify each file to load in the here package and use the here() function to read in any relevant data sets."
  },
  {
    "objectID": "14-workflow.html#code-style",
    "href": "14-workflow.html#code-style",
    "title": "5  Workflow and Other Skills",
    "section": "\n5.2 Code Style",
    "text": "5.2 Code Style\nWriting code that is “readable” is helpful not only for others but also for yourself, especially if the project you are working on is long-term. What constitutes “readable” code varies a bit, but there are some general principles that are more widely accepted for “good” code.\n\n\n\n\n\n\nNote\n\n\n\nMuch of the coding “style” you have seen so far has been imposed by me: I have my own style for writing code so naturally, I use that style in the code I write for our course materials.\n\n\nObject Names\nNames of objects that we create should be descriptive yet short. Sometimes, thinking of a name that makes sense can be very challenging! Some examples of “bad” names for objects include names that are too generic that we won’t be able to distinguish them later:\n\ndf1 &lt;- mtcars |&gt; filter(cyl == 4)\ndf2 &lt;- mtcars |&gt; filter(cyl == 6)\ndf3 &lt;- mtcars |&gt; filter(cyl == 8)\n\nBetter names for the above data frames would be cyl4_df, cyl6_df, and cyl8_df, respectively, as these names tell us more about what is in each data frame.\nOther “bad” names for objects are names that are too long:\n\ncars_with_4_cylinders_data_set &lt;- mtcars |&gt; filter(cyl == 4)\n\nLong names are descriptive but can be a pain to type out and to read.\nYou may have noticed that my coding “style” is to separate words in names with an _: cyl4_df. Others may choose to separate words in names with a .: cyl4.df while others may use capitalization for the second word: cyl4Df. The most important thing here is to be consistent in your choice. In other words, using _ instead of . isn’t necessarily better, but it would be poor practice to mix naming notation, as in:\n\ncyl4_df &lt;- mtcars |&gt; filter(cyl == 4)\ncyl6.df &lt;- mtcars |&gt; filter(cyl == 6)\n\nIf we mixed, then we always have to keep track of whether an object is named with _ or . or with capitalization.\nFinally, you may have noticed that most of our data frames our named with the suffix _df. I have worked that into my own coding style because I like keeping track of what is a dataframe (or tibble) and what isn’t. This is generally more helpful as you encounter different types of objects (model output, lists, matrices, vectors, etc.).\nExercise 2. Change the following object names to be “better:”\n\ncars_where_wt_is_larger_than_3_tons &lt;- mtcars |&gt; filter(wt &gt; 3)\n\n\ndataset &lt;- mtcars |&gt; group_by(am) |&gt;\n  summarise(mean_disp = mean(disp),\n            med_disp = median(disp),\n            sd_disp = sd(disp))"
  },
  {
    "objectID": "14-workflow.html#code-readability",
    "href": "14-workflow.html#code-readability",
    "title": "5  Workflow and Other Skills",
    "section": "\n5.3 Code Readability",
    "text": "5.3 Code Readability\nWe can also follow some general practices to make our code more “readable.” We have already been employing most of these practices throughout the semester: R Studio generally makes code readable by indenting appropriately.\nAppropriately using spacing can make code much more readable. Consider the following ggplot() code. For example, the following code chunk executes a scatterplot with a fitted regression line but it’s generally tough to read.\n\nggplot(data=mtcars,aes(x=wt,y=drat))+geom_point()+geom_smooth(method=\"lm\",se=FALSE)\n\n\n\n\nA couple of conventions can help: (1) spaces around any equal sign, plus sign, and after any comma and (2) putting code after each plus sign on a different line.\n\nggplot(data = mtcars, aes(x = wt, y = drat)) +\ngeom_point() +\ngeom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\nIndenting subsequent lines in ggplot2 code or in a dplyr pipeline shows that the subsequent lines “go with” the first line:\n\nggplot(data = mtcars, aes(x = wt, y = drat)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\nThe same concepts of using multiple lines holds for a piping statement as well. In general,\n\nmtcars |&gt; filter(cyl == 4) |&gt;\n  group_by(vs) |&gt;\n  summarise(mean_mpg = mean(mpg, na.rm = TRUE))\n#&gt; # A tibble: 2 × 2\n#&gt;      vs mean_mpg\n#&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1     0     26  \n#&gt; 2     1     26.7\n\nis easier to read than\n\nmtcars |&gt; filter(cyl == 4) |&gt; group_by(vs) |&gt; summarise(mean_mpg = mean(mpg, na.rm = TRUE))\n#&gt; # A tibble: 2 × 2\n#&gt;      vs mean_mpg\n#&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1     0     26  \n#&gt; 2     1     26.7\n\nExercise 3. Change the style of the following code to make the code more readable.\n\nggplot(data=mtcars,aes(x = mpg))+geom_histogram(colour=\"black\",fill=\"white\",bins=15) + facet_wrap(~cyl, ncol=1)"
  },
  {
    "objectID": "14-workflow.html#debugging-code",
    "href": "14-workflow.html#debugging-code",
    "title": "5  Workflow and Other Skills",
    "section": "\n5.4 Debugging Code",
    "text": "5.4 Debugging Code\nThe previous section on code readability can be seen as one step to helping with code debugging: code that is easier to read is code that is easier to spot errors in. Additionally, there are some other strategies we can take when our code is not working to figure out what the issue is.\nWe run R code for our data analyses from “top to bottom,” which makes it a bit easier to identify where the problem code is occurring. We can run our code from the top of our .qmd file, line by line, until we see the red Error message.\nOften this Error message will occur in a ggplot statement or a piping statement. If this is the case, then a further strategy is to run the ggplot statement + sign by + sign or to run the piping statement pipe by pipe to further isolate the error. For example, take the following ggplot code, which generates a somewhat cryptic error.\n\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(colour = disp) +\n  facet_wrap(~ cyl) \n\nIn this case, the error message does help us locate the issue, but that is not always the case. If we are not sure what the error is, what we can do is run\n\nggplot(data = mtcars, aes(x = wt, y = mpg))\n\nto see if we get an error. We don’t, so we move on to the code after the next + sign:\n\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point()\n\nWe still don’t get an error so we move on to the code after the next + sign:\n\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(colour = disp)\n\nWe have our error. So now, instead of isolating the error to a particular chunk of code, we have isolated the error to a particular line of code: we know the issue is something with how we are using geom_smooth(). (We are missing aes() to refer to the variable disp).\nThe same strategy can be used with piping. The following code, used to figure out the average bill length to bill depth ratio in Adelie penguins, does not give an error but instead outputs something that we might not expect: a tibble with an NaN (Not a Number) value (note that you must install the palmerpenguins package install.packages(\"palmerpenguins\") before loading the palmerpenguins library.\n\nlibrary(palmerpenguins)\npenguins |&gt; filter(species == \"Adeie\") |&gt;\n  mutate(bill_ratio = bill_length_mm / bill_depth_mm) |&gt;\n  summarise(mean_ratio = mean(bill_ratio))\n\nWe can troubleshoot by running the code “pipe by pipe,” starting with the code through the first filter() pipe:\n\npenguins |&gt; filter(species == \"Adeie\")\n\nRight away, we see a problem: we get a tibble with no data because we misspelled Adelie:\n\npenguins |&gt; filter(species == \"Adelie\")\n\nAfter correcting this issue, we can continue through the pipes:\n\npenguins |&gt; filter(species == \"Adelie\") |&gt;\n  mutate(bill_ratio = bill_length_mm / bill_depth_mm)\n\nThere doesn’t seem to be any issues in our mutate() statement so we can go to the next pipe.\n\npenguins |&gt; filter(species == \"Adelie\") |&gt;\n  mutate(bill_ratio = bill_length_mm / bill_depth_mm) |&gt;\n  summarise(mean_ratio = mean(bill_ratio))\n\nWe get an NA value, and we have isolated the issue to something with summarise(), or, possibly something with mutate() that does not set something up quite right for summarise(). Can you figure out the issue?\nIn addition to isolating the coding issue, a couple of other very basic strategies for trying to fix problematic code are to use a search engine like google to see if anyone else has a similar error message to the one you may have and to restart R to make sure that you are working from a clean slate.\nThe “restart R” strategy can be particularly helpful if you have code that will run but your .qmd file will not render. This can happen if you have, for example, created a data set that you use in a later chunk of code but have since deleted the code that created that data set. For example, suppose we create cyl4_df and make a plot:\n\ncyl4_df &lt;- mtcars |&gt; filter(cyl == 4)\n\nggplot(data = cyl4_df, aes(x = mpg)) +\n  geom_histogram()\n\n\n\n\nBut, later we delete the line creating cyl4_df. The plot will still work because cyl4_df is already in our environment but the file will not render because we are missing that crucial line of code. Restarting R can help us identify this issue because the plot will no longer work and we will get a sensible error message like cyl4_df not found."
  },
  {
    "objectID": "14-workflow.html#context-outliers-and-missing-values",
    "href": "14-workflow.html#context-outliers-and-missing-values",
    "title": "5  Workflow and Other Skills",
    "section": "\n5.5 Context, Outliers, and Missing Values",
    "text": "5.5 Context, Outliers, and Missing Values\nThe primary purpose of this section is to explore why we should always think critically about the data set we are analyzing as opposed to simply making summary tables without thinking about how they could be interpreted. In other words, we need to both examine the data set to see if there are things like missing values or outliers that could affect interpretation as well as consider the context that the data set comes from.\n\n5.5.1 Context\nConsidering context includes thinking about questions like:\n\nwhere did the data set come from? Who collected it?\nare there missing values coded as NAs in the data set. Would these affect our analysis or are they missing “at random.” Missing values coded as NA are referred to as explicitly missing values.\nare there missing values in the data set that are not actually observations at all? These are implicitly missing. An example might be collecting data on students attending this class. Students not present at the time the data was collected are implicitly missing.\ndoes the data come from an observational study or an experiment?\n\nThere are many other questions we could ask pertaining to context, but many such questions depend on the particular data collected. As an example, consider the data set on majors at SLU from 2015 to 2020. For now, you can ignore the extra code given to read in the data: pivoting functions and variable types are topics we will learn about in the upcoming weeks.\n\nmajors_df &lt;- read_csv(here(\"data/majors.csv\")) |&gt;\n  pivot_longer(-1, names_to = \"year\", values_to = \"n_majors\") |&gt;\n  mutate(year = as.numeric(year)) |&gt;\n  rename(major = `...1`)\nhead(majors_df)\n#&gt; # A tibble: 6 × 3\n#&gt;   major         year n_majors\n#&gt;   &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 Biochemistry  2005        2\n#&gt; 2 Biochemistry  2006        6\n#&gt; 3 Biochemistry  2007        5\n#&gt; 4 Biochemistry  2008        8\n#&gt; 5 Biochemistry  2009        3\n#&gt; 6 Biochemistry  2010        7\n\nIn the data, the n_majors variable represents the number of students graduating with that particular major in that particular year. For example, in the year 2005, there were just 2 students graduating with a Biochemistry major.\nSuppose we are interested in trends among majors in Estudios Hispanicos (Spanish). In the United States, there are many people who speak Spanish so we might expect this to be a somewhat popular major. In particular, we want to see if there is an increase or decrease in the number of these majors since 2005. We can make a line chart with:\n\nspanish_df &lt;- majors_df |&gt; filter(major == \"Estudios Hispanicos (Spanish)\")\nggplot(data = spanish_df, aes(x = year, y = n_majors)) +\n  geom_line() +\n  geom_smooth(se = FALSE)\n\n\n\n\nWhat would we conclude based on this plot?\nThe topic of this subsection is the context in which a data set arises in. So, you might guess that the conclusion one would make based on the line graph (that the spanish major at SLU seems to be in decline) does not tell the full story. In fact, about a decade ago, an International Economics Combined major was introduced, in which students complete courses in both Economics as well as foreign language studies. The most popular choice for the foreign language is Spanish.\nWe can make a graph of the number of International Economics Combined majors:\n\nint_econ_df &lt;- majors_df |&gt; filter(major == \"Int'l Economics (Combined)\")\nggplot(data = int_econ_df, aes(x = year, y = n_majors)) +\n  geom_line() +\n  geom_smooth(se = FALSE)\n\n\n\n\nHow does the new contextual information about the International Economics major influence your conclusions about the popularity of Spanish studies at SLU?\n\n\n\n\n\n\nImportant\n\n\n\nYou should find throughout the semester that the data sets on topics that you are more familiar with are easier to analyze than the data sets on topics that you are not as familiar with. A large part of the reasoning for this is that you have much more contextual information with data topics that you have prior knowledge on. That extra contextual information generally allows us to pose deeper questions, identify potentially erroneous data, and write more subtle conclusions.\n\n\nWe will discuss context more throughout the semester and will also have another focus on context when we discuss data ethics.\n\n5.5.2 Outliers and Missing Values\nOutliers in a data analysis can affect certain summary statistics, like the mean and the standard deviation (as you learned in STAT 113). They could also be observations that warrant further investigation if we are interested in why a particular point is an outlier.\n\n\n\n\n\n\nImportant\n\n\n\nMissing values can also cause us to reach a potentially misleading conclusion if we do not carefully consider why such values are missing.\n\n\nWe will talk about the consequences of outliers and missing values next, but first, we will discuss how to determine if there are outliers or missing values in the data set. An easy function to use for this purpose is the skim() function from the skimr package. Install the skimr package and then use the skim() function on thevideogame_clean.csv file, which contains variables on video games from 2004 - 2019, including\n\n\ngame, the name of the game\n\nrelease_date, the release date of the game\n\nrelease_date2, a second coding of release date\n\nprice, the price in dollars,\n\nowners, the number of owners (given in a range)\n\nmedian_playtime, the median playtime of the game\n\nmetascore, the score from the website Metacritic\n\nprice_cat, 1 for Low (less than 10.00 dollars), 2 for Moderate (between 10 and 29.99 dollars), and 3 for High (30.00 or more dollars)\n\nmeta_cat, Metacritic’s review system, with the following categories: “Overwhelming Dislike”, “Generally Unfavorable”, “Mixed Reviews”, “Generally Favorable”, “Universal Acclaim”.\n\nplaytime_miss, whether median play time is missing (TRUE) or not (FALSE)\n\nThe data set was modified from https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-07-30.\n\nlibrary(skimr)\nlibrary(here)\nvideogame_df &lt;- read_csv(here(\"data/videogame_clean.csv\"))\n\n\nskim(videogame_df)\n\nSee if you can find in the output the following:\n\nthe number of rows in the data set and the number of columns\nthe number of missing values for each variable\nthe number of unique values for each character variable\nthe completion rate (the proportion of values that are non-missing).\n\nIn particular, the number of missing values is given by nmissing and complete_rate gives the proportion of values that are non-missing. These give us some idea about if missing values exist for certain variables, and, if so, how many exist for each variable.\nAlso, at the bottom of the output, you should see tiny histograms of each numeric variable and some summary statistics. Looking at the min, max, and the histograms of each variable can inform us about whether each variable has any outliers. For example, we see that the histograms for price, median_playtime, and average_playtime all look extremely skewed right because of the outlier(s) on the upper end.\n\nSo, we now know that there are outliers and missing values for certain variables in the videogame data set. How might these affect the tables and graphs that we make?\nFirst, let’s focus on the metascore variable, which gives Metacritic’s overall aggregated review score for the videogames. Note that the complete_rate for the metascore variable is only 0.107: almost 90% of the videogames do not have a metascore.\nSo, suppose we are interested in exploring what the “typical” metascore is. We can figure out what the average metascore and what the median metascore of the non-missing videogames is with:\n\nvideogame_df |&gt; summarise(mean_meta = mean(metascore, na.rm = TRUE),\n                          med_meta = median(metascore, na.rm = TRUE))\n#&gt; # A tibble: 1 × 2\n#&gt;   mean_meta med_meta\n#&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1      71.9       73\n\nIgnoring the missing values, we would say that, on average, videogames receive metascores around 72 points. The question we need to ask ourselves is: “Is it reasonable to assume that the missing games receive similar reviews to the non-missing games so that we can thin about the 71.9 as the average review score of all games?”\nHow you answer might depend on what you understand about videogames and the review process. But I would argue that the missing games would be reviewed worse than the non-missing games. Major games usually get the most reviews and also usually have the most funding while there are many minor games that have little funding, would not get reviewed, and, if they did get reviewed, may get a lower rating.\n\n\n\n\n\n\nNote\n\n\n\nYou can certainly make a different argument: we don’t know if my argument is correct or not without further data. The most important thing to do is to at least think about and make clear possible limitations in your conclusions from a data analysis.\n\n\nAs a second example, consider an exploration of the relationship between median_playtime of a game and its metascore. We can make a scatterplot of the relationship, ignoring the missing values, with\n\nggplot(data = videogame_df, aes(x = metascore, y = median_playtime)) +\n  geom_point() +\n  geom_smooth()\n\n\n\n\nWe see some clear outliers, which we will talk about next, but would the missing values for metascore affect conclusions we draw from the graph? The answer would be “yes” if we think videogames with missing metascores would follow a different overall trend than those with non-missing metascores and “no” if we think that, if the videogames with missing metascores were rated, they would follow a similar trend as those already in the graph.\nFor this question, I would make the argument that the games would follow a similar trend. But again, that is an assumption I would need to make and need to be explicit about.\nWe also mentioned the idea of implicit missing values. These would be videogames that do not appear in the data set at all. In other words, was this set of videogames a sample or is it all videogames ever published in the United States? If it is a sample, how were they selected, and, if they are a convenience sample, what were the types of games that were left out?\n\nOutliers can also pose interesting challenges in data analysis. For example, consider again the graph of median_playtime vs. metascore. To focus on outliers now, we will ignore the missing values in the data.\n\nggplot(data = videogame_df, aes(x = metascore, y = median_playtime)) +\n  geom_point() +\n  geom_smooth()\n\n\n\n\nWe see some clear outliers in median_playtime: games with a median playtime of thousands of hours. Once again, having some knowledge about videogames can help us determine what to do with these outliers.\n\n\n\n\n\n\nImportant\n\n\n\nThe most important thing to do when dealing with outliers is to be explicit about what you, as the analyst, choose to keep in the graph or summary table and what you choose to remove. If you choose to remove values, give your reasoning, and, if there is space, you can also give a second graph that has the data without removing any outliers.\n\n\nIn this example, a median playtime of 3000+ hours seems a bit excessive, but it’s more challenging to determine what a reasonable cutoff for “excessive” is. Is it reasonable for a game to have a median playtime of 1000 hours? What aobut 2000 hours? 500 hours? Choosing which points to keep will affect the fit of the smoother. As you may have learned in STAT 113 or STAT 213, observations that have a high control over the fit of a smoother or regression line are influential.\nExercise 4. The STAT 113 survey data set contains responses from 397 STAT 113 students from a survey that students take at the beginning of the semester. There are 5 categorical variables and 7 numeric variables. Of the categorical variables, how many variables have 0 missing values? Of the numeric variables, how many variables have 0 missing values?\n\nlibrary(tidyverse)\nstat113_df &lt;- read_csv(here(\"data/stat113.csv\"))\n\n\n\n\n\n\n\nHint (only if you get stuck)\n\n\n\n\n\nUse the skim() function on stat113_df."
  },
  {
    "objectID": "14-workflow.html#reprexes-and-tibble",
    "href": "14-workflow.html#reprexes-and-tibble",
    "title": "5  Workflow and Other Skills",
    "section": "\n5.6 Reprexes and tibble\n",
    "text": "5.6 Reprexes and tibble\n\nA reproducible example, or reprex, is a chunk of code that we can give to someone else that runs without any outside data. These are used often on StackExchange. We can create a data set directly within R with the tibble() function in the tibble package. This is most useful when we want to make a small reproducible example so that someone else may help with our code.\nThe following code chunk is not a reprex because people would not necessarily have the data set parsedf.csv.\n\n## How do I get rid of the units from the values in\n## my variable `x`? Thanks!\nlibrary(tidyverse)\ntest_df &lt;- read_csv(here(\"data/parsedf.csv\"))\nhead(test_df)\n#&gt; # A tibble: 3 × 2\n#&gt;   x                   y\n#&gt;   &lt;chr&gt;           &lt;dbl&gt;\n#&gt; 1 20,000 dollars      1\n#&gt; 2 40 dollars          2\n#&gt; 3 only 13 dollars     3\n\nSuppose that we want to post on StackExchange for someone or ask a friend to help us convert a variable from a character vector with units to a numeric vector without units. We want to be able to give any possible helpers a small example data set to work with and isolate the problem or question that we have. For this, we can create our own tiny data set with tibble(), with the c() function that concatenates values together:\n\n## How do I get rid of the units from the values in\n## my variable `xvar`? Thanks!\nlibrary(tidyverse)\ntest_df2 &lt;- tibble(xvar = c(\"20,000 dollars\", \"40 dollars\"),\n                   yvar = c(1, 2))\ntest_df2\n#&gt; # A tibble: 2 × 2\n#&gt;   xvar            yvar\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 20,000 dollars     1\n#&gt; 2 40 dollars         2\n\nWhy is library(tidyverse) necessary in the code chunk above for my reprex?\nWe can copy and paste the code chunk above to our question: it’s code that anyone can run as long as they have the tidyverse package installed, and really encourages more people to help.\nAs a second example, we might have a question about how to find the mean for many numeric variables. For example, in the stat113.csv file, there are many numeric variables. We can compute the mean of each numeric variable by writing a separate summarise() statement for each variable. But we also may be interested in a quicker way. So, since our helper might not have the stat113.csv file, we can create a reprex for our problem:\n\n## is there a way to get a summary measure, like the mean, for \n## all numeric variables in a data set without writing a separate\n## summarise() statement for each variable?\n\nlibrary(tidyverse)\nsum_df &lt;- tibble(xvar = c(\"A\", \"B\"), yvar = c(1, 4), zvar = c(-1, 4),\n                 svar = c(\"G\", \"g\"), tvar = c(99, 100000))\n\nNote how we included some categorical variables in our reprex data set. We want code that will work even if there are categorical variables in our data set, so we must include them in our reprex in this example to be as general as possible.\nFor reference, the across() function can be used to answer our question (though it’s not the point of this section). The code below reads that we should summarise() across() all variables that are numeric (is.numeric) and that our summary measure should be the mean.\n\nsum_df |&gt; summarise(across(where(is.numeric), mean))\n#&gt; # A tibble: 1 × 3\n#&gt;    yvar  zvar   tvar\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1   2.5   1.5 50050.\n\n\n\n\n\n\n\nNote\n\n\n\nA reprex can also be created with data sets that are automatically loaded with R, such as mtcars, or data sets that are loaded with R packages, like the diamonds data set in the ggplot2 package. As long as others can easily obtain the data, they can help address your question.\n\n\nExercise 5. Use the tibble() function to make a data set with two variables: a variable called class that contains the four classes that you are currently taking and a variable called time, which gives the approximate number of hours per week that you spend on each class."
  },
  {
    "objectID": "14-workflow.html#practice",
    "href": "14-workflow.html#practice",
    "title": "5  Workflow and Other Skills",
    "section": "\n5.7 Practice",
    "text": "5.7 Practice\n\n5.7.1 Class Exercises\nClass Exercise 1. Click the “Packages” button in the lower-right hand window to bring up the packages menu. Instead of using library(name_of_package), you can click the check-box by the package name to load it into R. Try it out by un-checking and then re-checking tidyverse. Explain, from a reproducibility perspective, why loading packages this way is not good practice.\nClass Exercise 2. Find the error in the following code chunk by running the code “+ sign by + sign).\n\nggplot(data = mtcars, aes(x = hp, y = drat)) +\n  geom_point(aes(colour = factor(gear))) +\n  facet_wrap(cyl) +\n  geom_smooth()\n\nClass Exercise 3. Find the error in the following code chunk by running the code “pipe by pipe.”\n\npenguins |&gt; mutate(flipper_ratio = flipper_length_mm / body_mass_g) |&gt;\n  group_by(species, island) |&gt;\n  summarise(mean_flipper = mean(flipper_ratio, na.rm = TRUE)) |&gt;\n  arrange(flipper_ratio) |&gt;\n  pivot_wider(names_from = c(\"species\"), values_from = \"mean_flipper\")\n\nClass Exercise 4. Find the mean and median median_playtime for all videogames in the metacritic videogame data set. Then, remove the games with a median_playtime over 1000 hours. Compute the mean and median median_playtime of the data set without these games. Which measure, the mean or the median was more affected by having the outliers present?\nClass Exercise 5. For Project 2, we will work with some course evaluation data for a professor at SLU. Overall, you’ll answer some questions about how the professor can improve their courses at SLU by looking at course evaluation data. The variables and data set will be described in more detail in the project description.\nSuppose that you can’t figure out how to create a semester variable and a year variable from Term in evals_prof_S21.csv. (You want to split the Term variable into two variables: Semester with levels F and S and Year with levels 19, 20, and 21).\n\nlibrary(tidyverse)\nevals_df &lt;- read_csv(here(\"data/evals_prof_S21.csv\"))\nhead(evals_df)\n#&gt; # A tibble: 6 × 10\n#&gt;   Term  Course Question       `Agree strongly` Agree `Agree Somewhat` Neutral\n#&gt;   &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;                     &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 F19   113-02 1. Course has…                9     9                1       5\n#&gt; 2 F19   113-02 2. Effectivel…               12     8                1       2\n#&gt; 3 F19   113-02 3. Environmen…               11     8                2       3\n#&gt; 4 F19   113-02 5a. Fair Asse…                5    13                3       1\n#&gt; 5 F19   113-02 5b. Timely As…                8    12                1       2\n#&gt; 6 F19   113-02 5c. Construct…                5     8                4       6\n#&gt; # ℹ 3 more variables: `Disagree Somewhat` &lt;dbl&gt;, Disagree &lt;dbl&gt;,\n#&gt; #   `Disagree Strongly` &lt;dbl&gt;\n\nPut together a reprex using tibble() that someone would be able to run to help you figure out your question.\n\n5.7.2 Your Turn\nYour Turn 1. Choose a variable in the STAT 113 data set that has some missing values that you would feel comfortable ignoring the missing values in a table or graph. Give a one to two sentence reason.\nYour Turn 2. Choose a variable in the STAT 113 data set that has some missing values that you would not feel comfortable ignoring the missing values in a table or graph. Give a one to two sentence reason.\nYour Turn 3. Find the error in the following code chunk by running the code “pipe by pipe.”\n\npenguins |&gt; mutate(flipper_ratio = flipper_length_mm / body_mass_g) |&gt;\n  filter(flipper_ratio &gt; median(flipper_ratio)) |&gt;\n  group_by(species) |&gt;\n  summarise(count_var = n())\n\nYour Turn 4. Choose one of the 7 dplyr functions below and suppose that you do not know that that function exists.\n\nfilter().\nslice().\narrange().\nselect().\nmutate().\ngroup_by().\nsummarise().\n\nWrite a reprex for a question you might have where using that function would be the solution.\nYour Turn 5. Consider again the dplyr functions from the previous exercise. Write a reprex for a question that you actually do not know the answer to involving one of the functions."
  },
  {
    "objectID": "04-tidyr.html#what-is-tidy-data",
    "href": "04-tidyr.html#what-is-tidy-data",
    "title": "6  Tidying with tidyr",
    "section": "\n6.1 What is Tidy Data?",
    "text": "6.1 What is Tidy Data?\nR usually (but not always) works best when your data is in tidy form. A tidy data set has a few characteristics.\n\n\n\n\n\n\nNote\n\n\n\nYou should already be quite familiar with tidy data because, up to this point, all of the data sets we have used in this class (and probably most of the data sets that you see in STAT 113 an all of the data sets that you may have seen in STAT 213) are tidy.\n\n\nThe definition of tidy data below is taken from R for Data Science:\n\nevery variable in the data set is stored in its own column\nevery case in the data set is stored in its own row\neach value of a variable is stored in one cell\nvalues in the data set should not contain units\nthere should not be any table headers or footnotes\n\nWe will begin by focusing on the first characteristic: every variable in a the data set should be stored in its own column (and correspondingly, number 3: each value of a variable should be stored in one cell)."
  },
  {
    "objectID": "04-tidyr.html#separate-and-unite-columns",
    "href": "04-tidyr.html#separate-and-unite-columns",
    "title": "6  Tidying with tidyr",
    "section": "\n6.2 separate() and unite() Columns",
    "text": "6.2 separate() and unite() Columns\nIn a fresh .qmd file (File -&gt; New File -&gt; Quarto) that is in your Notes project, copy and paste the following code into an R chunk:\n\nlibrary(tidyverse)\nlibrary(here)\npolls &lt;- read_csv(here(\"data/rcp-polls.csv\"), na = \"--\")\npolls\n#&gt; # A tibble: 7 × 8\n#&gt;   Poll             Date  Sample   MoE `Clinton (D)` `Trump (R)` `Johnson (L)`\n#&gt;   &lt;chr&gt;            &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt;\n#&gt; 1 Monmouth         7/14… 688 LV   3.7            45          43             5\n#&gt; 2 CNN/ORC          7/13… 872 RV   3.5            42          37            13\n#&gt; 3 ABC News/Wash P… 7/11… 816 RV   4              42          38             8\n#&gt; 4 NBC News/Wall S… 7/9 … 1000 …   3.1            41          35            11\n#&gt; 5 Economist/YouGov 7/9 … 932 RV   4.5            40          37             5\n#&gt; 6 Associated Pres… 7/7 … 837 RV  NA              40          36             6\n#&gt; # ℹ 1 more row\n#&gt; # ℹ 1 more variable: `Stein (G)` &lt;dbl&gt;\n\nSuppose that you wanted to know what the average sample size of the polls was. Using dplyr functions,\n\npolls |&gt; summarise(meansample = mean(Sample))\n\nWhat warning do you get? Why?\nYou would get a similar warning (or sometimes an error) any time that you want to try to use Sample size in plotting or summaries. The issue is that the Sample column actually contains two variables so the data set is not tidy.\n\n6.2.1 separate() a Column\nLet’s separate() the Sample column into Sample_size and Sample_type:\n\npolls |&gt;\n  separate(col = Sample, into = c(\"Sample_size\", \"Sample_type\"), \n           sep = \" \")\n#&gt; # A tibble: 7 × 9\n#&gt;   Poll          Date  Sample_size Sample_type   MoE `Clinton (D)` `Trump (R)`\n#&gt;   &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1 Monmouth      7/14… 688         LV            3.7            45          43\n#&gt; 2 CNN/ORC       7/13… 872         RV            3.5            42          37\n#&gt; 3 ABC News/Was… 7/11… 816         RV            4              42          38\n#&gt; 4 NBC News/Wal… 7/9 … 1000        RV            3.1            41          35\n#&gt; 5 Economist/Yo… 7/9 … 932         RV            4.5            40          37\n#&gt; 6 Associated P… 7/7 … 837         RV           NA              40          36\n#&gt; # ℹ 1 more row\n#&gt; # ℹ 2 more variables: `Johnson (L)` &lt;dbl&gt;, `Stein (G)` &lt;dbl&gt;\n\nThe arguments to separate() are fairly easy to learn:\n\ncol is the name of the column in the data set you want to separate.\ninto is the name of the new columns. These could be anything you want, and are entered in as a vector (with c() to separate the names)\nsep is the character that you want to separate the column by. In this case, the sample size and sample type were separated by whitespace, so our sep = \" \", white space.\n\nThe sep argument is the newest piece of information here.\nNote that even using sep = \"\" will produce an error (there is not a space now, so R doesn’t know what to separate by):\n\npolls |&gt;\n  separate(col = Sample, into = c(\"Sample_size\", \"Sample_type\"), \n           sep = \"\")\n\nSimilarly, we would like the Date column to be separated into a poll start date and a poll end date:\n\npolls_sep &lt;- polls |&gt;\n  separate(col = Date, into = c(\"Start\", \"End\"),\n           sep = \" - \")\n\n\n6.2.2 unite() Columns\nunite() is the “opposite” of separate(): use it when one variable is stored across multiple columns, but each row still represents a single case. The need to use unite() is less common than separate(). In our current data set, there is no need to use it at all. But, for the sake of seeing an example, let’s separate the Start date into month and day and then use unite() to re-unite those columns:\n\npolls_sillytest &lt;- polls_sep |&gt;\n  separate(col = Start, into = c(\"Start_month\", \"Start_day\"), \n           sep = \"/\")\npolls_sillytest\n#&gt; # A tibble: 7 × 10\n#&gt;   Poll     Start_month Start_day End   Sample   MoE `Clinton (D)` `Trump (R)`\n#&gt;   &lt;chr&gt;    &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1 Monmouth 7           14        7/16  688 LV   3.7            45          43\n#&gt; 2 CNN/ORC  7           13        7/16  872 RV   3.5            42          37\n#&gt; 3 ABC New… 7           11        7/14  816 RV   4              42          38\n#&gt; 4 NBC New… 7           9         7/13  1000 …   3.1            41          35\n#&gt; 5 Economi… 7           9         7/11  932 RV   4.5            40          37\n#&gt; 6 Associa… 7           7         7/11  837 RV  NA              40          36\n#&gt; # ℹ 1 more row\n#&gt; # ℹ 2 more variables: `Johnson (L)` &lt;dbl&gt;, `Stein (G)` &lt;dbl&gt;\n\nThis situation could occur in practice: for example, the date variable may be in multiple columns, one for month and one for day (and if there are multiple years, there could be a third for year). We would use unite() to combine these two columns into a single Date, called New_start_date:\n\npolls_sillytest |&gt;\n  unite(\"New_start_date\", c(Start_month, Start_day),\n        sep = \"/\")\n#&gt; # A tibble: 7 × 9\n#&gt;   Poll            New_start_date End   Sample   MoE `Clinton (D)` `Trump (R)`\n#&gt;   &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1 Monmouth        7/14           7/16  688 LV   3.7            45          43\n#&gt; 2 CNN/ORC         7/13           7/16  872 RV   3.5            42          37\n#&gt; 3 ABC News/Wash … 7/11           7/14  816 RV   4              42          38\n#&gt; 4 NBC News/Wall … 7/9            7/13  1000 …   3.1            41          35\n#&gt; 5 Economist/YouG… 7/9            7/11  932 RV   4.5            40          37\n#&gt; 6 Associated Pre… 7/7            7/11  837 RV  NA              40          36\n#&gt; # ℹ 1 more row\n#&gt; # ℹ 2 more variables: `Johnson (L)` &lt;dbl&gt;, `Stein (G)` &lt;dbl&gt;\n\n\n\n\n\n\n\nNote\n\n\n\nunite() just switches around the first two arguments of separate(). Argument 1 is now the name of the new column and Argument 2 is the names of columns in the data set that you want to combine.\n\n\nWe have also used the c() function in separate() and unite(). While c() is a very general R function and isn’t specific to tidy data, this is the first time that we’re seeing it in this course. c() officially stands for concatenate, but, in simpler terms, c() combines two or more “things”, separated by a comma.\n\nc(1, 4, 2)\n#&gt; [1] 1 4 2\nc(\"A\", \"A\", \"D\")\n#&gt; [1] \"A\" \"A\" \"D\"\n\nThis is useful if a function argument expects two or more “things”: for example, in separate(), the into argument requires two column names for this example. Those column names must be specified by combining the names together with c().\n\n6.2.3 Column Names and rename()\n\nYou might have noticed that the columns with percentage of votes for Clinton, Trump, etc. are surrounded by backticks ` ` when you print polls or polls_sep:\n\npolls_sep\n#&gt; # A tibble: 7 × 9\n#&gt;   Poll       Start End   Sample   MoE `Clinton (D)` `Trump (R)` `Johnson (L)`\n#&gt;   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt;\n#&gt; 1 Monmouth   7/14  7/16  688 LV   3.7            45          43             5\n#&gt; 2 CNN/ORC    7/13  7/16  872 RV   3.5            42          37            13\n#&gt; 3 ABC News/… 7/11  7/14  816 RV   4              42          38             8\n#&gt; 4 NBC News/… 7/9   7/13  1000 …   3.1            41          35            11\n#&gt; 5 Economist… 7/9   7/11  932 RV   4.5            40          37             5\n#&gt; 6 Associate… 7/7   7/11  837 RV  NA              40          36             6\n#&gt; # ℹ 1 more row\n#&gt; # ℹ 1 more variable: `Stein (G)` &lt;dbl&gt;\n\nThis happens because the column names have a space in them (this also would occur if the columns started with a number or had odd special characters in them). Then, any time you want to reference a variable, you need the include the backticks:\n\npolls_sep |&gt;\n  summarise(meanclinton = mean(Clinton (D))) ## throws an error\npolls_sep |&gt;\n  summarise(meanclinton = mean(`Clinton (D)`)) ## backticks save the day!\n\nHaving variable names with spaces doesn’t technically violate any principle of tidy data, but it can be quite annoying. Always using backticks can be a huge pain. We can rename variables easily with rename(), which just takes a series of new_name = old_name arguments.\n\npolls_new &lt;- polls_sep |&gt;\n  rename(Clinton_D = `Clinton (D)`, Trump_R = `Trump (R)`,\n         Johnson_L = `Johnson (L)`, Stein_G = `Stein (G)`)\npolls_new\n#&gt; # A tibble: 7 × 9\n#&gt;   Poll           Start End   Sample   MoE Clinton_D Trump_R Johnson_L Stein_G\n#&gt;   &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 Monmouth       7/14  7/16  688 LV   3.7        45      43         5       1\n#&gt; 2 CNN/ORC        7/13  7/16  872 RV   3.5        42      37        13       5\n#&gt; 3 ABC News/Wash… 7/11  7/14  816 RV   4          42      38         8       5\n#&gt; 4 NBC News/Wall… 7/9   7/13  1000 …   3.1        41      35        11       6\n#&gt; 5 Economist/You… 7/9   7/11  932 RV   4.5        40      37         5       2\n#&gt; 6 Associated Pr… 7/7   7/11  837 RV  NA          40      36         6       2\n#&gt; # ℹ 1 more row\n\nrename() can also be very useful if you have variable names that are very long to type out.\n\n\n\n\n\n\nNote\n\n\n\nrename() is actually from dplyr, not tidyr, but we did not have a need for it with any of the dplyr data sets.\n\n\nIn the following exercises, use the toy data set to practice with separate() and unite().\n\ntoy_df &lt;- tibble::tibble(show = c(\"Bojack Horseman: Netflix\",\n                                  \"VEEP: HBO\",\n                                  \"American Vandal: Netflix\",\n                                  \"Community: Peacock\"),\n                         seasons = c(\"1-6\", \"1-7\", \"1-2\", \"1-6\"),\n                         main_first = c(\"Bojack\", \"Selina\", \"Peter\", \"Jeff\"),\n                         main_last = c(\"Horseman\", \"Meyer\", \"Molganado\", \"Winger\"))\ntoy_df\n#&gt; # A tibble: 4 × 4\n#&gt;   show                     seasons main_first main_last\n#&gt;   &lt;chr&gt;                    &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;    \n#&gt; 1 Bojack Horseman: Netflix 1-6     Bojack     Horseman \n#&gt; 2 VEEP: HBO                1-7     Selina     Meyer    \n#&gt; 3 American Vandal: Netflix 1-2     Peter      Molganado\n#&gt; 4 Community: Peacock       1-6     Jeff       Winger\n\nExercise 1. separate() the show variable into a column that has the TV show name and a column that has the name of the network.\nExercise 2. separate() the seasons variable into a column that has the starting season and a column that has the ending season.\nExercise 3. unite() the main_first and main_last columns into a new column that gives the full name of the main character, with their first and last names separated by a single space."
  },
  {
    "objectID": "04-tidyr.html#reshaping-with-pivot_",
    "href": "04-tidyr.html#reshaping-with-pivot_",
    "title": "6  Tidying with tidyr",
    "section": "\n6.3 Reshaping with pivot_()\n",
    "text": "6.3 Reshaping with pivot_()\n\nWe will continue to use the polling data set to introduce the pivoting functions and data reshaping. To make sure that we are all working with the same data set, run the following line of code:\n\npolls_clean &lt;- polls |&gt;\n  separate(col = Sample, into = c(\"Sample_size\", \"Sample_type\"), \n           sep = \" \")  |&gt;\n  separate(col = Date, into = c(\"Start\", \"End\"),\n           sep = \" - \") |&gt; \n  rename(Clinton_D = `Clinton (D)`, Trump_R = `Trump (R)`,\n         Johnson_L = `Johnson (L)`, Stein_G = `Stein (G)`)\npolls_clean\n\nThe data set polls_clean still isn’t tidy!! The candidate variable is spread out over 4 different columns and the values in each of these 4 columns actually represent 1 variable: poll percentage.\nThinking about data “tidyness” using the definitions above can sometimes be a little bit confusing. In practice, oftentimes we will usually realize that a data set is untidy when we go to do something that should be super simple but that something turns out to not be super simple at all when the data is in its current form.\nFor example, one thing we might want to do is to make a plot that has poll Start time on the x-axis, polling numbers on the y-axis, and has candidates represented by different colours. For this small data set, we might not see any trends through time, but you could imagine this graph would be quite useful if we had polling numbers through June, July, August, September, etc.\nTake a moment to think about how you would make this graph in ggplot2: what is your x-axis variable? What variable are you specifying for the y-axis? For the colours?\nA first attempt in making a graph would be:\n\nggplot(data = polls_clean, aes(x = Start, y = Clinton_D)) + \n  geom_point(aes(colour = ....??????????))\n\nAnd we’re stuck. It’s certainly not impossible to make the graph with the data in its current form (keep adding geom_point() and re-specifying the aesthetics, then manually specify colours, then manually specify a legend), but it’s definitely a huge pain.\nThis is where pivot_longer() can help! https://www.youtube.com/watch?v=8w3wmQAMoxQ\n\n6.3.1 pivot_longer() to Gather Columns\npivot_longer() “pivots” the data set so that is has more rows (hence the “longer”) by collapsing multiple columns into two columns. One new column is a “key” column, which is the new variable containing the old data set’s column names. The second new column is a “value” column, which is the new variable containing the old data set’s values for each of the old data set’s column names. It’s easier to see this with an example. We know from our plotting exercise above that we’d really like a candidate variable to colour by and a poll_percent variable for the y-axis of our plot. So, we can use pivot_longer() to make these two columns:\n\npolls_clean |&gt;\n  pivot_longer(cols = c(Clinton_D, Trump_R, Johnson_L, Stein_G),\n               names_to = \"candidate\", values_to = \"poll_percent\")\n#&gt; # A tibble: 28 × 8\n#&gt;   Poll     Start End   Sample_size Sample_type   MoE candidate poll_percent\n#&gt;   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n#&gt; 1 Monmouth 7/14  7/16  688         LV            3.7 Clinton_D           45\n#&gt; 2 Monmouth 7/14  7/16  688         LV            3.7 Trump_R             43\n#&gt; 3 Monmouth 7/14  7/16  688         LV            3.7 Johnson_L            5\n#&gt; 4 Monmouth 7/14  7/16  688         LV            3.7 Stein_G              1\n#&gt; 5 CNN/ORC  7/13  7/16  872         RV            3.5 Clinton_D           42\n#&gt; 6 CNN/ORC  7/13  7/16  872         RV            3.5 Trump_R             37\n#&gt; # ℹ 22 more rows\n\npivot_longer() has three important arguments:\n\n\ncols, the names of the columns that you want to PIVOT!\n\nnames_to, the name of the new variable that will have the old column names (anything you want it to be!)\n\nvalues_to, the name of the new variable that will have the old column values (anything you want it to be!)\n\nNow we can make our plot using ggplot functions. But don’t forget to give a name to our new “long” data set first!\n\npolls_long &lt;- polls_clean |&gt;\n  pivot_longer(cols = c(Clinton_D, Trump_R, Johnson_L, Stein_G),\n               names_to = \"candidate\", values_to = \"poll_percent\")\n\n## ignore as.Date for now....we will get to dates later!\nggplot(data = polls_long,\n       aes(x = as.Date(Start, \"%m/%d\"), y = poll_percent,\n           colour = candidate)) +\n  geom_point() + labs(x = \"Poll Start Date\")\n\n\n\n\n\n6.3.2 pivot_wider() to Spread to Multiple Columns\nThe “opposite” of pivot_longer() is pivot_wider(). We need to use pivot_wider() when one case is actually spread across multiple rows. pivot_wider() has two main arguments:\n\n\nnames_from, the column in the old data set that will provide the names of the new columns and\n\nvalues_from, the column in the old data set that will provide the values that fill in the new columns\n\nOne common use case of pivot_wider() is the creation of a contingency table from summarised data. For example, suppose that, from the STAT 113 survey data, we have\n\nstat113_df &lt;- read_csv(here::here(\"data/stat113.csv\"))\nstat113_sum &lt;- stat113_df |&gt; filter(!is.na(Sex)) |&gt; group_by(Year, Sex) |&gt;\n  summarise(n_students = n())\nstat113_sum\n#&gt; # A tibble: 8 × 3\n#&gt; # Groups:   Year [4]\n#&gt;   Year      Sex   n_students\n#&gt;   &lt;chr&gt;     &lt;chr&gt;      &lt;int&gt;\n#&gt; 1 FirstYear F             90\n#&gt; 2 FirstYear M             84\n#&gt; 3 Junior    F             15\n#&gt; 4 Junior    M             18\n#&gt; 5 Senior    F              3\n#&gt; 6 Senior    M             10\n#&gt; # ℹ 2 more rows\n\nWe want to make a two-way (contingency) table of the number of students that has Sex on the rows and Year on the columns. pivot_wider() can put the levels of the Year variable as new columns in the data frame:\n\nstat113_sum |&gt; pivot_wider(names_from = Year, values_from = n_students)\n#&gt; # A tibble: 2 × 5\n#&gt;   Sex   FirstYear Junior Senior Sophomore\n#&gt;   &lt;chr&gt;     &lt;int&gt;  &lt;int&gt;  &lt;int&gt;     &lt;int&gt;\n#&gt; 1 F            90     15      3       100\n#&gt; 2 M            84     18     10        76\n\nWe will see more examples of pivot_wider() and pivot_longer() in the Exercises.\n\n\n\n\n\n\nImportant\n\n\n\nTidy data isn’t necessarily always better: you might find cases where you need to “untidy” the data by using pivot_longer() or pivot_wider(). However, most functions in R (and in other languages) work best with tidy data.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere are a few more topics to discuss in tidying data. We have not yet discussed the 4th or 5th characteristics of tidy data (cells should not contain units and there should be no headers or footers), but these are usually dealt with when we read in the data. Therefore, these issues will be covered when we discuss readr.\n\n\nExercise 4. With the stat113_df data, construct a table showing the average GPA (without missing GPA values) for each Sex-Sport combination. Then, pivot the table so that the columns are the levels of Sport and the rows are the levels of Sex.\n\n\n\n\n\n\nHint (only if you get stuck)\n\n\n\n\n\nTo create the (unpivoted) table, you will need to use group_by() and summarise(), adding in an na.rm = TRUE argument to summarise().\n\n\n\nExercise 5. Suppose that you have the following data set that has the number of deaths from a disease for various years in different locations.\n\ndisease_df &lt;- tibble::tibble(location = c(\"A\", \"B\", \"C\", \"D\", \"E\"),\n                             year1 = c(5, 1, 10, 400, 31),\n                             year2 = c(10, 80, 99, 100, 1),\n                             year3 = c(0, 0, 20, 0, 40))\n\nPivot the data set so that there is a single column that gives the year and a single column that gives the death count."
  },
  {
    "objectID": "04-tidyr.html#practice",
    "href": "04-tidyr.html#practice",
    "title": "6  Tidying with tidyr",
    "section": "\n6.4 Practice",
    "text": "6.4 Practice\n\n6.4.1 Class Exercises\nClass Exercise 1. Once you have a handle on data science terminology, it’s not too difficult to transfer what you’ve learned to a different language. For example, students in computer science might be more familiar with Python. Google something like “pivot from wide to long in python” to find help on achieving the equivalent of pivot_longer() in Python.\nClass Exercise 2. The UBSprices2 data set contains information on prices of common commodities in cities throughout the world in the years 2003 and 2009. The three commodities in the data set are Rice (1 kg worth), Bread (1 kg worth), and a Big Mac https://media1.giphy.com/media/Fw5LicDKem6nC/source.gif\n\nprices_df &lt;- read_csv(here(\"data/UBSprices2.csv\"))\n\nConvert the data set to a tidier form so that there is a year variable and a commodity variable that has 3 values: \"bigmac\", \"bread\", and \"rice\". Hint: At some point, you will need to separate the commodity from the year in, for example, bread2009. But, you’ll notice this is different than our other uses of separate() because there is no “-” or ” ” or “/” to use as a separating character. Look at the help for separate() and scroll down to the sep argument to see if you can figure out this issue.\nClass Exercise 3. Convert your data set from the previous exercise so that commodity is split up into 3 variables: bigmac price, rice price and bread price.\nClass Exercise 4. In which data set would it be easiest to make a line plot with year on the x-axis and price of rice on the y-axis with lines for each city? In which data set would it be easiest to make a line chart with 3 lines, one for each type of commodity, for the city of Amsterdam?\nNew Data:\nThe under5mortality.csv file contains data on mortality for people under the age of 5 in countries around the world (mortality in deaths per 1000 people). The data come from https://www.gapminder.org/data/. The data set is extremely wide in its current form, having a column for each year in the data set. Read in the data set with\n\nmortality_df &lt;- read_csv(here(\"data/under5mortality.csv\"))\nhead(mortality_df)\n#&gt; # A tibble: 6 × 217\n#&gt;   `Under five mortality` `1800` `1801` `1802` `1803` `1804` `1805` `1806`\n#&gt;   &lt;chr&gt;                   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 Abkhazia                  NA     NA     NA     NA     NA     NA     NA \n#&gt; 2 Afghanistan              469.   469.   469.   469.   469.   469.   470.\n#&gt; 3 Akrotiri and Dhekelia     NA     NA     NA     NA     NA     NA     NA \n#&gt; 4 Albania                  375.   375.   375.   375.   375.   375.   375.\n#&gt; 5 Algeria                  460.   460.   460.   460.   460.   460.   460.\n#&gt; 6 American Samoa            NA     NA     NA     NA     NA     NA     NA \n#&gt; # ℹ 209 more variables: `1807` &lt;dbl&gt;, `1808` &lt;dbl&gt;, `1809` &lt;dbl&gt;,\n#&gt; #   `1810` &lt;dbl&gt;, `1811` &lt;dbl&gt;, `1812` &lt;dbl&gt;, `1813` &lt;dbl&gt;, `1814` &lt;dbl&gt;,\n#&gt; #   `1815` &lt;dbl&gt;, `1816` &lt;dbl&gt;, `1817` &lt;dbl&gt;, `1818` &lt;dbl&gt;, `1819` &lt;dbl&gt;,\n#&gt; #   `1820` &lt;dbl&gt;, `1821` &lt;dbl&gt;, `1822` &lt;dbl&gt;, `1823` &lt;dbl&gt;, `1824` &lt;dbl&gt;,\n#&gt; #   `1825` &lt;dbl&gt;, `1826` &lt;dbl&gt;, `1827` &lt;dbl&gt;, `1828` &lt;dbl&gt;, `1829` &lt;dbl&gt;,\n#&gt; #   `1830` &lt;dbl&gt;, `1831` &lt;dbl&gt;, `1832` &lt;dbl&gt;, `1833` &lt;dbl&gt;, `1834` &lt;dbl&gt;,\n#&gt; #   `1835` &lt;dbl&gt;, `1836` &lt;dbl&gt;, `1837` &lt;dbl&gt;, `1838` &lt;dbl&gt;, `1839` &lt;dbl&gt;, …\n\nClass Exercise 5. Notice that there are 217 columns (at the top of the print out of the header, 217 is the second number). When we use tidyr, we aren’t going to want to type out c(2, 3, 4, 5, .....) all the way up to 217! R has short-hand notation that we can use with :. For example, type in 4:9 in your console window. Use this notation to tidy the mortality_df data set.\n\n\n\n\n\n\nNote\n\n\n\nYou’ll need to add something to your pivot_longer() function to convert the variable Year to numeric. We haven’t talked too much about variable types yet so, after your values_to = \"Mortality\" statement, add , names_transform = list(Year = as.numeric), making sure you have a second ) to close the pivot_longer() function.\n\n\nClass Exercise 6. Make a line plot to look at the overall under 5 mortality trends for each country.\nClass Exercise 7. What is the overall trend in under 5 mortality? Does every single country follow this trend? What looks strange about the plot, specifically about the data collected before 1900?\n\n6.4.2 Your Turn\nThe MLB salary data set contains salaries on all 862 players in Major League Baseball in 2016. The data set was obtained from http://www.usatoday.com/sports/mlb/salaries/2016/player/all/\nYour Turn 1. Read in the data using the following code chunk and, with your group, write a sentence or two that explains why the data set is not tidy.\n\nlibrary(tidyverse)\nlibrary(here)\nbaseball_df &lt;- read_csv(here(\"data/mlb2016.csv\"))\nhead(baseball_df)\n#&gt; # A tibble: 6 × 7\n#&gt;   Name             Team  POS   Salary       Years      Total.Value Avg.Annual\n#&gt;   &lt;chr&gt;            &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;     \n#&gt; 1 Clayton Kershaw  LAD   SP    $ 33,000,000 7 (2014-2… $ 215,000,… $ 30,714,…\n#&gt; 2 Zack Greinke     ARI   SP    $ 31,799,030 6 (2016-2… $ 206,500,… $ 34,416,…\n#&gt; 3 David Price      BOS   SP    $ 30,000,000 7 (2016-2… $ 217,000,… $ 31,000,…\n#&gt; 4 Miguel Cabrera   DET   1B    $ 28,000,000 10 (2014-… $ 292,000,… $ 29,200,…\n#&gt; 5 Justin Verlander DET   SP    $ 28,000,000 7 (2013-1… $ 180,000,… $ 25,714,…\n#&gt; 6 Yoenis Cespedes  NYM   CF    $ 27,328,046 3 (2016-1… $ 75,000,0… $ 25,000,…\n\nYour Turn 2. Tidy the data set just so that\n\n\nDuration of the salary contract (currently given in the Year column) is in its own column\nthe year range (also currently given in the Year column) is split into a variable called Start and a variable called End year that give the start and end years of the contract. You can still have special characters for now (like ( and )) in the start and end year.\n\nYour Turn 3. You should have received a warning message. What does this message mean? See if you can figure it out by typing View(baseball_df) in your console window and scrolling down to some of the rows that the warning mentions: 48, 59, 60, etc. With your group why that warning message was given.\nYour Turn 4. We won’t learn about parse_number() until readr, but the function is straightforward enough to mention here. It’s useful when you have extra characters in the values of a numeric variable (like a $ or a (), but you just want to grab the actual number:\n\nbaseball_df &lt;- baseball_df |&gt;\n  mutate(Salary = parse_number(Salary),\n         Total.Value = parse_number(Total.Value),\n         Avg.Annual = parse_number(Avg.Annual),\n         Start = parse_number(Start),\n         End = parse_number(End))\n\nRun the code above so that the parsing is saved to baseball_df.\nYour Turn 5. With a partner, brainstorm 2 different graphics you could construct to investigate how player Salary compares for players with different POS. Each of you should make 1 of the graphics you discussed. Then, state the reason why making that plot would not have worked before we tidied the data set.\nNew Data: For the rest of the exercises, we will use salary data from the National Football League obtained from FiveThirtyEight that were originally obtained from Spotrac.com.\nThe data set has the top 100 paid players for each year for each position from 2011 through 2018, broken down by player position. For those unfamiliar with American football, the positions in the data set are Quarterback, Running Back, Wide Receiver, Tight End, and Offensive Lineman for offense, Cornerback, Defensive Lineman, Linebacker, and Safety for Defense, and a separate category for Special Teams players that includes punters and kickers. You can review a summary of player positions  here.\nWe are interested in how salaries compare for the top 100 players in each position and on how salaries have changed through time for each position.\nRead in the data set with\n\nnfl_df &lt;- read_csv(here::here(\"data/nfl_salary.csv\"))\n\nYour Turn 6. Use the head() functions to look at the data, and then, with your group, explain why this data set is not in tidy form.\nYour Turn 7. Use a function in tidyr to make the data tidy, and give your tidy data set a new name.\n\n\n\n\n\n\nCuation\n\n\n\nThe following exercises are a bit more challenging. Even though they are “Your Turn” exercises, we will discuss them a bit as a class as well.\n\n\nYour Turn 8. To your data set in the previous exercise, add a ranking variable that ranks the salaries within each player position so that the highest paid players in each year-position combination all receive a 1, the second highest paid players receive a 2, etc. Compare your results for the default way that R uses to break ties between two salaries that are the same and using ties.method = \"first\".\nYour Turn 9. Find the maximum salary for each player position in each year. Then, create two different line graphs that shows how the maximum salary has changed from 2011 to 2018 for each position. For one line graph, make the colours of the lines different for each position. For the second line graph, facet by position. Which graph do you like better?\nYour Turn 10. The maximum salary is very dependent on one specific player. Make the same graph, but plot the average salary of the top 20 players in each position of each year. What do you notice? Any interesting patterns for any of the positions? If you’re a fan of football, provide a guess as to why one of the positions has had their salary plateau in recent years.\nYour Turn 11. Sometimes for graphs involving cost or salary, we want to take into account the inflation rate. Google what the inflation rate was between 2011 and 2018 (Google something like “inflation rate from 2011 to 2018” and you should be able to find something). Adjust all of the 2011 salaries for inflation so that they are comparable to the 2018 salaries. Then, make a similar line plot as above but ignore all of the years between 2012 and 2017 (so your line plot will just have 2 points per position).\nAfter adjusting for inflation, how many positions have average higher salaries for the top 20 players in that position?\nYour Turn 12. Construct a graph that shows how much salary decreases moving from higher ranked players to lower ranked players for each position in the year 2018. Why do you think the depreciation is so large for Quarterbacks?"
  },
  {
    "objectID": "06-basics.html#variable-classes-in-r",
    "href": "06-basics.html#variable-classes-in-r",
    "title": "7  Coding in Base R",
    "section": "\n7.1 Variable Classes in R\n",
    "text": "7.1 Variable Classes in R\n\nR has a few different classes that variables take, including numeric, factor, character Date, and logical. Before we delve into the specifics of what these classes mean, let’s try to make some plots to illustrate why we should care about what these classes mean.\nThe videogame_clean.csv file contains variables on video games from 2004 - 2019, including\n\n\ngame, the name of the game\n\nrelease_date, the release date of the game\n\nrelease_date2, a second coding of release date\n\nprice, the price in dollars,\n\nowners, the number of owners (given in a range)\n\nmedian_playtime, the median playtime of the game\n\nmetascore, the score from the website Metacritic\n\nprice_cat, 1 for Low (less than 10.00 dollars), 2 for Moderate (between 10 and 29.99 dollars), and 3 for High (30.00 or more dollars)\n\nmeta_cat, Metacritic’s review system, with the following categories: “Overwhelming Dislike”, “Generally Unfavorable”, “Mixed Reviews”, “Generally Favorable”, “Universal Acclaim”.\n\nplaytime_miss, whether median play time is missing (TRUE) or not (FALSE)\n\nThe data set was modified from https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-07-30.\nRun the code in the following R chunk to read in the data.\n\nlibrary(tidyverse)\nlibrary(here)\nvideogame_df &lt;- read_csv(here(\"data/videogame_clean.csv\"))\nhead(videogame_df)\n#&gt; # A tibble: 6 × 15\n#&gt;   game      release_date release_date2 price owners median_playtime metascore\n#&gt;   &lt;chr&gt;     &lt;chr&gt;        &lt;date&gt;        &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 Half-Lif… Nov 16, 2004 2004-11-16     9.99 10,00…              66        96\n#&gt; 2 Counter-… Nov 1, 2004  2004-11-01     9.99 10,00…             128        88\n#&gt; 3 Counter-… Mar 1, 2004  2004-03-01     9.99 10,00…               3        65\n#&gt; 4 Half-Lif… Nov 1, 2004  2004-11-01     4.99 5,000…               0        NA\n#&gt; 5 Half-Lif… Jun 1, 2004  2004-06-01     9.99 2,000…               0        NA\n#&gt; 6 CS2D      Dec 24, 2004 2004-12-24    NA    1,000…              10        NA\n#&gt; # ℹ 8 more variables: price_cat &lt;dbl&gt;, meta_cat &lt;chr&gt;, playtime_miss &lt;lgl&gt;,\n#&gt; #   number &lt;dbl&gt;, developer &lt;chr&gt;, publisher &lt;chr&gt;, average_playtime &lt;dbl&gt;,\n#&gt; #   meta_cat_factor &lt;chr&gt;\n\nA data frame or tibble holds variables that are allowed to be different classes. If a variable is a different class than you would expect, you’ll get some strange errors or results when trying to wrangle the data or make graphics.\nRun the following lines of code. In some cases, we are only using the first 100 observations in videogame_small. Otherwise, the code would take a very long time to run.\n\nvideogame_small &lt;- videogame_df |&gt; slice(1:100)\nggplot(data = videogame_small, aes(x = release_date, y = price)) +\n  geom_point() \n\n\n\n\nggplot(data = videogame_small, aes(x = release_date2, y = metascore)) +\n  geom_point(aes(colour = price_cat))\n\n\n\n\nIn the first plot, release_date isn’t ordered according to how you would expect (by date). Instead, R orders it alphabetically.\nIn the second plot, we would expect to get a plot with 3 different colours, one for each level of price_cat. Instead, we get a continuous colour scale, which doesn’t make sense, given that price_cat can only be 1, 2, or 3.\nBoth plots are not rendered correctly because the variable classes are not correct in the underlying data set. Up until this point, the data that has been provided has almost always had the correct variable classes, by default, but that won’t always be the case!\n\n\n\n\n\n\nNote\n\n\n\nWe’ve actually seen both of these issues before as well (the Date issue in the exercise data and the continuous colour scale in the cars data), but, in both of these instances, code was provided to “fix” the problem. After this section, you’ll have the tools to fix many class issues on your own!\n\n\nIf we examine the output of the following line of code\n\nhead(videogame_df)\n#&gt; # A tibble: 6 × 15\n#&gt;   game      release_date release_date2 price owners median_playtime metascore\n#&gt;   &lt;chr&gt;     &lt;chr&gt;        &lt;date&gt;        &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 Half-Lif… Nov 16, 2004 2004-11-16     9.99 10,00…              66        96\n#&gt; 2 Counter-… Nov 1, 2004  2004-11-01     9.99 10,00…             128        88\n#&gt; 3 Counter-… Mar 1, 2004  2004-03-01     9.99 10,00…               3        65\n#&gt; 4 Half-Lif… Nov 1, 2004  2004-11-01     4.99 5,000…               0        NA\n#&gt; 5 Half-Lif… Jun 1, 2004  2004-06-01     9.99 2,000…               0        NA\n#&gt; 6 CS2D      Dec 24, 2004 2004-12-24    NA    1,000…              10        NA\n#&gt; # ℹ 8 more variables: price_cat &lt;dbl&gt;, meta_cat &lt;chr&gt;, playtime_miss &lt;lgl&gt;,\n#&gt; #   number &lt;dbl&gt;, developer &lt;chr&gt;, publisher &lt;chr&gt;, average_playtime &lt;dbl&gt;,\n#&gt; #   meta_cat_factor &lt;chr&gt;\n\nwe’ll see that, at the very top of the output, right below the variable names, R provides you with the classes of variables in the tibble.\n\n\n&lt;chr&gt; is character, used for strings or text.\n\n&lt;fct&gt; is used for variables that are factors, typically used for character variables with a finite number of possible values the variable can take on.\n\n&lt;date&gt; is used for dates.\n\n&lt;dbl&gt; stands for double and is used for the numeric class.\n\n&lt;int&gt; is for numbers that are all integers. In practice, there is not much difference between this class and class dbl.\n\n&lt;lgl&gt; is for logical, variables that are either TRUE or FALSE.\n\n\n7.1.1 Referencing Variables and Using str()\n\nWe can use name_of_dataset$name_of_variable to look at a specific variable in a data set:\n\nvideogame_df$game\n\nprints the first thousand entries of the variable game. There are a few ways to get the class of this variable: the way that we will use most often is with str(), which stands for “structure”, and gives the class of the variable, the number of observations (26688), as well as the first couple of observations:\n\nstr(videogame_df$game)\n#&gt;  chr [1:26688] \"Half-Life 2\" \"Counter-Strike: Source\" ...\n\nWe can also get a variable’s class more directly with class()\n\nclass(videogame_df$game)\n#&gt; [1] \"character\""
  },
  {
    "objectID": "06-basics.html#classes-in-detail",
    "href": "06-basics.html#classes-in-detail",
    "title": "7  Coding in Base R",
    "section": "\n7.2 Classes in Detail",
    "text": "7.2 Classes in Detail\nThe following gives summary information about each class of variables in R:\n\n7.2.1 &lt;chr&gt; and &lt;fct&gt; Class\nWith the character class, R will give you a warning and/or a missing value if you try to perform any numerical operations:\n\nmean(videogame_df$game)\n#&gt; [1] NA\nvideogame_df |&gt; summarise(maxgame = max(game))\n#&gt; # A tibble: 1 × 1\n#&gt;   maxgame\n#&gt;   &lt;chr&gt;  \n#&gt; 1 &lt;NA&gt;\n\nWe also can’t convert a character class to numeric. We can, however, convert a character class to a &lt;fct&gt; class, using as.factor(). The &lt;fct&gt; class will be useful when we discuss the forcats package, but isn’t particularly useful now.\n\nclass(videogame_df$meta_cat)\n#&gt; [1] \"character\"\nclass(as.factor(videogame_df$meta_cat))\n#&gt; [1] \"factor\"\n\nIn general, as._____ will lets you convert between classes. Note, however, that we aren’t saving our converted variable anywhere. If we wanted the conversion to the factor to be saved in the data set, we can use mutate():\n\nvideogame_df &lt;- videogame_df |&gt;\n  mutate(meta_cat_factor = as.factor(meta_cat))\nstr(videogame_df$meta_cat_factor)\n\nFor most R functions, it won’t matter whether our variable is in class character or class factor. In general, though, character classes are for variables that have a ton of different levels, like the name of the videogame, whereas factors are reserved for categorical variables with a smaller finite number of levels.\n\n7.2.2 &lt;date&gt; Class\nThe &lt;date&gt; class is used for dates, and the &lt;datetime&gt; class is used for Dates with times. R requires a very specific format for dates and times. Note that, while to the human eye, both of the following variables contain dates, only one is of class &lt;date&gt;:\n\nstr(videogame_df$release_date)\nstr(videogame_df$release_date2)\n\nrelease_date is class character, which is why we had the issue with the odd ordering of the dates earlier. We can try converting it using as.Date, but this function doesn’t always work:\n\nas.Date(videogame_df$release_date)\n#&gt; Error in charToDate(x): character string is not in a standard unambiguous format\n\nDates and times can be pretty complicated. In fact, we will spend an entire week covering them using the lubridate package.\nOn variables that are in Date format, like release_date2, we can use numerical operations:\n\nmedian(videogame_df$release_date2, na.rm = TRUE)\nmean(videogame_df$release_date2, na.rm = TRUE)\n\nWhat do you think taking the median or taking the mean of a date class means?\n\n7.2.3 &lt;dbl&gt; and &lt;int&gt; Class\nClass &lt;dbl&gt; and &lt;int&gt; are probably the most self-explanatory classes. &lt;dbl&gt;, the numeric class, are just variables that have only numbers in them while &lt;int&gt; only have integers (…, -2, -1, 0, 1, 2, ….). We can do numerical operations on either of these classes (and we’ve been doing them throughout the semester). For our purposes, these two classes are interchangeable.\n\nstr(videogame_df$price)\n\nProblems arise when numeric variables are coded as something non-numeric, or when non-numeric variables are coded as numeric. For example, examine:\n\nstr(videogame_df$price_cat)\n#&gt;  num [1:26688] 1 1 1 1 1 NA 2 1 1 1 ...\n\nprice_cat is categorical but is coded as 1 for cheap games, 2 for moderately priced games, and 3 for expensive games. Therefore, R thinks that the variable is numeric, when, it’s actually a factor.\n\nstr(as.factor(videogame_df$price_cat))\n#&gt;  Factor w/ 3 levels \"1\",\"2\",\"3\": 1 1 1 1 1 NA 2 1 1 1 ...\n\nThis is the cause of the odd colour scale that we encountered earlier and can be fixed by converting price_cat to a factor:\n\nvideogame_df &lt;- videogame_df |&gt;\n  mutate(price_factor = as.factor(price_cat)) \nggplot(data = videogame_df, aes(x = release_date2, y = metascore)) +\n  geom_point(aes(colour = price_factor))\n\n\n\n\n\n7.2.4 &lt;lgl&gt; Class\nFinally, there is a class of variables called logical. These variables can only take 2 values: TRUE or FALSE. For example, playtime_miss, a variable for whether or not the median_playtime variable is missing or not, is logical:\n\nstr(videogame_df$playtime_miss)\n#&gt;  logi [1:26688] FALSE FALSE FALSE TRUE TRUE FALSE ...\n\nIt’s a little strange at first, but R can perform numeric operations on logical classes. All R does is treat every TRUE as a 1 and every FALSE as a 0. Therefore, sum() gives the total number of TRUEs and mean() gives the proportion of TRUEs. So, we can find the number and proportion of games that are missing their median_playtime as:\n\nsum(videogame_df$playtime_miss)\n#&gt; [1] 25837\nmean(videogame_df$playtime_miss)\n#&gt; [1] 0.968113\n\nThere’s a lot of games that are missing this information!\nWe’ve actually used the ideas of logical variables for quite some time now, particularly in statements involving if_else(), case_when(), filter(), and mutate().\nThe primary purpose of this section is to be able to identify variable classes and be able to convert between the different variable types with mutate() to “fix” variables with the incorrect class.\nWe will use the fitness data set again for this set of exercises, as the data set has some of the issues with variable class that we have discussed. However, in week 1, some of the work of the work to fix those issues was already done before you saw the data. Now, you’ll get to fix a couple of those issues! Read in the data set with:\n\nlibrary(tidyverse)\nfitness_df &lt;- read_csv(here(\"data/higham_fitness_notclean.csv\"))\n\nExercise 1. What is the issue with the following plot? After you figure out the issue, use mutate() and as.factor() to create a new variable that fixes the issue and then reconstruct the graph.\n\nggplot(data = fitness_df, aes(x = active_cals)) +\n  geom_freqpoly(aes(group = weekday, colour = weekday)) +\n  theme_minimal()\n\n\n\n\nExercise 2. Currently stepgoal (whether or not 10000 steps were cleared that day) is &lt;dbl&gt;. What is another class that stepgoal could be?\nExercise 3. Convert stepgoal to the class that is the answer to the previous exercise. Using this new variable, calculate the total number of days where the goal was met and the proportion of the days where the goal was met."
  },
  {
    "objectID": "06-basics.html#object-types-and-subsetting",
    "href": "06-basics.html#object-types-and-subsetting",
    "title": "7  Coding in Base R",
    "section": "\n7.3 Object Types and Subsetting",
    "text": "7.3 Object Types and Subsetting\nVariables of these different classes can be stored in a variety of different objects in R. We have almost exclusively used the tibble object type. The tidy tibble\n\nis “rectangular” and has a specific number of rows and columns.\nhas columns that are variables\neach column must have elements that are of the same class, but different columns can be of different classes. This allows us to have character and numeric variables in the same tibble.\n\n\n7.3.1 tibble and data.frame\n\nThe tibble object is very similar to the data.frame object. We can also check what type of object you’re working with using the str() command:\n\nstr(videogame_df) ## look at the beginning to see \"tibble\"\n\n\n\n\n\n\n\nNote\n\n\n\nWe mostly use tibble and data frame interchangeably: the main difference between the two is that a tibble prints in a more reader friendly way. Nearly all of the data sets we have worked with over the semester have been tibbles.\n\n\nWe will have a small section on tibbles in the coming weeks so we won’t focus on them here. But, we should take note that, to reference a specific element in a tibble, called indexing, we can use [# , #]. So, for example, videogame_df[5, 3] grabs the value in the fifth row and the third column:\n\nvideogame_df[5, 3]\n#&gt; # A tibble: 1 × 1\n#&gt;   release_date2\n#&gt;   &lt;date&gt;       \n#&gt; 1 2004-06-01\n\nMore often, we’d want to grab an entire row (or range of rows) or an entire column. We can do this by leaving the row number blank (to grab the entire column) or by leaving the column number blank (to grab the entire row):\n\nvideogame_df[ ,3] ## grab the third column\n\nvideogame_df[5, ] ## grab the fifth row\n\nWe can also grab a range of columns or rows using the : operator:\n\nvideogame_df[ ,3:7] ## grab columns 3 through 7\n\nvideogame_df[3:7, ] ## grab rows 3 through 7\n\nor we can grab different columns or rows using c():\n\nvideogame_df[ ,c(1, 3, 4)] ## grab columns 1, 3, and 4\n\nvideogame_df[c(1, 3, 4), ] ## grab rows 1, 3, and 4\n\nTo get rid of an entire row or column, use -: videogame_df[ ,-c(1, 2)] drops the first and second columns while videogame_df[-c(1, 2), ] drops the first and second rows.\n\n7.3.2 Vectors\nA vector is an object that holds “things”, or elements, of the same class. You can create a vector in R using the c() function, which stands for “concatenate”. We’ve used the c() function before to bind things together; we just hadn’t yet discussed it in the context of creating a vector.\n\nvec1 &lt;- c(1, 3, 2)\nvec2 &lt;- c(\"b\", 1, 2)\nvec3 &lt;- c(FALSE, FALSE, TRUE)\nstr(vec1); str(vec2); str(vec3)\n#&gt;  num [1:3] 1 3 2\n#&gt;  chr [1:3] \"b\" \"1\" \"2\"\n#&gt;  logi [1:3] FALSE FALSE TRUE\n\nNotice that vec2 is a character class. R requires all elements in a vector to be of one class; since R knows b can’t be numeric, it makes all of the numbers characters as well.\nUsing dataset$variable draws out a vector from a tibble or data.frame:\n\nvideogame_df$metascore\n\nIf we wanted to make the above vector “by hand”, we’d need to have a lot of patience: c(96, 88, 65, NA, NA, NA, 93, .........)\nJust like tibbles, we can save vectors as something for later use:\n\nmetavec &lt;- videogame_df$metascore\nmean(metavec, na.rm = TRUE)\n#&gt; [1] 71.89544\n\nVectors are one-dimensional: if we want to grab the 100th element of a vector we just use name_of_vector[100]:\n\nmetavec[100] ## 100th element is missing\n#&gt; [1] NA\n\n\n7.3.3 Lists\nLists are one of the more flexible objects in R: you can put objects of different classes in the same list and lists aren’t required to be rectangular (like tibbles are). Lists are extremely useful because of this flexibility, but, we won’t use them much in this class. Therefore, we will just see an example of a list before moving on:\n\ntestlist &lt;- list(\"a\", 4, c(1, 4, 2, 6),\n                 tibble(x = c(1, 2), y = c(3, 2)))\ntestlist\n#&gt; [[1]]\n#&gt; [1] \"a\"\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 4\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 1 4 2 6\n#&gt; \n#&gt; [[4]]\n#&gt; # A tibble: 2 × 2\n#&gt;       x     y\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1     3\n#&gt; 2     2     2\n\ntestlist has four elements: a single character \"a\", a single number 4, a vector of 1, 4, 2, 6, and a tibble with a couple of variables. Lists can therefore be used to store complex information that wouldn’t be as easily stored in a vector or tibble.\nExercise 4. Suppose you subset a data frame df with df[1:5, 2:4]. What are the two dplyr functions that you can use to perform this same operation (subsetting to get the first five rows and the second through fourth columns).\nExercise 5. From videogame_df, create a tibble called first100 that only has the first 100 days in the data set using both (1) indexing with [ , ] and (2) a dplyr function.\nExercise 6. From videogame_df, create a tibble that doesn’t have the owners variable using both (1) indexing with [ , ] and (2) a dplyr function."
  },
  {
    "objectID": "06-basics.html#other-useful-base-r-functions",
    "href": "06-basics.html#other-useful-base-r-functions",
    "title": "7  Coding in Base R",
    "section": "\n7.4 Other Useful Base R Functions",
    "text": "7.4 Other Useful Base R Functions\nIn addition to the base R functions we have discussed so far, there are many other useful base R functions. The following give some of the functions that I think are most useful for data science.\nGenerating Data: rnorm(), sample(), and set.seed()\nrnorm() can be used to generate a certain number of normal random variables with a given mean and standard deviation. It has three arguments: the sample size, the mean, and the standard deviation.\nsample() can be used to obtain a sample from a vector, either with or without replacement: it has two required arguments: the vector that we want to sample from and size, the size of the sample.\nset.seed() can be used to fix R’s random seed. This can be set so that, for example, each person in our class can get the same random sample as long we all set the same seed.\nThese can be combined to quickly generate toy data. For example, below we are generating two quantitative variables (that are normally distributed) and two categorical variables:\n\nset.seed(15125141)\ntoy_df &lt;- tibble(xvar = rnorm(100, 3, 4),\n                 yvar = rnorm(100, -5, 10),\n                 group1 = sample(c(\"A\", \"B\", \"C\"), size = 100, replace = TRUE),\n                 group2 = sample(c(\"Place1\", \"Place2\", \"Place3\"), size = 100,\n                                 replace = TRUE))\ntoy_df\n#&gt; # A tibble: 100 × 4\n#&gt;     xvar   yvar group1 group2\n#&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; \n#&gt; 1  0.516 -13.5  B      Place2\n#&gt; 2 -0.891 -13.3  A      Place2\n#&gt; 3  5.58  -14.3  B      Place2\n#&gt; 4  2.42   -4.91 C      Place1\n#&gt; 5  1.43   -5.86 B      Place2\n#&gt; 6  6.61   12.7  B      Place2\n#&gt; # ℹ 94 more rows\n\nTables: We can use the table() function with the $ operator to quickly generate tables of categorical variables:\n\ntable(toy_df$group1)\n#&gt; \n#&gt;  A  B  C \n#&gt; 27 39 34\n\ntable(toy_df$group1, toy_df$group2)\n#&gt;    \n#&gt;     Place1 Place2 Place3\n#&gt;   A     10      8      9\n#&gt;   B      9     20     10\n#&gt;   C     10     10     14\n\nOthers: There are quite a few other useful base R functions. nrow() can be used on a data frame to quickly look at the number of rows of the data frame and summary() can be used to get a quick summary of a vector:\n\nnrow(toy_df)\n#&gt; [1] 100\nsummary(toy_df$yvar)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt; -30.123 -12.938  -5.380  -6.630  -1.298  13.858\n\nThere are also some useful functions for viewing a data frame. View() function can be used in your console window on a data frame: View(toy_df) to pull up a spreadsheet-like view of the data set in a different window within R Studio.\nOptions to print() allow us to view more rows or more columns in the console printout:\n\ntoy_df |&gt;\n  print(n = 60) ## print out 60 rows\n#&gt; # A tibble: 100 × 4\n#&gt;      xvar    yvar group1 group2\n#&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; \n#&gt;  1  0.516 -13.5   B      Place2\n#&gt;  2 -0.891 -13.3   A      Place2\n#&gt;  3  5.58  -14.3   B      Place2\n#&gt;  4  2.42   -4.91  C      Place1\n#&gt;  5  1.43   -5.86  B      Place2\n#&gt;  6  6.61   12.7   B      Place2\n#&gt;  7 -2.04   -9.28  A      Place1\n#&gt;  8  7.56    1.89  A      Place3\n#&gt;  9 -0.425 -30.1   C      Place1\n#&gt; 10  4.14    2.65  C      Place2\n#&gt; 11  5.03   -8.82  C      Place1\n#&gt; 12  2.98  -22.7   C      Place1\n#&gt; 13  5.97   -2.67  B      Place3\n#&gt; 14  0.882   1.59  A      Place1\n#&gt; 15  2.14   -5.63  B      Place3\n#&gt; 16  7.74    5.79  C      Place3\n#&gt; 17  5.20   -3.17  B      Place2\n#&gt; 18  2.89   -0.697 A      Place1\n#&gt; 19  2.71    1.09  B      Place2\n#&gt; 20  5.87   -4.87  C      Place1\n#&gt; 21  5.65  -10.3   B      Place2\n#&gt; 22 -0.520  -4.77  B      Place3\n#&gt; 23 -0.130 -18.3   B      Place3\n#&gt; 24 -0.174 -18.9   A      Place3\n#&gt; 25  4.33    4.63  A      Place3\n#&gt; 26  0.462 -12.8   A      Place3\n#&gt; 27  5.53   -3.36  C      Place1\n#&gt; 28  1.66   -5.34  A      Place1\n#&gt; 29 -0.469 -13.2   C      Place2\n#&gt; 30  7.51  -13.4   B      Place1\n#&gt; 31 -1.82   -6.47  C      Place3\n#&gt; 32 -2.44   -2.17  C      Place3\n#&gt; 33  1.52  -12.6   C      Place2\n#&gt; 34  4.60   -6.69  A      Place2\n#&gt; 35  3.10  -25.5   A      Place2\n#&gt; 36 -0.682 -20.4   A      Place1\n#&gt; 37 -5.72    2.65  B      Place3\n#&gt; 38  0.976 -12.1   B      Place3\n#&gt; 39  1.39    2.78  B      Place2\n#&gt; 40  6.67  -14.6   A      Place1\n#&gt; 41  3.09  -10.4   B      Place2\n#&gt; 42 -1.98  -12.8   A      Place3\n#&gt; 43  0.225  13.9   C      Place1\n#&gt; 44  5.71   -3.50  A      Place3\n#&gt; 45  5.57   -2.02  B      Place3\n#&gt; 46  8.96   -1.86  B      Place2\n#&gt; 47  3.80  -11.3   C      Place1\n#&gt; 48  7.40   -1.32  C      Place2\n#&gt; 49  0.988   4.04  B      Place2\n#&gt; 50  1.93   -5.24  A      Place2\n#&gt; 51  5.23   13.2   C      Place2\n#&gt; 52 -2.13  -19.6   A      Place2\n#&gt; 53  6.05   -4.42  A      Place3\n#&gt; 54  0.865  -9.47  A      Place1\n#&gt; 55  4.16  -16.4   B      Place1\n#&gt; 56 -2.73   -4.09  B      Place2\n#&gt; 57 -0.532   7.70  C      Place3\n#&gt; 58 -2.96  -11.6   C      Place3\n#&gt; 59  4.34   -5.99  B      Place2\n#&gt; 60  6.72  -13.6   B      Place2\n#&gt; # ℹ 40 more rows\ntoy_df |&gt;\n  print(width = Inf) ## print out all of the columns\n#&gt; # A tibble: 100 × 4\n#&gt;     xvar   yvar group1 group2\n#&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; \n#&gt; 1  0.516 -13.5  B      Place2\n#&gt; 2 -0.891 -13.3  A      Place2\n#&gt; 3  5.58  -14.3  B      Place2\n#&gt; 4  2.42   -4.91 C      Place1\n#&gt; 5  1.43   -5.86 B      Place2\n#&gt; 6  6.61   12.7  B      Place2\n#&gt; # ℹ 94 more rows\n\nWe will stop here, but will surely encounter more base R functions as we run into different types of problems.\nExercise 7. Use the summary() function on the metascore variable in videogame_df."
  },
  {
    "objectID": "06-basics.html#practice",
    "href": "06-basics.html#practice",
    "title": "7  Coding in Base R",
    "section": "\n7.5 Practice",
    "text": "7.5 Practice\n\n7.5.1 Class Exercises\nClass Exercise 1. Use the following steps to create a new variable weekend_ind, which will be “weekend” if the day of the week is Saturday or Sunday and “weekday” if the day of the week is any other day. The current weekday variable is coded so that 1 represents Sunday, 2 represents Monday, …., and 7 represents Saturday.\n\nCreate a vector that has the numbers corresponding to the two weekend days. Name the vector and then create a second vector that has the numbers corresponding to the five weekday days.\nUse dplyr functions and the %in% operator to create the new weekend_ind variable.\n\nClass Exercise 2. Use dplyr and tidyr functions to re-create the tables generated from\n\ntable(toy_df$group1)\n#&gt; \n#&gt;  A  B  C \n#&gt; 27 39 34\n\ntable(toy_df$group1, toy_df$group2)\n#&gt;    \n#&gt;     Place1 Place2 Place3\n#&gt;   A     10      8      9\n#&gt;   B      9     20     10\n#&gt;   C     10     10     14\n\n\n7.5.2 Your Turn\nYour Turn 1. Read in the data set and use filter() to remove any rows with missing metascores, missing median playtime, or have a median playtime of 0 hours.\n\n\n\n\n\n\nNote\n\n\n\nWe usually don’t want to remove missing values without a valid reason. In this case, a missing metascore means that the game wasn’t “major” enough to get enough critic reviews, and a missing or 0 hour median playtime means that there weren’t enough users who uploaded their playtime to the database. Therefore, any further analyses are constructed to games that are popular enough to both get enough reviews on metacritic and have enough users upload their median playtimes.\n\n\n\nvideogame_df &lt;- read_csv(here(\"data/videogame_clean.csv\"))\n\nYour Turn 2. Make a scatterplot with median_playtime on the y-axis and metascore (the aggregate review score) on the x-axis with the filtered data set.\nYour Turn 3. Something you may notice is that many of the points directly overlap one another. This is common when at least one of the variables on a scatterplot is discrete: metascore can only take on integer values in this case. Change geom_point() in your previous plot to geom_jitter(). Then, use the help to write a sentence about what geom_jitter() does.\nYour Turn 4. Another option is to control point transparency with alpha. In your geom_jitter() statement, change alpha so that you can still see all of the points, but so that you can tell in the plot where a lot of points are overlapping.\nYour Turn 5. Label the points that have median playtimes above 1500 hours. You may want to use the ggrepel package so that the labels don’t overlap.\nYour Turn 6. With a partner, have each of you choose a game that was labeled with a very median long play time and google search that game to see what its median play time is, according to google. Is it in the vicinity as the median_playtime recorded in our data set?\nYour Turn 7. What should be done about the outliers? With your group, discuss advantages and disadvantages of keeping the outliers on the plot."
  },
  {
    "objectID": "07-forcats.html#change-factor-levels",
    "href": "07-forcats.html#change-factor-levels",
    "title": "8  Factors with forcats",
    "section": "\n8.1 Change Factor Levels",
    "text": "8.1 Change Factor Levels\nThe Data: The pokemon_allgen.csv data set contains observations on Pokemon from the first 6 Generations (the first 6 games). There are 20 variable in this data set, but, of particular interest for this chapter are\n\n\nType 1, the first Type characteristic of the Pokemon (a factor with 13 levels)\n\nType 2, the second Type characteristic of the Pokemon (a factor with 13 levels, NA if the Pokemon only has one type)\n\nGeneration, the generation the Pokemon first appeared in (a factor with 6 levels)\n\nRead in the data set with read_csv(). Then, use a mutate() statement to make a Generation_cat variable that is a factor.\n\nlibrary(tidyverse)\nlibrary(here)\npokemon_df &lt;- read_csv(here(\"data/pokemon_allgen.csv\")) |&gt;\n  mutate(Generation_cat = factor(Generation))\n\nOne easy way to get a quick summary of a factor variable is to use group_by() and n() within a summarise() statement:\n\npokemon_df |&gt; group_by(`Type 1`) |&gt;\n  summarise(counttype = n())\n#&gt; # A tibble: 18 × 2\n#&gt;   `Type 1` counttype\n#&gt;   &lt;chr&gt;        &lt;int&gt;\n#&gt; 1 Bug             75\n#&gt; 2 Dark            31\n#&gt; 3 Dragon          41\n#&gt; 4 Electric        90\n#&gt; 5 Fairy           18\n#&gt; 6 Fighting        27\n#&gt; # ℹ 12 more rows\n\n\n\n8.1.1 fct_recode() to Rename Levels\nNow, let’s make a bar plot that examines how many Legendary Pokemon first appear in each generation, using dplyr commands that we’ve used and a simple geom_col() GEOM:\n\npokemon_legend &lt;- pokemon_df |&gt; filter(Legendary == TRUE) |&gt;\n  group_by(Generation_cat) |&gt;\n  summarise(nlegend = n())\nggplot(data = pokemon_legend, aes(x = Generation_cat, y = nlegend)) +\n  geom_col() +\n  theme_minimal()\n\n\n\n\nWe’ve discussed how to change many aspects of ggplot2 graphs, but we haven’t discussed how to rename the labels of levels of a categorical variable, whether those appear in the x-axis or in a separate legend. The easiest way to do this is to rename the levels in the factor itself using fct_recode(). Suppose, for example, that we want to relabel the Generation number with the actual region corresponding to each game (Kanto, Johto, Hoenn, Sinnoh, Unova, and Kalos). The function fct_recode() takes the name of a factor already present in the data set as its first argument and then a series of renaming schemes (new_name = “old_name”) as its remaining arguments.\n\npokemon_legend &lt;- pokemon_legend |&gt;\n  mutate(Generation_cat2 = fct_recode(Generation_cat,\n                                      Kanto = \"1\", Johto = \"2\",\n                                      Hoenn = \"3\", Sinnoh = \"4\",\n                                      Unova = \"5\", Kalos = \"6\")) |&gt;\n  select(Generation_cat2, everything())\nhead(pokemon_legend)\n#&gt; # A tibble: 6 × 3\n#&gt;   Generation_cat2 Generation_cat nlegend\n#&gt;   &lt;fct&gt;           &lt;fct&gt;            &lt;int&gt;\n#&gt; 1 Kanto           1                    6\n#&gt; 2 Johto           2                    5\n#&gt; 3 Hoenn           3                   34\n#&gt; 4 Sinnoh          4                   17\n#&gt; 5 Unova           5                   27\n#&gt; 6 Kalos           6                   13\nggplot(data = pokemon_legend,\n       aes(x = Generation_cat2, y = nlegend)) +\n  geom_col() +\n  theme_minimal()\n\n\n\n\n\n8.1.2 Collapsing Many Levels Into Fewer Levels with fct_collapse()\n\nSometimes, we might want to collapse the levels of two or more factors into a single level. With the Pokemon data set, there isn’t an example where this really makes sense, but, in the exercises, we’ll see a good use for this function with the social survey data set. For practice, we can collapse the Ice and Dark type Pokemon into a new level called Coolest and we can collapse the Poison, Fighting, and Fire type Pokemon into a new level called Least_Cool.\n\npokemon_long &lt;- pokemon_df |&gt; pivot_longer(c(`Type 1`, `Type 2`),\n                            names_to = \"Number\",\n                            values_to = \"Type\")\npokemon_long |&gt;\n  mutate(new_type = fct_collapse(Type, Coolest = c(\"Ice\", \"Dark\"),\n                                 Least_Cool = c(\"Fire\", \"Fighting\", \"Poison\"))) |&gt;\n  select(new_type, Type, everything())\n#&gt; # A tibble: 1,894 × 22\n#&gt;   new_type   Type    `#` Name  Total    HP Attack Defense `Sp. Atk` `Sp. Def`\n#&gt;   &lt;fct&gt;      &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 Grass      Grass     1 Bulb…   318    45     49      49        65        65\n#&gt; 2 Least_Cool Pois…     1 Bulb…   318    45     49      49        65        65\n#&gt; 3 Grass      Grass     2 Ivys…   405    60     62      63        80        80\n#&gt; 4 Least_Cool Pois…     2 Ivys…   405    60     62      63        80        80\n#&gt; 5 Grass      Grass     3 Venu…   525    80     82      83       100       100\n#&gt; 6 Least_Cool Pois…     3 Venu…   525    80     82      83       100       100\n#&gt; # ℹ 1,888 more rows\n#&gt; # ℹ 12 more variables: Speed &lt;dbl&gt;, Generation &lt;dbl&gt;, Legendary &lt;lgl&gt;,\n#&gt; #   id &lt;chr&gt;, identifier &lt;chr&gt;, height &lt;dbl&gt;, weight &lt;dbl&gt;,\n#&gt; #   base_experience &lt;dbl&gt;, order &lt;dbl&gt;, is_default &lt;dbl&gt;,\n#&gt; #   Generation_cat &lt;fct&gt;, Number &lt;chr&gt;\n\nWhat happens to the levels that aren’t being re-specified?\n\nExercise 1. What dplyr function(s) could we also use to create the new levels that were created with fct_collapse()? Why might it be a little easier to use fct_collapse()?\nExercise 2. Use fct_recode() with mutate() to recode “A” to “Apple”, and “B” to “Banana” in the following toy data set:\n\nfruit_df &lt;- tibble::tibble(fruit = c(\"A\", \"B\", \"B\", \"A\"))"
  },
  {
    "objectID": "07-forcats.html#reorder-factor-levels",
    "href": "07-forcats.html#reorder-factor-levels",
    "title": "8  Factors with forcats",
    "section": "\n8.2 Reorder Factor Levels",
    "text": "8.2 Reorder Factor Levels\n\n8.2.1 Change the Order of Levels by a Quantitative Variable with fct_reorder()\n\nWe might also be interested in re-ordering the x or y-axis of a particular graph so that the order of the factors correspond to, for example, the median of a quantitative variable for each level. The reason we would want to do this is easiest to see with an example. For example, suppose we want to look at the most common Pokemon types across the first 6 generations. We first get rid of duplicate Pokemon (we’ll discuss this in an exercise). Then, we pivot the data so that type is in one column, and we remove observations with missing Type, which correspond to the second Type of Pokemon that only have a single Type:\n\npokemon_nodup &lt;- pokemon_df |&gt; group_by(`#`) |&gt; slice(1) |&gt;\n  ungroup()\npokemon_long &lt;- pokemon_nodup |&gt;\n  pivot_longer(c(`Type 1`, `Type 2`),\n               names_to = \"Number\",\n               values_to = \"Type\")\npokemon_sum &lt;- pokemon_long |&gt;\n  group_by(Type) |&gt;\n  summarise(count_type = n()) |&gt;\n  filter(!is.na(Type))\nggplot(data = pokemon_sum, aes(x = Type,\n                               y = count_type)) +\n  geom_col() +\n  coord_flip() + ## flips the x and y axes\n  theme_minimal()\n\n\n\n\nHow does R order the levels of the Type factor, by default? How might you like them to be ordered to make the graph more readable?\nThe following code creates a new factor variable called Type_ordered that orders type by the count_type variable. fct_reorder() takes a factor as its first argument and a numeric variable to re-order that factor by as its second argument. The bar plot is then reconstructed with this new variable.\n\npokemon_sum &lt;- pokemon_sum |&gt; \n  mutate(Type_ordered = fct_reorder(.f = Type, .x = count_type))\nggplot(data = pokemon_sum, aes(x = Type_ordered,\n                               y = count_type)) +\n  geom_col() +\n  coord_flip() +\n  theme_minimal()\n\n\n\n\n\n8.2.2 Lollipop Plots\nLollipop plots are a popular alternative to bar plots because they often look cleaner with less ink. To make a lollipop plot in R, we specify two different geoms: geom_segment() to form the stick of the lollipop and geom_point() to form the pop part of the lollipop. geom_segment() requires 4 aesthetics: x, xend, y, and yend.\n\nggplot(data = pokemon_sum, aes(x = Type_ordered,\n                               y = count_type)) +\n  geom_segment(aes(x = Type_ordered, xend = Type_ordered,\n                   y = 0, yend = count_type)) +\n  geom_point() +\n  coord_flip() +\n  theme_minimal()\n\n\n\n\nfct_reorder() also works with boxplots or simple point plots that show, for example, the median response for each level of a factor. The following set of plots investigate how the Defense stat changes for different Pokemon types\n\npokemon_long &lt;- pokemon_long |&gt;\n  filter(!is.na(Type)) |&gt;\n  mutate(Type_Deford = fct_reorder(.f = Type, .x = Defense,\n                                   .fun = median))\nggplot(data = pokemon_long, aes(x = Type_Deford,\n                               y = Defense)) +\n  geom_boxplot() + \n  coord_flip() +\n  theme_minimal()\n\n\n\n\nThe following code makes a point plot that shows the median defense for each type instead of boxplots.\n\npokemon_med &lt;- pokemon_long |&gt; group_by(Type_Deford) |&gt;\n  summarise(med_def = median(Defense)) |&gt;\n  mutate(Type_Deford = fct_reorder(.f = Type_Deford, .x = med_def,\n                                   .fun = median))\n\nggplot(data = pokemon_med, aes(x = med_def, y = Type_Deford)) +\n  geom_point() +\n  theme_minimal()\n\n\n\n\nFinally, we can make a lollipop plot of median defense.\n\nggplot(data = pokemon_med, aes(x = Type_Deford, y = med_def)) +\n  geom_segment(aes(xend = Type_Deford, y = 0, yend = med_def)) +\n  geom_point() +\n  coord_flip() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe previous two plots (the point plot of median defense and the lollipop plot of median defense) are really not the best types of plots to explore defense across the different Pokemon types. We will discuss why as part of a class exercise.\n\n\nNew Data. The gun_violence_us.csv data set was obtained from https://www.openintro.org/book/statdata/index.php?data=gun_violence_us and contains the following variables on gun violence in 2014:\n\n\nstate, the name of the U.S. state\n\nmortality_rate, number of deaths from gun violence per 100,000 people\n\nownership_rate, the proportion of adults who own a gun\n\nregion, region of the U.S. (South, West, NE, and MW)\n\n\nmortality_df &lt;- read_csv(here(\"data/gun_violence_us.csv\")) |&gt;\n  mutate(region = factor(region))\n\n\n8.2.3 Reordering Levels Manually with fct_relevel()\n\nSuppose that we want to investigate the relationship between mortality_rate and ownership_rate using this data set. Run the following code to create a scatterplot of mortality_rate vs. ownership_rate with fitted linear regression lines for each region of the United States:\n\nggplot(data = mortality_df,\n       aes(x = ownership_rate, y = mortality_rate, colour = region)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  theme_minimal()\n\n\n\n\nNotice the order of the levels in the legend. Most people would prefer the order to actually match up with where the lines in the plot end, not for the order to be alphabetical. To achieve this, we can reorder the levels manually with fct_relevel().\nFactors are ordered alphabetically by default. If we want precise control over the order of the levels of a factor, we can use fct_relevel(), which takes a factor and a vector of the new levels as inputs:\n\nmortality_df &lt;- mortality_df |&gt;\n  mutate(region_3 = fct_relevel(region, c(\"South\", \"West\", \"MW\", \"NE\")))\nggplot(data = mortality_df,\n       aes(x = ownership_rate, y = mortality_rate, colour = region_3)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  theme_minimal()\n\n\n\n\nReordering the levels of a factor manually might also be useful in fitting linear models. Recall that, by default, R makes the reference group in a linear model the first level alphabetically. If we’d like a different reference group, you can reorder the levels of the factor:\n\nmod &lt;- lm(mortality_rate ~ ownership_rate + region, data = mortality_df)\nmod3 &lt;- lm(mortality_rate ~ ownership_rate + region_3, data = mortality_df)\nsummary(mod)\nsummary(mod3)\n\nIf you have not taken STAT 213, you can ignore this discussion about the reference group in linear models.\nExercise 3. The .fun argument in fct_reorder() controls how the Type factor is ordered. In the boxplots of Pokemon defense, change this argument when making pokemon_long so that the boxplots are ordered by the maximum Defense instead of the median Defense.\n\nggplot(data = pokemon_long, aes(x = Type_Deford,\n                               y = Defense)) +\n  geom_boxplot() + \n  coord_flip() +\n  theme_minimal()"
  },
  {
    "objectID": "07-forcats.html#practice",
    "href": "07-forcats.html#practice",
    "title": "8  Factors with forcats",
    "section": "\n8.3 Practice",
    "text": "8.3 Practice\n\n8.3.1 Class Exercises\nClass Exercise 1. In the text, it is mentioned that the point plot of median defense and the lollipop plot of median defense are not great visualizations. Why? For what type of data would these visualizations be more appropriate?\nClass Exercise 2. For the pokemon data set, we mentioned that we got rid of duplicates with group_by(#) |&gt; slice(1) |&gt; ungroup(). Explain why the code would get rid of duplicates. We will then discuss an alternative method to examine duplicates with the janitor package.\nClass Exercise 3. Make the side-by-side boxplots again with the pokemon data but do not use ungroup() by running the following code.\n\npokemon_nodup &lt;- pokemon_df |&gt; group_by(`#`) |&gt; slice(1) ## |&gt;\n  ## ungroup()\npokemon_long &lt;- pokemon_nodup |&gt;\n  pivot_longer(c(`Type 1`, `Type 2`),\n               names_to = \"Number\",\n               values_to = \"Type\")\n\npokemon_long &lt;- pokemon_long |&gt;\n  filter(!is.na(Type)) |&gt;\n  mutate(Type_Deford = fct_reorder(.f = Type, .x = Defense,\n                                   .fun = median))\nggplot(data = pokemon_long, aes(x = Type_Deford,\n                               y = Defense)) +\n  geom_boxplot() + \n  coord_flip()\n\n\n\n\nWhy are the boxplots no longer ordered by median defense?\n\n8.3.2 Your Turn\nFor these your turn exercises, we will use a data set on National Football League standings from 2000 to 2020. Read in the data set with:\n\nlibrary(tidyverse)\nlibrary(here)\nstandings_df &lt;- read_csv(here(\"data/standings.csv\"))\nstandings_df\n#&gt; # A tibble: 638 × 15\n#&gt;   team         team_name  year  wins  loss points_for points_against\n#&gt;   &lt;chr&gt;        &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;\n#&gt; 1 Miami        Dolphins   2000    11     5        323            226\n#&gt; 2 Indianapolis Colts      2000    10     6        429            326\n#&gt; 3 New York     Jets       2000     9     7        321            321\n#&gt; 4 Buffalo      Bills      2000     8     8        315            350\n#&gt; 5 New England  Patriots   2000     5    11        276            338\n#&gt; 6 Tennessee    Titans     2000    13     3        346            191\n#&gt; # ℹ 632 more rows\n#&gt; # ℹ 8 more variables: points_differential &lt;dbl&gt;, margin_of_victory &lt;dbl&gt;,\n#&gt; #   strength_of_schedule &lt;dbl&gt;, simple_rating &lt;dbl&gt;,\n#&gt; #   offensive_ranking &lt;dbl&gt;, defensive_ranking &lt;dbl&gt;, playoffs &lt;chr&gt;,\n#&gt; #   sb_winner &lt;chr&gt;\n\nThe important variables that we will use include:\n\n\nteam, the city where the team is based in\n\nteam_name, the name of the team\n\nplayoffs, whether or not the team made the playoffs that year\n\nsb_winner, whether or not the team won the superbowl that year\n\nYour Turn 1. Use the table() function with table(name_of_data_frame$name_of_variable) to make a table of team_name. This is useful to use for categorical variables to give a quick summary of what the levels are and how many times each level appears in the data set.\nYour Turn 2. Until a few years ago, the Washington Commanders team used to be known as the Washington Redskins. Because of the obvious racism the name conveys, in 2022, the name was changed from Redskins to Commanders. Use a forcats function to rename the Redskins team_name to Commanders. Note that, usually, we have been renaming the new variable after we use a forcats function, but, oftentimes, it makes sense to just overwrite the old variable by using the same name in our mutate() statement.\nYour Turn 3. Use a function from tidyr to combine team and team_name into a single variable called franchise. You may want to specify sep = \" \" for consistency with the city names.\nYour Turn 4. There are a couple of franchises in the national football league that moved cities in the late 2010s. In particular, the San Diego Chargers became the Los Angeles Chargers and the St. Louis Rams became the Los Angeles Rams (this is another instance where being familiar with context is helpful here: it may have taken you much longer to figure this out, had you not known much about the NFL). Use a forcats function to put the San Diego Chargers and Los Angeles Chargers into a single level, San Diego LA Chargers, and to put the St. Louis Rams and Los Angeles Rams into a single level, St. Louis LA Rams.\nYour Turn 5. Using the updated data set, create a lollipop plot of the ten franchises who have made the playoffs most often. You will need to do some work with dplyr before making the plot.\nYour Turn 6. Customize your lollipop plot by changing the way the points look at the end and / or the way the “stems” of the lollipops look. You may use https://r-graph-gallery.com/301-custom-lollipop-chart.html for inspiration.\nThe following are additional exercises for forcats: I’ve left them in here in case you want some extra practice! We will use the general social survey data set, which is in the forcats library in R. You should some of this Wikipedia page to better understand where this data comes from Wikipedia.\nMost variables are self-explanatory, but a couple that aren’t are:\n\n\npartyid, political leaning and\n\ndenom, religious denomination (if unfamiliar with this, you can think of it as a “more specific” subset of a particular religion).\n\nNote that some of these exercises are from the R for Data Science textbook.\nLoad in the data set with\n\nlibrary(tidyverse)\ngss_cat\n\nAdditional Exercise 1. Using a forcats function, change the name of the level Not str republican to be Weak republican and change the name of the level Not str democrat to be Weak democrat. These names more closely match the levels Strong republican and Strong democrat. Then, create a table of counts that shows the number of respondents in each political party partyid.\nNote: Levels that aren’t specified in your forcats function do not change.\nNote 2: In naming something Weak republican, you’ll need to use backticks since there is a space in the level name.\nAdditional Exercise 2. Use a forcats function so that partyid just has 4 categories: Other (corresponding to No answer, Don’t know, Other party), Ind (corresponding to Ind,near rep, Independent, Ind, near dem), Rep (corresponding to Strong republican and Not str republican), and Dem (corresponding to Not str democrat and Strong democrat).\nAdditional Exercise 3. Run the code to create the following plot that shows the average number of hours of television people watch from various religions.\n\nrelig_summary &lt;- gss_cat |&gt;\n  group_by(relig) |&gt;\n  summarise(\n    age = mean(age, na.rm = TRUE),\n    tvhours = mean(tvhours, na.rm = TRUE),\n    n = n()\n  )\n\nggplot(data = relig_summary, aes(tvhours, relig)) +\n  geom_point()\n\nThen, use a forcats function create a new variable in the data set that reorders the religion factor levels and make a lollipop plot so that the religion watches the most television, on average, is on the top, and the religion that watches the least television, on average, is on the bottom.\nAdditional Exercise 4. Run the code to make the following line plot that shows age on the x-axis, the proportion on the y-axis, and is coloured by various marital statuses (married, divorced, widowed, etc.):\n\nby_age &lt;- gss_cat |&gt;\n  filter(!is.na(age)) |&gt;\n  count(age, marital) |&gt;\n  group_by(age) |&gt;\n  mutate(prop = n / sum(n))\n\nggplot(by_age, aes(age, prop,\n                  colour = marital)) +\n  geom_line(na.rm = TRUE) +\n  labs(colour = \"marital\")\n\nThen, use a forcats function to make the plot so that the legend labels line up better with the different coloured marital status lines (e.g. so that the label for widowed is the first that appears in the legend, the label for married is second, etc.).\nAdditional Exercise 5. We haven’t talked much about creating two-way tables (or contingency tables). These are generally quite difficult to make with the tidyverse functions, but you can use the base R table() and prop.table() functions to make these.\nUsing data only from the year 2014, run the following code to make 4 two-way tables with the party_small variable that was constructed earlier and race:\n\ngss_cat &lt;- gss_cat |&gt; mutate(party_small = fct_collapse(partyid,\n                                              Other = c(\"No answer\", \"Don't know\", \"Other party\"),\n                                              Ind = c(\"Ind,near rep\", \"Independent\", \"Ind,near dem\"),\n                                              Rep = c(\"Strong republican\", \"Not str republican\"),\n                                              Dem = c(\"Not str democrat\", \"Strong democrat\")))\n\ngss_recent &lt;- gss_cat |&gt; filter(year == 2014)\n\ntab1 &lt;- table(gss_recent$party_small, gss_recent$race)\ntab1\n#&gt;        \n#&gt;         Other Black White Not applicable\n#&gt;   Other     8    12    68              0\n#&gt;   Rep      22    17   498              0\n#&gt;   Ind     152   108   828              0\n#&gt;   Dem      80   249   496              0\nprop.table(tab1)\n#&gt;        \n#&gt;               Other       Black       White Not applicable\n#&gt;   Other 0.003152088 0.004728132 0.026792750    0.000000000\n#&gt;   Rep   0.008668243 0.006698188 0.196217494    0.000000000\n#&gt;   Ind   0.059889677 0.042553191 0.326241135    0.000000000\n#&gt;   Dem   0.031520883 0.098108747 0.195429472    0.000000000\nprop.table(tab1, margin = 1)\n#&gt;        \n#&gt;              Other      Black      White Not applicable\n#&gt;   Other 0.09090909 0.13636364 0.77272727     0.00000000\n#&gt;   Rep   0.04096834 0.03165736 0.92737430     0.00000000\n#&gt;   Ind   0.13970588 0.09926471 0.76102941     0.00000000\n#&gt;   Dem   0.09696970 0.30181818 0.60121212     0.00000000\nprop.table(tab1, margin = 2)\n#&gt;        \n#&gt;              Other      Black      White Not applicable\n#&gt;   Other 0.03053435 0.03108808 0.03597884               \n#&gt;   Rep   0.08396947 0.04404145 0.26349206               \n#&gt;   Ind   0.58015267 0.27979275 0.43809524               \n#&gt;   Dem   0.30534351 0.64507772 0.26243386\n\nUse the help on ?prop.table to figure out how each of these three tables are constructed.\nWhich table do you think is most informative? What conclusions does it help you to draw?"
  },
  {
    "objectID": "09-ethics.html#ethical-examples",
    "href": "09-ethics.html#ethical-examples",
    "title": "9  Data Ethics",
    "section": "\n9.1 Ethical Examples",
    "text": "9.1 Ethical Examples\nWe’ve tried to interweave issues of ethics throughout many examples used already in this course, but the purpose of this section is to put data ethics in direct focus.\nSome questions to consider for any data collected, especially data collected on human subjects:\n\nwho gets to use data and for what purposes?\nwho collected the data and does that organization have any conflicts of interest?\nis presentation of an analysis harmful to a particular person or group of people? Are there benefits of an analysis?\n\nhave the subjects of a data collection procedure been treated respectfully and have they given consent to their information being collected?\n\nWhen is consent needed and when is it not? For example, we have looked at data on professional athletes. Do they need to provide consent or is consent inherent in being in the spotlight?\nWe’ve also scraped data from SLU’s athletics website to look at data pertaining to some of you! Is that ethical? Is there a line you wouldn’t cross pertaining to data collected on named, individual people?\n\n\n\nExercise 1. Read Sections 8.1 - 8.3 in Modern Data Science with R. Then, write a one paragraph summary of the reading and how it might pertain to the way you use or interpret data.\nExercise 2. Read 2 of the subsections in 8.4 in Modern Data Science with R. These can be any two of the eight subsections of your choosing.\n\nAfter reading the two subsections, read the 12 principles to guide ethical action in 8.5 in Modern Data Science with R. For each of the two subsections you chose, select one of the 12 principles that you feel is most relevant to each example, explaining your reasoning.\nRead the two subsections in 8.5 in Modern Data Science with R corresponding with the two subsections that your read in section 8.4. What principles do the authors reflect on for your two examples?\n\nExercise 3. Data Feminism is related to data ethics, though the two terms are not synonymous. Recently, Catherine D’Ignazio and Lauren F. Klein published a book called Data Feminism https://datafeminism.io/ \nRead the following blog post on Data Feminism, focusing on the section on Missing Data. https://teachdatascience.com/datafem/ .\nPick one example from the bulleted list and write a 2 sentence explanation that explains why it might be important to acknowledge the missing data in an analysis.\nExercise 4. Choose 1 of the following two articles to read\n\n\nhttps://www.theguardian.com/world/2017/sep/08/ai-gay-gaydar-algorithm-facial-recognition-criticism-stanford  on the use of data in the LGBTQIA+ community\n\nhttps://towardsdatascience.com/5-steps-to-take-as-an-antiracist-data-scientist-89712877c214  on anti-racist data practices.\n\n\nFor the LGBTQIA+ article, write a two sentence summary for the side of the argument that research in facial recognition software to identify members of the LGBTQ+ community should not occur, even if this viewpoint isn’t your own.\n\nThen, write a two sentence summary for the side of the argument that research in facial recognition software to identify members of the LGBTQ+ community is okay as long as the results are used responsibly, even if this viewpoint isn’t your own.\n\nFor the anti-racist data science article, under Step 2, pick a News Article and read the first few paragraphs. Describe, in 2-3 sentences, what your article’s example of bias is and why the incidence of bias matters.\n\n\n9.1.1 Data Privacy\nRelated to data ethics is the idea of data privacy.\n\nWhat data is private and what data is public? For some examples, this may seem obvious, but for others (e.g. data on a government agency that collects data on people), the answer might not be as clear cut.\nIs anonymous data truly anonymous?\nWhat type of consent should be provided before collecting data on someone?\n\nWe will explore some of these issues in class."
  },
  {
    "objectID": "09-ethics.html#hypothesis-generation-vs.-confirmation",
    "href": "09-ethics.html#hypothesis-generation-vs.-confirmation",
    "title": "9  Data Ethics",
    "section": "\n9.2 Hypothesis Generation vs. Confirmation",
    "text": "9.2 Hypothesis Generation vs. Confirmation\nWe have focused on hypothesis generation for all data sets in this particular course. Read the following two articles that explain the difference between hypothesis generation and hypothesis confirmation:\nRead the following two very short articles, one from our textbook and one from another source:\n\nhttps://r4ds.had.co.nz/model-intro.html#hypothesis-generation-vs.-hypothesis-confirmation\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC6718169/\n\nWe will discuss those readings further in class."
  },
  {
    "objectID": "09-ethics.html#practice",
    "href": "09-ethics.html#practice",
    "title": "9  Data Ethics",
    "section": "\n9.3 Practice",
    "text": "9.3 Practice\n\n9.3.1 Class Exercises\nClass Exercise 1. How anonymous are SLU’s course evaluations? We will do an in-class activity to investigate this.\nClass Exercise 2. Suppose that I collect data on students in this Data Science class. In each setting (a) through (d), suppose that I give you a data set with the following variables collected on each student in the class. Which option, if any, would it be ethically okay for me to share the data with all students in the class.\n\ncurrent grade and time spent on Canvas.\ncurrent grade, class year, and whether or not the student is a stat major\nfavorite R package, whether or not the student took STAT 213, whether or not the student took CS 140, and Major\nfavorite R package, whether or not the student took STAT 213, whether or not the student took CS 140, and current grade in the course\n\n9.3.2 Your Turn\nYour Turn 1. In your group, explain the difference between hypothesis generation and hypothesis confirmation.\nYour Turn 2. Discuss in your group how many times you can use a single observation for hypothesis generation? for hypothesis confirmation?\nYour Turn 3. Again in your group, answer which of the following questions, pertaining to someone’s fitness, sound more suitable to be answered with Hypothesis Exploration? Which with Hypothesis Confirmation?\n\nYou want to know if, on average, this person exercises more on weekends or more on weekdays, with no other questions of interest.\nYou want to look at general trends in the person’s step count and try to determine if various events influenced the step count.\nYou want to know if the person exercises more in winter or more in summer, and you would also like to investigate other seasonal trends.\n\n\n\n\n\n\n\nNote\n\n\n\nPrediction is different from hypothesis confirmation, because you typically don’t really care which variables are associated with your response. You only want a model that gives the “best” predictions. Because of this, if your goal is prediction, you typically have a lot more freedom with how many times you can “use” a single observation. We will talk a little more about prediction later in the semester."
  },
  {
    "objectID": "08-import.html#readr-to-read-in-data",
    "href": "08-import.html#readr-to-read-in-data",
    "title": "10  Data Import",
    "section": "\n10.1 readr to Read in Data",
    "text": "10.1 readr to Read in Data\nUp to now, we have mostly worked with data that was “R Ready”: meaning that it was in a nice .csv file that could be read into R easily with read_csv() from the readr package. We will begin by looking at some options in the read_csv() function and then move into formats other than .csv that data are commonly stored in.\n\n10.1.1 read_csv() Options\nThe mtcarsex.csv has observations on different car models with variables that include things like gas mileage, number of cylinders, etc. Read in the mtcarsex.csv data set with the following code. Then, examine the data set with head().\n\nlibrary(tidyverse)\nlibrary(here)\ncars_df &lt;- read_csv(here(\"data/mtcarsex.csv\"))\nhead(cars_df)\n#&gt; # A tibble: 6 × 11\n#&gt;   This is a data set about …¹ ...2  ...3  ...4  ...5  ...6  ...7  ...8  ...9 \n#&gt;   &lt;chr&gt;                       &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n#&gt; 1 \"I'm a na\\x95ve data input… &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n#&gt; 2 \"mpg\"                       cyl   disp  hp    drat  wt    qsec  vs    am   \n#&gt; 3  &lt;NA&gt;                       &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n#&gt; 4  &lt;NA&gt;                       &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n#&gt; 5 \"-999\"                      6     160   110   3.9   2.62  16.46 0     1    \n#&gt; 6 \"21\"                        6     160   110   3.9   2.875 17.02 0     1    \n#&gt; # ℹ abbreviated name: ¹​`This is a data set about cars.`\n#&gt; # ℹ 2 more variables: ...10 &lt;chr&gt;, ...11 &lt;chr&gt;\n\nWhat do you notice about the data set that seems odd? Open the .csv file with Excel or some other program to examine the data set outside of R.\nType in ?read_csv in the bottom-left window and look at some of the options in read_csv(). In particular, we will use the na and the skip arguments to fix up our reading.\nLet’s start with skip so that we aren’t reading in the first two rows of the data set:\n\ncars_df &lt;- read_csv(here(\"data/mtcarsex.csv\"), skip = 2)\n## first two lines will be skipped\ncars_df\n#&gt; # A tibble: 34 × 11\n#&gt;      mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n#&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1   NA      NA    NA    NA NA    NA     NA      NA    NA    NA    NA\n#&gt; 2   NA      NA    NA    NA NA    NA     NA      NA    NA    NA    NA\n#&gt; 3 -999       6   160   110  3.9   2.62  16.5     0     1     4     4\n#&gt; 4   21       6   160   110  3.9   2.88  17.0     0     1     4     4\n#&gt; 5   22.8     4   108    93  3.85  2.32  18.6     1     1     4     1\n#&gt; 6   21.4     6   258   110  3.08  3.22  19.4     1     0     3     1\n#&gt; # ℹ 28 more rows\n\nThat looks better, but there are still a couple of problems. What do you notice?\nGo to the help and read about the na argument. Let’s add that as an option to fix the missing value issue.\n\ncars_df &lt;- read_csv(here(\"data/mtcarsex.csv\"), na = c(\"NA\", \"-999\"), skip = 2)\nhead(cars_df)\n#&gt; # A tibble: 6 × 11\n#&gt;     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  NA      NA    NA    NA NA    NA     NA      NA    NA    NA    NA\n#&gt; 2  NA      NA    NA    NA NA    NA     NA      NA    NA    NA    NA\n#&gt; 3  NA       6   160   110  3.9   2.62  16.5     0     1     4     4\n#&gt; 4  21       6   160   110  3.9   2.88  17.0     0     1     4     4\n#&gt; 5  22.8     4   108    93  3.85  2.32  18.6     1     1     4     1\n#&gt; 6  21.4     6   258   110  3.08  3.22  19.4     1     0     3     1\n\nNow look at the classes of each variable. Which classes look like they are incorrect?\nWe’ve talked about how to re-specify classes of variables using mutate() and the as.factor() or as.Date() or as.numeric() functions, but sometimes it’s easier just to respecify the class when we are reading in the data. Notice how, when we use read_csv(), R gives us a message about each of the column types. This is actually an argument in read_csv() called col_types. We can add a |&gt; spec() piping statement after a read_csv() statement to tell R to print the col_types so that it’s easy for us to copy and paste it into read_csv() and change any classes.\n\nread_csv(here(\"data/mtcarsex.csv\"), na = c(\"NA\", \"-999\"), skip = 2) |&gt;\n  spec()\n#&gt; cols(\n#&gt;   mpg = col_double(),\n#&gt;   cyl = col_double(),\n#&gt;   disp = col_double(),\n#&gt;   hp = col_double(),\n#&gt;   drat = col_double(),\n#&gt;   wt = col_double(),\n#&gt;   qsec = col_double(),\n#&gt;   vs = col_double(),\n#&gt;   am = col_double(),\n#&gt;   gear = col_double(),\n#&gt;   carb = col_double()\n#&gt; )\n\nFor example, notice how cyl = col_double() is changed to cyl = col_factor() in the code chunk below:\n\ncars_df &lt;- read_csv(here(\"data/mtcarsex.csv\"), na = c(NA, \"-999\"), skip = 2,\n  col_types = cols(\n  mpg = col_double(),\n  cyl = col_factor(),\n  disp = col_double(),\n  hp = col_double(),\n  drat = col_double(),\n  wt = col_double(),\n  qsec = col_double(),\n  vs = col_factor(),\n  am = col_double(),\n  gear = col_double(),\n  carb = col_double()\n))\n\nFinally, there are two rows with all missing values. These aren’t providing anything useful so we can slice() them out:\n\ncars_df &lt;- read_csv(here(\"data/mtcarsex.csv\"), na = c(\"NA\", \"-999\"), skip = 2,\n  col_types = cols(\n  mpg = col_double(),\n  cyl = col_factor(),\n  disp = col_double(),\n  hp = col_double(),\n  drat = col_double(),\n  wt = col_double(),\n  qsec = col_double(),\n  vs = col_factor(),\n  am = col_double(),\n  gear = col_double(),\n  carb = col_double()\n)) |&gt;\n  slice(-(1:2))\nhead(cars_df)\n#&gt; # A tibble: 6 × 11\n#&gt;     mpg cyl    disp    hp  drat    wt  qsec vs       am  gear  carb\n#&gt;   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  NA   6       160   110  3.9   2.62  16.5 0         1     4     4\n#&gt; 2  21   6       160   110  3.9   2.88  17.0 0         1     4     4\n#&gt; 3  22.8 4       108    93  3.85  2.32  18.6 1         1     4     1\n#&gt; 4  21.4 6       258   110  3.08  3.22  19.4 1         0     3     1\n#&gt; 5  NA   8       360   175  3.15  3.44  17.0 0         0     3     2\n#&gt; 6  18.1 6       225   105  2.76  3.46  20.2 1         0     3     1\n\nThere are many other possible file formats for data storage. For example, there is a data set called oscars.tsv, which is a tab-separated file. You can read it in with read_tsv() instead of read_csv().\n\noscars_df &lt;- read_tsv(here(\"data/oscars.tsv\"))\nhead(oscars_df)\n#&gt; # A tibble: 6 × 51\n#&gt;   FilmName  OscarYear Duration Rating DirectorName DirectorGender OscarWinner\n#&gt;   &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;                 &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1 Crash          2006      113      4 Haggis                    0           1\n#&gt; 2 Brokebac…      2006      134      4 Lee                       0           0\n#&gt; 3 Capote         2006      114      4 Miller                    0           0\n#&gt; 4 Good Nig…      2006       93      2 Clooney                   0           0\n#&gt; 5 Munich         2006      164      4 Spielberg                 0           0\n#&gt; 6 The Depa…      2007      151      4 Scorsese                  0           1\n#&gt; # ℹ 44 more variables: GenreName &lt;chr&gt;, Genre_Drama &lt;dbl&gt;, Genre_Bio &lt;dbl&gt;,\n#&gt; #   CountryName &lt;chr&gt;, ForeignandUSA &lt;dbl&gt;, ProductionName &lt;chr&gt;,\n#&gt; #   ProductionCompany &lt;dbl&gt;, BudgetRevised &lt;chr&gt;, Budget &lt;chr&gt;,\n#&gt; #   DomesticBoxOffice &lt;dbl&gt;, WorldwideRevised &lt;dbl&gt;,\n#&gt; #   WorldwideBoxOffice &lt;dbl&gt;, DomesticPercent &lt;dbl&gt;,\n#&gt; #   LimitedOpeningWnd &lt;dbl&gt;, LimitedTheaters &lt;dbl&gt;,\n#&gt; #   LimitedAveragePThtr &lt;dbl&gt;, WideOpeningWkd &lt;dbl&gt;, WideTheaters &lt;dbl&gt;, …\n\n\n\n\n\n\n\nNote\n\n\n\nWe’ll be able to work with .txt files and Excel files in the Exercises. Check out https://rawgit.com/rstudio/cheatsheets/master/data-import.pdf for a data import cheatsheet.\n\n\nThe final issue that we will discuss in this section occurs when a data set has units within its cells. Consider the earlier example that we used in the reprex section:\n\ntest_df &lt;- read_csv(here(\"data/parsedf.csv\"))\nhead(test_df)\n#&gt; # A tibble: 3 × 2\n#&gt;   x                   y\n#&gt;   &lt;chr&gt;           &lt;dbl&gt;\n#&gt; 1 20,000 dollars      1\n#&gt; 2 40 dollars          2\n#&gt; 3 only 13 dollars     3\n\nThe parse_number() function is really useful if we just want the number (no commas, no units, etc.). The function is often paired with mutate() since we are creating a new variable:\n\ntest_df |&gt; mutate(x2 = parse_number(x))\n#&gt; # A tibble: 3 × 3\n#&gt;   x                   y    x2\n#&gt;   &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 20,000 dollars      1 20000\n#&gt; 2 40 dollars          2    40\n#&gt; 3 only 13 dollars     3    13\n\nExercise 1. Recall the fitness data set.\n\nfitness_df &lt;- read_csv(here::here(\"data/higham_fitness_notclean.csv\"))\n\nUse the col_types argument in read_csv() so that stepgoal is read in as a logical variable with col_logical() and so that month and weekday are both read in as factors."
  },
  {
    "objectID": "08-import.html#data-scraping-with-rvest",
    "href": "08-import.html#data-scraping-with-rvest",
    "title": "10  Data Import",
    "section": "\n10.2 Data Scraping with rvest\n",
    "text": "10.2 Data Scraping with rvest\n\nSometimes, we might want data from a public website that isn’t provided in a file format. To obtain this data, we’ll need to use web scraping, a term which just means “getting data from a website.” The easiest way to do this in R is with the rvest package. Note that we could spend an entire semester talking about web scraping, but we will focus only on websites where the scraping of data is “easy” and won’t give us any major errors.\nGo to the following website and suppose that you wanted the table of gun violence statistics in R: https://en.wikipedia.org/wiki/Gun_violence_in_the_United_States_by_state. We could try copy-pasting the table into Excel and reading the data set in with read_excel(). Depending on the format of the table, that strategy may work but it may not. Another way is to scrape it directly with rvest. Additionally, if the website continually updates (standings for a sports league, enrollment data for a school, best-selling products for a company, etc.), then scraping is much more convenient, as we don’t need to continually copy-paste for updated data.\nIn the following code chunk, read_html() reads in the entire html file from the url provided while html_nodes() extracts only the tables on the website.\n\nlibrary(tidyverse)\nlibrary(rvest)\n\n## provide the URL and name it something (in this case, url).\nurl &lt;- \"https://en.wikipedia.org/wiki/Gun_violence_in_the_United_States_by_state\"\n\n## read_html() convert the html code from the URL into something R can read\ntab &lt;- read_html(url) |&gt; \n  html_nodes(\"table\") ## html_nodes can grab only the tables \n\nWe see that, for this example, there are 3 tables provided. The tables are stored in a list and we can reference the first table using [[1]], the second table using [[2]], etc.\n\n\n\n\n\n\nImportant\n\n\n\nFor the purposes of this class, we will figure out which of the 3 tables is the one we actually want using trial and error.\n\n\nThe html_table() function converts the table into a data.frame object.\n\ntest1 &lt;- tab[[1]] |&gt; html_table()\ntest1\ntest2 &lt;- tab[[2]] |&gt; html_table()\ntest2\ntest3 &lt;- tab[[3]] |&gt; html_table()\ntest3\n\nWhich of the 3 tables is the one that we would want to use for an analysis on gun violence in the United States?\nAs another example, consider scraping data from SLU’s athletics page. In particular, suppose we want to do an analysis on SLU’s baseball team.\nGo to the following website to look at the table of data that we want to scrape: https://saintsathletics.com/sports/baseball/stats/2023.\nAfter looking at the website, use the following code to scrape the data set.\n\nurl &lt;- \"https://saintsathletics.com/sports/baseball/stats/2023\"\n\ntab &lt;- read_html(url) |&gt; html_nodes(\"table\")\ntab\nobj &lt;- tab[[1]] |&gt; html_table()\nobj\nobj2 &lt;- tab[[2]] |&gt; html_table()\nobj2\n\nThere’s now 72 different tables! See if you can figure out where the first few tables are coming from on the website.\nExercise 2. SLU keeps track of diversity of students and makes this data public on the following website: https://www.stlawu.edu/offices/institutional-research/student-diversity-2021. Use rvest to scrape one of the data tables into R."
  },
  {
    "objectID": "08-import.html#json-files-with-jsonlite",
    "href": "08-import.html#json-files-with-jsonlite",
    "title": "10  Data Import",
    "section": "\n10.3 JSON Files with jsonlite\n",
    "text": "10.3 JSON Files with jsonlite\n\nA final common data format that we will discuss is JSON (JavaScript Object Notation). We will only cover the very basics of JSON data and use the jsonlite package in R to read in some .json files. JSON files are read in to R as a list object.\n\n10.3.1 Everything Working Well\nFirst, consider data from the mobile game Clash Royale. Install the jsonlite package and then use it to read in the json file with the function fromJSON():\n\n## install.packages(\"jsonlite\")\nlibrary(jsonlite)\ncr_cards &lt;- fromJSON(here(\"data/clash_royale_card_info.json\"))\n\nNext, type View(cr_cards) in your console (bottom-left) window to look at the data. See if you can pull out the data set by clicking on some things in the View() window.\nThe following give a couple of ways to grab the data using code. The as_tibble() function converts a rectangular object into our familiar tibble.\nThe first option specifies the name of the table that’s in the JSON file (in this case, the name is \"cards\"):\n\nlibrary(tidyverse)\ncr_cards_flat &lt;- cr_cards[[\"cards\"]]\ncr_cards_df &lt;- as_tibble(cr_cards_flat)\ncr_cards_df\n#&gt; # A tibble: 93 × 8\n#&gt;   key     name      elixir type  rarity arena description                  id\n#&gt;   &lt;chr&gt;   &lt;chr&gt;      &lt;int&gt; &lt;chr&gt; &lt;chr&gt;  &lt;int&gt; &lt;chr&gt;                     &lt;int&gt;\n#&gt; 1 knight  Knight         3 Troop Common     0 A tough melee fighter. … 2.6 e7\n#&gt; 2 archers Archers        3 Troop Common     0 A pair of lightly armor… 2.60e7\n#&gt; 3 goblins Goblins        2 Troop Common     1 Three fast, unarmored m… 2.60e7\n#&gt; 4 giant   Giant          5 Troop Rare       0 Slow but durable, only … 2.60e7\n#&gt; 5 pekka   P.E.K.K.A      7 Troop Epic       4 A heavily armored, slow… 2.60e7\n#&gt; 6 minions Minions        3 Troop Common     0 Three fast, unarmored f… 2.60e7\n#&gt; # ℹ 87 more rows\n\nThe second method uses the flatten() function from the purrr package, the only package in the core tidyverse that we do not talk about in detail in this class. There is also a different flatten() function in the jsonlite package. In the code below, we specify that we want to use flatten() from purrr with purrr::flatten(). If we wanted to use flatten() from jsonlite, we’d use jsonlite::flatten()\n\ncr_cards_flat2 &lt;- purrr::flatten(cr_cards)\ncr_cards_df2 &lt;- as_tibble(cr_cards_flat2)\ncr_cards_df2\n#&gt; # A tibble: 93 × 8\n#&gt;   key     name      elixir type  rarity arena description                  id\n#&gt;   &lt;chr&gt;   &lt;chr&gt;      &lt;int&gt; &lt;chr&gt; &lt;chr&gt;  &lt;int&gt; &lt;chr&gt;                     &lt;int&gt;\n#&gt; 1 knight  Knight         3 Troop Common     0 A tough melee fighter. … 2.6 e7\n#&gt; 2 archers Archers        3 Troop Common     0 A pair of lightly armor… 2.60e7\n#&gt; 3 goblins Goblins        2 Troop Common     1 Three fast, unarmored m… 2.60e7\n#&gt; 4 giant   Giant          5 Troop Rare       0 Slow but durable, only … 2.60e7\n#&gt; 5 pekka   P.E.K.K.A      7 Troop Epic       4 A heavily armored, slow… 2.60e7\n#&gt; 6 minions Minions        3 Troop Common     0 Three fast, unarmored f… 2.60e7\n#&gt; # ℹ 87 more rows\n\nBoth methods give a tibble that we can then use our usual tidyverse tools ggplot2, dplyr, tidyr, etc. on.\n\n10.3.2 Things Aren’t Always So Easy\nNow let’s try to look at some animal crossing data that were obtained from https://github.com/jefflomacy/villagerdb. We first just want to look at the data from one individual villager (ace) in the file ace.json.\n\nacedata &lt;- fromJSON(here(\"data/ace.json\"))\naceflat &lt;- purrr::flatten(acedata)\naceflat\n#&gt; $gender\n#&gt; [1] \"male\"\n#&gt; \n#&gt; $species\n#&gt; [1] \"bird\"\n#&gt; \n#&gt; $birthday\n#&gt; [1] \"3-13\"\n#&gt; \n#&gt; $ac\n#&gt; $ac$personality\n#&gt; [1] \"jock\"\n#&gt; \n#&gt; $ac$clothes\n#&gt; [1] \"spade-shirt\"\n#&gt; \n#&gt; $ac$song\n#&gt; [1] \"K.K. Parade\"\n#&gt; \n#&gt; $ac$phrase\n#&gt; [1] \"ace\"\n#&gt; \n#&gt; \n#&gt; $`afe+`\n#&gt; $`afe+`$personality\n#&gt; [1] \"jock\"\n#&gt; \n#&gt; $`afe+`$clothes\n#&gt; [1] \"spade-shirt\"\n#&gt; \n#&gt; $`afe+`$song\n#&gt; [1] \"K.K. Parade\"\n#&gt; \n#&gt; \n#&gt; $name\n#&gt; [1] \"Ace\"\n#&gt; \n#&gt; $id\n#&gt; [1] \"ace\"\n\nThings are now….more complicated. This example is just to show that it’s not always easy working with JSON data. Lists can be nested and that creates problems when trying to convert a deeply nested list into our “rectangular” format that’s easy to work with.\nThere’s also the added problem of reading in the .json files from all villagers at the same time We could do this with a for loop or a mapping function from purrr to download and read in the JSON files for all villagers. We won’t delve any more deeply into this, but there’s a lot more to all of the file formats that we’ve discussed this week, particularly web scraping and .json files."
  },
  {
    "objectID": "08-import.html#practice",
    "href": "08-import.html#practice",
    "title": "10  Data Import",
    "section": "\n10.4 Practice",
    "text": "10.4 Practice\n\n10.4.1 Class Exercises\nClass Exercise 1. The birthdays.txt file has information on the birthdays of various animals on my Animal Crossing island. There are also columns for the Animal’s Name, Animal Type, and how long the animal has lived on the island (in weeks). Click on the file to open it to look at the format of the data.\nStart with the following code chunk and use the options of read_delim() to read in the data (?read_delim). The delim argument that’s already provided specifies that the delimiter (separator) that you’ll use is a -, as opposed to, for example, a , in a .csv file. Arguments that you may need to change include\n\nskip\ncol_names\nna\ntrim_ws\ncol_types\n\n\nlibrary(tidyverse)\ndf &lt;- read_delim(here(\"data/birthdays.txt\"), delim = \" - \")\nhead(df)\n\nClass Exercise 2. Another common format for data to be stored in is an Excel file. Often, it’s easiest just to save the Excel file as a .csv file and read it in using read_csv(). But, sometimes this route can be difficult (for example, if your Excel file has thousands of sheets). To read in directly from Excel, you’ll need to install the readxl with install.packages(\"readxl\"). Once installed, load the package with library(readxl), and read in the first sheet evals_prof.xlsx data set, a similar data set as the one that will be used for Project 2, with the read_excel() function.\nClass Exercise 3. Now, read in the second sheet in the Excel file, using the help file for ?read_excel to change one of the arguments.\nClass Exercise 4. A common issue when scraping tables with rvest is that there are tables that have two rows of headers, often with duplicate names in one or both of the rows. Examine this issue at https://en.wikipedia.org/wiki/Josh_Allen_(quarterback), scrolling down to the NFL career statistics subsection.\nTry to scrape Josh Allen’s stats using the “usual” data scraping method with rvest:\n\nurl1 &lt;- \"https://en.wikipedia.org/wiki/Josh_Allen_(quarterback)\"\ntab_allen_stats &lt;- read_html(url1) |&gt; html_nodes(\"table\")\nallen_df &lt;- tab_allen_stats[[6]] |&gt; html_table()\nallen_df\n\nTry to perform any operation on allen_df (a filter(), mutate(), arrange(), or anything else). What error do you get?\nTo fix this issue, we can use the header = FALSE argument to html_table() and then “manually” construct the column names:\n\nallen_stats &lt;- tab_allen_stats[[6]] |&gt; html_table(header = FALSE) \nallen_stats\n\nnewnames &lt;- paste(allen_stats[1, ], allen_stats[2, ])\nallen_stats |&gt; set_names(newnames) |&gt;\n  slice(-1, -2)\n\nExplain what the code above is doing to fix the duplicate name issue.\n\n10.4.2 Your Turn\nYour Turn 1. Choose a topic/person/place/etc. that interests you that has tables on Wikipedia and scrape the table that is related to that topic.\nYour Turn 2. Choose a sports team at SLU, and go to that team’s website (by simply googling SLU name_of_sport). Scrape the data tables from the “Results” or “Statistics” section of this sport. After you scrape the data, tidy the data set. Then, choose one of the following options (different options might make more/less sense for different sports)\n\nSummarise different team statistics, either numerically or graphically. Perhaps make some graphs showing different statistics through time.\nSummarise different individual statistics, either numerically or graphically.\n\n\n\n\n\n\n\nNote\n\n\n\nA few sports (men’s and women’s golf, for example), give results in PDF format. PDF format is generally a horrible way to record and share data, as it’s very difficult to read in to almost any program. Therefore, avoid sports with PDF results for the purposes of this exercise.\n\n\nYour Turn 3. For either your topic of choice data set or your sports team data set, ask and answer any other questions that make sense for the particular topic or sport that you are looking at!"
  },
  {
    "objectID": "10-merging.html#stacking-rows-and-appending-columns",
    "href": "10-merging.html#stacking-rows-and-appending-columns",
    "title": "11  Merging with dplyr",
    "section": "\n11.1 Stacking Rows and Appending Columns",
    "text": "11.1 Stacking Rows and Appending Columns\n\n11.1.1 Stacking with bind_rows()\n\nFirst, we will talk about combining two data sets by “stacking” them on top of each other to form one new data set. The bind_rows() function can be used for this purpose if the two data sets have identical column names.\nA common instance where this is useful is if two data sets come from the same source and have different locations or years, but the same exact column names.\nFor example, examine the following website and notice how there are .csv files given for each year of matches in the ATP (Association of (men’s) Tennis Professionals). https://github.com/JeffSackmann/tennis_atp.\nThen, read in the data sets, and look at how many columns each has.\n\nlibrary(tidyverse)\nlibrary(here)\natp_2019 &lt;- read_csv(here(\"data/atp_matches_2019.csv\"))\natp_2018 &lt;- read_csv(here(\"data/atp_matches_2018.csv\"))\nhead(atp_2019) \nhead(atp_2018)\n\nTo combine results from both data sets,\n\natp_df &lt;- bind_rows(atp_2018, atp_2019)\n#&gt; Error in `bind_rows()`:\n#&gt; ! Can't combine `..1$winner_seed` &lt;double&gt; and `..2$winner_seed` &lt;character&gt;.\n\nThe issue is that the winner_seed and loser_seed variables are of different variable classes in the atp_2018 data vs. the atp_2019 data. The following code forces the seed variables in the 2018 data set to be characters.\n\natp_2018 &lt;- read_csv(here(\"data/atp_matches_2018.csv\"),\n                     col_types = cols(winner_seed = col_character(),\n                                      loser_seed = col_character()))\n\nWe can try combining the data sets now.\n\natp_df &lt;- bind_rows(atp_2018, atp_2019)\natp_df\n\nDo a quick check to make sure the number of rows in atp_2018 plus the number of rows in atp_2019 equals the number of rows in atp_df.\nIt might seem a little annoying, but, by default bind_rows() will only combine two data sets by stacking rows if the data sets have identical column names and identical column classes, as we saw in the previous example.\nNow run the following and look at the output.\n\ndf_test2a &lt;- tibble(xvar = c(1, 2))\ndf_test2b &lt;- tibble(xvar = c(1, 2), y = c(5, 1))\nbind_rows(df_test2a, df_test2b)\n#&gt; # A tibble: 4 × 2\n#&gt;    xvar     y\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1    NA\n#&gt; 2     2    NA\n#&gt; 3     1     5\n#&gt; 4     2     1\n\nWe can see from this toy example that, if a variable is missing entirely in one of the data sets we are binding together, then NA values are filled in for that variable.\n\n11.1.2 Binding Columns with bind_cols()\n\nbind_cols() combines two data sets by combining their columns into one data set. We won’t spend much time talking about how to bind together columns because it’s generally a little dangerous.\nWe will use a couple of test data sets, df_test1a and df_test1b, to see it in action:\n\ndf_test1a &lt;- tibble(xvar = c(1, 2), yvar = c(5, 1))\ndf_test1b &lt;- tibble(x = c(1, 2), y = c(5, 1))\nbind_cols(df_test1a, df_test1b)\n#&gt; # A tibble: 2 × 4\n#&gt;    xvar  yvar     x     y\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1     5     1     5\n#&gt; 2     2     1     2     1\n\nExercise 1. Run the following and explain why R does not simply stack the rows. Then, fix the issue with the rename() function.\n\ndf_test1a &lt;- tibble(xvar = c(1, 2), yvar = c(5, 1))\ndf_test1b &lt;- tibble(x = c(1, 2), y = c(5, 1))\nbind_rows(df_test1a, df_test1b)\n#&gt; # A tibble: 4 × 4\n#&gt;    xvar  yvar     x     y\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1     5    NA    NA\n#&gt; 2     2     1    NA    NA\n#&gt; 3    NA    NA     1     5\n#&gt; 4    NA    NA     2     1"
  },
  {
    "objectID": "10-merging.html#mutating-joins",
    "href": "10-merging.html#mutating-joins",
    "title": "11  Merging with dplyr",
    "section": "\n11.2 Mutating Joins",
    "text": "11.2 Mutating Joins\nIf the goal is to combine two data sets using some common variable(s) that both data sets have, we need different tools than simply stacking rows or appending columns. When merging together two or more data sets, we need to have a matching identification variable in each data set. This variable is commonly called a key. A key can be an identification number, a name, a date, etc, but must be present in both data sets.\nAs a simple first example, consider\n\nlibrary(tidyverse)\ndf1 &lt;- tibble(name = c(\"Emily\", \"Miguel\", \"Tonya\"), fav_sport = c(\"Swimming\", \"Football\", \"Tennis\"))\ndf2 &lt;- tibble(name = c(\"Tonya\", \"Miguel\", \"Emily\"),\n              fav_colour = c(\"Robin's Egg Blue\", \"Tickle Me Pink\", \"Goldenrod\"))\n\nOur goal is to combine the two data sets so that the people’s favorite sports and favorite colours are in one data set.\nIdentify the key in the example above.\n\n11.2.1 Keep All Rows of Data Set 1 with left_join()\n\nConsider the babynames R package, which has the following data sets:\n\n\nlifetables: cohort life tables for different sex and different year variables, starting at the year 1900.\n\nbirths: the number of births in the United States in each year, since 1909\n\nbabynames: popularity of different baby names per year and sex since the year 1880.\n\n\n##install.packages(\"babynames\")\nlibrary(babynames)\nlife_df &lt;- babynames::lifetables\nbirth_df &lt;- babynames::births\nbabynames_df &lt;- babynames::babynames\n\nhead(babynames)\nhead(births)\nhead(lifetables)\n\nRead about each data set with ?babynames, ?births and ?lifetables.\nSuppose that you want to combine the births data set with the babynames data set, so that each row of babynames now has the total number of births for that year. We first need to identify the key in each data set that we will use for the joining. In this case, each data set has a year variable, and we can use left_join() to keep all observations in babynames_df, even for years that are not in the births_df data set.\n\ncombined_left &lt;- left_join(babynames_df, birth_df, by = join_by(year == year))\nhead(combined_left)\n#&gt; # A tibble: 6 × 6\n#&gt;    year sex   name          n   prop births\n#&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;  &lt;dbl&gt;  &lt;int&gt;\n#&gt; 1  1880 F     Mary       7065 0.0724     NA\n#&gt; 2  1880 F     Anna       2604 0.0267     NA\n#&gt; 3  1880 F     Emma       2003 0.0205     NA\n#&gt; 4  1880 F     Elizabeth  1939 0.0199     NA\n#&gt; 5  1880 F     Minnie     1746 0.0179     NA\n#&gt; 6  1880 F     Margaret   1578 0.0162     NA\ntail(combined_left)\n#&gt; # A tibble: 6 × 6\n#&gt;    year sex   name       n       prop  births\n#&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;int&gt;      &lt;dbl&gt;   &lt;int&gt;\n#&gt; 1  2017 M     Zyhier     5 0.00000255 3855500\n#&gt; 2  2017 M     Zykai      5 0.00000255 3855500\n#&gt; 3  2017 M     Zykeem     5 0.00000255 3855500\n#&gt; 4  2017 M     Zylin      5 0.00000255 3855500\n#&gt; 5  2017 M     Zylis      5 0.00000255 3855500\n#&gt; 6  2017 M     Zyrie      5 0.00000255 3855500\n\nWhy are births missing in head(combined_left) but not in tail(combined_left)?\n\n11.2.2 Keep All Rows of Data Set 2 with right_join()\n\nRecall from the accompanying handout that there is no need to ever use right_join() because it is the same as using a left_join() with the first two data set arguments switched:\n\n## these will always do the same exact thing\nright_join(babynames_df, birth_df, by = join_by(year == year))\n#&gt; # A tibble: 1,839,952 × 6\n#&gt;    year sex   name         n   prop  births\n#&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;    &lt;int&gt;  &lt;dbl&gt;   &lt;int&gt;\n#&gt; 1  1909 F     Mary     19259 0.0523 2718000\n#&gt; 2  1909 F     Helen     9250 0.0251 2718000\n#&gt; 3  1909 F     Margaret  7359 0.0200 2718000\n#&gt; 4  1909 F     Ruth      6509 0.0177 2718000\n#&gt; 5  1909 F     Dorothy   6253 0.0170 2718000\n#&gt; 6  1909 F     Anna      5804 0.0158 2718000\n#&gt; # ℹ 1,839,946 more rows\nleft_join(birth_df, babynames_df, by = join_by(year == year))\n#&gt; # A tibble: 1,839,952 × 6\n#&gt;    year  births sex   name         n   prop\n#&gt;   &lt;dbl&gt;   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;    &lt;int&gt;  &lt;dbl&gt;\n#&gt; 1  1909 2718000 F     Mary     19259 0.0523\n#&gt; 2  1909 2718000 F     Helen     9250 0.0251\n#&gt; 3  1909 2718000 F     Margaret  7359 0.0200\n#&gt; 4  1909 2718000 F     Ruth      6509 0.0177\n#&gt; 5  1909 2718000 F     Dorothy   6253 0.0170\n#&gt; 6  1909 2718000 F     Anna      5804 0.0158\n#&gt; # ℹ 1,839,946 more rows\n\nTherefore, it’s usually easier to just always use left_join() and ignore right_join() completely.\n\n11.2.3 Keep All Rows of Both Data Sets with full_join()\n\nIn addition to keeping any rows with a matching key in the other data frame, a full_join() will keep all rows in data set 1 that don’t have a matching key in data set 2, and will also keep all rows in data set 2 that don’t have a matching key in data set 1, filling in NA for missing values when necessary. For our example of merging babynames_df with birth_df,\n\nfull_join(babynames_df, birth_df, by = join_by(year == year))\n\n\n11.2.4 Keep Only Rows with Matching Keys with inner_join()\n\nWe can also keep only rows with matching keys with inner_join(). For this join, any row in data set 1 without a matching key in data set 2 is dropped, and any row in data set 2 without a matching key in data set 1 is also dropped.\n\ninner_join(babynames_df, birth_df, by = join_by(year == year))\n#&gt; # A tibble: 1,839,952 × 6\n#&gt;    year sex   name         n   prop  births\n#&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;    &lt;int&gt;  &lt;dbl&gt;   &lt;int&gt;\n#&gt; 1  1909 F     Mary     19259 0.0523 2718000\n#&gt; 2  1909 F     Helen     9250 0.0251 2718000\n#&gt; 3  1909 F     Margaret  7359 0.0200 2718000\n#&gt; 4  1909 F     Ruth      6509 0.0177 2718000\n#&gt; 5  1909 F     Dorothy   6253 0.0170 2718000\n#&gt; 6  1909 F     Anna      5804 0.0158 2718000\n#&gt; # ℹ 1,839,946 more rows\n\n\n\n11.2.5 Which xxxx_join()?\nWhich join function we use will depend on the context of the data and what questions you will be answering in your analysis.\n\n\n\n\n\n\nImportant\n\n\n\nIf you’re using a left_join(), right_join() or inner_join(), you’re potentially cutting out some data. It’s important to be aware of what data you’re omitting. For example, with the babynames and births data, we would want to keep a note that a left_join() removed all observations before 1909 from joined data set.\n\n\nExercise 2. Examine the following two joins that we’ve done, and explain why one resulting data set has fewer observations (rows) than the other.\n\nleft_join(babynames_df, birth_df, by = join_by(year == year))\n#&gt; # A tibble: 1,924,665 × 6\n#&gt;    year sex   name          n   prop births\n#&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;  &lt;dbl&gt;  &lt;int&gt;\n#&gt; 1  1880 F     Mary       7065 0.0724     NA\n#&gt; 2  1880 F     Anna       2604 0.0267     NA\n#&gt; 3  1880 F     Emma       2003 0.0205     NA\n#&gt; 4  1880 F     Elizabeth  1939 0.0199     NA\n#&gt; 5  1880 F     Minnie     1746 0.0179     NA\n#&gt; 6  1880 F     Margaret   1578 0.0162     NA\n#&gt; # ℹ 1,924,659 more rows\nleft_join(birth_df, babynames_df, by = join_by(year == year))\n#&gt; # A tibble: 1,839,952 × 6\n#&gt;    year  births sex   name         n   prop\n#&gt;   &lt;dbl&gt;   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;    &lt;int&gt;  &lt;dbl&gt;\n#&gt; 1  1909 2718000 F     Mary     19259 0.0523\n#&gt; 2  1909 2718000 F     Helen     9250 0.0251\n#&gt; 3  1909 2718000 F     Margaret  7359 0.0200\n#&gt; 4  1909 2718000 F     Ruth      6509 0.0177\n#&gt; 5  1909 2718000 F     Dorothy   6253 0.0170\n#&gt; 6  1909 2718000 F     Anna      5804 0.0158\n#&gt; # ℹ 1,839,946 more rows"
  },
  {
    "objectID": "10-merging.html#filtering-joins",
    "href": "10-merging.html#filtering-joins",
    "title": "11  Merging with dplyr",
    "section": "\n11.3 Filtering Joins",
    "text": "11.3 Filtering Joins\nFiltering joins (semi_join() and anti_join()) are useful if we would only like to keep the variables in one data set, but we want to filter out observations by a variable in the second data set.\nConsider again the two data sets on men’s tennis matches in 2018 and in 2019.\n\natp_2019 &lt;- read_csv(here(\"data/atp_matches_2019.csv\"))\natp_2018 &lt;- read_csv(here(\"data/atp_matches_2018.csv\"))\natp_2019\n#&gt; # A tibble: 2,781 × 49\n#&gt;   tourney_id tourney_name surface draw_size tourney_level tourney_date\n#&gt;   &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt;\n#&gt; 1 2019-M020  Brisbane     Hard           32 A                 20181231\n#&gt; 2 2019-M020  Brisbane     Hard           32 A                 20181231\n#&gt; 3 2019-M020  Brisbane     Hard           32 A                 20181231\n#&gt; 4 2019-M020  Brisbane     Hard           32 A                 20181231\n#&gt; 5 2019-M020  Brisbane     Hard           32 A                 20181231\n#&gt; 6 2019-M020  Brisbane     Hard           32 A                 20181231\n#&gt; # ℹ 2,775 more rows\n#&gt; # ℹ 43 more variables: match_num &lt;dbl&gt;, winner_id &lt;dbl&gt;, winner_seed &lt;chr&gt;,\n#&gt; #   winner_entry &lt;chr&gt;, winner_name &lt;chr&gt;, winner_hand &lt;chr&gt;,\n#&gt; #   winner_ht &lt;dbl&gt;, winner_ioc &lt;chr&gt;, winner_age &lt;dbl&gt;, loser_id &lt;dbl&gt;,\n#&gt; #   loser_seed &lt;chr&gt;, loser_entry &lt;chr&gt;, loser_name &lt;chr&gt;, loser_hand &lt;chr&gt;,\n#&gt; #   loser_ht &lt;dbl&gt;, loser_ioc &lt;chr&gt;, loser_age &lt;dbl&gt;, score &lt;chr&gt;,\n#&gt; #   best_of &lt;dbl&gt;, round &lt;chr&gt;, minutes &lt;dbl&gt;, w_ace &lt;dbl&gt;, w_df &lt;dbl&gt;, …\natp_2018\n#&gt; # A tibble: 2,889 × 49\n#&gt;   tourney_id tourney_name surface draw_size tourney_level tourney_date\n#&gt;   &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt;\n#&gt; 1 2018-M020  Brisbane     Hard           32 A                 20180101\n#&gt; 2 2018-M020  Brisbane     Hard           32 A                 20180101\n#&gt; 3 2018-M020  Brisbane     Hard           32 A                 20180101\n#&gt; 4 2018-M020  Brisbane     Hard           32 A                 20180101\n#&gt; 5 2018-M020  Brisbane     Hard           32 A                 20180101\n#&gt; 6 2018-M020  Brisbane     Hard           32 A                 20180101\n#&gt; # ℹ 2,883 more rows\n#&gt; # ℹ 43 more variables: match_num &lt;dbl&gt;, winner_id &lt;dbl&gt;, winner_seed &lt;dbl&gt;,\n#&gt; #   winner_entry &lt;chr&gt;, winner_name &lt;chr&gt;, winner_hand &lt;chr&gt;,\n#&gt; #   winner_ht &lt;dbl&gt;, winner_ioc &lt;chr&gt;, winner_age &lt;dbl&gt;, loser_id &lt;dbl&gt;,\n#&gt; #   loser_seed &lt;dbl&gt;, loser_entry &lt;chr&gt;, loser_name &lt;chr&gt;, loser_hand &lt;chr&gt;,\n#&gt; #   loser_ht &lt;dbl&gt;, loser_ioc &lt;chr&gt;, loser_age &lt;dbl&gt;, score &lt;chr&gt;,\n#&gt; #   best_of &lt;dbl&gt;, round &lt;chr&gt;, minutes &lt;dbl&gt;, w_ace &lt;dbl&gt;, w_df &lt;dbl&gt;, …\n\n\n11.3.1 Filtering with semi_join()\n\nSuppose that we only want to keep matches in 2019 where the winning player had 10 or more wins in 2018. This might be useful if we want to not consider players in 2018 that only played in a couple of matches, perhaps because they got injured or perhaps because they received a special wildcard into the draw of only one event.\nTo accomplish this, we can first create a data set that has the names of all of the players that won 10 or more matches in 2018, using functions that we learned from dplyr earlier in the semester:\n\nwin10 &lt;- atp_2018 |&gt; group_by(winner_name) |&gt;\n  summarise(nwin = n()) |&gt; \n  filter(nwin &gt;= 10)\nwin10\n#&gt; # A tibble: 93 × 2\n#&gt;   winner_name       nwin\n#&gt;   &lt;chr&gt;            &lt;int&gt;\n#&gt; 1 Adrian Mannarino    26\n#&gt; 2 Albert Ramos        21\n#&gt; 3 Alex De Minaur      29\n#&gt; 4 Alexander Zverev    58\n#&gt; 5 Aljaz Bedene        19\n#&gt; 6 Andreas Seppi       24\n#&gt; # ℹ 87 more rows\n\nNext, we apply semi_join(), which takes the names of two data sets (the second is the one that contains information about how the first should be “filtered”). The third argument gives the name of the key (winner_name) in this case.\n\ntennis_2019_10 &lt;- semi_join(atp_2019, win10,\n                            by = join_by(winner_name == winner_name))\ntennis_2019_10$winner_name\n\n\n\n\n\n\n\nNote\n\n\n\nThis strategy only keeps the matches in 2019 where the winner had 10 or more match wins in 2018. It drops any matches where the loser lost against someone who did not have 10 or more match wins in 2018. So this isn’t yet perfect and would take a little more thought into which matches we actually want to keep for a particular analysis.\n\n\n\n11.3.2 Filtering with anti_join()\n\nNow suppose that we want to only keep the matches in 2019 where the winning player did not have any wins in 2018. We might think of these players as “emerging players” in 2019, players who are coming back from an injury, etc.. To do this, we can use anti_join(), which only keeps the rows in the first data set that do not have a match in the second data set.\n\nnew_winners &lt;- anti_join(atp_2019, atp_2018,\n                         by = join_by(winner_name == winner_name)) \nnew_winners$winner_name\n\nWe can then examine how many wins each of these “new” (or perhaps previously injured) players had in 2019:\n\nnew_winners |&gt; group_by(winner_name) |&gt;\n  summarise(nwin = n()) |&gt;\n  arrange(desc(nwin))\n#&gt; # A tibble: 59 × 2\n#&gt;   winner_name           nwin\n#&gt;   &lt;chr&gt;                &lt;int&gt;\n#&gt; 1 Christian Garin         32\n#&gt; 2 Juan Ignacio Londero    22\n#&gt; 3 Miomir Kecmanovic       22\n#&gt; 4 Hugo Dellien            12\n#&gt; 5 Attila Balazs            7\n#&gt; 6 Cedrik Marcel Stebe      7\n#&gt; # ℹ 53 more rows\n\nThe filtering join functions are useful if you want to filter out observations by some criterion in a different data set.\nExercise 3. Examine the following data sets (the first is df1 and the second is df2) and then, without running any code, answer the following questions.\n\n\n\n\n\n\n\n\nid\nxvar\n\n\n\nA\n1\n\n\nB\n2\n\n\nC\n3\n\n\nE\n1\n\n\nF\n2\n\n\n\n\n\n\n\n\n\n\n\nid\nyvar\n\n\n\nA\n2\n\n\nC\n1\n\n\nD\n2\n\n\nE\n1\n\n\nG\n1\n\n\nH\n4\n\n\n\n\n\n\nHow many rows would be in the data set from left_join(df1, df2, by = join_by(id == id))?\nHow many rows would be in the data set from left_join(df2, df1, by = join_by(id == id))?\nHow many rows would be in the data set from full_join(df1, df2, by = join_by(id == id))?\nHow many rows would be in the data set from inner_join(df1, df2, by = join_by(id == id))?\nHow many rows would be in the data set from semi_join(df1, df2, by = join_by(id == id))?\nHow many rows would be in the data set from anti_join(df1, df2, by = join_by(id == id))?"
  },
  {
    "objectID": "10-merging.html#practice",
    "href": "10-merging.html#practice",
    "title": "11  Merging with dplyr",
    "section": "\n11.4 Practice",
    "text": "11.4 Practice\n\n11.4.1 Class Exercises\nClass Exercise 1. Why might bind_cols() be a dangerous way to combine data? What must you be sure of about the way the data was collected in order to combine data in this way?\nClass Exercise 2. The key variable is very important for joining and is not always available in a “perfect” form. Recall the college majors data sets we have, called slumajors_df, which information on majors at SLU. Another data set, collegemajors_df, has different statistics on college majors nationwide. There’s lots of interesting variables in these data sets, but we’ll focus on the Major variable here. Read in and examine the two data sets with:\n\nslumajors_df &lt;- read_csv(here(\"data/SLU_Majors_17_21.csv\"))\ncollegemajors_df &lt;- read_csv(here(\"data/college-majors.csv\"))\nhead(slumajors_df)\n#&gt; # A tibble: 6 × 3\n#&gt;   Major                        nfemales nmales\n#&gt;   &lt;chr&gt;                           &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 Anthropology                       35     13\n#&gt; 2 Art & Art History                  62     11\n#&gt; 3 Biochemistry                       15      6\n#&gt; 4 Biology                           152     58\n#&gt; 5 Business in the Liberal Arts      192    301\n#&gt; 6 Chemistry                          28     20\nhead(collegemajors_df)\n#&gt; # A tibble: 6 × 12\n#&gt;   Major         Total   Men Women Major_category Employed Full_time Part_time\n#&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 PETROLEUM EN…  2339  2057   282 Engineering        1976      1849       270\n#&gt; 2 MINING AND M…   756   679    77 Engineering         640       556       170\n#&gt; 3 METALLURGICA…   856   725   131 Engineering         648       558       133\n#&gt; 4 NAVAL ARCHIT…  1258  1123   135 Engineering         758      1069       150\n#&gt; 5 CHEMICAL ENG… 32260 21239 11021 Engineering       25694     23170      5180\n#&gt; 6 NUCLEAR ENGI…  2573  2200   373 Engineering        1857      2038       264\n#&gt; # ℹ 4 more variables: Unemployed &lt;dbl&gt;, Median &lt;dbl&gt;, P25th &lt;dbl&gt;,\n#&gt; #   P75th &lt;dbl&gt;\n\n\nAttempt to join the two data sets together with a key.\nWhy did the collegemajors_df give only NA values when we tried to join by major?\n\n\n\n\n\n\n\nNote\n\n\n\nThis example underscores the importance of having a key that matches exactly. Some, but not all, of the issues involved in joining these two data sets can be solved with functions in the stringr package (discussed in a few weeks). But, the underlying problem remains and, if we wanted to combine the two data sets fully, we would need to recode some of the major names by hand.\n\n\nClass Exercise 3. Read in the gun violence data set, and suppose that you want to add a row to this data set that has the statistics on gun ownership and mortality rate in the District of Columbia (Washington D.C., which is in the NE region, has 16.7 deaths per 100,000 people, and a gun ownership rate of 8.7%). To do so, create a tibble() that has a single row representing D.C. and then combine your new tibble with the overall gun violence data set. Name this new data set all_df.\n\nlibrary(tidyverse)\nmortality_df &lt;- read_csv(here(\"data/gun_violence_us.csv\"))\n\nClass Exercise 4. Examine the following data sets that are in R’s base library on demographic statistics about the U.S. states and state abbreviations:\n\ndf1 &lt;- as_tibble(state.x77)\ndf2 &lt;- as_tibble(state.abb)\ndf1\n#&gt; # A tibble: 50 × 8\n#&gt;   Population Income Illiteracy `Life Exp` Murder `HS Grad` Frost   Area\n#&gt;        &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1       3615   3624        2.1       69.0   15.1      41.3    20  50708\n#&gt; 2        365   6315        1.5       69.3   11.3      66.7   152 566432\n#&gt; 3       2212   4530        1.8       70.6    7.8      58.1    15 113417\n#&gt; 4       2110   3378        1.9       70.7   10.1      39.9    65  51945\n#&gt; 5      21198   5114        1.1       71.7   10.3      62.6    20 156361\n#&gt; 6       2541   4884        0.7       72.1    6.8      63.9   166 103766\n#&gt; # ℹ 44 more rows\ndf2\n#&gt; # A tibble: 50 × 1\n#&gt;   value\n#&gt;   &lt;chr&gt;\n#&gt; 1 AL   \n#&gt; 2 AK   \n#&gt; 3 AZ   \n#&gt; 4 AR   \n#&gt; 5 CA   \n#&gt; 6 CO   \n#&gt; # ℹ 44 more rows\n\nCombine the two data sets with bind_cols() and name the new data set states_df. What are you assuming about the data sets in order to use this function?\nClass Exercise 5. Use a join function to combine the mortality data set with D.C. (which we named all_df) with the states data set from the previous exercise (states_df). For this exercise, keep the row with Washington D.C., having it take on NA values for any variable not observed in the states data.\nClass Exercise 6. Repeat the previous exercise, but now drop Washington D.C. in your merging process. Practice doing this with a join function (as opposed to slice()-ing it out explicitly).\nClass Exercise 7. Use semi_join() to create a subset of states_df that are in the NE region. Hint: You will need to filter all_df from Exercise 1 first to contain only states in the NE region.\nClass Exercise 8. Do the same thing as in the previous exercise (that is, create a subset of states_df that are in the NE region), but this time, use anti_join().\n\n11.4.2 Your Turn\nYour Turn 1. In your group, evaluate whether each of the following statements are true or false:\n\nAn inner_join() will always result in a data set with the same or fewer rows than a full_join().\nAn inner_join() will always result in a data set with the same or fewer rows than a left_join().\nA left_join() will always result in a data set with the same number of rows as a semi_join() on the same two data sets.\n\nYour Turn 2. Examine the following data sets (the first is df3 and the second is df4) and then, without running any code, answer the following questions in your group. Keep in mind that there are some duplicate keys!\n\n\n\n\n\n\n\n\nid\nxvar\n\n\n\nA\n1\n\n\nA\n2\n\n\nC\n3\n\n\nC\n1\n\n\nF\n2\n\n\nF\n6\n\n\n\n\n\n\n\n\n\n\n\nid\nyvar\n\n\n\nA\n2\n\n\nB\n1\n\n\nC\n2\n\n\nD\n1\n\n\nD\n1\n\n\nD\n4\n\n\n\n\n\n\nHow many rows would be in the data set from left_join(df3, df4, by = join_by(id == id))?\nHow many rows would be in the data set from left_join(df4, df3, by = join_by(id == id))?\nHow many rows would be in the data set from full_join(df3, df4, by = join_by(id == id))?\nHow many rows would be in the data set from inner_join(df3, df4, by = join_by(id == id))?\nHow many rows would be in the data set from semi_join(df3, df4, by = join_by(id == id))?\nHow many rows would be in the data set from anti_join(df3, df4, by = join_by(id == id))?\n\nYour Turn 3. Examine again the gun violence data set, and explain why each attempt at combining the D.C. data with the overall data doesn’t work or is incorrect.\n\ntest1 &lt;- tibble(state = \"Washington D.C.\", mortality_rate = 16.7,\n                ownership_rate = 8.7, region = \"NE\")\nbind_rows(mortality_df, test1)\n\ntest2 &lt;- tibble(state = \"Washington D.C.\", mortality_rate = 16.7,\n       ownership_rate = 0.087, region = NE)\n#&gt; Error in eval_tidy(xs[[j]], mask): object 'NE' not found\nbind_rows(mortality_df, test2)\n#&gt; Error in list2(...): object 'test2' not found\n\ntest3 &lt;- tibble(state = \"Washington D.C.\", mortality_rate = \"16.7\",\n       ownership_rate = \"0.087\", region = \"NE\")\nbind_rows(mortality_df, test3)\n#&gt; Error in `bind_rows()`:\n#&gt; ! Can't combine `..1$mortality_rate` &lt;double&gt; and `..2$mortality_rate` &lt;character&gt;."
  },
  {
    "objectID": "11-lubridate.html#converting-variables-to-date",
    "href": "11-lubridate.html#converting-variables-to-date",
    "title": "12  Dates with lubridate",
    "section": "\n12.1 Converting Variables to <date>\n",
    "text": "12.1 Converting Variables to &lt;date&gt;\n\nThe lubridate package is built to easily work with Date objects and DateTime objects. R does not actually have a class that stores Time objects (unless you install a separate package). Dates tend to be much more common than Times, so, we will primarily focus on Dates, but most functions we will see have easy extensions to Times.\nTo begin, install the lubridate package, and load the package with library(). The today() function prints today’s date while now() prints today’s date and time. These can sometimes be useful in other contexts, but we will just run the code to see how R stores dates and date-times.\n\nlibrary(tidyverse)\nlibrary(lubridate)\ntoday()\n#&gt; [1] \"2024-03-01\"\nnow()\n#&gt; [1] \"2024-03-01 08:19:11 EST\"\n\nThis first section will deal with how to convert a variable in R to be a Date. We will use a data set that has the holidays of Animal Crossing from January to April. The columns in this data set are:\n\n\nHoliday, the name of the holiday and\nvarious other columns with different date formats\n\nRead in the data set with\n\nlibrary(here)\nholiday_df &lt;- read_csv(here(\"data/animal_crossing_holidays.csv\"))\nholiday_df\n#&gt; # A tibble: 6 × 10\n#&gt;   Holiday         Date1     Date2  Date3 Date4 Date5 Month  Year   Day Month2\n#&gt;   &lt;chr&gt;           &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; \n#&gt; 1 New Year's Day  1-Jan-20  Jan-1… 1/1/… 1/1/… 2020…     1  2020     1 Janua…\n#&gt; 2 Groundhog Day   2-Feb-20  Feb-2… 2/2/… 2/2/… 2020…     2  2020     2 Febru…\n#&gt; 3 Valentine's Day 14-Feb-20 Feb-1… 2/14… 2020… 2020…     2  2020    14 Febru…\n#&gt; 4 Shamrock Day    17-Mar-20 Mar-1… 3/17… 2020… 2020…     3  2020    17 March \n#&gt; 5 Bunny Day       12-Apr-20 Apr-1… 4/12… 12/4… 2020…     4  2020    12 April \n#&gt; 6 Earth Day       22-Apr-20 Apr-2… 4/22… 2020… 2020…     4  2020    22 April\n\nWhich columns were specified as Dates? In this example, none of the columns have the &lt;date&gt; specification: all of the date columns are read in as character variables.\n\n12.1.1 From &lt;chr&gt; to &lt;date&gt;\n\nWe will use the dmy() series of functions in lubridate to convert character variables to dates.\nThere are a series of dmy()-type variables, each corresponding to a different Day-Month-Year order.\n\n\ndmy() is used to parse a date from a character vector that has the day first, month second, and year last.\n\nymd() is used to parse a date that has year first, month second, and date last\n\nydm() is used to parse a date that has year first, day second, and month last,….\n\nand dym(), mdy(), and myd() work similarly. lubridate is usually “smart” and picks up dates in all kinds of different formats (e.g. it can pick up specifying October as the month and Oct as the month and 10 as the month).\n\n\n\n\n\n\nImportant\n\n\n\nWe will typically pair these lubridate functions with a mutate() statement: much like the forcats functions, we are almost always creating a new variable.\n\n\nLet’s try it out on Date1 and Date2:\n\nholiday_df |&gt; mutate(Date_test = dmy(Date1)) |&gt;\n  select(Date_test, everything())\n#&gt; # A tibble: 6 × 11\n#&gt;   Date_test  Holiday   Date1 Date2 Date3 Date4 Date5 Month  Year   Day Month2\n#&gt;   &lt;date&gt;     &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; \n#&gt; 1 2020-01-01 New Year… 1-Ja… Jan-… 1/1/… 1/1/… 2020…     1  2020     1 Janua…\n#&gt; 2 2020-02-02 Groundho… 2-Fe… Feb-… 2/2/… 2/2/… 2020…     2  2020     2 Febru…\n#&gt; 3 2020-02-14 Valentin… 14-F… Feb-… 2/14… 2020… 2020…     2  2020    14 Febru…\n#&gt; 4 2020-03-17 Shamrock… 17-M… Mar-… 3/17… 2020… 2020…     3  2020    17 March \n#&gt; 5 2020-04-12 Bunny Day 12-A… Apr-… 4/12… 12/4… 2020…     4  2020    12 April \n#&gt; 6 2020-04-22 Earth Day 22-A… Apr-… 4/22… 2020… 2020…     4  2020    22 April\nholiday_df |&gt; mutate(Date_test = mdy(Date2)) |&gt;\n  select(Date_test, everything())\n#&gt; # A tibble: 6 × 11\n#&gt;   Date_test  Holiday   Date1 Date2 Date3 Date4 Date5 Month  Year   Day Month2\n#&gt;   &lt;date&gt;     &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; \n#&gt; 1 2020-01-01 New Year… 1-Ja… Jan-… 1/1/… 1/1/… 2020…     1  2020     1 Janua…\n#&gt; 2 2020-02-02 Groundho… 2-Fe… Feb-… 2/2/… 2/2/… 2020…     2  2020     2 Febru…\n#&gt; 3 2020-02-14 Valentin… 14-F… Feb-… 2/14… 2020… 2020…     2  2020    14 Febru…\n#&gt; 4 2020-03-17 Shamrock… 17-M… Mar-… 3/17… 2020… 2020…     3  2020    17 March \n#&gt; 5 2020-04-12 Bunny Day 12-A… Apr-… 4/12… 12/4… 2020…     4  2020    12 April \n#&gt; 6 2020-04-22 Earth Day 22-A… Apr-… 4/22… 2020… 2020…     4  2020    22 April\n\nA Reminder: Why do &lt;date&gt; objects even matter? Compare the following two plots: one made where the date is in &lt;chr&gt; form and the other where date is in its appropriate &lt;date&gt; form.\n\nggplot(data = holiday_df, aes(x = Date1, y = Holiday)) +\n  geom_point()\n\n\n\n\nholiday_df &lt;- holiday_df |&gt; mutate(Date_test_plot = dmy(Date1)) |&gt;\n  select(Date_test_plot, everything())\nggplot(data = holiday_df, aes(x = Date_test_plot, y = Holiday)) +\n  geom_point()\n\n\n\n\nIn which plot does the ordering on the x-axis make more sense?\n\n12.1.2 Making a &lt;date&gt; variable from Date Components\nAnother way to create a Date object is to assemble it with make_date() from a month, day, and year components, each stored in a separate column:\n\nholiday_df |&gt; mutate(Date_test2 = make_date(year = Year,\n                                             month = Month,\n                                             day = Day)) |&gt;\n  select(Date_test2, everything())\n#&gt; # A tibble: 6 × 12\n#&gt;   Date_test2 Date_test_plot Holiday Date1 Date2 Date3 Date4 Date5 Month  Year\n#&gt;   &lt;date&gt;     &lt;date&gt;         &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 2020-01-01 2020-01-01     New Ye… 1-Ja… Jan-… 1/1/… 1/1/… 2020…     1  2020\n#&gt; 2 2020-02-02 2020-02-02     Ground… 2-Fe… Feb-… 2/2/… 2/2/… 2020…     2  2020\n#&gt; 3 2020-02-14 2020-02-14     Valent… 14-F… Feb-… 2/14… 2020… 2020…     2  2020\n#&gt; 4 2020-03-17 2020-03-17     Shamro… 17-M… Mar-… 3/17… 2020… 2020…     3  2020\n#&gt; 5 2020-04-12 2020-04-12     Bunny … 12-A… Apr-… 4/12… 12/4… 2020…     4  2020\n#&gt; 6 2020-04-22 2020-04-22     Earth … 22-A… Apr-… 4/22… 2020… 2020…     4  2020\n#&gt; # ℹ 2 more variables: Day &lt;dbl&gt;, Month2 &lt;chr&gt;\n\nBut, when Month is stored as a character (e.g. February) instead of a number (e.g. 2), problems arise with the make_date() function:\n\nholiday_df |&gt; mutate(Date_test2 = make_date(year = Year,\n                                             month = Month2,\n                                             day = Day)) |&gt;\n  select(Date_test2, everything())\n#&gt; # A tibble: 6 × 12\n#&gt;   Date_test2 Date_test_plot Holiday Date1 Date2 Date3 Date4 Date5 Month  Year\n#&gt;   &lt;date&gt;     &lt;date&gt;         &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 NA         2020-01-01     New Ye… 1-Ja… Jan-… 1/1/… 1/1/… 2020…     1  2020\n#&gt; 2 NA         2020-02-02     Ground… 2-Fe… Feb-… 2/2/… 2/2/… 2020…     2  2020\n#&gt; 3 NA         2020-02-14     Valent… 14-F… Feb-… 2/14… 2020… 2020…     2  2020\n#&gt; 4 NA         2020-03-17     Shamro… 17-M… Mar-… 3/17… 2020… 2020…     3  2020\n#&gt; 5 NA         2020-04-12     Bunny … 12-A… Apr-… 4/12… 12/4… 2020…     4  2020\n#&gt; 6 NA         2020-04-22     Earth … 22-A… Apr-… 4/22… 2020… 2020…     4  2020\n#&gt; # ℹ 2 more variables: Day &lt;dbl&gt;, Month2 &lt;chr&gt;\n\nSo the make_date() function requires a specific format for the year, month, and day columns. It may take a little pre-processing to put a particular data set in that format.\nExercise 1. What’s the issue with trying to convert Date4 to a &lt;date&gt; form? You may want to investigate Date4 further to answer this question.\n\nholiday_df |&gt; mutate(Date_test = ymd(Date4)) |&gt;\n  select(Date_test, everything())\n#&gt; # A tibble: 6 × 12\n#&gt;   Date_test  Date_test_plot Holiday Date1 Date2 Date3 Date4 Date5 Month  Year\n#&gt;   &lt;date&gt;     &lt;date&gt;         &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 2001-01-20 2020-01-01     New Ye… 1-Ja… Jan-… 1/1/… 1/1/… 2020…     1  2020\n#&gt; 2 2002-02-20 2020-02-02     Ground… 2-Fe… Feb-… 2/2/… 2/2/… 2020…     2  2020\n#&gt; 3 NA         2020-02-14     Valent… 14-F… Feb-… 2/14… 2020… 2020…     2  2020\n#&gt; 4 NA         2020-03-17     Shamro… 17-M… Mar-… 3/17… 2020… 2020…     3  2020\n#&gt; 5 2012-04-20 2020-04-12     Bunny … 12-A… Apr-… 4/12… 12/4… 2020…     4  2020\n#&gt; 6 NA         2020-04-22     Earth … 22-A… Apr-… 4/22… 2020… 2020…     4  2020\n#&gt; # ℹ 2 more variables: Day &lt;dbl&gt;, Month2 &lt;chr&gt;\n\nExercise 2. Practice converting Date3 and Date5 to &lt;date&gt; variables with lubridate functions."
  },
  {
    "objectID": "11-lubridate.html#functions-for-date-variables",
    "href": "11-lubridate.html#functions-for-date-variables",
    "title": "12  Dates with lubridate",
    "section": "\n12.2 Functions for <date> Variables",
    "text": "12.2 Functions for &lt;date&gt; Variables\nOnce an object is in the &lt;date&gt; format, there are some special functions in lubridate that can be used on that date variable. To investigate some of these functions, we will pull stock market data from Yahoo using the quantmod package. Install the package, and run the following code, which gets stock market price data on Apple, Nintendo, Chipotle, and the S & P 500 Index from 2011 to now.\n\n\n\n\n\n\nNote\n\n\n\nWe have the ability to understand all of the code below, but we will skip over this code for now to focus more on the new information in this section (information about date functions).\n\n\n\n## install.packages(\"quantmod\")\nlibrary(quantmod)\n\nstart &lt;- ymd(\"2011-01-01\")\nend &lt;- ymd(\"2021-5-19\")\ngetSymbols(c(\"AAPL\", \"NTDOY\", \"CMG\", \"SPY\"), src = \"yahoo\",\n           from = start, to = end)\n#&gt; [1] \"AAPL\"  \"NTDOY\" \"CMG\"   \"SPY\"\n\ndate_tib &lt;- as_tibble(index(AAPL)) |&gt;\n  rename(start_date = value)\napp_tib &lt;- as_tibble(AAPL)\nnint_tib &lt;- as_tibble(NTDOY)\nchip_tib &lt;- as_tibble(CMG)\nspy_tib &lt;- as_tibble(SPY)\nall_stocks &lt;- bind_cols(date_tib, app_tib, nint_tib, chip_tib, spy_tib)\n\nstocks_long &lt;- all_stocks |&gt;\n  select(start_date, AAPL.Adjusted, NTDOY.Adjusted,\n                      CMG.Adjusted, SPY.Adjusted) |&gt;\n  pivot_longer(2:5, names_to = \"Stock_Type\", values_to = \"Price\") |&gt;\n  mutate(Stock_Type = fct_recode(Stock_Type,\n                                 Apple = \"AAPL.Adjusted\",\n                                 Nintendo = \"NTDOY.Adjusted\",\n                                 Chipotle = \"CMG.Adjusted\",\n                                 `S & P 500` = \"SPY.Adjusted\"\n                                 ))\ntail(stocks_long)\n#&gt; # A tibble: 6 × 3\n#&gt;   start_date Stock_Type  Price\n#&gt;   &lt;date&gt;     &lt;fct&gt;       &lt;dbl&gt;\n#&gt; 1 2021-05-17 Chipotle   1332. \n#&gt; 2 2021-05-17 S & P 500   399. \n#&gt; 3 2021-05-18 Apple       123. \n#&gt; 4 2021-05-18 Nintendo     14.1\n#&gt; 5 2021-05-18 Chipotle   1325. \n#&gt; 6 2021-05-18 S & P 500   395.\n\nYou’ll have a chance in class to choose your own stocks to investigate. For now, we’ve made a data set with three variables:\n\n\nstart_date, the opening date for the stock market\n\nStock_Type, a factor with 4 levels: Apple, Nintendo, Chipotle, and S & P 500\n\n\nPrice, the price of the stock?\n\nFirst, let’s make a line plot that shows how the S & P 500 has changed over time:\n\nstocks_sp &lt;- stocks_long |&gt; filter(Stock_Type == \"S & P 500\")\nggplot(data = stocks_sp, aes(x = start_date, y = Price)) +\n  geom_line()\n\n\n\n\nBut, there’s other information that we can get from the start_date variable. We might be interested in things like day of the week, monthly trends, or yearly trends. To extract variables like “weekday” and “month” from a &lt;date&gt; variable, there are a series of functions that are fairly straightforward to use. We will discuss the year() month(), mday(), yday(), and wday() functions.\n\n12.2.1 year(), month(), and mday()\n\nThe functions year(), month(), and mday() can grab the year, month, and day of the month, respectively, from a &lt;date&gt; variable.\n\n\n\n\n\n\nNote\n\n\n\nLike the forcats functions and the earlier lubridate functions, the year(), month(), and mday() functions will almost always be paired with a mutate() statement because they will create a new variable.\n\n\n\nstocks_long |&gt; mutate(year_stock = year(start_date))\nstocks_long |&gt; mutate(month_stock = month(start_date))\nstocks_long |&gt; mutate(day_stock = mday(start_date))\n\n\n12.2.2 yday() and wday()\n\nThe yday() function grabs the day of the year from a &lt;date&gt; object. For example,\n\ntest &lt;- mdy(\"November 4, 2020\")\nyday(test)\n#&gt; [1] 309\n\nreturns 309, indicating that November 4th is the 309th day of the year 2020. Using this function in a mutate() statement creates a new variable that has yday for each observation:\n\nstocks_long |&gt; mutate(day_in_year = yday(start_date))\n#&gt; # A tibble: 10,444 × 4\n#&gt;   start_date Stock_Type  Price day_in_year\n#&gt;   &lt;date&gt;     &lt;fct&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1 2011-01-03 Apple        9.96           3\n#&gt; 2 2011-01-03 Nintendo     7.34           3\n#&gt; 3 2011-01-03 Chipotle   224.             3\n#&gt; 4 2011-01-03 S & P 500   99.6            3\n#&gt; 5 2011-01-04 Apple       10.0            4\n#&gt; 6 2011-01-04 Nintendo     7.1            4\n#&gt; # ℹ 10,438 more rows\n\nFinally, the function wday() grabs the day of the week from a &lt;date&gt;. By default, wday() puts the day of the week as a numeric, but I find this confusing, as I can’t ever remember whether a 1 means Sunday or a 1 means Monday. Adding, label = TRUE creates the weekday variable as Sunday, Monday, Tuesday, etc.:\n\nstocks_long |&gt; mutate(day_of_week = wday(start_date))\nstocks_long |&gt; mutate(day_of_week = wday(start_date,\n                                          label = TRUE, abbr = FALSE))\n\nPossible uses for these functions are:\n\nwe want to look at differences between years (with year())\nwe want to look at differences between months (with month())\nwe want to look at differences between days of the week (with wday())\nwe want to see whether there are yearly trends within years (with yday())\n\n\n\n\n\n\n\nNote\n\n\n\nWorking with times is extremely similar to working with dates. Instead of ymd(), mdy(), etc., you tack on a few extra letters to specify the order that the hour, minute, and seconds appear in the variable: ymd_hms() converts a character vector that has the order year, month, day, hour, minute, second to a &lt;datetime&gt;.\nAdditionally, the functions hour(), minute(), and second() grab the hour, minute, and second from a &lt;datetime&gt; variable.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThings can get complicated with dates and times, especially if you start to consider things like time duration. Consider how the following might affect an analysis involving time duration:\n\ntime zones\nleap years (not all years have the same number of days)\ndiffering number of days in a given month\ndaylight saving time (not all days have the same number of hours)\n\n\n\nExercise 3. The month() function gives the numbers corresponding to each month by default. Type ?month and figure out which argument you would need to change to get the names (January, February, etc.) instead of the month numbers. What about the abbreviations (Jan, Feb, etc.) of each month instead of the month numbers? Try making the changes in the mutate() statement below.\n\nstocks_long |&gt; mutate(month_stock = month(start_date))"
  },
  {
    "objectID": "11-lubridate.html#practice",
    "href": "11-lubridate.html#practice",
    "title": "12  Dates with lubridate",
    "section": "\n12.3 Practice",
    "text": "12.3 Practice\n\n12.3.1 Class Exercises\nClass Exercise 1. The truncated argument to ymd(), dmy(), mdy(), etc. will allow R to parse dates that aren’t actually complete. For example,\n\nlibrary(lubridate)\nymd(\"2019\", truncated = 2)\n#&gt; [1] \"2019-01-01\"\n\nparses 2019 to be January 1, 2019 when the month and day are missing. The 2 means that the last two parts of the date (in this case, month and day) are allowed to be missing. Similarly,\n\ndmy(\"19-10\", truncated = 1)\n#&gt; [1] \"0000-10-19\"\n\ntruncates the year (which is given as 0000). The truncate function is usually most useful in the context of the first example with a truncated month and/or day.\nExamine the ds_google.csv, which contains\n\n\nMonth, the year and month from 2004 to now\n\nData_Science, the relative popularity of data science (Google keeps how it calculates “popularity” as somewhat of a mystery but it is likely based off of the number of times people search for the term “Data Science”)\n\n\nlibrary(tidyverse)\nlibrary(lubridate)\nds_df &lt;- read_csv(here(\"data/ds_google.csv\"))\nds_df\n#&gt; # A tibble: 202 × 2\n#&gt;   Month   Data_Science\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 2004-01           14\n#&gt; 2 2004-02            8\n#&gt; 3 2004-03           16\n#&gt; 4 2004-04           11\n#&gt; 5 2004-05            5\n#&gt; 6 2004-06            8\n#&gt; # ℹ 196 more rows\n\nUse a lubridate function with the truncated option to convert the Month variable to be in the &lt;date&gt; format.\nClass Exercise 2. Make a plot of the popularity of Data Science through Time. Add a smoother to your plot. What patterns do you notice?\nClass Exercise 3. Modify the code in the tutorial section on the Stocks data to get a data frame on stock prices for the now infamous Gamestop stock. Construct a line plot of the price through time.\nClass Exercise 4. Use the lag() function to create a new variable that is the previous day’s stock price. Can you predict the current stock price based on the previous day’s stock price accurately? Why or why not? Use either graphical or numerical evidence.\n\n12.3.2 Your Turn\nThe data in the Class Exercises was obtained from Google Trends: Google Trends. Google Trends is incredibly cool to explore, even without R.\nYour Turn 1. On Google Trends, Enter in a search term, and change the Time dropdown menu to be 2004-present. Then, enter in a second search term that you want to compare. You can also change the country if you want to (or, you can keep the country as United States).\n\n\n\n\n\n\nNote\n\n\n\nMy search terms will be “super smash” and “animal crossing”, but yours should be something that interests you!\n\n\nIn the top-right window of the graph, you should click the down arrow to download the data set. Delete the first two rows of your data set (either in Excel or R), read in the data set, and change the date variable so that it’s in a Date format.\nYour Turn 2. Make a plot of your Popularity variables through time.\n\n\n\n\n\n\nHint (only if you get stuck)\n\n\n\n\n\nYou will need to use a function from tidyr to tidy the data set first.\n\n\n\nYour Turn 3. Using your data set that explored a variable or two from 2004 through now, make a table of the average popularity for each year.\n\n\n\n\n\n\nHint (only if you get stuck)\n\n\n\n\n\nYou’ll need a lubridate function to extract the year variable from the date object.\n\n\n\nYour Turn 4. Clear your search and now enter a search term that you’d like to investigate for the past 90 days. Mine will be “Pittsburgh Steelers” but, again, yours should be something that interests you.\nAgain, click the download button again and read in the data to R. Convert the date variable to be in &lt;date&gt; format.\nYour Turn 5. Make a plot of your popularity variable through time, adding a smoother.\nYour Turn 6. Using your data set that explored a variable from the past 90 days, construct a table that compares the average popularity on each day of the week (Monday through Saturday).\nYour Turn 7. Examine the ds_df data set again, the data set on data science in Google Trends, and suppose that you have an observation for each day of every year (not just one observation per month). You want to look at whether data science is more popular on certain days of the week. Explain why the following strategy wouldn’t really work that well.\n\ncreate a weekday variable with wday()\n\nuse summarise() and group_by() to find the average popularity for each day of the week"
  },
  {
    "objectID": "12-stringr.html#text-analysis",
    "href": "12-stringr.html#text-analysis",
    "title": "13  Text Data with tidytext and stringr",
    "section": "\n13.1 Text Analysis",
    "text": "13.1 Text Analysis\nBeyonce is a legend. For this example, we will work through a text analysis on lyrics from songs from Beyonce’s albums, utilizing functions from both stringr to parse strings and tidytext to convert text data into a tidy format. To begin, read in a data set of Beyonce’s lyrics:\n\nlibrary(tidyverse)\nlibrary(here)\nbeyonce &lt;- read_csv(here(\"data/beyonce_lyrics.csv\"))\nhead(beyonce)\n\nWe will be most focused on the line variable, as each value for this variable contains a line from a Beyonce song. There’s other variables present as well, such as the song_name and the artist_name (the data set originally came from a data set with artists other than Beyonce).\nYou can look at the first 4 values of line with\n\nbeyonce$line[1:4]\n#&gt; [1] \"If I ain't got nothing, I got you\"                       \n#&gt; [2] \"If I ain't got something, I don't give a damn\"           \n#&gt; [3] \"'Cause I got it with you\"                                \n#&gt; [4] \"I don't know much about algebra, but I know 1+1 equals 2\"\n\nOur end goal is to construct a plot that shows the most popular words in Beyonce’s albums. This is much more challenging than it sounds because we will have to deal with the nuances of working with text data.\nThe tidytext package makes it a lot easier to work with text data in many regards. Let’s use the unnest_tokens() functions from tidytext to separate the lines into individual words. We’ll name this new data set beyonce_unnest:\n\nlibrary(tidytext)\nbeyonce_unnest &lt;- beyonce |&gt; unnest_tokens(output = \"word\", input = \"line\")\nbeyonce_unnest\n#&gt; # A tibble: 164,740 × 6\n#&gt;   song_id song_name artist_id artist_name song_line word   \n#&gt;     &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;  \n#&gt; 1   50396 1+1             498 Beyoncé             1 if     \n#&gt; 2   50396 1+1             498 Beyoncé             1 i      \n#&gt; 3   50396 1+1             498 Beyoncé             1 ain't  \n#&gt; 4   50396 1+1             498 Beyoncé             1 got    \n#&gt; 5   50396 1+1             498 Beyoncé             1 nothing\n#&gt; 6   50396 1+1             498 Beyoncé             1 i      \n#&gt; # ℹ 164,734 more rows\n\nWe’ll want to make sure that either all words are capitalized or no words are capitalized, for consistency (remember that R is case-sensitive). To that end, we’ll modify the word variable and use stringr’s str_to_lower() to change all letters to lower-case:\n\nbeyonce_unnest &lt;- beyonce_unnest |&gt; mutate(word = str_to_lower(word))\n\nLet’s try counting up Beyonce’s most popular words from the data set we just made:\n\nbeyonce_unnest |&gt; group_by(word) |&gt;\n  summarise(n = n()) |&gt;\n  arrange(desc(n))\n#&gt; # A tibble: 6,469 × 2\n#&gt;   word      n\n#&gt;   &lt;chr&gt; &lt;int&gt;\n#&gt; 1 you    7693\n#&gt; 2 i      6669\n#&gt; 3 the    4719\n#&gt; 4 me     3774\n#&gt; 5 to     3070\n#&gt; 6 it     2999\n#&gt; # ℹ 6,463 more rows\n\nWhat’s the issue here?\nTo remedy this, we can use what are called stop words: words that are very common and carry little to no meaningful information. For example the, it, are, etc. are all stop words. We need to eliminate these from the data set before we continue on. Luckily, the tidytext package also provides a data set of common stop words in a data set named stop_words:\n\nhead(stop_words)\n#&gt; # A tibble: 6 × 2\n#&gt;   word      lexicon\n#&gt;   &lt;chr&gt;     &lt;chr&gt;  \n#&gt; 1 a         SMART  \n#&gt; 2 a's       SMART  \n#&gt; 3 able      SMART  \n#&gt; 4 about     SMART  \n#&gt; 5 above     SMART  \n#&gt; 6 according SMART\n\nLet’s join the Beyonce lyrics data set to the stop words data set and elminate any stop words:\n\nbeyonce_stop &lt;- anti_join(beyonce_unnest, stop_words, by = join_by(word == word))\n\nThen, we can re-make the table with the stop words removed:\n\nbeyonce_sum &lt;- beyonce_stop |&gt; group_by(word) |&gt;\n  summarise(n = n()) |&gt;\n  arrange(desc(n)) |&gt;\n  print(n = 25)\n#&gt; # A tibble: 5,937 × 2\n#&gt;    word        n\n#&gt;    &lt;chr&gt;   &lt;int&gt;\n#&gt;  1 love     1362\n#&gt;  2 baby     1024\n#&gt;  3 girl      592\n#&gt;  4 wanna     564\n#&gt;  5 hey       499\n#&gt;  6 boy       494\n#&gt;  7 yeah      491\n#&gt;  8 feel      488\n#&gt;  9 time      452\n#&gt; 10 uh        408\n#&gt; 11 halo      383\n#&gt; 12 check     366\n#&gt; 13 tonight   342\n#&gt; 14 girls     341\n#&gt; 15 ya        327\n#&gt; 16 run       325\n#&gt; 17 crazy     308\n#&gt; 18 world     301\n#&gt; 19 body      287\n#&gt; 20 ooh       281\n#&gt; 21 ladies    269\n#&gt; 22 top       241\n#&gt; 23 gotta     240\n#&gt; 24 beyoncé   238\n#&gt; 25 night     213\n#&gt; # ℹ 5,912 more rows\nbeyonce_sum\n#&gt; # A tibble: 5,937 × 2\n#&gt;   word      n\n#&gt;   &lt;chr&gt; &lt;int&gt;\n#&gt; 1 love   1362\n#&gt; 2 baby   1024\n#&gt; 3 girl    592\n#&gt; 4 wanna   564\n#&gt; 5 hey     499\n#&gt; 6 boy     494\n#&gt; # ℹ 5,931 more rows\n\nLooking through the list, there are still some stop words in there that were not picked up on in the stop_words data set.\nExercise 1. Look at the remaining words. Do any of them look like stop words that were missed with the stop words from the tidytext package? Create a tibble with a few of the remaining stop words (like ooh, gotta, ya, uh, and yeah) not picked up by the tidytext package, and use a join function to drop these words from the data set.\n\n\n\n\n\n\nHint (only if you get stuck)\n\n\n\n\n\nThe join function you will need to use here is anti_join().\n\n\n\nExercise 2. With the new data set, construct a lollipop plot or a bar plot that shows the 20 most common words Beyonce uses, as well as the number of times each word is used.\nExercise 3. Use the wordcloud() function in the wordcloud library and the code below to make a wordcloud of Beyonce’s words.\n\n## install.packages(\"wordcloud\")\nlibrary(wordcloud)\nbeyonce_small &lt;- beyonce_sum |&gt; filter(n &gt; 50)\nwordcloud(beyonce_small$word, beyonce_small$n, \n          colors = brewer.pal(8, \"Dark2\"), scale = c(5, .2),\n          random.order = FALSE, random.color = FALSE)\n\nThere is not anything else you need to do for this exercise: just make the word cloud!\n\n\n\n\n\n\nNote\n\n\n\nIf you want to delve into text data more, you’ll need to learn about regular expressions , or regexes. If interested, you can read more in the R4DS textbook. Starting out is not too bad, but learning about escaping special characters in R can be much more challenging!\n\n\nWe analyzed a short text data set, but, you can imagine extending this type of analysis to things like:\n\nsong lyrics, if you have the lyrics to all of the songs from an artist https://rpubs.com/RosieB/taylorswiftlyricanalysis\n\nbook analysis, if you have the text of an entire book or series of books\ntv analysis, if you have the scripts to all episodes of a tv show\n\nIf you were doing one of these analyses, there are lots of cool functions in tidytext to help you out! We will do one more example, this time looking at Donald Trump’s twitter account in 2016."
  },
  {
    "objectID": "12-stringr.html#introduction-to-stringr",
    "href": "12-stringr.html#introduction-to-stringr",
    "title": "13  Text Data with tidytext and stringr",
    "section": "\n13.2 Introduction to stringr\n",
    "text": "13.2 Introduction to stringr\n\nIn the previous examples, the string data that we had consisted primarily of words. The tools in tidytext make working with data consisting of words not too painful. However, some data exists as strings that are not words. For a non-trivial example, consider data sets obtained from https://github.com/JeffSackmann/tennis_MatchChartingProject, a repository for professional tennis match charting put together by Jeff Sackmann. Some of the following code was modified from a project completed by James Wolpe in a data visualization course.\nFrom this repository, I have put together a data set on one particular tennis match to make it a bit easier for us to get started. The match I have chosen is the 2021 U.S. Open Final between Daniil Medvedev and Novak Djokovic. Why this match? This was arguably the most important match of Djokovic’s career: if he won, he would win all four grand slams in a calendar year. I don’t like Djokovic and he lost so looking back at the match brings me joy. Read in the data set with:\n\nlibrary(here)\nlibrary(tidyverse)\nmed_djok_df &lt;- read_csv(here(\"data/med_djok.csv\"))\nmed_djok_df\n#&gt; # A tibble: 182 × 46\n#&gt;   point      Serving match_id    Pt  Set1  Set2   Gm1   Gm2 Pts   `Gm#` TbSet\n#&gt;   &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;\n#&gt; 1 4f2d@      ND      2021091…     1     0     0     0     0 0-0   1 (1)     1\n#&gt; 2 6d         ND      2021091…     2     0     0     0     0 15-0  1 (2)     1\n#&gt; 3 6b29f3b3b… ND      2021091…     3     0     0     0     0 15-15 1 (3)     1\n#&gt; 4 4b28f1f2f… ND      2021091…     4     0     0     0     0 30-15 1 (4)     1\n#&gt; 5 5b37b3b3b… ND      2021091…     5     0     0     0     0 40-15 1 (5)     1\n#&gt; 6 6f28f1f1f… ND      2021091…     6     0     0     0     0 40-30 1 (6)     1\n#&gt; # ℹ 176 more rows\n#&gt; # ℹ 35 more variables: `TB?` &lt;dbl&gt;, TBpt &lt;lgl&gt;, Svr &lt;dbl&gt;, Ret &lt;dbl&gt;,\n#&gt; #   `1st` &lt;chr&gt;, `2nd` &lt;chr&gt;, Notes &lt;chr&gt;, `1stSV` &lt;dbl&gt;, `2ndSV` &lt;dbl&gt;,\n#&gt; #   `1stIn` &lt;dbl&gt;, `2ndIn` &lt;dbl&gt;, isAce &lt;lgl&gt;, isUnret &lt;lgl&gt;,\n#&gt; #   isRallyWinner &lt;lgl&gt;, isForced &lt;lgl&gt;, isUnforced &lt;lgl&gt;, isDouble &lt;lgl&gt;,\n#&gt; #   PtWinner &lt;dbl&gt;, isSvrWinner &lt;dbl&gt;, rallyCount &lt;dbl&gt;, `Player 1` &lt;chr&gt;,\n#&gt; #   `Player 2` &lt;chr&gt;, `Pl 1 hand` &lt;chr&gt;, `Pl 2 hand` &lt;chr&gt;, Gender &lt;chr&gt;, …\n\nThe observations of the data set correspond to points played (so there is one row per point). There are a ton of variables in this data set, but the most important variable is the first variable, point, which contains a string with information about the types of shots that were played during the point. The coding of the point variable includes:\n\n\n4 for serve out wide, 5 for serve into the body, and 6 for a serve “down the t (center)”.\n\nf for forehand stroke, b for backhand stroke.\n\n1 to a right-hander’s forehand side, 2 for down the middle of the court, and 3 to a right-hander’s backhand side.\n\nd for a ball hit deep, w for a ball hit wide, and n for a ball hit into the net\n\n@ symbol at the end if the point ended in an unforced error\nand there’s lots of other numbers and symbols that correspond to other things (volleys, return depths, hitting the top of the net, etc.)\n\nFor example, Djokovic served the 7th point of the match, which has a point value of 4f18f1f2b3b2f1w@. This reads that\n\n\n4: Djokovic served out wide,\n\nf18: Medvedev hit a forehand cross-court to Djokovic’s forehand side\n\nf1: Djokovic hit a forehand cross-court to Medvedev’s forehand side\n\nf2: Medvedev hit a forehand to the center of the court\n\nb3: Djokovic hit a backhand to Medvedev’s backhand side\n\nb2: Medvedev hit a backhand to the center of the court\n\nf1w@: Djokovic hit a forehand to Medvedev’s forehand side, but the shot landed wide and was recorded as an unforced error.\n\nClearly, there is a lot of data encoded in the point variable. We are going to introduce stringr by answering a relatively simple question: what are the serving patterns of Medvedev and Djokovic during this match?\n\n13.2.1 Regular Expressions\nA regex, or regular expression, is a string used to identify particular patterns in string data.\n\n\n\n\n\n\nNote\n\n\n\nRegular expressions are used in many languages (so, if you google something about a regular expression, you do not need to limit yourself to just looking at resources pertaining to R).\n\n\nRegex’s can be used with the functions in the stringr package.\n\n\n\n\n\n\nNote\n\n\n\nThe functions in the stringr package begin with str_(), much like the functions in forcats began with fct_().\n\n\nWe will first focus on the str_detect() function, which detects whether a particular regex is present in a string variable. str_detect() takes the name of the string as its first argument and the regex as the second argument. For example,\n\nstr_detect(med_djok_df$point, pattern = \"f\")\n#&gt;   [1]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n#&gt;  [13]  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n#&gt;  [25]  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE\n#&gt;  [37]  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE\n#&gt;  [49] FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE FALSE\n#&gt;  [61]  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n#&gt;  [73] FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE\n#&gt;  [85]  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n#&gt;  [97]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n#&gt; [109] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE  TRUE\n#&gt; [121]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE\n#&gt; [133]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE\n#&gt; [145] FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE\n#&gt; [157]  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE\n#&gt; [169] FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n#&gt; [181] FALSE FALSE\n\nreturns a TRUE if the letter f appears anywhere in the string and a FALSE if it does not. So, we can examine how many points a forehand was hit in the Medvedev-Djokovic match. As a second example,\n\nstr_detect(med_djok_df$point, pattern = \"d@\")\n\nreturns TRUE if d@ appears in a string and FALSE if not. Note that d@ must appear together and in that order to return a TRUE. This lets us examine how many points a ball is hit deep and is recorded an unforced error. It looks like\n\nsum(str_detect(med_djok_df$point, pattern = \"d@\"))\n#&gt; [1] 21\n\npoints ended in an unforced error where the ball was hit deep,\n\nsum(str_detect(med_djok_df$point, pattern = \"w@\"))\n#&gt; [1] 19\n\npoints ended in an unforced error where the ball was hit wide, and\n\nsum(str_detect(med_djok_df$point, pattern = \"n@\"))\n#&gt; [1] 22\n\npoints ended in an unforced error where the ball was hit into the net.\n\n13.2.2 stringr Functions with dplyr\n\nWe can combine the stringr functions with dplyr functions that we already know and love. For example, if we are only interested in points the end in an unforced error (so points that have the @ symbol in them), we can filter out the points that don’t have an @:\n\nmed_djok_df |&gt; filter(str_detect(point, pattern = \"@\") == TRUE)\n#&gt; # A tibble: 63 × 46\n#&gt;   point      Serving match_id    Pt  Set1  Set2   Gm1   Gm2 Pts   `Gm#` TbSet\n#&gt;   &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;\n#&gt; 1 4f2d@      ND      2021091…     1     0     0     0     0 0-0   1 (1)     1\n#&gt; 2 6b29f3b3b… ND      2021091…     3     0     0     0     0 15-15 1 (3)     1\n#&gt; 3 5b37b3b3b… ND      2021091…     5     0     0     0     0 40-15 1 (5)     1\n#&gt; 4 4f18f1f2b… ND      2021091…     7     0     0     0     0 40-40 1 (7)     1\n#&gt; 5 5b28f1f2f… ND      2021091…     8     0     0     0     0 40-AD 1 (8)     1\n#&gt; 6 6b27f2f2b… DM      2021091…    13     0     0     0     1 40-15 2 (5)     1\n#&gt; # ℹ 57 more rows\n#&gt; # ℹ 35 more variables: `TB?` &lt;dbl&gt;, TBpt &lt;lgl&gt;, Svr &lt;dbl&gt;, Ret &lt;dbl&gt;,\n#&gt; #   `1st` &lt;chr&gt;, `2nd` &lt;chr&gt;, Notes &lt;chr&gt;, `1stSV` &lt;dbl&gt;, `2ndSV` &lt;dbl&gt;,\n#&gt; #   `1stIn` &lt;dbl&gt;, `2ndIn` &lt;dbl&gt;, isAce &lt;lgl&gt;, isUnret &lt;lgl&gt;,\n#&gt; #   isRallyWinner &lt;lgl&gt;, isForced &lt;lgl&gt;, isUnforced &lt;lgl&gt;, isDouble &lt;lgl&gt;,\n#&gt; #   PtWinner &lt;dbl&gt;, isSvrWinner &lt;dbl&gt;, rallyCount &lt;dbl&gt;, `Player 1` &lt;chr&gt;,\n#&gt; #   `Player 2` &lt;chr&gt;, `Pl 1 hand` &lt;chr&gt;, `Pl 2 hand` &lt;chr&gt;, Gender &lt;chr&gt;, …\n\nWe can then use mutate() with case_when() to create a variable corresponding to error type and then summarise() the error types made from the two players.\n\nmed_djok_df |&gt; filter(str_detect(point, pattern = \"@\") == TRUE) |&gt;\n  mutate(error_type = case_when(str_detect(point, pattern = \"d@\") ~ \"deep error\",\n                                   str_detect(point, pattern = \"w@\") ~ \"wide error\",\n            str_detect(point, pattern = \"n@\") ~ \"net error\")) |&gt;\n  group_by(PtWinner, error_type) |&gt;\n  summarise(n_errors = n())\n#&gt; # A tibble: 7 × 3\n#&gt; # Groups:   PtWinner [2]\n#&gt;   PtWinner error_type n_errors\n#&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;int&gt;\n#&gt; 1        1 deep error        9\n#&gt; 2        1 net error         6\n#&gt; 3        1 wide error       10\n#&gt; 4        2 deep error       12\n#&gt; 5        2 net error        16\n#&gt; 6        2 wide error        9\n#&gt; # ℹ 1 more row\n\nIn the output above, a PtWinner of 1 corresponds to points that Djokovic won (and therefore points where Medvedev made the unforced error) while a PtWinner of 2 corresponds to points that Medvedev won (and therefore points where Djokovic made the unforced error). So we see that, in this match, Djokovic had more unforced errors overall. Medvedev’s unforced errors tended to be deep or wide while the highest proportion of Djokovic’s unforced errors were balls that went into the net.\nWe will explore our original “service patterns” question in the exercises. To close out this section, we will just emphasize that we have done a very simple introduction into regexes. These can get very cumbersome, especially as the patterns you want to extract get more complicated. Consider the examples below.\n\nDetect which points are aces, which are coded in the variable as *. Regexes have “special characters, like \\, *, ., which, if present in the variable need to be”escaped” with a backslash. But, the backslash is a special character, so it needs to be escaped too: so we need two \\\\ in front of * to pull the points with a *.\n\n\nstr_detect(med_djok_df$point, pattern = \"\\\\*\")\n\n\n\nDetect which points start with a 4 using ^ to denote “at the beginning”:\n\n\nstr_detect(med_djok_df$point, pattern = \"^4\")\n\n\n\nDetect which points end with an @ using $ to denote “at the end” (this is safer than what we did in the code above, where we just assumed that @ did not appear anywhere else in the string except at the end).\n\n\nstr_detect(med_djok_df$point, pattern = \"@$\")\n\n\n\nExtract all of the forehand shots that were hit with str_extract_all(). The regex here says to extract anything with an f followed by any number of digits before another non-digit symbol.\n\n\nstr_extract_all(med_djok_df$point, pattern = \"f[:digit:]+\")\n\nThe purpose of these examples is just to show that things can get more complicated with strings.\nExercise 4. Use str_detect() and the dplyr functions mutate() and case_when() to create a variable for serve_location that is either \"wide\" if the point starts with a 4, \"body\" if the point starts with a 5, and \"down the center\" if the point starts with a 6."
  },
  {
    "objectID": "12-stringr.html#practice",
    "href": "12-stringr.html#practice",
    "title": "13  Text Data with tidytext and stringr",
    "section": "\n13.3 Practice",
    "text": "13.3 Practice\n\n13.3.1 Class Exercises\nClass Exercise 1. We will use a provided .qmd file to replicate what is called a sentiment analysis on Trump’s twitter account from 2016. This analysis was used in conjunction with a major news story that hypothesized that Trump himself wrote tweets from an Android device while his campaign staff wrote tweets for him from an iPhone device. We will investigate what properties of his tweets led the author to believe this.\nThe .qmd file used for this is posted on Canvas. We will see more uses of stringr for this particular analysis. For this entire section, you should be able to follow along and understand what each line of code is doing. However, unlike other topics, you will not be expected to do a sentiment analysis on your own.\n\n13.3.2 Your Turn\nYour Turn 1. Use dplyr functions and the Serving variable to count the number of serve locations for each player. (i.e. for how many points did Medvedev hit a serve out wide?).\nYour Turn 2. Use dplyr functions, the Serving variable, and the isSrvWinner variable to find the proportion of points each player won for each of their serving locations (i.e. if Medvedev won 5 points while serving out wide and lost 3 points, the proportion would be 5 / 8 = 0.625).\n\n\n\n\n\n\nNote\n\n\n\nThe isSrvWinner variable is coded as a 1 if the serving player won the point and 0 if the serving player lost the point.\n\n\nYour Turn 3. The letters v, z, i, and k denote volleys (of different types). Use str_detect() and dplyr functions to figure out the proportion of points where a volley was hit."
  },
  {
    "objectID": "13-knn.html#introduction-to-classification",
    "href": "13-knn.html#introduction-to-classification",
    "title": "14  Predictive Modeling with knn",
    "section": "\n14.1 Introduction to Classification",
    "text": "14.1 Introduction to Classification\nk-nearest neighbors (or knn) is an introductory supervised machine learning algorithm, most commonly used as a classification algorithm. Classification refers to prediction of a categorical response variable with two or more categories. For example, for a data set with SLU students, we might be interested in predicting whether or not each student graduates in four years (so the response has two categories: graduates in 4 years or doesn’t). We might want to classify this response based on various student characteristics like anticipated major, GPA, standardized test scores, etc.\n\n\n\n\n\n\nNote\n\n\n\nknn can also be used to predict a quantitative response, but we’ll focus on categorical response variables only throughout this section.\n\n\nIf you’ve had STAT 213, you might try to draw some parallels to knn and classification using logistic regression. Note, however, that logistic regression required the response to have two levels while knn can classify a response variable that has two or more levels.\nTo introduce knn, we will be using pokemon_full.csv data. Pokemon have different Types: we will use Type as a categorical response that we are interested in predicting. For simplicity, we will only use Pokemon’s primary type and we will only use 4 different types:\n\nset.seed(1119)\nlibrary(tidyverse)\nlibrary(here)\npokemon &lt;- read_csv(here(\"data/pokemon_full.csv\")) |&gt;\n  filter(Type %in% c(\"Steel\", \"Dark\", \"Fire\", \"Ice\"))\n\nOur goal is to develop a k-nearest-neighbors model that is able to classify/predict Pokemon Type from a set of predictors, like Pokemon HP, Attack, Defense, etc.\n\n14.1.1 Training and Test Data\nIn order to develop our knn model (note that we still haven’t discussed what knn actually is yet!), we first need to discuss terms that apply to almost all predictive/classification modeling: training and test data. A training data set is a subset of the full data set used to fit various models. For the example below, the training data set will be just 15 observations for pedagogical purposes. More commonly, the training data set will contain 50%-80% of the observations in the full data set.\nA test data set consists of the remaining 20%-50% of the observations not in the training data set. A test data set is used to assess different the performances of various models that were fit using the training data set.\n\n\n\n\n\n\nImportant\n\n\n\nWhy do we need to do this division? Using the full data set for both training a model and testing that model is “cheating:” the model will perform better than it should because we are using each observation twice: once for fitting and once for testing. Having a separate test data set that wasn’t used to fit the model gives the model a more “fair” test, as these observations are supposed to be new data that the model hasn’t yet seen.\n\n\nThe following code uses that slice_sample() function to randomly select 15 observations to be in the training data set. anti_join() then makes a test data set without the 15 pokemon in the training data set.\n\ntrain_sample &lt;- pokemon |&gt;\n  slice_sample(n = 15)\ntest_sample &lt;- anti_join(pokemon, train_sample)\n\ntrain_sample |&gt; head()\n#&gt; # A tibble: 6 × 14\n#&gt;    ...1 Name      Type     HP Attack Defense Speed SpAtk SpDef Generation\n#&gt;   &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1   491 Darkrai   Dark     70     90      90   125   135    90          4\n#&gt; 2   136 Flareon   Fire     65    130      60    65    95   110          1\n#&gt; 3   571 Zoroark   Dark     60    105      60   105   120    60          5\n#&gt; 4   221 Piloswine Ice     100    100      80    50    60    60          2\n#&gt; 5   668 Pyroar    Fire     86     68      72   106   109    66          6\n#&gt; 6   262 Mightyena Dark     70     90      70    70    60    60          3\n#&gt; # ℹ 4 more variables: Legendary &lt;lgl&gt;, height &lt;dbl&gt;, weight &lt;dbl&gt;,\n#&gt; #   base_experience &lt;dbl&gt;\ntest_sample |&gt; head()\n#&gt; # A tibble: 6 × 14\n#&gt;    ...1 Name       Type     HP Attack Defense Speed SpAtk SpDef Generation\n#&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1     4 Charmander Fire     39     52      43    65    60    50          1\n#&gt; 2     5 Charmeleon Fire     58     64      58    80    80    65          1\n#&gt; 3    37 Vulpix     Fire     38     41      40    65    50    65          1\n#&gt; 4    38 Ninetales  Fire     73     76      75   100    81   100          1\n#&gt; 5    58 Growlithe  Fire     55     70      45    60    70    50          1\n#&gt; 6    59 Arcanine   Fire     90    110      80    95   100    80          1\n#&gt; # ℹ 4 more variables: Legendary &lt;lgl&gt;, height &lt;dbl&gt;, weight &lt;dbl&gt;,\n#&gt; #   base_experience &lt;dbl&gt;\n\nThe ideas of a training data set and test data set are pervasive in predictive and classification models, including models not related to knn.\n\n\n\n\n\n\nNote\n\n\n\nWe are using the training vs. test sample method in this class because it’s the simplest: if we wanted to take this a step further, we’d repeat the training and test process 5 or 10 times, using what’s known as k-fold cross-validation.\n\n\n\nExplain what anti_join() joins on when by isn’t specified and why not specifying a by argument works for this example."
  },
  {
    "objectID": "13-knn.html#knn-introduction",
    "href": "13-knn.html#knn-introduction",
    "title": "14  Predictive Modeling with knn",
    "section": "\n14.2 knn Introduction",
    "text": "14.2 knn Introduction\n\n14.2.1 knn with k = 1 and 1 Predictor\nSuppose that we have just those 15 pokemon in our training data set. We want to predict Type from just one predictor, Defense. Below is a plot that shows the defenses of the 15 pokemon in our training data set, and has points coloured by Type and with different shapes for Type.\n\nggplot(data = train_sample, aes(x = Defense, y = 1, colour = Type, shape = Type)) +\n  geom_point(size = 4) +  theme(axis.title.y=element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())\n\n\n\n\nWe see from the plot that Steel type Pokemon tend to have pretty high defense values. Now suppose that we want to predict the Type for one of the Pokemon in our test data set, Dialga. We know that Dialga has a Defense stat of 120: the plot below shows Dialga marked with a large black X.\n\ndialga &lt;- test_sample |&gt; slice(63)\nggplot(data = train_sample, aes(x = Defense, y = 1, colour = Type, shape = Type)) +\n  geom_point(size = 4) +  theme(axis.title.y=element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks.y=element_blank()) +\n  geom_point(data = dialga, colour = \"black\", shape = 4, size = 7)\n\n\n\n\nWhat would your prediction for Dialga be? Why? According to knn with k = 1, we would predict Dialga to be Fire type. k = 1 means that we are using the 1st nearest neighbor: in this case the point that is closest to Dialga is a green triangle, corresponding to a Fire type Pokemon.\n\n14.2.2 knn with k &gt; 1 and One Predictor\nBut, we might not necessarily want to predict the response value based on the single nearest neighbor. Dialga is also near many purple plus signs: should those factor in at all? We can extend knn to different values for k. For example, \\(k = 3\\) looks at the 3 nearest neighbors, and assigns a prediction as the category that appears the most among those 3 nearest neighbors.\nUsing k = 3, what would the prediction for Dialga be? Why?\n\n14.2.3 knn with k &gt; 1 and More Than One Predictor\nWe can increase the number of predictors in our knn model as well. We can generally include as many predictors as we would like, but visualizing becomes challenging with more than 2 predictors and nearly impossible with more than 3 predictors. For the case of two predictors, suppose that we want to use Defense and Speed as our predictors for Type. Dialga, the Pokemon we want to predict for, is again marked with a large black X.\n\nggplot(data = train_sample, aes(x = Defense, y = Speed, colour = Type, shape = Type)) +\n  geom_point(size = 3) +\n  geom_point(data = dialga, colour = \"black\", shape = 4, size = 5)\n\n\n\n\nFor \\(k = 1\\), we would predict the Dialga is Steel, as the closest point is the purple + sign in the top-left corner of the graph. For \\(k = 3\\), what Type would you predict for Dialga? For this question, it’s a little hard to tell which three points are closest to Dialga without computing the distances numerically, which is something we will let R do with the knn() function.\n\n14.2.4 Scaling Predictor Variables before Using knn\nIn general, we want to scale any quantitative predictors when using knn because it relies on distances between points in its predictions. This is easiest to see with an example. Suppose, in our Pokemon example, that we want to use height and weight as our predictors in the knn model. We just have 2 observations in our training data set: a Dark Type pokemon with a height of 15 centimeters and a weight of 505 pounds, and a Fire Type Pokemon with a height of 9 centimeters and a weight of 250 pounds.\n\ntrain_tiny &lt;- train_sample |&gt; slice(1:2)\nnewobs &lt;- tibble(height = 15, weight = 350, Type = \"Unknown\")\nggplot(data = train_tiny, aes(x = height, y = weight, shape = Type)) +\n  geom_point(size = 5, aes(colour = Type)) + xlim(c(7, 17)) + ylim(c(200, 550)) +\n  geom_point(data = newobs, shape = 4, size = 10)\n\n\n\n\nOn the plot is also given a Pokemon in our test data set that we wish to predict the Type of, marked with a black X. Upon visual inspection, with k = 1, it looks like we would classify this pokemon as Dark. However, the units of weight and height are on very different scales. We will compute the actual distances in class to see if the conclusion from the calculation matches with the visual conclusion.\n\nTo get around this issue, it is customary to scale all quantitative predictors before applying knn. One method of doing this is applying\n\\[\nscaled_x = \\frac{x - min(x)}{max(x) - min(x)}\n\\]\nFor example, scaling weight for the 15 original pokemon:\n\ntrain_sample |&gt; select(weight) |&gt; head()\n#&gt; # A tibble: 6 × 1\n#&gt;   weight\n#&gt;    &lt;dbl&gt;\n#&gt; 1    505\n#&gt; 2    250\n#&gt; 3    811\n#&gt; 4    558\n#&gt; 5    815\n#&gt; 6    370\n\nputs all weights between 0 and 1:\n\ntrain_sample |&gt; mutate(weight_s = (weight - min(weight)) / \n                          (max(weight) - min(weight))) |&gt;\n  select(weight_s) |&gt;\n  head()\n#&gt; # A tibble: 6 × 1\n#&gt;   weight_s\n#&gt;      &lt;dbl&gt;\n#&gt; 1   0.187 \n#&gt; 2   0.0835\n#&gt; 3   0.312 \n#&gt; 4   0.209 \n#&gt; 5   0.314 \n#&gt; 6   0.132\n\nIf we do the same with height, then the variables will contribute more “equally” to the distance metric used in knn.\nThe code below scales all numeric variables in a data set, using the across() function. across() applies a transformation to every column in a data set that satisfies the condition given in the where argument.\n\n## ?across\nlibrary(pander)\ntrain_sample |&gt;\n  mutate(across(where(is.numeric), ~ (.x - min(.x)) /\n                                 (max(.x) - min(.x)))) |&gt;\n  slice(1:3)\n#&gt; # A tibble: 3 × 14\n#&gt;    ...1 Name    Type     HP Attack Defense Speed SpAtk SpDef Generation\n#&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1 0.720 Darkrai Dark  0.417  0.444     0.4 1     1     0.658        0.6\n#&gt; 2 0.193 Flareon Fire  0.333  0.889     0.1 0.368 0.619 0.921        0  \n#&gt; 3 0.838 Zoroark Dark  0.25   0.611     0.1 0.789 0.857 0.263        0.8\n#&gt; # ℹ 4 more variables: Legendary &lt;lgl&gt;, height &lt;dbl&gt;, weight &lt;dbl&gt;,\n#&gt; #   base_experience &lt;dbl&gt;\n\n\n14.2.5 Exercises\n\nConsider again the toy example with just two observations in the training data set and unscaled weight and height as predictors.\n\n\nggplot(data = train_tiny, aes(x = height, y = weight, shape = Type)) +\n  geom_point(size = 5, aes(colour = Type)) + xlim(c(7, 17)) +\n  ylim(c(200, 550)) +\n  geom_point(data = newobs, shape = 4, size = 10)\n\n\n\n\nThe actual (height, weight) coordinates of the Fire pokemon are (9, 250), the actual coordinates of the Dark pokemon are (15, 505), and the actual coordinates of the test pokemon are (15, 350). We mentioned that, visually, the pokemon looks “closer” to the Dark type pokemon. Verify that this is not actually the case by computing the actual distances numerically. Recall that the formula for the distance between two points at (\\(x1, y1\\)) and (\\(x2, y2\\)) is:\n\\[\n\\sqrt{(x_1 - x_2) ^ 2 + (y1 - y2) ^ 2}\n\\]\nWe’ll need to use this formula twice: once for the distance between the fire pokemon and the test pokemon and once for the distance between the dark pokemon and the test pokemon.\n\nAfter scaling according to the formula in this section, the coordinates (height, weight) of the Fire pokemon are (0, 0) and the coordinates of the Dark pokemon are (1, 1). (Since there are only two observations, the formula doesn’t give any output between 0 and 1 for this tiny example). The scaled coordinates for the test pokemon are (1, 0.39). Verify that, after scaling, the test pokemon is “closer” to the Dark type pokemon by numerically computing distances.\n\n\nConsider again the example with 15 pokemon in the training data set and a single predictor, Defense.\n\n\nggplot(data = train_sample, aes(x = Defense, y = 1, colour = Type, shape = Type)) +\n  geom_point(size = 4) +  theme(axis.title.y=element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks.y=element_blank()) +\n  geom_point(data = dialga, colour = \"black\", shape = 4, size = 7)\n\n\n\n\nWith k = 2, there is a tie between Fire and Steel. Come up with a way in which you might break ties in a knn algorithm.\n\nExplain what knn would use as a prediction for all test observations if k equals the number of observations in the training data set.\nWhat are some advantages for making k smaller and what are some advantages for making k larger?"
  },
  {
    "objectID": "13-knn.html#choosing-predictors-and-k",
    "href": "13-knn.html#choosing-predictors-and-k",
    "title": "14  Predictive Modeling with knn",
    "section": "\n14.3 Choosing Predictors and k",
    "text": "14.3 Choosing Predictors and k\nWe now know how knn classifies observations in the test data set, but how do we choose which predictors should be used by the knn algorithm? And how do we choose the number of neighbors, k? We want to measure how “good” models with different predictors and different k’s do, but we first need to define what “good” means.\nMuch of the “choosing predictors” part will be trial and error by evaluating different models with a criterion that we will talk about in the next section. However, it is always helpful to explore the data set with graphics to get us to a good starting point. A scatterplot matrix is a useful exploratory tool. The following is a scatterplot matrix with the response variable, Type, and just three candidate predictors, HP, Attack, and Defense, created with the GGally (“g-g-ally”) package.\n\n## install.packages(\"GGally\")\nlibrary(GGally)\nggpairs(data = train_sample, columns = c(4, 5, 6, 3), \n        lower = list(combo = wrap(ggally_facethist, bins = 15)))\n\n\n\n\nThe lower argument changes the number of bins in the faceted histograms in the bottom row.\nThe columns argument is important: it allows you to specify which columns you want to look at. I prefer putting the response, Type (column 3) in the last slot.\nWe can examine this to see which variables seem to have a relationship with Type. Where would we want to look for this?\n\n14.3.1 The Confusion Matrix\nBut, we still need a metric to evaluate models with different predictors. One definition of a “good” model in the classification context is a model that has a high proportion of correct predictions in the test data set. This should make some intuitive sense, as we would hope that a “good” model correctly classifies most Dark pokemon as Dark, most Fire pokemon as Fire, etc.\nIn order to examine the performance of a particular model, we’ll create a confusion matrix that shows the results of the model’s classification on observations in the test data set. An equivalent name for the confusion matrix is a classification table.\nThe following video explains confusion matrices in more detail and should also cement the ideas of training and test data. https://www.youtube.com/watch?v=Kdsp6soqA7o.\n\n14.3.2 Using knn in R\n\nTo make a confusion matrix for a model using the pokemon data set, we first need to obtain predictions from a model. We’ll use the class library to fit a knn model to the pokemon data.\n\n\n\n\n\n\nNote\n\n\n\nInstead of having 15 Pokemon in our training data set, we now have 70 pokemon to give a more reasonable number. The test set has the remaining 50 pokemon.\n\n\nThe following code chunk sets a seed so that we all get the same training and test samples, scales all numeric variables in the pokemon data set, and then randomly selects 70 pokemon to be in the training sample.\n\nlibrary(tidyverse)\nset.seed(11232020) ## run this line so that you get the same \n## results as I do!\n\n## scale the quantitative predictors\npokemon_scaled &lt;- pokemon |&gt;\n    mutate(across(where(is.numeric), ~ (.x - min(.x)) /\n                                 (max(.x) - min(.x)))) \n\ntrain_sample_2 &lt;- pokemon_scaled |&gt;\n  slice_sample(n = 70)\ntest_sample_2 &lt;- anti_join(pokemon_scaled, train_sample_2)\n\nThe first knn model we will investigate will have HP, Attack, Defense, and Speed as predictors. The class library can fit knn models with a knn() function but requires the training and test data sets to have only the predictors that we want to use to fit the model. The knn() function also requires the response variable, Type, to be given as a vector.\n\n## install.packages(\"class\")\nlibrary(class)\n\n## create a data frame that only has the predictors\n## that we will use\ntrain_small &lt;- train_sample_2 |&gt; select(HP, Attack, Defense, Speed)\ntest_small &lt;- test_sample_2 |&gt; select(HP, Attack, Defense, Speed)\n\n## put our response variable into a vector\ntrain_cat &lt;- train_sample_2$Type\ntest_cat &lt;- test_sample_2$Type\n\nNow that the data has been prepared for the knn() function in the class library, we fit the model with 9 nearest neighbors. The arguments to knn() are\n\n\ntrain, a data set with the training data that contains only the predictors we want to use (and not other predictors or the response).\n\ntest, a data set with the test data that contains only the predictors we want to use (and not other predictors or the response).\n\ncl, a vector of the response variable for the training data.\n\nk, the number of nearest neighbors.\n\n\n## fit the knn model with 9 nearest neighbors\nknn_mod &lt;- knn(train = train_small, test = test_small,\n               cl = train_cat, k = 9)\nknn_mod\n#&gt;  [1] Ice   Fire  Fire  Fire  Fire  Fire  Steel Fire  Ice   Fire  Fire  Fire \n#&gt; [13] Fire  Ice   Ice   Steel Ice   Dark  Ice   Fire  Steel Fire  Fire  Ice  \n#&gt; [25] Fire  Ice   Steel Fire  Fire  Ice   Dark  Fire  Fire  Fire  Dark  Ice  \n#&gt; [37] Ice   Fire  Ice   Fire  Fire  Fire  Fire  Fire  Fire  Fire  Fire  Fire \n#&gt; [49] Ice   Fire \n#&gt; Levels: Dark Fire Ice Steel\n\nThe output of knn_mod gives the predicted categories for the test sample. We can compare the predictions from the knn model with the actual pokemon Types in the test sample with table(), which makes a confusion matrix:\n\ntable(knn_mod, test_cat) \n#&gt;        test_cat\n#&gt; knn_mod Dark Fire Ice Steel\n#&gt;   Dark     0    3   0     0\n#&gt;   Fire     6   13   7     4\n#&gt;   Ice      5    5   2     1\n#&gt;   Steel    0    1   0     3\n\nThe columns of the confusion matrix give the actual Pokemon types in the test data while the rows give the predicted types from our knn model. The above table tells us that there were 0 pokemon that were Dark type that our knn model correctly classified as Dark. There were 6 pokemon that were Dark type that our knn model incorrectly classified as Fire. There were 5 pokemon that were Dark type and that our knn model incorrectly classified as Ice. In other words, correct predictions appear on the diagonal, while incorrect predictions appear on the off-diagonal.\nOne common metric used to assess overall model performance is the model’s classification rate, which is computed as the number of correct classifications divided by the total number of observations in the test data set. In this case, our classification rate is\n\n(0 + 13 + 2 + 3) / 50\n#&gt; [1] 0.36\n\nCode to automatically obtain the classification rate from a confusion matrix is\n\ntab &lt;- table(knn_mod, test_cat) \nsum(diag(tab)) / sum(tab)\n#&gt; [1] 0.36\n\nWhat does diag() seem to do in the code above?\n\n14.3.3 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-13).\n\nChange the predictors used or change k to improve the classification rate of the model with k = 9 and Attack, Defense, HP, and Speed as predictors."
  },
  {
    "objectID": "13-knn.html#practice",
    "href": "13-knn.html#practice",
    "title": "14  Predictive Modeling with knn",
    "section": "\n14.4 Practice",
    "text": "14.4 Practice\nThere will be no class exercises nor any your turn exercises for this section. Instead, you will apply some of these concepts in your third project."
  },
  {
    "objectID": "15-connections.html#stat-113",
    "href": "15-connections.html#stat-113",
    "title": "15  Connections to STAT 113, STAT 213, and CS 140",
    "section": "\n15.1 STAT 113",
    "text": "15.1 STAT 113\nIn this section, we discuss how what we have learned in this data science course connects to some concepts that you learned about in STAT 113. As a quick refresher, a few concepts that you learned about in STAT 113 are:\n\nexploring data through numerical and graphical summaries. The connection to what we have been doing in this class is fairly straightforward: we’ve learned a lot about how to actually compute those numerical summaries and make appropriate graphics for potentially messy data.\nexplaining what sampling distributions are and how they relate to confidence intervals and hypothesis tests. This topic is probably the least connected to what we have learned so far in this class.\nconducting hypothesis tests and creating confidence intervals to answer questions of interest. We will focus on this third general objective in this section.\n\nThe example we will use is an experiment designed to assess the effects of race and sex on whether or not an employee received a callback for a job. In order to conduct the experiment, researchers randomly assigned names to resumes with each name associated with a particular race and gender, sent the resumes to employers, and recorded whether or not the resume received a callback. In addition to race, sex, and whether or not an employee received a callback, a few more variables were collected, like resume quality, whether or not the applicant had computer skills, years of experience, etc. A 1 for the received_callback indicates that the applicant received a callback.\n\n\n\n\n\n\nNote\n\n\n\nYou may recall this example from STAT 113: we used it to introduce a chi-square test of association. In that example and others like it, an appropriate graphic and summary statistics were provided. Here, we create these ourselves.\n\n\nThe data set is called resume in the openintro package: you’ll need to install this package with install.packages(\"openintro\"). Then, load in the data with\n\nlibrary(openintro)\nresume\n#&gt; # A tibble: 4,870 × 30\n#&gt;   job_ad_id job_city job_industry               job_type   job_fed_contractor\n#&gt;       &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;                      &lt;chr&gt;                   &lt;dbl&gt;\n#&gt; 1       384 Chicago  manufacturing              supervisor                 NA\n#&gt; 2       384 Chicago  manufacturing              supervisor                 NA\n#&gt; 3       384 Chicago  manufacturing              supervisor                 NA\n#&gt; 4       384 Chicago  manufacturing              supervisor                 NA\n#&gt; 5       385 Chicago  other_service              secretary                   0\n#&gt; 6       386 Chicago  wholesale_and_retail_trade sales_rep                   0\n#&gt; # ℹ 4,864 more rows\n#&gt; # ℹ 25 more variables: job_equal_opp_employer &lt;dbl&gt;, job_ownership &lt;chr&gt;,\n#&gt; #   job_req_any &lt;dbl&gt;, job_req_communication &lt;dbl&gt;, job_req_education &lt;dbl&gt;,\n#&gt; #   job_req_min_experience &lt;chr&gt;, job_req_computer &lt;dbl&gt;,\n#&gt; #   job_req_organization &lt;dbl&gt;, job_req_school &lt;chr&gt;,\n#&gt; #   received_callback &lt;dbl&gt;, firstname &lt;chr&gt;, race &lt;chr&gt;, gender &lt;chr&gt;,\n#&gt; #   years_college &lt;int&gt;, college_degree &lt;dbl&gt;, honors &lt;int&gt;, …\n\n\n15.1.1 Chi-square Test of Association\nOur goal is to assess whether there is evidence of racial discrimination in the study. In other words, are the variables race and received_callback associated?\nPrepare. Let’s start by writing the null and alternative hypotheses.\n\\(H_0:\\) There is no association between race and received_callback.\n\\(H_a:\\) There is an association between race and received_callback.\nNext, we can construct a summary graphic. One graphic to explore two categorical variables is a stacked bar plot.\n\nlibrary(tidyverse)\nresume_sum &lt;- resume |&gt; \n  mutate(received_callback = received_callback) |&gt;\n           group_by(race, received_callback) |&gt;\n  summarise(count = n())\nggplot(data = resume_sum, aes(x = race, y = count)) +\n  geom_col(aes(fill = received_callback)) +\n  scale_fill_viridis_c()\n\n\n\n\nWhat do you notice about the recieved_callback variable scale? How could we fix that?\n\nresume &lt;- resume |&gt;\n  mutate(received_callback = as.factor(received_callback))\nresume_sum &lt;- resume |&gt; \n           group_by(race, received_callback) |&gt;\n  summarise(count = n())\nggplot(data = resume_sum, aes(x = race, y = count)) +\n  geom_col(aes(fill = received_callback)) +\n  scale_fill_viridis_d()\n\n\n\n\nWe might also want to generate a two-way table:\n\nresume |&gt; group_by(race, received_callback) |&gt;\n  summarise(count = n()) |&gt;\n  pivot_wider(names_from = race,\n              values_from = count)\n#&gt; # A tibble: 2 × 3\n#&gt;   received_callback black white\n#&gt;   &lt;fct&gt;             &lt;int&gt; &lt;int&gt;\n#&gt; 1 0                  2278  2200\n#&gt; 2 1                   157   235\n\nCheck: The two assumptions for the test are independence of observations and that all expected counts are larger than 5. We don’t have time to discuss these in detail but we will assume that they are satisfied here.\nCalculate: We next want to calculate a p-value for the hypothesis test. The core tidyverse packages do not offer functionality for hypothesis testing. Instead, there are some functions in base R that perform the various tests. You may have used the lm() function in STAT 213 to perform hypothesis testing in the regression context. t.test() and chisq.test() are a couple of other functions that can perform a one and two-sample t-test (t.test()) or a chi-square goodness-of-fit test and chi-square test of association chisq.test(). The arguments to chisq.test() for a test of association are two vectors. Because the arguments are not data.frames, we need to specify the appropriate vectors directly with resume$race and resume$received_callback.\n\nchisq.test(x = resume$race, y = resume$received_callback)\n#&gt; \n#&gt;  Pearson's Chi-squared test with Yates' continuity correction\n#&gt; \n#&gt; data:  resume$race and resume$received_callback\n#&gt; X-squared = 16.449, df = 1, p-value = 4.998e-05\n\nThe output of chisq.test() gives a p-value of 0.00004998 with a chi-square statistic of 16.449 and 1 degree of freedom.\nConclude. Finally, we write a conclusion in context of the problem.\nThere is strong evidence that race and callback are associated (p-value = 0.00004998). The graph shows that white applicants receive a callback more often than black applicants do and the hypothesis test shows that this is statistically significant.\n\n15.1.2 Additional Analysis\nIn addition to carrying out the steps of a statistical hypothesis test, we can also use the skills we have learned in this course to provide further information about the study. Some questions we might answer include:\n\nwhat is the distribution of job types job_type and job industries job_industry in the study?\ndo some of the first names (firstname) used have more bias than other first names?\nwhat other variables are associated with whether or not the applicant received a callback?\n\nTo answer the question about the distribution of job types and job industries used in the study, we can make a simple bar plot:\n\nggplot(data = resume, aes(x = fct_rev(fct_infreq(job_type)))) +\n  geom_bar() +\n  coord_flip() +\n  labs(x = \"Job Type\")\n\n\n\n\nIn the code, fct_infreq() orders the levels of job_type from the highest count/frequency to the lowest. fct_rev() reverses the order so that, on the resulting bar plot, the level with the highest count appears first.\n\nggplot(data = resume, aes(x = fct_rev(fct_infreq(job_industry)))) +\n  geom_bar() +\n  coord_flip() +\n  labs(x = \"Job Industry\")\n\n\n\n\nTo answer the question about whether some first names are more biased than others, we might make a graph of the proportion of resumes that received a callback for each first name.\n\nresume_firstname &lt;- resume |&gt;\n  group_by(firstname) |&gt;\n  summarise(propcallback = mean(received_callback == \"1\"),\n            gender = unique(gender),\n            race = unique(race)) |&gt;\n  arrange(desc(propcallback)) |&gt;\n  unite(\"gender_race\", c(gender, race))\n\nggplot(data = resume_firstname, aes(x = gender_race, y = propcallback)) +\n  geom_point()\n\n\n\n\nWe can the label the name with the lowest callback rate and the name with the highest callback rate.\n\nlibrary(ggrepel)\nlabel_df &lt;- resume_firstname |&gt; \n  filter(propcallback == max(propcallback) |\n           propcallback == min(propcallback))\n\nggplot(data = resume_firstname, aes(x = gender_race, y = propcallback)) +\n  geom_point() +\n  geom_text_repel(data = label_df, aes(label = firstname))\n\n\n\n\nExercise 1. Construct a graphic or make a table that explores whether one of the other variables in the data set is associated with whether the applicant receives a callback for the job. Other variables include gender, years_college, college_degree, honors, worked_during_school, years_experience, computer_skills, special_skills, volunteer, military, employment_holes, and resume_quality.\nExercise 2. Construct a graphic or make a table that explores one of the other variables in the data set is associated with whether the applicant receives a callback for the job. If your variable in Exercise 1 was categorical, choose a quantitative variable for this exercise. If your variable in Exercise 1 was quantitative, choose a categorical variable for this exercise.\nExercise 3. For the categorical variable that you chose, conduct a Chi-square test of association to see if there is statistical evidence that the variable is associated with received_callback. In your test, (a), write the null and alternative hypotheses, run the test in chisq.test() and make a note of whether or not you get a warning about assumptions for the test, and write a conclusion in context of the problem.\nExercise 4. Write a short, one paragraph summary on your major take-aways from this section."
  },
  {
    "objectID": "15-connections.html#stat-213",
    "href": "15-connections.html#stat-213",
    "title": "15  Connections to STAT 113, STAT 213, and CS 140",
    "section": "\n15.2 STAT 213",
    "text": "15.2 STAT 213\nMuch of the same concepts in connecting STAT 113 to this course hold for connecting STAT 213 with this course. We can still use what we have learned to explore a data set, conduct a hypothesis test, and perform further analysis and exploration on the data set.\nFor this section, however, we will focus more on a tidy approach to modeling. In particular, we will use the broom package to return tibbles with model summary information that we can then use for further analysis, plotting, or presentation.\nWe will use the coffee_ratings data set, which contains observations on ratings of various coffees throughout the world. The data was obtained from the Github account (https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-07-07/readme.md).\nA description of each variable in the data set is given below.\n\n\ntotal_cup_points, the score of the coffee by a panel of experts (our response variable for this section)\n\nspecies, the species of the coffee bean (Arabica or Robusta)\n\naroma, aroma (smell) grade\n\nflavor, flavor grade\n\naftertaste, aftertaste grade\n\nacidity, acidity grade\n\nbody, body grade\n\nbalance, balance grade\n\nuniformity, uniformity grade\n\nclean_cup, clean cup grade\n\nsweetness, sweetness grade\n\nmoisture, moisture grade\n\ncategory_one_defects, count of category one defects\n\nquakers, quakers\n\ncategory_two_defects, the number of category two defects\n\n\n15.2.1 broom Package Functions\nThe broom package consists of three primary functions: tidy(), glance(), and augment().\ntidy()\ntidy() is analagous to summary() for a linear model object. Let’s start by fitting a linear model with lm() with total_cup_points as the response and species, aroma, flavor, sweetness, and moisture as predictors.\nRead in the data, load the broom package (and install it with install.packages(\"broom\")), and fit the model with\n\nlibrary(broom)\nlibrary(here)\ncoffee_df &lt;- read_csv(here(\"data/coffee_ratings.csv\"))\ncoffee_mod &lt;- lm(total_cup_points ~ species + aroma + flavor +\n                   sweetness + moisture,\n   data = coffee_df)\n\nIn STAT 213, you likely used summary() to look at the model output:\n\nsummary(coffee_mod)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = total_cup_points ~ species + aroma + flavor + sweetness + \n#&gt;     moisture, data = coffee_df)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -9.5132 -0.3705  0.0726  0.5610  5.5844 \n#&gt; \n#&gt; Coefficients:\n#&gt;                Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)     7.04039    0.77377   9.099  &lt; 2e-16 ***\n#&gt; speciesRobusta  2.85365    0.26861  10.624  &lt; 2e-16 ***\n#&gt; aroma           1.95188    0.14575  13.392  &lt; 2e-16 ***\n#&gt; flavor          5.09440    0.14042  36.281  &lt; 2e-16 ***\n#&gt; sweetness       2.23956    0.06553  34.173  &lt; 2e-16 ***\n#&gt; moisture       -1.88033    0.67368  -2.791  0.00533 ** \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 1.168 on 1333 degrees of freedom\n#&gt; Multiple R-squared:  0.8891, Adjusted R-squared:  0.8887 \n#&gt; F-statistic:  2137 on 5 and 1333 DF,  p-value: &lt; 2.2e-16\n\nHowever, there are a few inconveniences involving summary(). First, it’s just not that nice to look at: the output isn’t formatted in a way that is easy to look at. Second, it can be challenging to pull items from the summary output with code. For example, if you want to pull the p-value for moisture, you would need to write something like:\n\nsummary(coffee_mod)$coefficients[\"moisture\", 4]\n#&gt; [1] 0.005327594\n\ntidy() is an alternative that puts the model coefficients, standard errors, t-stats, and p-values in a tidy tibble:\n\ntidy(coffee_mod)\n#&gt; # A tibble: 6 × 5\n#&gt;   term           estimate std.error statistic   p.value\n#&gt;   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)        7.04    0.774       9.10 3.23e- 19\n#&gt; 2 speciesRobusta     2.85    0.269      10.6  2.31e- 25\n#&gt; 3 aroma              1.95    0.146      13.4  1.82e- 38\n#&gt; 4 flavor             5.09    0.140      36.3  4.73e-201\n#&gt; 5 sweetness          2.24    0.0655     34.2  2.41e-184\n#&gt; 6 moisture          -1.88    0.674      -2.79 5.33e-  3\n\nThe advantage of this format of output is that we can now use other tidyverse functions on the output. To pull the p-values,\n\ntidy(coffee_mod) |&gt; select(p.value)\n#&gt; # A tibble: 6 × 1\n#&gt;     p.value\n#&gt;       &lt;dbl&gt;\n#&gt; 1 3.23e- 19\n#&gt; 2 2.31e- 25\n#&gt; 3 1.82e- 38\n#&gt; 4 4.73e-201\n#&gt; 5 2.41e-184\n#&gt; 6 5.33e-  3\n\nor, to grab the output for a particular variable of interest:\n\ntidy(coffee_mod) |&gt; filter(term == \"aroma\")\n#&gt; # A tibble: 1 × 5\n#&gt;   term  estimate std.error statistic  p.value\n#&gt;   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 aroma     1.95     0.146      13.4 1.82e-38\n\nglance()\nglance() puts some model summary statistics into a tidy tibble. For example, if we run\n\nglance(coffee_mod)\n#&gt; # A tibble: 1 × 12\n#&gt;   r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n#&gt;       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     0.889         0.889  1.17     2137.       0     5 -2105. 4224. 4260.\n#&gt; # ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nyou should notice a lot of statistics that you are familiar with from STAT 213, including r.squared, adj.r.squared, sigma (the residual standard error), statistic (the overall F-statistic), AIC, and BIC.\naugment()\naugment() is my personal favourite of the three. The function returns a tibble that contains all of the variables used to fit the model appended with commonly used diagnostic statistics like the fitted values (.fitted), cook’s distance (.cooksd), .hat values for leverage, and residuals (.resid).\n\naugment(coffee_mod)\n#&gt; # A tibble: 1,339 × 12\n#&gt;   total_cup_points species aroma flavor sweetness moisture .fitted  .resid\n#&gt;              &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1             90.6 Arabica  8.67   8.83        10     0.12    91.1 -0.537 \n#&gt; 2             89.9 Arabica  8.75   8.67        10     0.12    90.5 -0.538 \n#&gt; 3             89.8 Arabica  8.42   8.5         10     0       89.2  0.577 \n#&gt; 4             89   Arabica  8.17   8.58        10     0.11    88.9  0.114 \n#&gt; 5             88.8 Arabica  8.25   8.5         10     0.12    88.6  0.214 \n#&gt; 6             88.8 Arabica  8.58   8.42        10     0.11    88.9 -0.0411\n#&gt; # ℹ 1,333 more rows\n#&gt; # ℹ 4 more variables: .hat &lt;dbl&gt;, .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;,\n#&gt; #   .std.resid &lt;dbl&gt;\n\naugment() the data set makes it really easy to do things like:\n\n\nfilter() the data set to examine values with high cook’s distance that might be influential\n\n\naugment_df &lt;- augment(coffee_mod)\naugment_df |&gt; filter(.cooksd &gt; 1)\n#&gt; # A tibble: 1 × 12\n#&gt;   total_cup_points species aroma flavor sweetness moisture .fitted .resid\n#&gt;              &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1                0 Arabica     0      0         0     0.12    6.81  -6.81\n#&gt; # ℹ 4 more variables: .hat &lt;dbl&gt;, .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;,\n#&gt; #   .std.resid &lt;dbl&gt;\n\nWe see right away that there is a potentially influential observation with 0 total_cup_points. Examining this variable further, we see that it is probably a data entry error that can be removed from the data.\n\nggplot(data = coffee_df, aes(x = total_cup_points)) +\n  geom_histogram(bins = 15, fill = \"white\", colour = \"black\")\n\n\n\n\nWe could also find observations with high leverage\n\naugment_df |&gt; filter(.hat &gt; 0.2)\n#&gt; # A tibble: 2 × 12\n#&gt;   total_cup_points species aroma flavor sweetness moisture .fitted .resid\n#&gt;              &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1             59.8 Arabica   7.5   6.67      1.33     0.1    58.4    1.38\n#&gt; 2              0   Arabica   0     0         0        0.12    6.81  -6.81\n#&gt; # ℹ 4 more variables: .hat &lt;dbl&gt;, .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;,\n#&gt; #   .std.resid &lt;dbl&gt;\n\nor observations that are outliers:\n\naugment_df |&gt; filter(.std.resid &gt; 3 | .std.resid &lt; -3)\n#&gt; # A tibble: 25 × 12\n#&gt;   total_cup_points species aroma flavor sweetness moisture .fitted .resid\n#&gt;              &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1             82.8 Arabica  8.08   8.17     10        0.12    86.6  -3.85\n#&gt; 2             82.4 Arabica  5.08   7.75     10        0.11    78.6   3.79\n#&gt; 3             82.3 Arabica  7.75   8.08      6.67     0.11    78.1   4.27\n#&gt; 4             80.7 Arabica  7.67   7.5       6.67     0       75.2   5.51\n#&gt; 5             80   Arabica  7.58   7.75     10        0       83.7  -3.71\n#&gt; 6             79.9 Arabica  7.83   7.67     10        0       83.8  -3.87\n#&gt; # ℹ 19 more rows\n#&gt; # ℹ 4 more variables: .hat &lt;dbl&gt;, .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;,\n#&gt; #   .std.resid &lt;dbl&gt;\n\nFinally, we can use our ggplot2 skills to construct plots like a residuals versus fitted values plot (filtering out the outlying observation first):\n\nggplot(data = augment_df |&gt; filter(.fitted &gt; 25), aes(x = .fitted, y = .resid)) +\n  geom_point() \n\n\n\n\n\n15.2.2 Exploring Concepts\nWe can also use our data science skills to explore concepts that you may or may not have found difficult in STAT 213. In this section, we will use plotting to help interpret coefficient estimates in models with two quantitative predictors: one with an interaction and one without an interaction.\nFirst, consider a model with flavor and moisture as predictors on the coffee data set that removes the observation with a total_cup_points rating of 0:\n\\(Y = \\beta_0 + \\beta_1 flavor + \\beta_2 aroma + \\epsilon\\)\n\ncoffee_noout &lt;- coffee_df |&gt; filter(total_cup_points &gt; 0)\ncoffee_fa &lt;- lm(total_cup_points ~ flavor + aroma,\n   data = coffee_noout)\ncoffee_fa |&gt; tidy()\n#&gt; # A tibble: 3 × 5\n#&gt;   term        estimate std.error statistic   p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)    29.8      1.01      29.6  1.79e-148\n#&gt; 2 flavor          5.55     0.177     31.4  1.33e-162\n#&gt; 3 aroma           1.39     0.191      7.27 6.11e- 13\n\nYou might recall that, you can interpret the estimate for \\(\\hat{\\beta}_1\\) as something like:\n“For a one point increase in flavor grade, we expect average coffee score to increase by 5.55 points, if aroma is held constant.”\n(or, if aroma does not change, or if aroma is fixed).\nBut, what if someone asked you, what does that mean, “if aroma is held constant.” Instead of using words to explain, we can use our ggplot2 skills to construct a plot. The plot that we are going to create is going to show the fitted model for 5 distinct values of aroma: the minimum value, Q1, the median, Q3, and the maximum (though these values can also be chosen arbitrarily). We will make use of the modelr package to gather predictions to make the lines in our plot. First, we are creating a grid of values for predictions:\n\n## install.packages(\"modelr\")\nlibrary(modelr)\ngrid_vals &lt;- coffee_noout |&gt;\n  modelr::data_grid(\n    flavor = quantile(flavor),\n    aroma = quantile(aroma)\n  )\ngrid_vals\n#&gt; # A tibble: 25 × 2\n#&gt;   flavor aroma\n#&gt;    &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1   6.08  5.08\n#&gt; 2   6.08  7.42\n#&gt; 3   6.08  7.58\n#&gt; 4   6.08  7.75\n#&gt; 5   6.08  8.75\n#&gt; 6   7.33  5.08\n#&gt; # ℹ 19 more rows\n\nWe then gather these predictions in a data set, and use that data set to make the lines in our plot. The plot will show total_cup_points on the y-axis, flavor on the x-axis, and have coloured lines for a few different values of aroma that we specified in the grid function:\n\ngrid &lt;- grid_vals |&gt;\n  modelr::gather_predictions(coffee_fa)\ngrid\n#&gt; # A tibble: 25 × 4\n#&gt;   model     flavor aroma  pred\n#&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 coffee_fa   6.08  5.08  70.7\n#&gt; 2 coffee_fa   6.08  7.42  73.9\n#&gt; 3 coffee_fa   6.08  7.58  74.1\n#&gt; 4 coffee_fa   6.08  7.75  74.4\n#&gt; 5 coffee_fa   6.08  8.75  75.8\n#&gt; 6 coffee_fa   7.33  5.08  77.6\n#&gt; # ℹ 19 more rows\n\n\nggplot(data = coffee_noout, aes(x = flavor, y = total_cup_points)) +\n  geom_point(alpha = 0.2) +\n  geom_line(data = grid, aes(colour = fct_rev(factor(aroma)),\n                             y = pred), size = 1.2) +\n  scale_colour_viridis_d() +\n  labs(colour = \"Aroma\") +\n  theme_minimal()\n\n\n\n\nFrom this model, we might more easily be able to explain that slope interpretation earlier: for a fixed coloured line of aroma, if flavor increases by one point, we expect coffee rating to increase by 5.55 points, on average.\nBut, we can really see the power of modelr and plotting models if we fit more complicated models. For example, next, we will fit a model with an interaction between flavor and aroma. You may recall that an interaction allows the association between one predictor (flavor) and the response (total_cup_points) to change depending on the value of the other predictor (aroma).\n\\(Y = \\beta_0 + \\beta_1 flavor + \\beta_2 aroma + \\beta_3 flavor * aroma + \\epsilon\\)\n\ncoffee_fa_int &lt;- lm(total_cup_points ~ flavor + aroma + flavor:aroma,\n   data = coffee_noout)\ncoffee_fa_int |&gt; tidy()\n#&gt; # A tibble: 4 × 5\n#&gt;   term         estimate std.error statistic  p.value\n#&gt;   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 (Intercept)    -38.5     13.4       -2.88 4.02e- 3\n#&gt; 2 flavor          14.6      1.77       8.24 4.06e-16\n#&gt; 3 aroma           10.5      1.79       5.88 5.26e- 9\n#&gt; 4 flavor:aroma    -1.21     0.235     -5.13 3.34e- 7\n\nYou may also recall that, in a model with an interaction between two quantitative variables, the coefficient estimates are not interpretable on their own. We cannot say something like “For a one point increase in flavor, we expect average coffee rating to increase by 14.6 points, as long as aroma and the interaction between flavor and aroma is held constant” because it does not make sense to talk about an increase in one unit of flavor while also holding the interaction fixed.\nSo, for interpreting the model output, we can again use the modelr package and out ggplot2 knowledge to construct a plot that shows the relationship between total_cup_points and flavor for a few different values of aroma:\n\ngrid &lt;- coffee_noout |&gt;\n  modelr::data_grid(\n    flavor = quantile(flavor),\n    aroma = quantile(aroma), \n  ) |&gt;\n  modelr::gather_predictions(coffee_fa_int)\nggplot(data = coffee_noout, aes(x = flavor, y = total_cup_points)) +\n  geom_point(alpha = 0.2) +\n  geom_line(data = grid, aes(colour = fct_rev(factor(aroma)), y = pred), size = 1.2) +\n  scale_colour_viridis_d() +\n  labs(colour = \"Aroma\") +\n  theme_minimal()\n\n\n\n\nFrom the plot, we can understand the nature of the interaction a little better. It looks like, no matter what the value of aroma, flavor and total_cup_points have a positive association. But, for larger values of aroma, the slope is smaller while, for smaller values of aroma, the slope is larger. The model says that, when aroma is small, we would expect a larger increase in total_cup_points (on average) for a one point increase in flavor while, when aroma is large, we would expect a smaller increase in total_cup_points (on average) for a one point increase in flavor.\nExercise 1. Add a couple of more predictors to the linear model that we fit earlier. Then, use glance() to obtain some model fit statistics. Which model is “better” according to some of the metrics you learned about in STAT 213?\nExercise 2. For one of your fitted models, construct a histogram of the residuals to assess the normality assumption (using ggplot2 and augment()).\nExercise 3. Make a table of the 5 coffees that have the highest predicted coffee rating, according to one of your models.\nExercise 4. Write a short, one paragraph summary on your major take-aways from this section."
  },
  {
    "objectID": "15-connections.html#cs-140",
    "href": "15-connections.html#cs-140",
    "title": "15  Connections to STAT 113, STAT 213, and CS 140",
    "section": "\n15.3 CS 140",
    "text": "15.3 CS 140\nIn this section, we will repeat a couple of topics from CS 140, which is in Python, in R. In particular we will,\n\nwrite our own function. The syntax for doing so in R is very similar to Python.\nperform iteration to repeat a similar task multiple times.\n\nTo start, suppose that we are interested in scraping some hitting data on SLU’s baseball team from the web address https://saintsathletics.com/sports/baseball/stats/2022. After we have the hitting data, we want to create a statistic for each player’s weighted on-base-average (wOBA). Information on what the wOBA is can be found here: https://www.mlb.com/glossary/advanced-stats/weighted-on-base-average. Some of the following code was modified from a project completed by Jack Sylvia in a data visualization course.\nCode to do such a task is given in the following chunk.\n\nlibrary(tidyverse)\nlibrary(rvest)\n\nurl_SLU &lt;- \"https://saintsathletics.com/sports/baseball/stats/2022\"\ntab_SLU &lt;- read_html(url_SLU) |&gt; html_nodes(\"table\")\nSLU_Hitting &lt;- tab_SLU[[1]] |&gt; html_table(fill = TRUE) |&gt;\n  head(-2) |&gt;\n  select(-23) |&gt;\n  mutate(wOBA = (0.69 * BB + 0.72 * HBP + 0.89 * (H-`2B`-`3B`-`HR`) + 1.27 * `2B` + 1.62 * `3B` + 2.10 * HR) / (AB + BB + SF + HBP))\n\nWe can make sure that the statistic was calculated with:\n\nSLU_Hitting |&gt; select(wOBA, everything()) |&gt; arrange(desc(wOBA))\n#&gt; # A tibble: 20 × 23\n#&gt;    wOBA   `#` Player    AVG   OPS `GP-GS`    AB     R     H  `2B`  `3B`    HR\n#&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n#&gt; 1 0.514     7 \"Brink… 0.556  1.16 5-1         9     3     5     0     0     0\n#&gt; 2 0.497    25 \"Liber… 0.379  1.16 25-19      66    19    25     8     0     4\n#&gt; 3 0.46      1 \"Verra… 0.5    1.1  4-0         2     1     1     0     0     0\n#&gt; 4 0.452    13 \"Butle… 0.325  1.08 35-35     126    31    41     9     5     7\n#&gt; 5 0.433     6 \"Clark… 0.367  1.02 29-19      79    24    29     5     2     3\n#&gt; 6 0.425    11 \"Circe… 0.252  1.00 35-33     111    27    28     9     0    11\n#&gt; # ℹ 14 more rows\n#&gt; # ℹ 11 more variables: RBI &lt;int&gt;, TB &lt;int&gt;, `SLG%` &lt;dbl&gt;, BB &lt;int&gt;,\n#&gt; #   HBP &lt;int&gt;, SO &lt;int&gt;, GDP &lt;int&gt;, `OB%` &lt;dbl&gt;, SF &lt;int&gt;, SH &lt;int&gt;,\n#&gt; #   `SB-ATT` &lt;chr&gt;\n\n\n15.3.1 Functions\nNow, suppose that we might want to repeat the scraping and calculation of wOBA for other years at SLU or for other teams.\n\n\n\n\n\n\nNote\n\n\n\nWe could, of course, obtain the new URL address and copy and paste the code that we used above, replacing the old URL address with the new one. This would be a reasonable thing to do if we only wanted to do this for one other url. But, if we wanted to do this for 10, 20, 50, 1000, urls, we might consider writing a function to scrape the data and calculate the wOBA.\n\n\nThe format of a function in R is:\n\nname_of_function &lt;- function(argument1, argument2, ....) {\n  body_of_function ## performs various tasks with the arguments\n  \n  return(output) ## tells the function what to return\n}\n\nWe have used functions throughout the entire semester, but they have always been functions that others have defined and are imported into R through packages. As we expand our toolbox, we might encounter situations where we want to write our own specialized functions for performing tasks that are not covered by functions that others have written.\nBefore we get back to our example, let’s write a very simple function, called get_sum_squares, that computes the sum of squares from a numeric vector argument named x_vec. A sum of squares function would take each number in x_vec, square it, and then add the numbers up.\n\nget_sum_squares &lt;- function(x_vec) {\n  \n  sum_of_squares &lt;- sum(x_vec ^ 2)\n  \n  return(sum_of_squares)\n}\n\nNow, let’s test our function on the numeric vector c(2, 4, 1)\n\nget_sum_squares(x_vec = c(2, 4, 1))\n#&gt; [1] 21\n\nNow, we will move back to our example. We want to write a function called get_hitting_data that takes a url_name, scrapes the data from that url, and calculates the wOBA from the variables that were scraped.\n\n\n\n\n\n\nNote\n\n\n\nOur function will only work on urls that contain a data table formatted with the various baseball statistics as column names.\n\n\nTo create this function, we can simply copy and paste the code above and replace the SLU url web address with the argument url_name in the body of the function.\n\nget_hitting_data &lt;- function(url_name) {\n  \n  tab &lt;- read_html(url_name) |&gt; html_nodes(\"table\")\n  \n  hitting &lt;- tab[[1]] |&gt; html_table(fill = TRUE) |&gt;\n    head(-2) |&gt;\n    select(-23) |&gt;\n    mutate(wOBA = (0.69 * BB + 0.72 * HBP + 0.89 *\n                     (H- `2B` - `3B` - `HR`) +\n                     1.27 * `2B` + 1.62 * `3B` + 2.10 * HR) / \n             (AB + BB + SF + HBP),\n           url_name = url_name,\n           `#` = as.character(`#`))\n  \n  return(hitting)\n}\n\nWe can then test our function on the SLU url:\n\nget_hitting_data(url_name = \"https://saintsathletics.com/sports/baseball/stats/2022\")\n#&gt; # A tibble: 20 × 24\n#&gt;   `#`   Player    AVG   OPS `GP-GS`    AB     R     H  `2B`  `3B`    HR   RBI\n#&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n#&gt; 1 6     \"Clark… 0.367 1.02  29-19      79    24    29     5     2     3    23\n#&gt; 2 19    \"Delan… 0.33  0.817 34-34     115    18    38     5     1     1    24\n#&gt; 3 13    \"Butle… 0.325 1.08  35-35     126    31    41     9     5     7    36\n#&gt; 4 30    \"Watso… 0.318 0.853 35-33     110    29    35     6     0     1    20\n#&gt; 5 5     \"Desja… 0.284 0.741 28-20      67    23    19     1     0     0     9\n#&gt; 6 20    \"Court… 0.281 0.646 27-17      64     6    18     2     0     0    13\n#&gt; # ℹ 14 more rows\n#&gt; # ℹ 12 more variables: TB &lt;int&gt;, `SLG%` &lt;dbl&gt;, BB &lt;int&gt;, HBP &lt;int&gt;,\n#&gt; #   SO &lt;int&gt;, GDP &lt;int&gt;, `OB%` &lt;dbl&gt;, SF &lt;int&gt;, SH &lt;int&gt;, `SB-ATT` &lt;chr&gt;,\n#&gt; #   wOBA &lt;dbl&gt;, url_name &lt;chr&gt;\n\n\n15.3.2 Iteration\nNow suppose that we want to use our function to scrape the 2022 baseball statistics for all teams in the Liberty League. There are 10 teams in total. The websites for each team’s statistics as well as the school name is given in the tibble below:\n\nschool_df &lt;- tibble(school_name = c(\"SLU\", \"Clarkson\", \"Rochester\", \"RIT\", \"Ithaca\", \"Skidmore\", \"RPI\", \"Union\", \"Bard\", \"Vassar\"),\n                    hitting_web_url = c(\"https://saintsathletics.com/sports/baseball/stats/2022\",\n                 \"https://clarksonathletics.com/sports/baseball/stats/2022\", \n                 \"https://uofrathletics.com/sports/baseball/stats/2022\",\n                 \"https://ritathletics.com/sports/baseball/stats/2022\",\n                 \"https://athletics.ithaca.edu/sports/baseball/stats/2022\",\n                 \"https://skidmoreathletics.com/sports/baseball/stats/2022\",\n                 \"https://rpiathletics.com/sports/baseball/stats/2022\",\n                 \"https://unionathletics.com/sports/baseball/stats/2022\",\n                 \"https://bardathletics.com/sports/baseball/stats/2022\",\n                 \"https://www.vassarathletics.com/sports/baseball/stats/2022\"))\nschool_df\n\nOne option we have to obtain the hitting statistics for all 10 teams and calculating the wOBA (assuming that the tables are structured the same way on each web page) would be to apply our function 10 times and then bind together the results. The first three applications of the function are shown below.\n\nget_hitting_data(url_name = \"https://saintsathletics.com/sports/baseball/stats/2022\")\nget_hitting_data(url_name = \"https://clarksonathletics.com/sports/baseball/stats/2022\")\nget_hitting_data(url_name = \"https://uofrathletics.com/sports/baseball/stats/2022\")\n\nFor just 10 teams, this approach is certainly doable but is a bit annoying. And, what if we wanted to do this type of calculation for a league with more teams, such as the MLB (Major League Baseball)? Or, for multiple years for each team?\nA better approach is to use iteration and write code to repeatedly scrape the data from each website and calculate the wOBA statistic with our function. In CS 140, the primary form of iteration you used was probably a for loop. for loops in R have very similar syntax to for loops in Python. However, in general, for loops are clunky, can take up a lot of lines of code, and can be difficult to read.\nFor this section, we will instead focus on a functional programming approach to iteration through the map() function family in purrr. purrr is part of the core tidyverse so the package gets loaded in with library(tidyverse). The map() function has two arguments: the first is a vector or a list and the second is a function. map() applies the function in the second argument to each element of the vector or list in the first argument. For example, consider applying the get_sum_squares function we wrote earlier to a list of vectors:\n\nnum_list &lt;- list(vec1 = c(1, 4, 5), vec2 = c(9, 8, 3, 5), vec3 = 1)\nmap(num_list, get_sum_squares)\n#&gt; $vec1\n#&gt; [1] 42\n#&gt; \n#&gt; $vec2\n#&gt; [1] 179\n#&gt; \n#&gt; $vec3\n#&gt; [1] 1\n\nThe output is a list of sums of squares calculated with our get_sum_squares() function.\nTo apply the map() approach to iteration to our baseball web urls, we will create an object called url_vec that has the urls of each school.\n\nurl_vec &lt;- school_df$hitting_web_url\n\nThen, we apply map() with the first argument being the url_vec and the second argument being the function get_hitting_data() that we wrote earlier.\n\nhitting_list &lt;- map(url_vec, get_hitting_data)\nhitting_list\n\nScraping and performing the wOBA calculation will take a few seconds. The output is a list of 10 tibbles. The bind_rows() function that we used to stack rows of different data frames or tibbles can also be used to stack rows of data frames or tibbles given in a list. We apply the function to the scraped data and then add the name of the school to the data frame with a left_join():\n\nhitting_ll &lt;- hitting_list |&gt; bind_rows() |&gt;\n  left_join(school_df, by = join_by(url_name == hitting_web_url)) \nhitting_ll\n#&gt; # A tibble: 213 × 25\n#&gt;   `#`   Player    AVG   OPS `GP-GS`    AB     R     H  `2B`  `3B`    HR   RBI\n#&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n#&gt; 1 6     \"Clark… 0.367 1.02  29-19      79    24    29     5     2     3    23\n#&gt; 2 19    \"Delan… 0.33  0.817 34-34     115    18    38     5     1     1    24\n#&gt; 3 13    \"Butle… 0.325 1.08  35-35     126    31    41     9     5     7    36\n#&gt; 4 30    \"Watso… 0.318 0.853 35-33     110    29    35     6     0     1    20\n#&gt; 5 5     \"Desja… 0.284 0.741 28-20      67    23    19     1     0     0     9\n#&gt; 6 20    \"Court… 0.281 0.646 27-17      64     6    18     2     0     0    13\n#&gt; # ℹ 207 more rows\n#&gt; # ℹ 13 more variables: TB &lt;int&gt;, `SLG%` &lt;dbl&gt;, BB &lt;int&gt;, HBP &lt;int&gt;,\n#&gt; #   SO &lt;int&gt;, GDP &lt;int&gt;, `OB%` &lt;dbl&gt;, SF &lt;int&gt;, SH &lt;int&gt;, `SB-ATT` &lt;chr&gt;,\n#&gt; #   wOBA &lt;dbl&gt;, url_name &lt;chr&gt;, school_name &lt;chr&gt;\n\nWith this data set, we can now do things like figure out the top 3 hitters from each team, according to the wOBA metric:\n\nhitting_ll |&gt; group_by(school_name) |&gt;\n  arrange(desc(wOBA)) |&gt;\n  slice(1:3) |&gt;\n  select(Player, school_name, wOBA)\n#&gt; # A tibble: 30 × 3\n#&gt; # Groups:   school_name [10]\n#&gt;   Player                                                    school_name  wOBA\n#&gt;   &lt;chr&gt;                                                     &lt;chr&gt;       &lt;dbl&gt;\n#&gt; 1 \"Toby, Jared\\r\\n                                        … Bard        0.488\n#&gt; 2 \"Dumper, Sam\\r\\n                                        … Bard        0.475\n#&gt; 3 \"Myers, Jordan\\r\\n                                      … Bard        0.431\n#&gt; 4 \"Cantor, Danny\\r\\n                                      … Clarkson    0.805\n#&gt; 5 \"Price, Grant\\r\\n                                       … Clarkson    0.475\n#&gt; 6 \"Doyle, Caleb\\r\\n                                       … Clarkson    0.473\n#&gt; # ℹ 24 more rows\n\nor find the players on each team with the most at bats AB:\n\nhitting_ll |&gt; group_by(school_name) |&gt;\n  arrange(desc(AB)) |&gt;\n  slice(1:3) |&gt;\n  select(Player, school_name, AB)\n#&gt; # A tibble: 30 × 3\n#&gt; # Groups:   school_name [10]\n#&gt;   Player                                                    school_name    AB\n#&gt;   &lt;chr&gt;                                                     &lt;chr&gt;       &lt;int&gt;\n#&gt; 1 \"Toby, Jared\\r\\n                                        … Bard          107\n#&gt; 2 \"Myers, Jordan\\r\\n                                      … Bard          107\n#&gt; 3 \"Luscher, Alex\\r\\n                                      … Bard          101\n#&gt; 4 \"Brouillette, Colby\\r\\n                                 … Clarkson      127\n#&gt; 5 \"Wilson, Kent\\r\\n                                       … Clarkson      126\n#&gt; 6 \"Doyle, Caleb\\r\\n                                       … Clarkson      103\n#&gt; # ℹ 24 more rows\n\nExamine the code below scrapes data from a wikipedia page listing the billboard end of year “hot 100” songs for the year 2021.\n\nlibrary(rvest)\nlibrary(tidyverse)\n\nyear_scrape &lt;- 2021\nurl &lt;- paste0(\"https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_\", year_scrape)\n\n## convert the html code into something R can read\nbillboard_tab &lt;- read_html(url) |&gt; html_nodes(\"table\")\n\n## grabs the tables\nbillboard_df &lt;- billboard_tab[[1]] |&gt; html_table() |&gt;\n  mutate(year = year_scrape)\nbillboard_df\n#&gt; # A tibble: 100 × 4\n#&gt;     No. Title                 `Artist(s)`                   year\n#&gt;   &lt;int&gt; &lt;chr&gt;                 &lt;chr&gt;                        &lt;dbl&gt;\n#&gt; 1     1 \"\\\"Levitating\\\"\"      Dua Lipa                      2021\n#&gt; 2     2 \"\\\"Save Your Tears\\\"\" The Weeknd and Ariana Grande  2021\n#&gt; 3     3 \"\\\"Blinding Lights\\\"\" The Weeknd                    2021\n#&gt; 4     4 \"\\\"Mood\\\"\"            24kGoldn featuring Iann Dior  2021\n#&gt; 5     5 \"\\\"Good 4 U\\\"\"        Olivia Rodrigo                2021\n#&gt; 6     6 \"\\\"Kiss Me More\\\"\"    Doja Cat featuring SZA        2021\n#&gt; # ℹ 94 more rows\n\nExercise 1. Wrap the code above in a function that scrapes data from Wikipedia for a user-provided year_scrape argument.\nExercise 2. Create either a vector of the years 2014 through 2021 or a list of the years 2014 through 2021. Use your vector or list, along with the function you wrote in Exercise 1 and the map() function, to scrape data tables from each year.\nExercise 3. Combine the data frames you scraped in Exercise 2 with bind_rows() and use the combined data set to figure out which artist appears the highest number of times in the billboard hot 100 list within the years 2014 through 2021. To save time, once you have the data frames combined you should be able to do something like:\n\ncombined_df |&gt; group_by(`Artist(s)`) |&gt;\n  summarise(n_appear = n()) |&gt;\n  arrange(desc(n_appear))\n\nExercise 4. Write a short, one paragraph summary on your major take-aways from this section."
  },
  {
    "objectID": "15-connections.html#practice",
    "href": "15-connections.html#practice",
    "title": "15  Connections to STAT 113, STAT 213, and CS 140",
    "section": "\n15.4 Practice",
    "text": "15.4 Practice\nThere are no practice exercises for this section on connections to STAT 113, STAT 213, and CS 140."
  },
  {
    "objectID": "16-sql.html#what-is-a-database",
    "href": "16-sql.html#what-is-a-database",
    "title": "16  Introduction to SQL with dbplyr",
    "section": "\n16.1 What is a Database",
    "text": "16.1 What is a Database\nThe R for Data Science textbook defines a database as “a collection of data frames,” each called a database table. There a few key differences between a data frame (what we’ve been using the entire semester) and a database table. They are summarised from R for Data Science here as:\n\na database table can be larger and is stored on disk while a data frame is stored in memory so their size is more limited.\na database table usually indices while data frames do not.\nmany, but not all, data base tables are “row-oriented” while tidy data frames are “column-oriented.”\n\nDatabases are run through Database Management Systems. The R for Data Science textbook divides Database Management Systems into 3 types:\n\n\nclient-server like PostgreSQL and SQL Server\n\nCloud-based like Amazon’s Redshift\n\nIn-process like SQLite\n\nWe won’t really discuss these any further, but an advanced course in database systems through the CS department would give more information about Database Management Systems (and databases in general).\nHow to connect to a database from R depends on the type of database management system. For our purposes, because how to connect to a Database management system depends so heavily on the type, we will focus on a database management system that is contained in the R package duckdb.\nWe also need a database interface to connect to the database tables in duckdb in the DBI package.\n\n\n\n\n\n\nNote\n\n\n\nThis section on connecting to a database management systems may be confusing, particularly if you do not have a computer science background. But don’t let that derail your learning for this rest of this chapter, which will consist of primarily of R code from here on! The take-home message is that we need a way to connect to the system within R. It’s challenging to give specific directions because the connection depends on the type of system, so we are avoiding most of that by connecting to a database management system in the duckdb R package using functions from the DBI package.\n\n\nSQL is short from Structured Query Language. We first load in the duckdb and DBI libraries and make a connection to the database management system, which we will name con:\n\nlibrary(DBI)\nlibrary(duckdb)\ncon &lt;- DBI::dbConnect(duckdb::duckdb())\n\nWe can type in con to see what it stores:\n\ncon\n\nWe’ve created a brand-new database, so we can next add some data tables with the duckdb_read_csv() function. Compared to read_csv() from the readr package, duckdb_read_csv() has a couple of extra arguments: a conn argument giving the database management connection and a name argument giving the name that we want to give to the data table:\n\nlibrary(here)\nduckdb_read_csv(conn = con, name = \"tennis2018\", \n                files = here(\"data/atp_matches_2018.csv\"))\nduckdb_read_csv(conn = con, name = \"tennis2019\", \n                files = here(\"data/atp_matches_2019.csv\"))\n\nThe doListTables() function lists the names of the data tables in the database we just created:\n\ndbListTables(con)\n#&gt; [1] \"tennis2018\" \"tennis2019\"\n\nAnd, dbExistsTable() can be used to examine whether or not a data table exists in the current database:\n\ndbExistsTable(con, \"tennis2019\")\n#&gt; [1] TRUE\ndbExistsTable(con, \"tennis2020\")\n#&gt; [1] FALSE\n\nNote that, in many practical situations, the data tables will already exist in the database you are working with, so the step of duckdb_read_csv() would not be necessary.\nTo use raw SQL code and query the database that we just created, we can create a string of SQL code, name it sql, and pass it to the dbGetQuery() function. We also load in the tidyverse package here to use the as_tibble() function to convert the data.frame to a tibble.\n\nlibrary(tidyverse)\n\nsql &lt;- \"\n  SELECT surface, winner_name, loser_name, w_ace, l_ace, minutes\n  FROM tennis2019 \n  WHERE minutes &gt; 240\n\"\ndbGetQuery(con, sql)|&gt;\n  as_tibble()\n#&gt; # A tibble: 30 × 6\n#&gt;   surface winner_name           loser_name            w_ace l_ace minutes\n#&gt;   &lt;chr&gt;   &lt;chr&gt;                 &lt;chr&gt;                 &lt;int&gt; &lt;int&gt;   &lt;int&gt;\n#&gt; 1 Hard    Joao Sousa            Guido Pella              19    18     241\n#&gt; 2 Hard    Jeremy Chardy         Ugo Humbert              29    20     244\n#&gt; 3 Hard    Roberto Bautista Agut Andy Murray               7    19     249\n#&gt; 4 Hard    Joao Sousa            Philipp Kohlschreiber    28    20     258\n#&gt; 5 Hard    Alex Bolt             Gilles Simon             11    14     244\n#&gt; 6 Hard    Milos Raonic          Stan Wawrinka            39    28     241\n#&gt; # ℹ 24 more rows\n\nExercise 1. Though we do not know SQL code, we can probably figure out what the code (the string that is being assigned to sql) above is doing. Which matches are being returned from our query?\nExercise 2. What is the dplyr equivalent function to WHERE in the SQL code above? What is the dplyr equivalent function to SELECT in the SQL code above?"
  },
  {
    "objectID": "16-sql.html#dbplyr-a-database-version-of-dplyr",
    "href": "16-sql.html#dbplyr-a-database-version-of-dplyr",
    "title": "16  Introduction to SQL with dbplyr",
    "section": "\n16.2 dbplyr: A Database Version of dplyr\n",
    "text": "16.2 dbplyr: A Database Version of dplyr\n\ndbplyr is a package that will allow us to continue to write dplyr-style code to query databases instead of writing native SQL, as in the code-chunk above.\nWe begin by loading in the package and creating a database table object with the tbl() function. In this case, we create a database table with the tennis2019 data and name it tennis_db:\n\nlibrary(dbplyr)\ntennis_db &lt;- tbl(con, \"tennis2019\")\ntennis_db\n#&gt; # Source:   table&lt;tennis2019&gt; [?? x 49]\n#&gt; # Database: DuckDB 0.3.5-dev1410 [root@Darwin 22.6.0:R 4.2.1/:memory:]\n#&gt;   tourney_id tourney_name surface draw_size tourney_level tourney_date\n#&gt;   &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;       &lt;int&gt; &lt;chr&gt;                &lt;int&gt;\n#&gt; 1 2019-M020  Brisbane     Hard           32 A                 20181231\n#&gt; 2 2019-M020  Brisbane     Hard           32 A                 20181231\n#&gt; 3 2019-M020  Brisbane     Hard           32 A                 20181231\n#&gt; 4 2019-M020  Brisbane     Hard           32 A                 20181231\n#&gt; 5 2019-M020  Brisbane     Hard           32 A                 20181231\n#&gt; 6 2019-M020  Brisbane     Hard           32 A                 20181231\n#&gt; # ℹ more rows\n#&gt; # ℹ 43 more variables: match_num &lt;int&gt;, winner_id &lt;int&gt;, winner_seed &lt;chr&gt;,\n#&gt; #   winner_entry &lt;chr&gt;, winner_name &lt;chr&gt;, winner_hand &lt;chr&gt;,\n#&gt; #   winner_ht &lt;int&gt;, winner_ioc &lt;chr&gt;, winner_age &lt;dbl&gt;, loser_id &lt;int&gt;,\n#&gt; #   loser_seed &lt;chr&gt;, loser_entry &lt;chr&gt;, loser_name &lt;chr&gt;, loser_hand &lt;chr&gt;,\n#&gt; #   loser_ht &lt;int&gt;, loser_ioc &lt;chr&gt;, loser_age &lt;dbl&gt;, score &lt;chr&gt;,\n#&gt; #   best_of &lt;int&gt;, round &lt;chr&gt;, minutes &lt;int&gt;, w_ace &lt;int&gt;, w_df &lt;int&gt;, …\n\nExamine the print for tennis_db, which should look similar to the print for a tibble or data.frame. Let’s use some dplyr code to obtain only the matches that lasted longer than 240 minutes and keep only a few of the columns. We will name the result tennis_query1:\n\ntennis_query1 &lt;- tennis_db |&gt; \n  filter(minutes &gt; 240) |&gt; \n  select(minutes, winner_name, loser_name, minutes, tourney_name)\ntennis_query1\n#&gt; # Source:   SQL [?? x 4]\n#&gt; # Database: DuckDB 0.3.5-dev1410 [root@Darwin 22.6.0:R 4.2.1/:memory:]\n#&gt;   minutes winner_name           loser_name            tourney_name   \n#&gt;     &lt;int&gt; &lt;chr&gt;                 &lt;chr&gt;                 &lt;chr&gt;          \n#&gt; 1     241 Joao Sousa            Guido Pella           Australian Open\n#&gt; 2     244 Jeremy Chardy         Ugo Humbert           Australian Open\n#&gt; 3     249 Roberto Bautista Agut Andy Murray           Australian Open\n#&gt; 4     258 Joao Sousa            Philipp Kohlschreiber Australian Open\n#&gt; 5     244 Alex Bolt             Gilles Simon          Australian Open\n#&gt; 6     241 Milos Raonic          Stan Wawrinka         Australian Open\n#&gt; # ℹ more rows\n\nWe should note that the result is still a database object: it’s not our “usual” tibble. One major difference between the database object and the usual tibble is that our tennis_query1 does not tell us how many rows are in the data (see the ?? and the specification with more rows). The code that we wrote is not actually looking in the entire data set for matches that are longer than 240 minutes: it is saving time by only performing our query on part of the database table. This is very useful behaviour for database tables that are very, very large, where code might take a long time to run.\nIf we want to obtain the result of our query as a tibble, we can use the collect() function:\n\ntennis_query1 |&gt;\n  collect()\n#&gt; # A tibble: 30 × 4\n#&gt;   minutes winner_name           loser_name            tourney_name   \n#&gt;     &lt;int&gt; &lt;chr&gt;                 &lt;chr&gt;                 &lt;chr&gt;          \n#&gt; 1     241 Joao Sousa            Guido Pella           Australian Open\n#&gt; 2     244 Jeremy Chardy         Ugo Humbert           Australian Open\n#&gt; 3     249 Roberto Bautista Agut Andy Murray           Australian Open\n#&gt; 4     258 Joao Sousa            Philipp Kohlschreiber Australian Open\n#&gt; 5     244 Alex Bolt             Gilles Simon          Australian Open\n#&gt; 6     241 Milos Raonic          Stan Wawrinka         Australian Open\n#&gt; # ℹ 24 more rows\n\nThe result is a tibble that we can now use any R functions on (not just functions from dplyr and a few other packages).\nThe show_query() function can be used on our tennis_query1 to give the SQL code that was executed:\n\ntennis_query1 |&gt;\n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT \"minutes\", \"winner_name\", \"loser_name\", \"tourney_name\"\n#&gt; FROM \"tennis2019\"\n#&gt; WHERE (\"minutes\" &gt; 240.0)\n\nTo get a better idea about what SQL code looks like, let’s make one more query with dplyr code and use the show_query() function to give the native SQL:\n\nmedvedev_query &lt;- tennis_db |&gt;\n  pivot_longer(c(winner_name, loser_name), names_to = \"win_loss\",\n               values_to = \"player\") |&gt;\n  filter(player == \"Daniil Medvedev\") |&gt;\n  group_by(win_loss) |&gt;\n  summarise(win_loss_count = n())\nmedvedev_query\n#&gt; # Source:   SQL [2 x 2]\n#&gt; # Database: DuckDB 0.3.5-dev1410 [root@Darwin 22.6.0:R 4.2.1/:memory:]\n#&gt;   win_loss    win_loss_count\n#&gt;   &lt;chr&gt;                &lt;dbl&gt;\n#&gt; 1 winner_name             59\n#&gt; 2 loser_name              21\nshow_query(medvedev_query)\n#&gt; &lt;SQL&gt;\n#&gt; SELECT \"win_loss\", COUNT(*) AS \"win_loss_count\"\n#&gt; FROM (\n#&gt;   (\n#&gt;     SELECT\n#&gt;       \"tourney_id\",\n#&gt;       \"tourney_name\",\n#&gt;       \"surface\",\n#&gt;       \"draw_size\",\n#&gt;       \"tourney_level\",\n#&gt;       \"tourney_date\",\n#&gt;       \"match_num\",\n#&gt;       \"winner_id\",\n#&gt;       \"winner_seed\",\n#&gt;       \"winner_entry\",\n#&gt;       \"winner_hand\",\n#&gt;       \"winner_ht\",\n#&gt;       \"winner_ioc\",\n#&gt;       \"winner_age\",\n#&gt;       \"loser_id\",\n#&gt;       \"loser_seed\",\n#&gt;       \"loser_entry\",\n#&gt;       \"loser_hand\",\n#&gt;       \"loser_ht\",\n#&gt;       \"loser_ioc\",\n#&gt;       \"loser_age\",\n#&gt;       \"score\",\n#&gt;       \"best_of\",\n#&gt;       \"round\",\n#&gt;       \"minutes\",\n#&gt;       \"w_ace\",\n#&gt;       \"w_df\",\n#&gt;       \"w_svpt\",\n#&gt;       \"w_1stIn\",\n#&gt;       \"w_1stWon\",\n#&gt;       \"w_2ndWon\",\n#&gt;       \"w_SvGms\",\n#&gt;       \"w_bpSaved\",\n#&gt;       \"w_bpFaced\",\n#&gt;       \"l_ace\",\n#&gt;       \"l_df\",\n#&gt;       \"l_svpt\",\n#&gt;       \"l_1stIn\",\n#&gt;       \"l_1stWon\",\n#&gt;       \"l_2ndWon\",\n#&gt;       \"l_SvGms\",\n#&gt;       \"l_bpSaved\",\n#&gt;       \"l_bpFaced\",\n#&gt;       \"winner_rank\",\n#&gt;       \"winner_rank_points\",\n#&gt;       \"loser_rank\",\n#&gt;       \"loser_rank_points\",\n#&gt;       'winner_name' AS \"win_loss\",\n#&gt;       \"winner_name\" AS \"player\"\n#&gt;     FROM \"tennis2019\"\n#&gt;   )\n#&gt;   UNION ALL\n#&gt;   (\n#&gt;     SELECT\n#&gt;       \"tourney_id\",\n#&gt;       \"tourney_name\",\n#&gt;       \"surface\",\n#&gt;       \"draw_size\",\n#&gt;       \"tourney_level\",\n#&gt;       \"tourney_date\",\n#&gt;       \"match_num\",\n#&gt;       \"winner_id\",\n#&gt;       \"winner_seed\",\n#&gt;       \"winner_entry\",\n#&gt;       \"winner_hand\",\n#&gt;       \"winner_ht\",\n#&gt;       \"winner_ioc\",\n#&gt;       \"winner_age\",\n#&gt;       \"loser_id\",\n#&gt;       \"loser_seed\",\n#&gt;       \"loser_entry\",\n#&gt;       \"loser_hand\",\n#&gt;       \"loser_ht\",\n#&gt;       \"loser_ioc\",\n#&gt;       \"loser_age\",\n#&gt;       \"score\",\n#&gt;       \"best_of\",\n#&gt;       \"round\",\n#&gt;       \"minutes\",\n#&gt;       \"w_ace\",\n#&gt;       \"w_df\",\n#&gt;       \"w_svpt\",\n#&gt;       \"w_1stIn\",\n#&gt;       \"w_1stWon\",\n#&gt;       \"w_2ndWon\",\n#&gt;       \"w_SvGms\",\n#&gt;       \"w_bpSaved\",\n#&gt;       \"w_bpFaced\",\n#&gt;       \"l_ace\",\n#&gt;       \"l_df\",\n#&gt;       \"l_svpt\",\n#&gt;       \"l_1stIn\",\n#&gt;       \"l_1stWon\",\n#&gt;       \"l_2ndWon\",\n#&gt;       \"l_SvGms\",\n#&gt;       \"l_bpSaved\",\n#&gt;       \"l_bpFaced\",\n#&gt;       \"winner_rank\",\n#&gt;       \"winner_rank_points\",\n#&gt;       \"loser_rank\",\n#&gt;       \"loser_rank_points\",\n#&gt;       'loser_name' AS \"win_loss\",\n#&gt;       \"loser_name\" AS \"player\"\n#&gt;     FROM \"tennis2019\"\n#&gt;   )\n#&gt; ) \"q01\"\n#&gt; WHERE (\"player\" = 'Daniil Medvedev')\n#&gt; GROUP BY \"win_loss\"\n\nThe show_query() shows the native SQL code for a pivot: yikes! Remember that SQL was not designed for data analysis, so it doesn’t always look pretty. We’ll do one more simpler query:\n\nover20aces &lt;- tennis_db |&gt; filter(w_ace &gt; 20) |&gt;\n  select(w_ace, winner_name) |&gt;\n  group_by(winner_name) |&gt;\n  summarise(nmatch = n()) |&gt;\n  arrange(desc(nmatch))\nover20aces\n#&gt; # Source:     SQL [?? x 2]\n#&gt; # Database:   DuckDB 0.3.5-dev1410 [root@Darwin 22.6.0:R 4.2.1/:memory:]\n#&gt; # Ordered by: desc(nmatch)\n#&gt;   winner_name      nmatch\n#&gt;   &lt;chr&gt;             &lt;dbl&gt;\n#&gt; 1 John Isner           15\n#&gt; 2 Reilly Opelka        14\n#&gt; 3 Milos Raonic         10\n#&gt; 4 Sam Querrey           9\n#&gt; 5 Nick Kyrgios          8\n#&gt; 6 Alexander Bublik      7\n#&gt; # ℹ more rows\n\nover20aces |&gt; show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT \"winner_name\", COUNT(*) AS \"nmatch\"\n#&gt; FROM (\n#&gt;   SELECT \"w_ace\", \"winner_name\"\n#&gt;   FROM \"tennis2019\"\n#&gt;   WHERE (\"w_ace\" &gt; 20.0)\n#&gt; ) \"q01\"\n#&gt; GROUP BY \"winner_name\"\n#&gt; ORDER BY \"nmatch\" DESC\n\nCan you match some of the SQL code with the corresponding dplyr functions used?\nExercise 3. Obtain the distribution of the surface variable by making a table of the total number of matches played on each surface in the 2019 season using dplyr functions on tennis_db. Then, use show_query() to show the corresponding SQL code."
  },
  {
    "objectID": "16-sql.html#sql",
    "href": "16-sql.html#sql",
    "title": "16  Introduction to SQL with dbplyr",
    "section": "\n16.3 SQL",
    "text": "16.3 SQL\nThe purpose of this section is to explore SQL syntax a little more, focusing on its connections to dplyr.\n\n\n\n\n\n\nNote\n\n\n\nKnowing dplyr is quite helpful in learning this SQL syntax because, while the syntax differs, the concepts are quite similar.\n\n\nMuch of the text in this section is paraphrased from the R for Data Science textbook. There are five core components of an SQL query. The two most basic are a SELECT statement (similar to select(), and, as discussed below, mutate() and summarise()) and a FROM statement (similar to the data argument). Using the show_query() function directly on tennis_db shows an SQL query that SELECTs all columns (denoted by the *), FROM the tennis2019 database.\n\ntennis_db |&gt; show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT *\n#&gt; FROM \"tennis2019\"\n\nThe WHERE and ORDER BY statements control which rows are returned (similar to filter()) and in what order those rows get returned (similar to arrange()):\n\ntennis_db |&gt; filter(winner_hand == \"L\") |&gt;\n  arrange(desc(tourney_date)) |&gt;\n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT *\n#&gt; FROM \"tennis2019\"\n#&gt; WHERE (\"winner_hand\" = 'L')\n#&gt; ORDER BY \"tourney_date\" DESC\n\nFinally, GROUP BY is used for aggregation (similar to the dplyr group_by() and summarise() combination).\n\ntennis_db |&gt;\n  group_by(winner_name) |&gt;\n  summarise(meanace = mean(w_ace, na.rm = TRUE)) |&gt;\n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT \"winner_name\", AVG(\"w_ace\") AS \"meanace\"\n#&gt; FROM \"tennis2019\"\n#&gt; GROUP BY \"winner_name\"\n\nIn the above code chunk, remove the na.rm = TRUE argument and run the query. What do you learn?\nThe SQL syntax must always follow the order SELECT, FROM, WHERE, GROUP BY, ORDER BY, even though the operations can be performed in a different order than what is specified. This is one aspect that makes SQL harder to pick up than something like dplyr, where we specify what we want done in the order that we want.\nBelow we give a little more detail about the 5 operations.\nSELECT: SELECT covers a lot of dplyr functions. In the code below, we explore how it is used in SQL to choose which columns get returned, rename columns, and create new variables:\n\n\nSELECT to choose which columns to return:\n\n\ntennis_db |&gt; select(1:4) |&gt; show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT \"tourney_id\", \"tourney_name\", \"surface\", \"draw_size\"\n#&gt; FROM \"tennis2019\"\n\n\n\nSELECT to rename columns:\n\n\ntennis_db |&gt; rename(tournament = tourney_name) |&gt;\n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT\n#&gt;   \"tourney_id\",\n#&gt;   \"tourney_name\" AS \"tournament\",\n#&gt;   \"surface\",\n#&gt;   \"draw_size\",\n#&gt;   \"tourney_level\",\n#&gt;   \"tourney_date\",\n#&gt;   \"match_num\",\n#&gt;   \"winner_id\",\n#&gt;   \"winner_seed\",\n#&gt;   \"winner_entry\",\n#&gt;   \"winner_name\",\n#&gt;   \"winner_hand\",\n#&gt;   \"winner_ht\",\n#&gt;   \"winner_ioc\",\n#&gt;   \"winner_age\",\n#&gt;   \"loser_id\",\n#&gt;   \"loser_seed\",\n#&gt;   \"loser_entry\",\n#&gt;   \"loser_name\",\n#&gt;   \"loser_hand\",\n#&gt;   \"loser_ht\",\n#&gt;   \"loser_ioc\",\n#&gt;   \"loser_age\",\n#&gt;   \"score\",\n#&gt;   \"best_of\",\n#&gt;   \"round\",\n#&gt;   \"minutes\",\n#&gt;   \"w_ace\",\n#&gt;   \"w_df\",\n#&gt;   \"w_svpt\",\n#&gt;   \"w_1stIn\",\n#&gt;   \"w_1stWon\",\n#&gt;   \"w_2ndWon\",\n#&gt;   \"w_SvGms\",\n#&gt;   \"w_bpSaved\",\n#&gt;   \"w_bpFaced\",\n#&gt;   \"l_ace\",\n#&gt;   \"l_df\",\n#&gt;   \"l_svpt\",\n#&gt;   \"l_1stIn\",\n#&gt;   \"l_1stWon\",\n#&gt;   \"l_2ndWon\",\n#&gt;   \"l_SvGms\",\n#&gt;   \"l_bpSaved\",\n#&gt;   \"l_bpFaced\",\n#&gt;   \"winner_rank\",\n#&gt;   \"winner_rank_points\",\n#&gt;   \"loser_rank\",\n#&gt;   \"loser_rank_points\"\n#&gt; FROM \"tennis2019\"\n\n\n\nSELECT to create a new variable\n\n\ntennis_db |&gt; mutate(prop_first_won = w_1stIn / w_1stWon) |&gt;\n  select(prop_first_won, winner_name) |&gt;\n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT \"w_1stIn\" / \"w_1stWon\" AS \"prop_first_won\", \"winner_name\"\n#&gt; FROM \"tennis2019\"\n\n\n\nSELECT to create a new variable that is a summary:\n\n\ntennis_db |&gt; summarise(mean_length = mean(minutes)) |&gt;\n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT AVG(\"minutes\") AS \"mean_length\"\n#&gt; FROM \"tennis2019\"\n\n\nGROUP BY: GROUP BY covers aggregation in a similar way as dplyr’s group_by() function:\n\ntennis_db |&gt; group_by(winner_name) |&gt;\n  summarise(meanlength = mean(minutes)) |&gt;\n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT \"winner_name\", AVG(\"minutes\") AS \"meanlength\"\n#&gt; FROM \"tennis2019\"\n#&gt; GROUP BY \"winner_name\"\n\nWHERE: WHERE is used for filter(), though SQL uses different Boolean operators than R (for example, & becomes AND, | becomes or).\n\ntennis_db |&gt; filter(winner_age &gt; 35 | loser_age &gt; 35) |&gt;\n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT *\n#&gt; FROM \"tennis2019\"\n#&gt; WHERE (\"winner_age\" &gt; 35.0 OR \"loser_age\" &gt; 35.0)\n\nORDER BY: ORDER BY is used for arrange(). This one is quite straightforward:\n\ntennis_db |&gt; arrange(desc(winner_rank_points)) |&gt;\n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT *\n#&gt; FROM \"tennis2019\"\n#&gt; ORDER BY \"winner_rank_points\" DESC\n\nSQL also has corresponding syntax for the xxxx_join() family of functions, but we do not have time to discuss this in detail. Note that we have really just scratched the surface of SQL. There are entire courses devoted to learning SQL syntax and more about databases in general. If you ever do find yourself in a situation where you need to learn SQL, either for a course or for a job, you should have a major head-start with your dplyr knowledge!\nIn much of this section, we have created code with dplyr and seen how that code translates to SQL. In this exercise, you will instead be given SQL code and asked to write dplyr code that achieves the same thing.\nExercise 4. Examine the SQL code below and write equivalent dplyr code.\n\nSELECT * \nFROM \"tennis2019\"\nWHERE (\"tourney_name\" = 'Wimbledon')"
  },
  {
    "objectID": "16-sql.html#practice",
    "href": "16-sql.html#practice",
    "title": "16  Introduction to SQL with dbplyr",
    "section": "\n16.4 Practice",
    "text": "16.4 Practice\n\n16.4.1 Class Exercises\nClass Exercise 1. Examine the SQL code below and write equivalent dplyr code.\n\nSELECT \"winner_name\", \"loser_name\", \"w_ace\", \"l_ace\", \"w_ace\" - \"l_ace\" AS \"ace_diff\"\nFROM \"tennis2019\"\nORDER BY \"ace_diff\" DESC\n\nClass Exercise 2. Create a new variable that is the difference in the winner_rank_points and loser_rank_points using a dplyr function. Then, have your query return only the column you just created, the winner_name column, and the loser_name column. Use the show_query() function to show the corresponding SQL code.\n\n16.4.2 Your Turn\nYour Turn 1. Examine the SQL code below and write equivalent dplyr code.\n\nSELECT \"tourney_name\", AVG(\"minutes\") AS \"mean_min\"\nFROM \"tennis2019\"\nGROUP BY \"tourney_name\"\n\nYour Turn 2. Perform a query of your choosing on tennis_db and use the show_query() function to show the corresponding SQL code.\nYour Turn 3. Try to run a function from lubridate or forcats on tennis_db with mutate(). Does the function work? Did you expect it to work?\nYour Turn 4. Run the following code and write how the ! is translated to SQL.\n\ntennis_db |&gt; filter(winner_name != \"Daniil Medvedev\") |&gt;\n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT *\n#&gt; FROM \"tennis2019\"\n#&gt; WHERE (\"winner_name\" != 'Daniil Medvedev')\n\nYour Turn 5. Run the following code and write how the %in% symbol is translated to SQL.\n\ntennis_db |&gt;\n  filter(winner_name %in% c(\"Daniil Medvedev\", \"Dominic Thiem\")) |&gt;\n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT *\n#&gt; FROM \"tennis2019\"\n#&gt; WHERE (\"winner_name\" IN ('Daniil Medvedev', 'Dominic Thiem'))"
  }
]
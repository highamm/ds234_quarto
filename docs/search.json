[
  {
    "path": "index.html",
    "id": "syllabus-and-course-information",
    "chapter": " 1 Syllabus and Course Information",
    "heading": " 1 Syllabus and Course Information",
    "text": "",
    "code": ""
  },
  {
    "path": "index.html",
    "id": "general-information",
    "chapter": " 1 Syllabus and Course Information",
    "heading": "1.1 General Information",
    "text": "Instructor InformationProfessor: Matt HighamOffice: Bewkes 123Email: mhigham@stlawu.eduSemester: Fall 2022Sections:\nMW 2:30 - 4:00\nMW 2:30 - 4:00Office Hours: 15 minute slots bookable calendly page.\nNote must book time office hours least 12 hours advance guarantee present available time.\nNote must book time office hours least 12 hours advance guarantee present available time.Course MaterialsSTAT 234 Materials Bundle. primary source materials.Textbooks (used references):\nModern Data Science R Baumer, Kaplan, Horton, found free online version.\nR Data Science Grolemund Wickham, found free online version.\nModern Data Science R Baumer, Kaplan, Horton, found free online version.R Data Science Grolemund Wickham, found free online version.Computer Internet access.",
    "code": ""
  },
  {
    "path": "index.html",
    "id": "course-information",
    "chapter": " 1 Syllabus and Course Information",
    "heading": "1.2 Course Information",
    "text": "Welcome STAT 234! overall purpose course learn data science skills necessary complete large-scale data analysis projects. tool using achieve goal statistical software language R. work wide variety interesting data sets throughout semester build R skills. particular, focus Data Analysis Life Cycle (Grolemund Wickham 2020):put emphasis Import, Tidy, Transform, Visualize, Communicate parts cycle, introduction Modeling part covered STAT 213.",
    "code": ""
  },
  {
    "path": "index.html",
    "id": "use-of-r-and-rstudio",
    "chapter": " 1 Syllabus and Course Information",
    "heading": "1.2.1 Use of R and RStudio",
    "text": "use statistical software R construct graphs analyze data. notes:R RStudio free use.primarily using SLU R Studio server first: rstudio.stlawu.local:8787.Additionally, using RMarkdown data analysis reports. Note: ’s always nice start assignments projects early possible, particularly important assignments projects involving R. ’s fun try figure code working last minute. start early enough though, plenty time seek help therefore won’t waste lot time coding error.",
    "code": ""
  },
  {
    "path": "index.html",
    "id": "general-course-outcomes",
    "chapter": " 1 Syllabus and Course Information",
    "heading": "1.3 General Course Outcomes",
    "text": "Import data different types R analysis.Import data different types R analysis.Tidy data form can easily visualized, summarised, modeled.Tidy data form can easily visualized, summarised, modeled.Transform, Wrangle, Visualize variables data set assess patterns data.Transform, Wrangle, Visualize variables data set assess patterns data.Communicate results analysis target audience written report, , possibly oral presentation.Communicate results analysis target audience written report, , possibly oral presentation.Practice reproducible statistical practices use Quarto data analysis projects.Practice reproducible statistical practices use Quarto data analysis projects.Explain ethically important consider context data set comes .Explain ethically important consider context data set comes .Develop necessary skills able ask answer future data analysis questions , either using R another program, Python.Develop necessary skills able ask answer future data analysis questions , either using R another program, Python.paraphrase R Data Science textbook, 80% skills necessary complete data analysis project can learned coursework classes like one. , 20% particular project involve learning new things specific project. Achieving Goal # 6 allow learn extra 20% .",
    "code": ""
  },
  {
    "path": "index.html",
    "id": "how-you-will-be-assessed",
    "chapter": " 1 Syllabus and Course Information",
    "heading": "1.4 How You Will Be Assessed",
    "text": "components grade described :ModulesEach week, submit 60-point Module, consisting following:Exercise Set (10 points): Due Mondays usually STAT 234 Materials Bundle. Exercises graded completion solutions typically provided submit Canvas. Collaboration allowed.Take-Home Quiz (20 points): Due Wednesdays. Take-Home quizzes graded correctness. Collaboration allowed.-Class Quiz (30 points): Given Wednesdays start class. -Class quizzes graded correctness. Collaboration allowed. questions -class quiz based exercises complete class, exercise sets complete, take-home quiz questions.13 modules total. three modules, complete 50-point Project instead two quizzes. Project tasks complete particular data set. lowest module dropped grade total number points available 12 * 60 = 720 points.Additionally, one module, permitted take -class quiz take-home turn following Monday.Finally, choose take (optional) -person final exam (described ), score earn final replace second third-lowest module scores.ClassClass participation assessed three times throughout semester 20 point rubric total 60 points. rubric used shared first day class.Final ProjectThere one final project, worth 100 points. primary purpose final project give opportunity assemble topics throughout course one coherent data analysis. able choose data set use final project, might begin thinking particular topic data set interested exploring. final project presented format decided later semester.Final ExamThere optional Final Exam, worth 120 points total, consisting 20 points Exercises (graded completeness), 40 points take-home portion, 60 points -class portion. must campus final exam time take final exam.two options final exam:Option 1: Skip final exam (skip components: exercises, take-home, -class) assign average percentage 12 highest modules percentage score 120 point final. example, suppose 13 module scores : 60, 60, 59, 58, 57, 53, 53, 53, 53, 45, 43, 30, 0. , drop lowest score (0) score final : (60 + 60 + 59 + 58 + 57 + 53 + 53 + 53 + 53 + 45 + 43 + 30 + 30) / 720 * 120 = 109 / 120 points.Option 1: Skip final exam (skip components: exercises, take-home, -class) assign average percentage 12 highest modules percentage score 120 point final. example, suppose 13 module scores : 60, 60, 59, 58, 57, 53, 53, 53, 53, 45, 43, 30, 0. , drop lowest score (0) score final : (60 + 60 + 59 + 58 + 57 + 53 + 53 + 53 + 53 + 45 + 43 + 30 + 30) / 720 * 120 = 109 / 120 points.Option 2: Take final exam, consists 20 points Exercises, 30 point Take-Home portion, 50 point -Class portion. score items used 100 points devoted final exam. Additionally, final exam score better 2nd 3rd lowest module grades, replaced final exam score. example, suppose 13 module scores : 60, 60, 59, 58, 57, 53, 53, 53, 53, 45, 43, 30, 0. take final exam score 110 / 120. , score final 110 / 120 points new module scores : 60, 60, 59, 58, 57, 53, 53, 53, 53, 45, 55, 55, 0. 0 still dropped lowest module score.Option 2: Take final exam, consists 20 points Exercises, 30 point Take-Home portion, 50 point -Class portion. score items used 100 points devoted final exam. Additionally, final exam score better 2nd 3rd lowest module grades, replaced final exam score. example, suppose 13 module scores : 60, 60, 59, 58, 57, 53, 53, 53, 53, 45, 43, 30, 0. take final exam score 110 / 120. , score final 110 / 120 points new module scores : 60, 60, 59, 58, 57, 53, 53, 53, 53, 45, 55, 55, 0. 0 still dropped lowest module score.",
    "code": ""
  },
  {
    "path": "index.html",
    "id": "breakdown",
    "chapter": " 1 Syllabus and Course Information",
    "heading": "1.4.1 Breakdown",
    "text": "720 points Modules60 points Class Participation100 points Final Project120 points (optional) -person Final ExamPoints add 1000 grade end semester number points ’ve earned across categories divided 1000.",
    "code": ""
  },
  {
    "path": "index.html",
    "id": "grading-scale",
    "chapter": " 1 Syllabus and Course Information",
    "heading": "1.4.2 Grading Scale",
    "text": "following rough grading scale. reserve right make changes scale necessary.",
    "code": ""
  },
  {
    "path": "index.html",
    "id": "collaboration-diversity-accessibility-and-academic-integrity",
    "chapter": " 1 Syllabus and Course Information",
    "heading": "1.5 Collaboration, Diversity, Accessibility, and Academic Integrity",
    "text": "",
    "code": ""
  },
  {
    "path": "index.html",
    "id": "rules-for-collaboration",
    "chapter": " 1 Syllabus and Course Information",
    "heading": "1.5.1 Rules for Collaboration",
    "text": "Collaboration classmates exercises, take-home quizzes, projects encouraged, must follow guidelines:must state name(s) collaborated top assessment.work must . means never send someone code via email let someone directly type code screen. Instead, can talk strategies solving problems help ask someone coding error.may use Internet StackExchange, also copy paste code directly website, without citing .isn’t rule, keep mind collaboration permitted quizzes, exams, limited collaboration permitted final project. Therefore, working someone, make sure really learning can success non-collaborative assessments.",
    "code": ""
  },
  {
    "path": "index.html",
    "id": "diversity-statement",
    "chapter": " 1 Syllabus and Course Information",
    "heading": "1.5.2 Diversity Statement",
    "text": "Diversity encompasses differences age, colour, ethnicity, national origin, gender, physical mental ability, religion, socioeconomic background, veteran status, sexual orientation, marginalized groups. interaction different human characteristics brings positive learning environment. Diversity respected valued classroom.",
    "code": ""
  },
  {
    "path": "index.html",
    "id": "accessibility-statement",
    "chapter": " 1 Syllabus and Course Information",
    "heading": "1.5.3 Accessibility Statement",
    "text": "specific learning profile, medical mental health condition need accommodations, please sure contact Student Accessibility Services Office right away can help get accommodations require. need use accommodations class, please meet instructor early provide Individualized Educational Accommodation Plan (IEAP) letter can best possible experience semester.Although required, instructor like know accommodations needed least 10 days quiz test. Please proactive set appointment meet someone Student Accessibility Services Office.Color-Vision Deficiency: Color-Vision Deficient, Student Accessibility Services office loan glasses students color vision deficient. Please contact office make appointment.specific information setting appointment Student Accessibility Services please see listed options :Telephone: 315.229.5537Email: studentaccessibility@stlawu.eduFor information Student Accessibility Services can check website : https://www.stlawu.edu/student-accessibility-services",
    "code": ""
  },
  {
    "path": "index.html",
    "id": "academic-dishonesty",
    "chapter": " 1 Syllabus and Course Information",
    "heading": "1.5.4 Academic Dishonesty",
    "text": "Academic dishonesty tolerated. specific policies course supplementary theHonor Code. According St. Lawrence University Academic Honor Policy,assumed work done student unless instructor/mentor/employer gives specific permission collaboration.Cheating examinations tests consists knowingly giving using attempting use unauthorized assistance examinations tests.Dishonesty work outside examinations tests consists handing presenting original work original, originality required.Claims ignorance academic personal pressure unacceptable excuses academic dishonesty. Students must learn constitutes one’s work work others must acknowledged.information, refer www.stlawu.edu/acadaffairs/academic_honor_policy.pdf.avoid academic dishonesty, important follow directions collaboration rules ask clarification questions acceptable particular assignment exam. suspect academic dishonesty, score zero given entire assignment academic dishonesty occurred individuals involved Academic Honor Council notified. pattern academic dishonesty found occurred, grade 0.0 entire course can given.important work way maximizes learning. aware students rely much others homework projects tend poorly quizzes exams.Please note addition , assignments score reduced due academic dishonesty dropped according quiz policy e.g., receive zero quiz academic dishonesty, dropped grade.",
    "code": ""
  },
  {
    "path": "index.html",
    "id": "tentative-schedule",
    "chapter": " 1 Syllabus and Course Information",
    "heading": "1.6 Tentative Schedule",
    "text": "three projects tentatively scheduled due September 28, October 19, November 2, though subject change.",
    "code": ""
  },
  {
    "path": "intro.html",
    "id": "intro",
    "chapter": " 2 Getting Started with R and R Studio",
    "heading": " 2 Getting Started with R and R Studio",
    "text": "Goals:Download R R StudioDownload R R StudioUse Quarto code chunksUse Quarto code chunksLoad data R StudioLoad data R StudioRun code change things within codeRun code change things within codeCorrect common errors running code RCorrect common errors running code R",
    "code": ""
  },
  {
    "path": "intro.html",
    "id": "intro-to-r-and-r-studio",
    "chapter": " 2 Getting Started with R and R Studio",
    "heading": "2.1 Intro to R and R Studio",
    "text": "R statistical computing software used many statisticians well professionals fields, biology, ecology, business, psychology. goal Week 0 provide basic familiarity R Quarto, using entire semester.",
    "code": ""
  },
  {
    "path": "intro.html",
    "id": "installing-r-and-r-studio",
    "chapter": " 2 Getting Started with R and R Studio",
    "heading": "2.1.1 Installing R and R Studio",
    "text": "R Studio server computer set-carry R-based analyses students remote access computer (case, SLU Login credentials). might helpful think server large machine keyboard screen: ’s purpose execute code. may used R Studio server different stats course. server benefits, asusing server ensures using version R. theory, one person gets error, everyone get error.using server ensures using version R. theory, one person gets error, everyone get error.installing R R Studio personal device much easier ’ve experience using server.installing R R Studio personal device much easier ’ve experience using server.don’t need computer capable running R use server (can use tablet Chromebook since server actual computation).don’t need computer capable running R use server (can use tablet Chromebook since server actual computation)., however, move away server install R R Studio devices. Though server advantages, also disadvantages:won’t SLU login forever, , wanted use R post graduation, ’d need know install .won’t SLU login forever, , wanted use R post graduation, ’d need know install .haven’t experience installing R packages. quite easy , ’ve installed necessary R packages server us haven’t worry step.haven’t experience installing R packages. quite easy , ’ve installed necessary R packages server us haven’t worry step.server requires good Internet access also potential crash.server requires good Internet access also potential crash.next section, work installing R R Studio personal laptop. following videos provide instructions install R R Studio laptop computer. easiest complete steps consecutively one sitting. Watch follow along video installing R.  Watch follow along video installing R Studio.  Watch follow along video installing R packages changing options. ",
    "code": ""
  },
  {
    "path": "intro.html",
    "id": "relevant-websites",
    "chapter": " 2 Getting Started with R and R Studio",
    "heading": "2.1.2 Relevant Websites",
    "text": "Install R: http://lib.stat.cmu.edu/R/CRAN/Install R: http://lib.stat.cmu.edu/R/CRAN/Install R Studio (free option): https://www.rstudio.com/products/rstudio/download/Install R Studio (free option): https://www.rstudio.com/products/rstudio/download/",
    "code": ""
  },
  {
    "path": "intro.html",
    "id": "creating-an-r-project",
    "chapter": " 2 Getting Started with R and R Studio",
    "heading": "2.1.3 Creating an R Project",
    "text": "R R Studio installed, open R Studio Applications. Create new folder Desktop (place easy access remember). Make sure folder name spaces ., R Studio, create R Project Clicking File -> New Project -> Existing Directory. Navigate DATA234 folder made, click Create Project. see new window R Studio open .Next, want put data folder folder newly created R Project. Download data.zip file Canvas (Resources) move zip file folder R project (can drag drop downloads, copy/paste downloads, move whatever method usually move files ). Clicking data.zip file (either window bottom-left window R Studio) created data folder data sets want use throughout semester.Finally, want create new Quarto file clicking File -> New File -> Quarto Document. can give new Quarto file title want, click okay.also going change one option routinely Quarto files. Change first lines file something like:Note self-contained: true option added. ensures figures, images, tables, etc. contained one .html file, important , quizzes exercises, typically turn .html file.moving , click Render button top-left window top menu bar. Make sure file renders pretty-looking .html file. newly rendered .html file can now found folder R project.",
    "code": "---\ntitle: \"Your Title\"\nauthor: \"Your Name\"\nformat: \n  html:\n    self-contained: true\n---"
  },
  {
    "path": "intro.html",
    "id": "what-are-r-r-studio-and-quarto",
    "chapter": " 2 Getting Started with R and R Studio",
    "heading": "2.2 What are R, R Studio, and Quarto?",
    "text": "distinction 3 become clear later . now,R statistical coding software used heavily data analysis statistical procedures.R statistical coding software used heavily data analysis statistical procedures.R Studio nice IDE (Integrated Development Environment) R lot convenient features. Think just convenient User Interface.R Studio nice IDE (Integrated Development Environment) R lot convenient features. Think just convenient User Interface.Quarto allows users mix regular Microsoft-Word-style text code. .qmd file ending denotes Quarto file. Quarto many options use heavily throughout semester, ’s need worry now.Quarto allows users mix regular Microsoft-Word-style text code. .qmd file ending denotes Quarto file. Quarto many options use heavily throughout semester, ’s need worry now.",
    "code": ""
  },
  {
    "path": "intro.html",
    "id": "r-packages-and-the-tidyverse",
    "chapter": " 2 Getting Started with R and R Studio",
    "heading": "2.2.1 R Packages and the tidyverse",
    "text": "can think R packages add-ons R let things R able . ’re video games, can think R packages extra Downloadable Content (DLC). , unlike gaming DLC, R packages always free make heavy use R packages.tidyverse series R packages useful data science. order encounter class, core tidyverse packages :ggplot2 plotting datadplyr data wrangling summarizingtidyr data tidying reshapingreadr data importtibble data storedstringr text dataforcats factor (categorical) datapurrr, functional programming, one core 8 won’t get useWe use packages outside core tidyverse well, tidyverse main focus.",
    "code": ""
  },
  {
    "path": "intro.html",
    "id": "installing-r-packages",
    "chapter": " 2 Getting Started with R and R Studio",
    "heading": "2.2.2 Installing R Packages",
    "text": "R Studio server, either one statistics faculty members installed packages ’ve needed use server globally. However, want use package isn’t installed server, , want use package using R Studio personal computer, need install first.Installation needs happen (upgrade R, usually doesn’t happen often), whereas package needs loaded library() every time open R. analogy lightbulb might helpful. need screw lightbulb socket , , every time want lightbulb provide light, need flip light switch.lightbulb analogy, putting lightbulb socket correspond ? flipping light switch correspond ?Now R computer, ’ll need install packages want use (, remember just need install package ). Try installing tidyverse package, collection many useful data science packages, :",
    "code": "\ninstall.packages(\"tidyverse\")"
  },
  {
    "path": "intro.html",
    "id": "putting-code-in-a-.qmd-file",
    "chapter": " 2 Getting Started with R and R Studio",
    "heading": "2.3 Putting Code in a .qmd File",
    "text": "first thing involves code load package R library() function. package just R add-lets just R . Load tidyverse package R typing running library(tidyverse) line. create code chunk, click Insert -> R. Within code chunk, type library(tidyverse) run code eitherClicking “Run” button menu bar top-left window R Studio orClicking “Run” button menu bar top-left window R Studio (Recommended) Clicking “Command + Enter” Mac “Control + Enter” PC.(Recommended) Clicking “Command + Enter” Mac “Control + Enter” PC.Note code appears grey boxes surrounded three backticks normal text different colour background backticks.run previous line, text appear bottom-left window. won’t worry much text means now, also won’t ignore completely. able spot 8 core tidyverse packages listed well numbers follow package. numbers correspond package version. ’s things , long text start “Error:”, ’re good go!Congrats running first line code class! particular code isn’t particularly exciting doesn’t really anything can see.run R code using R chunk. R chunk, new line, try typing basic calculation, like 71 + 9 4 / 3, run line observe result., still wasn’t super exciting. R can perform basic calculations, just use calculator Excel . order look things bit interesting, need data.",
    "code": "\nlibrary(tidyverse)"
  },
  {
    "path": "intro.html",
    "id": "alcohol-data-example",
    "chapter": " 2 Getting Started with R and R Studio",
    "heading": "2.4 Alcohol Data Example",
    "text": "looking two data sets just get little bit preview things working rest semester. Important: worry understanding following code point. plenty time understand weeks ahead. purpose section just get used using R: detailed explanations exercises functions used various options coming weeks. particular, following code uses ggplot2, dplyr, tidyr packages, cover detail throughout first ~ 3-4 weeks course.Data first part obtained fivethirtyeight Five Thirty Eight GitHub page.first step read data set R. Though already downloaded alcohol.csv data zip, still need load R. Check make sure alcohol.csv data folder bottom-right hand window. following code can copied R code chunk read data:Note need full file extension data set R project.something show console window? , great! , make sure data set data folder R project set .like name data set something easily reference later, name data set using <- operator, inYou can name data set whatever want (restrictions). ’ve named alcohol_data. Now, run line code name data set, run alcohol_data, see data set appear:’s data set? see variables columns:country: name countrybeer_servings: average number beer servings per person per yearspirit_servings: average number spirit (hard alcohol) servings per person per yearwine_servings: average number wine servings per person per yeartotal_litres_of_pure_alcohol: average total litres pure alcohol consumed per person per year.One goal class able pose questions data set use tools learn answer questions. example, might want know distribution total litres alcohol consumed per person looks like across countries. , can make plot ggplot2 package, one packages automatically loads tidyverse. might start constructing following plot. Reminder: goal everyone understand code plot, don’t worry much .now want see United States (USA) falls distribution drawing red vertical line total litres alcohol consumed United States. , ’ll first use filter() function dplyr package (, learn function detail later). Copy paste following lines code new R chunk. , run lines.looks like countries consume little alcohol. might want know countries :looks like 13 countries data set consume alcohol. Note , chunk , use total_litres_of_pure_alcohol variable name name variable data set. Even something like spelling litres American English liters (total_liters_of_pure_alcohol) throw error isn’t exact name variable data set. something can aggravating first learning coding language.Now suppose want know 3 countries consume beer, 3 countries consume spirits, 3 countries consume wine per person. ’re trivia person, can form guesses. Without cheating, going guess (Germany, USA, UK) beer, (Spain, Italy, USA) wine, (Russia, Poland, Lithuania) spirits. Let’s beer first!Let’s thing Wine Spirits:Finally, suppose want know country consumes wine relative beer consumption? Let’s first look question graphically. need tidy data first pivot_longer() function tidyr package:x-axis corresponds beer servings y-axis corresponds wine servings. reference line given countries line consuming wine beer. get make plot like later: now, copy code chunk change labeled point corresponds country interests (Denmark).\nmight able better answer original question numerically computing wine beer ratio country ordering largest ratio smallest ratio:one ratios Inf?",
    "code": "\nread_csv(\"data/alcohol.csv\")\nalcohol_data <- read_csv(\"data/alcohol.csv\")\nalcohol_data\n#> # A tibble: 193 × 5\n#>    country           beer_servings spirit_…¹ wine_…² total…³\n#>    <chr>                     <dbl>     <dbl>   <dbl>   <dbl>\n#>  1 Afghanistan                   0         0       0     0  \n#>  2 Albania                      89       132      54     4.9\n#>  3 Algeria                      25         0      14     0.7\n#>  4 Andorra                     245       138     312    12.4\n#>  5 Angola                      217        57      45     5.9\n#>  6 Antigua & Barbuda           102       128      45     4.9\n#>  7 Argentina                   193        25     221     8.3\n#>  8 Armenia                      21       179      11     3.8\n#>  9 Australia                   261        72     212    10.4\n#> 10 Austria                     279        75     191     9.7\n#> # … with 183 more rows, and abbreviated variable names\n#> #   ¹​spirit_servings, ²​wine_servings,\n#> #   ³​total_litres_of_pure_alcohol\n#> # ℹ Use `print(n = ...)` to see more rows\nggplot(data = alcohol_data,\n       mapping = aes(total_litres_of_pure_alcohol)) +\n  geom_histogram(colour = \"black\", fill = \"white\", bins = 15)\nsmall_df <- alcohol_data |> filter(country == \"USA\")\nggplot(data = alcohol_data,\n       mapping = aes(total_litres_of_pure_alcohol)) +\n  geom_histogram(colour = \"black\", fill = \"white\", bins = 15) +\n  geom_vline(data = small_df,\n             aes(xintercept = total_litres_of_pure_alcohol),\n             colour = \"red\")\nalcohol_data |> filter(total_litres_of_pure_alcohol == 0)\n#> # A tibble: 13 × 5\n#>    country          beer_servings spirit_s…¹ wine_…² total…³\n#>    <chr>                    <dbl>      <dbl>   <dbl>   <dbl>\n#>  1 Afghanistan                  0          0       0       0\n#>  2 Bangladesh                   0          0       0       0\n#>  3 North Korea                  0          0       0       0\n#>  4 Iran                         0          0       0       0\n#>  5 Kuwait                       0          0       0       0\n#>  6 Libya                        0          0       0       0\n#>  7 Maldives                     0          0       0       0\n#>  8 Marshall Islands             0          0       0       0\n#>  9 Mauritania                   0          0       0       0\n#> 10 Monaco                       0          0       0       0\n#> 11 Pakistan                     0          0       0       0\n#> 12 San Marino                   0          0       0       0\n#> 13 Somalia                      0          0       0       0\n#> # … with abbreviated variable names ¹​spirit_servings,\n#> #   ²​wine_servings, ³​total_litres_of_pure_alcohol\nalcohol_data |> mutate(rankbeer = rank(desc(beer_servings))) |>\n  arrange(rankbeer) |> \n  filter(rankbeer <= 3)\nalcohol_data |> mutate(rankwine = rank(desc(wine_servings))) |>\n  arrange(rankwine) |> \n  filter(rankwine <= 3)\n\nalcohol_data |> mutate(rankspirits = rank(desc(spirit_servings))) |>\n  arrange(rankspirits) |> \n  filter(rankspirits <= 3)\nonecountry_df <- alcohol_data |> \n  filter(country == \"Denmark\")\n\nlibrary(ggrepel)\nggplot(data = alcohol_data,\n       mapping = aes(x = beer_servings, y = wine_servings)) + \n  geom_point(alpha = 0.5) +\n  geom_label_repel(data = onecountry_df, aes(label = country),\n    colour = \"purple\") +\n  geom_point(data = onecountry_df, colour = \"purple\",\n             size = 2.5, shape = 1) +\n  geom_abline(aes(slope = 1, intercept = 0), alpha = 0.3)\nalcohol_data |>\n  mutate(wbratio = wine_servings / beer_servings) |>\n  arrange(desc(wbratio)) |>\n  select(country, beer_servings, wine_servings, wbratio)\n#> # A tibble: 193 × 4\n#>    country             beer_servings wine_servings wbratio\n#>    <chr>                       <dbl>         <dbl>   <dbl>\n#>  1 Cook Islands                    0            74  Inf   \n#>  2 Qatar                           1             7    7   \n#>  3 Montenegro                     31           128    4.13\n#>  4 Timor-Leste                     1             4    4   \n#>  5 Syria                           5            16    3.2 \n#>  6 France                        127           370    2.91\n#>  7 Georgia                        52           149    2.87\n#>  8 Italy                          85           237    2.79\n#>  9 Equatorial Guinea              92           233    2.53\n#> 10 Sao Tome & Principe            56           140    2.5 \n#> # … with 183 more rows\n#> # ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "path": "intro.html",
    "id": "exercise-1-1",
    "chapter": " 2 Getting Started with R and R Studio",
    "heading": "2.4.1 Exercises",
    "text": "shape distribution total alcohol consumption? Left-skewed, right-skewed, approximately symmetric? Unimodal multimodal?shape distribution total alcohol consumption? Left-skewed, right-skewed, approximately symmetric? Unimodal multimodal?histogram total alcohol consumption, pick country USA interests . See can change code chunk made histogram red vertical line drawn country interests .histogram total alcohol consumption, pick country USA interests . See can change code chunk made histogram red vertical line drawn country interests .Hint: Use View() function look alcohol data set typing View(alcohol_data) bottom-left window help see countries data set.Note: careful capitalization: R case sensitive USA different usa.histogram total alcohol consumption, change fill colour bins histogram : changed code chunk?histogram total alcohol consumption, change fill colour bins histogram : changed code chunk?spirit rankings, think 2 countries showed instead 3? Can investigation case?spirit rankings, think 2 countries showed instead 3? Can investigation case?rankings code, wanted look top 5 countries instead top 3? See change code.rankings code, wanted look top 5 countries instead top 3? See change code.Change wine beer ratio code example find countries highest beer wine consumption (instead wine beer consumption).Change wine beer ratio code example find countries highest beer wine consumption (instead wine beer consumption).",
    "code": "\nView(alcohol_data)"
  },
  {
    "path": "intro.html",
    "id": "athlete-data-example",
    "chapter": " 2 Getting Started with R and R Studio",
    "heading": "2.5 Athlete Data Example",
    "text": "Secondly, look data set top 100 highest paid athletes 2014. athletesdata obtained https://github.com/ali-ce/datasets data set information following variables 100 highest paid athletes 2014, according Forbes (pay includes salary endorsements):Name (name athlete)Rank (athlete ranks, 1 highest paid)Sport (sport athlete plays)endorsements (money sponsorships companies)totalpay (millions year 2014, salary + endorsements)salary (money tournaments contract salary)age athlete 2014Gender (Male Female)first read data set name athletes. can use head() function look first rows data set.many different interesting questions answer data set. First, might interested relationship athlete age salary top 100 athletes. Recall earlier stat course one appropriate graphic examine relationship scatterplot:see anything strange scatterplot? think y-axis tick labels 2.5e+07, 5.0e+07, etc. mean?Now let’s see can count number athletes Top 100 personal favourite sport, Tennis:looks like 6 athletes: can see sort Rank :Finally, let’s see can compare ratio endorsements (commercials products) salary professional athletes Top 100 2 sports: Football (referring American Football) Basketball. Recall earlier Stat class might want use side--side boxplots make comparison since one categorical variable (Sport Type) one quantitative variable (Ratio Endorsements Salary).graph endorsements / salary ratio 1 indicates person makes half overall pay endorsements half overall pay salary.sport looks like tends receive larger proportion overall pay endorsements athletes top 100?",
    "code": "\nathletes <- read_csv(\"data/athletesdata.csv\")\nhead(athletes)\n#> # A tibble: 6 × 9\n#>    ...1 Name         Rank Sport endor…¹ total…² salary   age\n#>   <dbl> <chr>       <dbl> <chr>   <dbl>   <dbl>  <dbl> <dbl>\n#> 1     1 Aaron Rodg…    55 Foot… 7500000  2.2 e7 1.45e7    31\n#> 2     2 Adam Scott     95 Golf  9000000  1.77e7 8.7 e6    34\n#> 3     3 Adrian Gon…    60 Base…  400000  2.15e7 2.11e7    32\n#> 4     4 Alex Rodri…    48 Base…  300000  2.29e7 2.26e7    39\n#> 5     5 Alfonso So…    93 Base…   50000  1.80e7 1.8 e7    38\n#> 6     6 Amar'e Sto…    27 Bask… 5000000  2.67e7 2.17e7    32\n#> # … with 1 more variable: Gender <chr>, and abbreviated\n#> #   variable names ¹​endorsements, ²​totalpay\n#> # ℹ Use `colnames()` to see all variable names\nggplot(data = athletes, mapping = aes(x = age, y = salary)) + \n  geom_point() +\n  geom_smooth(se = FALSE)\nathletes |> group_by(Sport) |>\n  summarise(counts = n()) |>\n  filter(Sport == \"Tennis\")\n#> # A tibble: 1 × 2\n#>   Sport  counts\n#>   <chr>   <int>\n#> 1 Tennis      6\nathletes |>\n  filter(Sport == \"Tennis\") |>\n  arrange(Rank)\n#> # A tibble: 6 × 9\n#>    ...1 Name         Rank Sport endor…¹ total…² salary   age\n#>   <dbl> <chr>       <dbl> <chr>   <dbl>   <dbl>  <dbl> <dbl>\n#> 1    82 Roger Fede…     7 Tenn…   5.2e7  5.62e7 4.2 e6    33\n#> 2    78 Rafael Nad…     9 Tenn…   3  e7  4.45e7 1.45e7    28\n#> 3    72 Novak Djok…    17 Tenn…   2.1e7  3.31e7 1.21e7    27\n#> 4    64 Maria Shar…    34 Tenn…   2.2e7  2.44e7 2.4 e6    27\n#> 5    60 Li Na          41 Tenn…   1.8e7  2.36e7 5.6 e6    32\n#> 6    89 Serena Wil…    55 Tenn…   1.1e7  2.2 e7 1.1 e7    33\n#> # … with 1 more variable: Gender <chr>, and abbreviated\n#> #   variable names ¹​endorsements, ²​totalpay\n#> # ℹ Use `colnames()` to see all variable names\nfootball_basketball <- athletes |>\n  filter(Sport == \"Football\" | Sport == \"Basketball\")\n\nggplot(data = football_basketball,\n       aes(x = Sport, y = endorsements / salary)) + \n  geom_boxplot() +\n  labs(y = \"Endorsements / Salary\")"
  },
  {
    "path": "intro.html",
    "id": "exercise-1-2",
    "chapter": " 2 Getting Started with R and R Studio",
    "heading": "2.5.1 Exercises",
    "text": "Instead looking relationship age salary top 100 athletes 2014, change plot look relationship age endorsements. change code ? Try !Instead looking relationship age salary top 100 athletes 2014, change plot look relationship age endorsements. change code ? Try !Pick Sport Tennis see can count number athletes top 100 sport well sort Rank. Careful: sports athletes Top 100.Pick Sport Tennis see can count number athletes top 100 sport well sort Rank. Careful: sports athletes Top 100.many athletes top 100 sport chose?endorsements / salary example, change one sports sport choice make comparison. sport tends receive larger proportion overall pay endorsements.endorsements / salary example, change one sports sport choice make comparison. sport tends receive larger proportion overall pay endorsements.qualification might want make statement previous exercise? (random sample athletes sport? matter?).qualification might want make statement previous exercise? (random sample athletes sport? matter?).side--side boxplots comparing endorsements salary ratio two different sports, ’ve changed y-axis label Endorsements / Salary using labs(y = \"Endorsements / Salary\") statement. Try changing x-axis label something else. think need add plot?side--side boxplots comparing endorsements salary ratio two different sports, ’ve changed y-axis label Endorsements / Salary using labs(y = \"Endorsements / Salary\") statement. Try changing x-axis label something else. think need add plot?",
    "code": ""
  },
  {
    "path": "intro.html",
    "id": "finishing-up-common-errors-in-r",
    "chapter": " 2 Getting Started with R and R Studio",
    "heading": "2.6 Finishing Up: Common Errors in R",
    "text": "now talk little bit getting errors R can done correct common errors.may encountered errors point document. Let’s go common errors well discuss comment code.missing parenthesis: open parenthesis ( needs close ). Try running following code chunk without fixing anything.Notice bottom-left window > symbol starts line changes +. generally bad!! means forgot close parenthesis ) quote (' \"). code run since R thinks still trying type something function. fix issue, click cursor bottom-left window press Esc. , try find error code chunk.Can find missing closing parenthesis ?Missing Comma. Try running following code chunk without fixing anything.R gives “Error: unexpected symbol ….”. Oftentimes, means missing comma spelled variable name incorrectly.Can find missed comma ?Capitalization IssuesIn original data set, variable Sport capitalized. capitalizing means R won’t able find proclaims “object sport found”.Forgetting Quotes. Character strings need quotation marks around . discuss later, graph labels titles need quotes around since don’t directly refer columns rows data set:error forgetting quotes typically “Unexpected Symbol” though error also given issues.quotes missing code chunk ?Finally, can add comment code chunk # symbol (always use double ## reason though). allows type comment code chunk isn’t code:Comments useful longer code chunks, allow remember something. also tell someone ’ve shared code something.Save file clicking File -> Save using keyboard shortcut Command + s (Control + s PC). Render file clicking Render button top-left window. see .html file pop , errors code!",
    "code": "ggplot(data = athletes, aes(x = Sport, y = salary) + \n  geom_boxplot()ggplot(data = athletes aes(x = Sport, y = salary)) + \n  geom_boxplot()\nathletes |> filter(sport == \"Tennis\")ggplot(data = athletes, aes(x = Sport, y = endorsements)) + \n  geom_boxplot() + xlab(Popularity Measure)\n## this is a comment\n## this calculation might be useful later\n7 * 42\n#> [1] 294"
  },
  {
    "path": "intro.html",
    "id": "chapexercise-1",
    "chapter": " 2 Getting Started with R and R Studio",
    "heading": "2.7 Chapter Exercises",
    "text": "Note: Usually, exercises ask write code using week’s chapter reference. However, initial chapter, something little different.Open new .qmd file (File -> New File -> Quarto Document -> OK) delete text explaining Quarto . Make sure Quarto document self-contained using something like following first lines file:, complete following exercises.Exercise 1. Read short paper https://joss.theoj.org/papers/10.21105/joss.01686 Introduction tidyverse, answer questions Quarto file. ’m imagining whole exercise take ~ 20-25 minutes.Answer following questions typing answers .qmd document. need make new code chunks, questions don’t ask coding!two major areas tidyverse doesn’t provide tools ?two major areas tidyverse doesn’t provide tools ?authors define “tidy”?authors define “tidy”?mean tidyverse “human-centred”?mean tidyverse “human-centred”?2 sentences, describe data science “cycle” given diagram top page 3.2 sentences, describe data science “cycle” given diagram top page 3.Exercise 2. may continue use .qmd file answer questions. question, type answer new line, line space answers. questions answered outside code chunks since answers text, code.name class year (first-year, sophomore, junior, senior)?name class year (first-year, sophomore, junior, senior)?/major(s) minor(s), either actual intended?/major(s) minor(s), either actual intended?taking course? (Major requirement?, Minor requirement?, recommended advisor student?, exploring field?, etc.). taking major minor requirement, decide major minor statistics data science?taking course? (Major requirement?, Minor requirement?, recommended advisor student?, exploring field?, etc.). taking major minor requirement, decide major minor statistics data science?semester year take STAT 113 professor?semester year take STAT 113 professor?taken STAT 213? taken CS 140?taken STAT 213? taken CS 140?hometown: city, state, country?hometown: city, state, country?play sport campus? , sport? , activity -campus?play sport campus? , sport? , activity -campus?favorite TV show movie band/musical artist?favorite TV show movie band/musical artist?Tell something .Tell something .Take look learning outcomes listed syllabus. excited ?Take look learning outcomes listed syllabus. excited ?expectations class /hope gain class?expectations class /hope gain class?Take moment scroll advice students took course Fall semester 2021. one piece advice hope apply course semester?Take moment scroll advice students took course Fall semester 2021. one piece advice hope apply course semester?Render .qmd file .html file submit rendered .html file Canvas. file won’t render, submit .qmd file instead. submit either file, first need get file server onto computer can upload Canvas.Nice work: dive ggplot() ggplot2 package next!",
    "code": "---\ntitle: \"Your Title\"\nauthor: \"Your Name\"\nformat: \n  html:\n    self-contained: true\n---"
  },
  {
    "path": "intro.html",
    "id": "solutions-1",
    "chapter": " 2 Getting Started with R and R Studio",
    "heading": "2.8 Exercise Solutions",
    "text": "sections, exercise solutions posted end section. However, R brand new, coding exercises class first section.",
    "code": ""
  },
  {
    "path": "ggplot2.html",
    "id": "ggplot2",
    "chapter": " 3 Plotting with ggplot2",
    "heading": " 3 Plotting with ggplot2",
    "text": "Goals:Use ggplot2 package make exploratory plots STAT 113 single quantitative variable, two quantitative variables, quantitative categorical variable, single categorical variable, two categorical variables.Use ggplot2 package make exploratory plots STAT 113 single quantitative variable, two quantitative variables, quantitative categorical variable, single categorical variable, two categorical variables.Use plots produced answer questions Presidential election data set Fitness data set.Use plots produced answer questions Presidential election data set Fitness data set.practice running code R.practice running code R.",
    "code": ""
  },
  {
    "path": "ggplot2.html",
    "id": "introduction-and-basic-terminology",
    "chapter": " 3 Plotting with ggplot2",
    "heading": "3.1 Introduction and Basic Terminology",
    "text": "begin data science journey plotting ggplot2 package. starting plotting couple reasons:Plotting cool! get see immediate result coding efforts form nice--look-plot.Plotting cool! get see immediate result coding efforts form nice--look-plot.exploratory data analysis, typically start making plots data.exploratory data analysis, typically start making plots data.Plotting can lead us ask subsequently investigate interesting questions, see first example.Plotting can lead us ask subsequently investigate interesting questions, see first example.first use data set 2000 United States Presidential election former President George Bush Al Gore obtained http://www.econometrics.com/intro/votes.htm. unfamiliar U.S. political elections, enough know state allocated certain number “electoral votes” president: states award electoral votes candidate receives ballots state. can read strange system Wikipedia.Florida typically highly-contentious “battleground” state. data set following variables, recorded 67 counties Florida:Gore, number people voted Al Gore 2000Bush, number people voted George Bush 2000Buchanan, number people voted third-party candidate BuchananNader, number people voted third-party candidate NaderOther, number people voted candidate previous 4 listedCounty, name county FloridaTo get started exploring data, complete following steps learned Week 0:Open R Project double clicking .RProj icon folder desktop, , opening R Studio clicking File -> Open Project.Open R Project double clicking .RProj icon folder desktop, , opening R Studio clicking File -> Open Project.Create new .qmd file folder Notes R Project using File -> New File -> Quarto.Create new .qmd file folder Notes R Project using File -> New File -> Quarto.Finally, read name data set pres_df, take look data set running head(pres_df) line, shows first observations data set:Finally, read name data set pres_df, take look data set running head(pres_df) line, shows first observations data set:Pay special attention variable names: ’ll need use names make plots. , R case-sensitive, meaning , example, need use Gore, gore.trying go light technical code terminology start (come back things later semester). terminology make lot sense ’ve actually worked data. , three terms thrown around quite bit next weeks: function, argument, object.function R always* (*always class) followed open ( ended closed ). non-technical terms, function something inputs often analogous English verb. example, mean() function calculates mean, rank() functions ranks variable lowest highest, labs() used add labels plot. Every function help file can accessed typing ?name_of_function. Try typing ?mean lower left window.function R always* (*always class) followed open ( ended closed ). non-technical terms, function something inputs often analogous English verb. example, mean() function calculates mean, rank() functions ranks variable lowest highest, labs() used add labels plot. Every function help file can accessed typing ?name_of_function. Try typing ?mean lower left window.argument something goes inside parentheses function. Arguments include objects, might . bottom-left window, type ?mean view Help file R function. see mean() 3 arguments: x, R object, trim, na.rm. trim = 0 default, means , default, R trim numbers computing mean.argument something goes inside parentheses function. Arguments include objects, might . bottom-left window, type ?mean view Help file R function. see mean() 3 arguments: x, R object, trim, na.rm. trim = 0 default, means , default, R trim numbers computing mean.object something created R, usually <-. , looking code read data, pres_df R object.object something created R, usually <-. , looking code read data, pres_df R object.make sense go first couple weeks.",
    "code": "\nlibrary(tidyverse)\npres_df <- read_table(\"data/PRES2000.txt\") \n## don't worry about the `read_table` function....yet\nhead(pres_df)\n#> # A tibble: 6 × 6\n#>     Gore   Bush Buchanan Nader Other County  \n#>    <dbl>  <dbl>    <dbl> <dbl> <dbl> <chr>   \n#> 1  47365  34124      263  3226   751 ALACHUA \n#> 2   2392   5610       73    53    26 BAKER   \n#> 3  18850  38637      248   828   242 BAY     \n#> 4   3075   5414       65    84    35 BRADFORD\n#> 5  97318 115185      570  4470   852 BREVARD \n#> 6 386561 177323      788  7101  1623 BROWAR"
  },
  {
    "path": "ggplot2.html",
    "id": "basic-plot-structure",
    "chapter": " 3 Plotting with ggplot2",
    "heading": "3.2 Basic Plot Structure",
    "text": "use ggplot() function ggplot2 package construct visualizations data. ggplot() function 3 basic components:data argument, specifying name data set (pres_df )mapping argument, specifying specifies aesthetics plot (aes()). Common aesthetics x position, y position, colour, size, shape, group, fill.geom_    () component, specifying geometric shape used display data.components combined following form:structure ggplot() plots based Grammar Graphics https://www.springer.com/gp/book/9780387245447. new things, components easier think examples.",
    "code": "ggplot(data = name_of_data, aes(x = name_of_x_var, \n                                          y = name_of_y_var,\n                                          colour = name_of_colour_var,\n                                          etc.)) +\n  geom_nameofgeom() +\n  .....<other stuff>"
  },
  {
    "path": "ggplot2.html",
    "id": "graphing-a-single-variable",
    "chapter": " 3 Plotting with ggplot2",
    "heading": "3.3 Graphing a Single Variable",
    "text": "",
    "code": ""
  },
  {
    "path": "ggplot2.html",
    "id": "histograms-and-frequency-plots-for-a-quantitative-variable",
    "chapter": " 3 Plotting with ggplot2",
    "heading": "3.3.1 Histograms and Frequency Plots for a Quantitative Variable",
    "text": "Let’s go ahead begin exploration data making histogram number people voted Gore county. Recall histogram useful like graph single quantitative variable. Copy following code R chunk run code:1e+05, 2e+05, etc. labels x-axis mean?R gives us message “Pick better value binwidth” instead default bins = 30. Add , bins = 15 inside parentheses geom_histogram() change number bins histogram.Change colour inside bins “darkred”. think colour inside bins maps colour fill? Try !couple observations high vote values. explain large outliers?Another graph useful visualizing single quantitative variable frequency plot. code make frequency plot given . simply replacing geom_histogram() geom_freqpoly().frequency plot just like histogram counts connected line instead represented bins. can see relate including geom_freqpoly() geom_histogram() plot, though doesn’t make prettiest graph:",
    "code": "\nggplot(data = pres_df, aes(x = Gore)) +\n  geom_histogram(colour = \"black\", fill = \"white\") +\n  xlab(\"Votes for Gore in Florida\")\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\nggplot(data = pres_df, aes(x = Gore)) +\n  geom_freqpoly(colour = \"black\") +\n  xlab(\"Votes for Gore in Florida\") \n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\nggplot(data = pres_df, aes(x = Gore)) +\n  geom_freqpoly(colour = \"black\") +\n  xlab(\"Votes for Gore in Florida\") +\n  geom_histogram() \n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`."
  },
  {
    "path": "ggplot2.html",
    "id": "r-code-style",
    "chapter": " 3 Plotting with ggplot2",
    "heading": "3.3.2 R Code Style",
    "text": "want code readable possible. benefits people may read code (like ), also benefits , particularly read code future. try follow Style Guide Advanced R book: http://adv-r..co.nz/Style.html. Feel free skim , don’t need worry much: able pick important elements just going course. might actually end better code style haven’t previous coding experience.quick example code style can important, consider following two code chunks, produce graph.code chunk want read two years now? code chunk want classmate/friend/coworker read? (assuming like classmate/friend/coworker….)",
    "code": "\nggplot(data=pres_df,mapping=aes(x=Gore))+geom_histogram(colour=\"black\",fill=\"white\")+\n  xlab(\"Votes for Gore in Florida\")\nggplot(data = pres_df, aes(x = Gore)) +\n  geom_histogram(colour = \"black\", fill = \"white\") +\n  xlab(\"Votes for Gore in Florida\")"
  },
  {
    "path": "ggplot2.html",
    "id": "bar-plots-for-a-categorical-variable",
    "chapter": " 3 Plotting with ggplot2",
    "heading": "3.3.3 Bar Plots for a Categorical Variable",
    "text": "Recall STAT 113 bar plots useful want examine distribution one categorical variable. Side--side bar plots stacked bar plots plots useful looking relationship two categorical variables. actually aren’t categorical variables interesting plot data set, ’ll make one, called winner using code don’t need understand next week. winner \"Gore\" Gore won county \"Bush\" Bush won county. ’ll name new data set pres_cat.Using data set, can make bar plot geom_bar(). beauty ggplot() code super-similar used histograms frequency plots!Note , sometimes, data format one column contains levels categorical variable another column contains counts directly. example, can create data set using code learn next week:data set just two observations contains column two major presidential candidates column number counties candidate won. wanted make barplot showing number wins candidate, can’t use geom_bar(). Predict result running following code.Instead, can use geom_col(), takes x aesthetic giving column names levels categorical variable, y aesthetic giving column counts:",
    "code": "\npres_cat <- pres_df |> mutate(winner = if_else(Gore > Bush,\n                                                true = \"Gore\",\n                                                false = \"Bush\"))\npres_cat\n#> # A tibble: 67 × 7\n#>      Gore   Bush Buchanan Nader Other County    winner\n#>     <dbl>  <dbl>    <dbl> <dbl> <dbl> <chr>     <chr> \n#>  1  47365  34124      263  3226   751 ALACHUA   Gore  \n#>  2   2392   5610       73    53    26 BAKER     Bush  \n#>  3  18850  38637      248   828   242 BAY       Bush  \n#>  4   3075   5414       65    84    35 BRADFORD  Bush  \n#>  5  97318 115185      570  4470   852 BREVARD   Bush  \n#>  6 386561 177323      788  7101  1623 BROWAR    Gore  \n#>  7   2155   2873       90    39    17 CALHOUN   Bush  \n#>  8  29645  35426      182  1462   181 CHARLOTTE Bush  \n#>  9  25525  29765      270  1379   261 CITRUS    Bush  \n#> 10  14632  41736      186   562   237 CLAY      Bush  \n#> # … with 57 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\nggplot(data = pres_cat, aes(x = winner)) +\n  geom_bar()\npres_cat2 <- pres_cat |> group_by(winner) |>\n  summarise(nwins = n())\npres_cat2\n#> # A tibble: 2 × 2\n#>   winner nwins\n#>   <chr>  <int>\n#> 1 Bush      51\n#> 2 Gore      16\nggplot(pres_cat2, aes(x = winner)) +\n  geom_bar()\nggplot(pres_cat2, aes(x = winner, y = nwins)) +\n  geom_col()"
  },
  {
    "path": "ggplot2.html",
    "id": "exercise-2-1",
    "chapter": " 3 Plotting with ggplot2",
    "heading": "3.3.4 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 3.7.Change frequency plot plot number votes Bush instead number Gore. obvious outliers Bush frequency plot?Change frequency plot plot number votes Bush instead number Gore. obvious outliers Bush frequency plot?preference histograms preference frequency plots? Can think situation one desirable ?preference histograms preference frequency plots? Can think situation one desirable ?looks like Bush won lot ….necessarily mean Bush won votes total Florida? ?looks like Bush won lot ….necessarily mean Bush won votes total Florida? ?using survey data STAT 113 2018-2019 academic year many exercises section. may taken STAT 113 AP credit another reason, STAT 113 survey given students STAT 113 across sections. analyses Intro Stat carried using survey.data set contains following variables:Year, FirstYear, Sophomore, Junior, SeniorSex, M F (data set, Sex considered binary).Hgt, height, inches.Wgt, weight, pounds.Haircut, much paid haircut, typically.GPAExercise, amount hours exercise typical week.Sport, whether student plays varsity sport.TV, amount hours spent watching TV typical week.Award, Award preferred: choices Olympic Medal, Nobel Prize, Academy Award.Pulse, pulse rate, beats per minute.SocialMedia, used social media platform (Instagram, SnapChat, FaceBook, Twitter, , None).* Create histogram Exercise variable, change x-axis label “Exercise (hours per typical week)”, change number bins 14, change fill bins “lightpink2” outline colour bins black.* Create histogram Exercise variable, change x-axis label “Exercise (hours per typical week)”, change number bins 14, change fill bins “lightpink2” outline colour bins black.* can change y-axis histogram “density” instead raw count. means bar shows proportion cases instead raw count. Google something like “geom_histogram density” figure create y aes() show density instead count.* can change y-axis histogram “density” instead raw count. means bar shows proportion cases instead raw count. Google something like “geom_histogram density” figure create y aes() show density instead count.Construct histogram using quantitative variable choice. Change fill colour using http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf help choose colours.Construct histogram using quantitative variable choice. Change fill colour using http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf help choose colours.Construct bar plot variable choosing. find?Construct bar plot variable choosing. find?format STAT 113 data set need construct bar plot geom_col() instead geom_bar()?format STAT 113 data set need construct bar plot geom_col() instead geom_bar()?",
    "code": "\nlibrary(tidyverse)\nstat113_df <- read_csv(\"data/stat113.csv\")\nhead(stat113_df)\n#> # A tibble: 6 × 12\n#>   Year   Sex     Hgt   Wgt Haircut   GPA Exerc…¹ Sport    TV\n#>   <chr>  <chr> <dbl> <dbl>   <dbl> <dbl>   <dbl> <chr> <dbl>\n#> 1 Sopho… M        66   155       0  2.9       15 Yes       8\n#> 2 First… F        69   170      17  3.87      14 Yes      12\n#> 3 First… F        64   130      40  3.3        5 No        5\n#> 4 First… M        68   157      35  3.21      10 Yes      15\n#> 5 First… M        72   175      20  3.1        2 No        5\n#> 6 Junior F        62   150      50  3.3        8 Yes       5\n#> # … with 3 more variables: Award <chr>, Pulse <dbl>,\n#> #   SocialMedia <chr>, and abbreviated variable name\n#> #   ¹​Exercise\n#> # ℹ Use `colnames()` to see all variable names"
  },
  {
    "path": "ggplot2.html",
    "id": "graphing-two-quantitative-variables-faceting-and-aes-options",
    "chapter": " 3 Plotting with ggplot2",
    "heading": "3.4 Graphing Two Quantitative Variables, Faceting, and aes() Options",
    "text": "",
    "code": ""
  },
  {
    "path": "ggplot2.html",
    "id": "scatterplots",
    "chapter": " 3 Plotting with ggplot2",
    "heading": "3.4.1 Scatterplots",
    "text": "Moving back 2000 presidential election data set, thus far, ’ve figured couple counties large numbers votes Gore large number votes Bush. don’t know reason (counties democratic, republican, counties just populous). counties large number votes Bush also tend large number votes Gore? candidates: interesting patterns?Let’s start making scatterplot number votes Gore number votes Bush. Note geom_ making scatterplot called geom_point() adding layer points plot.patterns see scatterplot?Now, change x variable Gore Buchanan. notice something strange scatterplot. Try come one explanation outlying point many votes Buchanan.trying come explanation, nice figure Florida county outlying point nice knew something Florida counties. remedy first issue, recall can type View(pres_df) pull data set. new window open, click column heading Buchanan sort votes Buchanan high low figure county outlier.Use Google sleuthing skills find explanation: try search “2000 united states presidential election [name outlier county]”. Write sentence find. Hint: nothing useful pops , try adding term “butterfly ballot” search.used 2000 Presidential data set find something really interesting! particular, used exploratory data analysis examine data set, without specific question interest want answer. type exploring often really useful, drawbacks, discuss later semester.",
    "code": "\nggplot(data = pres_df, aes(x = Gore, y = Bush)) +\n  geom_point()"
  },
  {
    "path": "ggplot2.html",
    "id": "aesthetics-in-aes",
    "chapter": " 3 Plotting with ggplot2",
    "heading": "3.4.2 Aesthetics in aes()",
    "text": "remainder chapter, work fitness data collected Apple Watch since November 2018. higham_fitness_clean.csv contains information following variables:Start, month, day, year fitness data recorded onmonth, monthweekday, day weekdayofyear, day year (304 corresponds 304th day year)distance, distance walked milessteps, number steps takenflights, number flights stairs climbedactive_cals, number calories burned activitystepgoal, whether reached 10,000 steps dayweekend_ind, variable whether day week weekend day (Saturday Sunday) weekday (Monday - Friday).First, let’s make basic scatterplot illustrate ’s important plot data. ’ll use variable distance x-variable active_cals y-variable.One aspect plot may notice observations burned 0 active calories, yet walked/jogged/ran/moved distance. possible burn calories move ~ 4 miles? Probably , let’s drop observations data set make note dropped observations. Unfortunately, don’t tools yet, just run following chunk code without worrying much syntax.Let’s make plot fitness data set instead fitness_full see outliers actually gone. time, put aes() geom_point() function:Putting aes() ggplot() putting aes() geom_point() results graph case. put aes() ggplot(), R perpetuates aes() aesthetics geom_s plotting command. However, put aes() geom_point(), future geoms use need re-specify different aes(). ’ll see example exercises.aes() OptionsIn addition x y, can also use aes() map variables things like colour, size, shape. example, might make scatterplot Start x-axis (date) active_cals y-axis, colouring whether day week weekend.anything useful notice plot? anything plot improved?Instead using colour, can also specify point shape. useful, example, printing something black white.prefer colour shape? ?Finally, another common aes() size. example, make size points scatterplot change depending many flights stairs climbed.don’t think previous three plots necessarily “best” need work, , part fun exploratory data analysis making trying different plots see “works.”Inside vs Outside aes()’ve changed colour points correspond weekend_ind, just wanted change colour points colour, \"purple\". Try running following code chunk:graph look like? expected?Putting colour = ____ inside aes() outside aes() achieves different things. general,want map something data set (fitness) something plot (x, y, colour, size, etc.), put inside aes() geom_point(aes(colour = weekend_ind)).want map something data set (fitness) something plot (x, y, colour, size, etc.), put inside aes() geom_point(aes(colour = weekend_ind)).assign fixed characteristics don’t come data, put outside aes(), geom_point(colour = \"purple\").assign fixed characteristics don’t come data, put outside aes(), geom_point(colour = \"purple\").can also change overall point size shape. standard size 1 following code chunk makes points bigger. standard shape 19: can try changing integers see shapes can get.",
    "code": "\nlibrary(tidyverse)\nfitness_full <- read_csv(\"data/higham_fitness_clean.csv\") |> mutate(weekend_ind = case_when(weekday == \"Sat\" | weekday == \"Sun\" ~ \"weekend\",\n  TRUE ~ \"weekday\"))\n#> Rows: 993 Columns: 9\n#> ── Column specification ────────────────────────────────────\n#> Delimiter: \",\"\n#> chr  (2): month, weekday\n#> dbl  (6): active_cals, distance, flights, steps, dayofye...\n#> date (1): Start\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nggplot(data = fitness_full, aes(x = distance, y = active_cals)) +\n  geom_point()\n## drop observations that have active calories < 50. \n## assuming that these are data errors or \n## days where the Apple Watch wasn't worn.\nfitness <- fitness_full |>\n  filter(active_cals > 50)\nggplot(data = fitness) +\n  geom_point(aes(x = distance, y = active_cals))\nggplot(data = fitness) +\n  geom_point(aes(x = Start, y = active_cals, colour = weekend_ind))\nggplot(data = fitness) +\n  geom_point(aes(x = Start, y = active_cals, shape = weekend_ind))\nggplot(data = fitness) +\n  geom_point(aes(x = Start, y = active_cals, size = flights))\nggplot(data = fitness) +\n  geom_point(aes(x = Start, y = active_cals, colour = \"purple\"))\nggplot(data = fitness) +\n  geom_point(aes(x = Start, y = active_cals), size = 1.5, shape = 19)"
  },
  {
    "path": "ggplot2.html",
    "id": "using-more-than-one-geom",
    "chapter": " 3 Plotting with ggplot2",
    "heading": "3.4.3 Using More Than One geom()",
    "text": "might also interested fitting smooth curve scatterplot. want put one “geom” plot, can use multiple geoms. Since want aes() apply geom_point() geom_smooth(), going move aes() command overall ggplot() line code:Within geom_smooth(), can set se = FALSE get rid grey standard errors around lines, can setmethod = \"lm\" fit straight linear regression lines instead smooth curves:look like increasing overall trend? decreasing? make sense use line model relationship prefer smooth curve?",
    "code": "\nggplot(data = fitness, aes(x = Start, y = active_cals)) +\n  geom_point() +\n  geom_smooth(span = 0.3)\n#> `geom_smooth()` using method = 'loess' and formula 'y ~ x'\nggplot(data = fitness, aes(x = Start, y = active_cals)) +\n  geom_point() +\n  geom_smooth(se = FALSE, method = \"lm\")\n#> `geom_smooth()` using formula 'y ~ x'"
  },
  {
    "path": "ggplot2.html",
    "id": "line-plots-with-geom_line",
    "chapter": " 3 Plotting with ggplot2",
    "heading": "3.4.4 Line Plots with geom_line()",
    "text": "Line plots often useful quantitative variable ’d like explore time. y-axis quantitative variable x-axis typically time. generally, line plots often used x-axis variable one discrete value y-axis variable. example, suppose want explore step count changed time past couple years. Compare standard scatterplot following line plot: prefer?Can spot start pandemic graph? seemed happen step count?",
    "code": "\nggplot(data = fitness, aes(x = Start, y = steps)) +\n  geom_point() + geom_smooth() + xlab(\"Date\")\n#> `geom_smooth()` using method = 'loess' and formula 'y ~ x'\nggplot(data = fitness, aes(x = Start, y = steps)) +\n  geom_line() + geom_smooth() + xlab(\"Date\")\n#> `geom_smooth()` using method = 'loess' and formula 'y ~ x'"
  },
  {
    "path": "ggplot2.html",
    "id": "faceting",
    "chapter": " 3 Plotting with ggplot2",
    "heading": "3.4.5 Faceting",
    "text": "Using colour colour points different levels categorical variable generally fine just couple levels /little overlap among levels. , lot two categories colour . example, let’s move back STAT 113 survey data set investigate relationship Pulse Exercise different class Year’s. might hypothesize students get exercise tend lower pulse rates.many different categories categorical variable (4 categories Year, particular plot still bit difficult read), can sometimes useful facet plot variable instead trying use different colours shapes.eliminated colour = argument added facet_wrap( ~ name_of_facet_variable). creates different scatterplot smooth line level name_of_facet_variable.can see plot harder see plot colour?data seem support hypothesis exercise associated lower pulse rates sample students?",
    "code": "\nggplot(data = stat113_df, aes(x = Exercise, y = Pulse,\n                           colour = Year)) +\n  geom_point() +\n  geom_smooth(se = TRUE)\n#> `geom_smooth()` using method = 'loess' and formula 'y ~ x'\nggplot(data = stat113_df, aes(x = Exercise, y = Pulse)) +\n  geom_point() +\n  geom_smooth(se = TRUE) +\n  facet_wrap(~ Year)\n#> `geom_smooth()` using method = 'loess' and formula 'y ~ x'"
  },
  {
    "path": "ggplot2.html",
    "id": "exercise-2-2",
    "chapter": " 3 Plotting with ggplot2",
    "heading": "3.4.6 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 3.7.Fix code chunk tried specify colour points purple actually make points “purple” moving colour = \"purple\" outside parentheses aes() (still inside geom_point()).Fix code chunk tried specify colour points purple actually make points “purple” moving colour = \"purple\" outside parentheses aes() (still inside geom_point()).console (bottom-left) window, type ?geom_smooth scroll “Arguments.” Find span, read , , within geom_smooth() argument line plot steps vs. date, add span argument make smooth line wigglier.console (bottom-left) window, type ?geom_smooth scroll “Arguments.” Find span, read , , within geom_smooth() argument line plot steps vs. date, add span argument make smooth line wigglier.Explain doesn’t make sense construct line plot Exercise vs. GPA.Explain doesn’t make sense construct line plot Exercise vs. GPA.* Make scatterplot Hgt y-axis Wgt x-axis, colouring Sport. Add smooth fitted curve scatterplot. , move colour = Sport aes() ggplot() function aes() geom_point() function. changes plot? Can give explanation change occurs?* Make scatterplot Hgt y-axis Wgt x-axis, colouring Sport. Add smooth fitted curve scatterplot. , move colour = Sport aes() ggplot() function aes() geom_point() function. changes plot? Can give explanation change occurs?* Faceting can used types plots ! Make pair faceted histograms quantitative variable choosing faceted categorical variable choosing.* Faceting can used types plots ! Make pair faceted histograms quantitative variable choosing faceted categorical variable choosing.",
    "code": ""
  },
  {
    "path": "ggplot2.html",
    "id": "boxplots-stacked-barplots-and-others",
    "chapter": " 3 Plotting with ggplot2",
    "heading": "3.5 Boxplots, Stacked Barplots and Others",
    "text": "common geoms useful throughout semester. skim surface: ’ll come back plotting weeks, ’re able data wrangling reshaping.",
    "code": ""
  },
  {
    "path": "ggplot2.html",
    "id": "graphing-a-quant.-variable-vs.-a-cat.-variable",
    "chapter": " 3 Plotting with ggplot2",
    "heading": "3.5.1 Graphing a Quant. Variable vs. a Cat. Variable",
    "text": "Another common plot used Intro Stat courses boxplot. Side--side boxplots particularly useful want compare quantitative response variable across two levels categorical variable. Let’s stick STAT 113 survey data examine relationship Exercise Award preference.can conclude plot?alternative side--side boxplots violin plots:Read Violin plots typing ?geom_violin console (bottom-left window). different boxplots?",
    "code": "\nggplot(data = stat113_df, aes(x = Award, y = Exercise)) +\n  geom_boxplot()\nggplot(data = stat113_df, aes(x = Award, y = Exercise)) +\n  geom_violin()"
  },
  {
    "path": "ggplot2.html",
    "id": "graphing-two-categorical-variables",
    "chapter": " 3 Plotting with ggplot2",
    "heading": "3.5.2 Graphing Two Categorical Variables",
    "text": "combination two variables yet explore two variables categorical. Let’s look relationship Year SocialMedia first using stacked bar plot.make graph, specify position = \"fill\" bars “filled” stepgoal.patterns notice plot? anything plot improved?",
    "code": "\nggplot(data = stat113_df, aes(x = Year, fill = SocialMedia)) +\n  geom_bar(position = \"fill\") +\n  ylab(\"Proportion\")"
  },
  {
    "path": "ggplot2.html",
    "id": "exercise-2-3",
    "chapter": " 3 Plotting with ggplot2",
    "heading": "3.5.3 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 3.7.* Change colour inside boxplots Exercise vs. Award graph \"blue\". think ’ll use colour = \"blue\" fill = \"blue\"?* Change colour inside boxplots Exercise vs. Award graph \"blue\". think ’ll use colour = \"blue\" fill = \"blue\"?* Create side--side boxplot compares GPAs students prefer different Awards. change fill boxplot colour choice. notice plot?* Create side--side boxplot compares GPAs students prefer different Awards. change fill boxplot colour choice. notice plot?* making previous plot, R gives us warning message “Removed 70 rows containing non-finite values”. R’s robotic way telling us 70 GPA values missing data set. Use know data collected (Fall Spring semester 2018-2019 school-year) guess missing.* making previous plot, R gives us warning message “Removed 70 rows containing non-finite values”. R’s robotic way telling us 70 GPA values missing data set. Use know data collected (Fall Spring semester 2018-2019 school-year) guess missing.* Make stacked bar plot two variables choosing STAT 113 data set. Comment something notice plot.* Make stacked bar plot two variables choosing STAT 113 data set. Comment something notice plot.",
    "code": ""
  },
  {
    "path": "ggplot2.html",
    "id": "chapexercise-2",
    "chapter": " 3 Plotting with ggplot2",
    "heading": "3.6 Chapter Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 3.7.* default geom_smooth() use LOESS (locally estimated scatterplot smoothing). Read LOESS : . Write one two sentences explaining LOESS .* default geom_smooth() use LOESS (locally estimated scatterplot smoothing). Read LOESS : . Write one two sentences explaining LOESS .* Thus far, faceted single variable. Use Google figure facet two variables make plot shows relationship GPA (y-axis) Exercise (x-axis) four facets: one male students play sport, one female students play sport, one male students play sport, one female students play sport.* Thus far, faceted single variable. Use Google figure facet two variables make plot shows relationship GPA (y-axis) Exercise (x-axis) four facets: one male students play sport, one female students play sport, one male students play sport, one female students play sport.* Intro-Stat, boxplots typically introduced using * symbol identify outliers. Using combination help ?geom_boxplot Googling “R point shapes”, figure modify side--side boxplots outliers shown using *, default dots. , using Google, figure add mean boxplot “darkgreen” diamond-shaped symbol stat_summary().* Intro-Stat, boxplots typically introduced using * symbol identify outliers. Using combination help ?geom_boxplot Googling “R point shapes”, figure modify side--side boxplots outliers shown using *, default dots. , using Google, figure add mean boxplot “darkgreen” diamond-shaped symbol stat_summary().common theme ’ll see throughout course ’s advantageous know much background information possible data set analyzing. Data sets easier analyze pose questions ’re familiar subject matter.common theme ’ll see throughout course ’s advantageous know much background information possible data set analyzing. Data sets easier analyze pose questions ’re familiar subject matter.Give example something know STAT 113 survey data set helped answer pose question someone another university (therefore unfamiliar intro stat course) wouldn’t know.Give example something don’t know fitness data set person owns fitness data know. give advantage person familiar fitness data?",
    "code": ""
  },
  {
    "path": "ggplot2.html",
    "id": "solutions-2",
    "chapter": " 3 Plotting with ggplot2",
    "heading": "3.7 Exercise Solutions",
    "text": "",
    "code": ""
  },
  {
    "path": "ggplot2.html",
    "id": "introduction-etc.-s",
    "chapter": " 3 Plotting with ggplot2",
    "heading": "3.7.1 Introduction etc. S",
    "text": "",
    "code": ""
  },
  {
    "path": "ggplot2.html",
    "id": "basic-plot-structure-s",
    "chapter": " 3 Plotting with ggplot2",
    "heading": "3.7.2 Basic Plot Structure S",
    "text": "",
    "code": ""
  },
  {
    "path": "ggplot2.html",
    "id": "graphing-a-single-variable-s",
    "chapter": " 3 Plotting with ggplot2",
    "heading": "3.7.3 Graphing a Single Variable S",
    "text": "* Create histogram Exercise variable, change x-axis label “Exercise (hours per typical week)”, change number bins 14, change fill bins “lightpink2” outline colour bins black.* can change y-axis histogram “density” instead raw count. means bar shows proportion cases instead raw count. Google something like “geom_histogram density” figure create y aes() show density instead count.",
    "code": "\nggplot(data = stat113_df, aes(x = Exercise)) +\n  geom_histogram(bins = 14, fill = \"lightpink2\", colour = \"black\") +\n  xlab(\"Exercise (hours per typical week)\")\nggplot(data = stat113_df, aes(x = Exercise, y = ..density..)) +\n  geom_histogram(bins = 14, fill = \"lightpink2\", colour = \"black\") +\n  xlab(\"Exercise (hours per typical week)\")"
  },
  {
    "path": "ggplot2.html",
    "id": "graphing-two-quant.-etc.-s",
    "chapter": " 3 Plotting with ggplot2",
    "heading": "3.7.4 Graphing Two Quant. etc. S",
    "text": "* Make scatterplot Hgt y-axis Wgt x-axis, colouring Sport. Add smooth fitted curve scatterplot. , move colour = Sport aes() ggplot() function aes() geom_point() function. changes plot? Can give explanation change occurs?points now coloured Sport one smooth fitted line. makes sense geom_point() now two global aesthetics x y, well colour aesthetic. geom_smooth() longer colour aesthetic still inherits two global aesthetics, x y.* Faceting can used types plots ! Make pair faceted histograms quantitative variable choosing faceted categorical variable choosing.Answers vary:",
    "code": "\nggplot(data = stat113_df, aes(x = Wgt, y = Hgt, colour = Sport)) +\n  geom_point() +\n  geom_smooth()\n\nggplot(data = stat113_df, aes(x = Wgt, y = Hgt)) +\n  geom_point(aes(colour = Sport)) +\n  geom_smooth()\nggplot(data = stat113_df, aes(x = GPA)) + \n  geom_histogram(bins = 15) +\n  facet_wrap( ~ Sport)"
  },
  {
    "path": "ggplot2.html",
    "id": "boxplots-stacked-etc.-s",
    "chapter": " 3 Plotting with ggplot2",
    "heading": "3.7.5 Boxplots, Stacked, etc. S",
    "text": "* Change colour inside boxplots Exercise vs. Award graph \"blue\". think ’ll use colour = \"blue\" fill = \"blue\"?fill ’s inside boxplots want modify. colour modify outline colour.* Create side--side boxplot compares GPAs students prefer different Awards. change fill boxplot colour choice. notice plot?outlier students, three groups overall seem similar GPAs.* making previous plot, R gives us warning message “Removed 70 rows containing non-finite values”. R’s robotic way telling us 70 GPA values missing data set. Use know data collected (Fall Spring semeseter 2018-2019 school-year) guess missing.STAT 113 first-year students: first-years taking course fall GPA report. Additionally, another reason might student chose report GPA.* Make stacked bar plot two variables choosing STAT 113 data set. Comment something notice plot.Answers vary.might expect, seem like higher proportion students play sport prefer win Olympic medal, compared students play sport.",
    "code": "\nggplot(data = stat113_df, aes(x = Award, y = Exercise)) +\n  geom_boxplot(fill = \"blue\")\nggplot(data = stat113_df, aes(x = Award, y = GPA)) +\n  geom_boxplot(fill = \"lightpink1\")\nggplot(data = stat113_df, aes(x = Sport, fill = Award)) +\n  geom_bar(position = \"fill\")"
  },
  {
    "path": "ggplot2.html",
    "id": "chapexercise-2-S",
    "chapter": " 3 Plotting with ggplot2",
    "heading": "3.7.6 Chapter Exercises S",
    "text": "* default geom_smooth() use LOESS (locally estimated scatterplot smoothing). Read LOESS : . Write one two sentences explaining LOESS .Loess uses bunch local regressions predict y-variable point, giving weight observations near point interest x-axis. done every point, predictions connected smooth curve.* Thus far, faceted single variable. Use Google figure facet two variables make plot shows relationship GPA (y-axis) Exercise (x-axis) four facets: one male students play sport, one female students play sport, one male students play sport, one female students play sport.* Intro-Stat, boxplots typically introduced using * symbol identify outliers. Using combination help ?geom_boxplot Googling “R point shapes”, figure modify side--side boxplots outliers shown using *, default dots., using Google, figure add mean boxplot “darkgreen” diamond-shaped symbol stat_summary().",
    "code": "\nggplot(data = stat113_df |> filter(!is.na(Sport) & !is.na(Sex)),\n  aes(x = Exercise, y = GPA)) + \n  geom_point() + geom_smooth() +\n  facet_grid(Sex ~ Sport)\n#> `geom_smooth()` using method = 'loess' and formula 'y ~ x'\nggplot(data = stat113_df, aes(x = Sex, y = GPA)) +\n  geom_boxplot(fill = \"lightpink1\", outlier.shape = 8) +\n  stat_summary(fun = mean, shape = 18, colour = \"darkgreen\")"
  },
  {
    "path": "ggplot2.html",
    "id": "rcode-2",
    "chapter": " 3 Plotting with ggplot2",
    "heading": "3.8 Non-Exercise R Code",
    "text": "",
    "code": "\nlibrary(tidyverse)\npres_df <- read_table(\"data/PRES2000.txt\") \n## don't worry about the `read_table` function....yet\nhead(pres_df)\nggplot(data = pres_df, aes(x = Gore)) +\n  geom_histogram(colour = \"black\", fill = \"white\") +\n  xlab(\"Votes for Gore in Florida\")\nggplot(data = pres_df, aes(x = Gore)) +\n  geom_freqpoly(colour = \"black\") +\n  xlab(\"Votes for Gore in Florida\") \nggplot(data = pres_df, aes(x = Gore)) +\n  geom_freqpoly(colour = \"black\") +\n  xlab(\"Votes for Gore in Florida\") +\n  geom_histogram() \npres_cat <- pres_df |> mutate(winner = if_else(Gore > Bush,\n                                                true = \"Gore\",\n                                                false = \"Bush\"))\npres_cat\nggplot(data = pres_cat, aes(x = winner)) +\n  geom_bar()\npres_cat2 <- pres_cat |> group_by(winner) |>\n  summarise(nwins = n())\npres_cat2\nggplot(pres_cat2, aes(x = winner)) +\n  geom_bar()\nggplot(pres_cat2, aes(x = winner, y = nwins)) +\n  geom_col()\nggplot(data = pres_df, aes(x = Gore, y = Bush)) +\n  geom_point()\nlibrary(tidyverse)\nfitness_full <- read_csv(\"data/higham_fitness_clean.csv\") |> mutate(weekend_ind = case_when(weekday == \"Sat\" | weekday == \"Sun\" ~ \"weekend\",\n  TRUE ~ \"weekday\"))\nggplot(data = fitness_full, aes(x = distance, y = active_cals)) +\n  geom_point()\n## drop observations that have active calories < 50. \n## assuming that these are data errors or \n## days where the Apple Watch wasn't worn.\nfitness <- fitness_full |>\n  filter(active_cals > 50)\nggplot(data = fitness) +\n  geom_point(aes(x = distance, y = active_cals))\nggplot(data = fitness) +\n  geom_point(aes(x = Start, y = active_cals, colour = weekend_ind))\nggplot(data = fitness) +\n  geom_point(aes(x = Start, y = active_cals, shape = weekend_ind))\nggplot(data = fitness) +\n  geom_point(aes(x = Start, y = active_cals, size = flights))\nggplot(data = fitness) +\n  geom_point(aes(x = Start, y = active_cals, colour = \"purple\"))\nggplot(data = fitness) +\n  geom_point(aes(x = Start, y = active_cals), size = 1.5, shape = 19)\nggplot(data = fitness, aes(x = Start, y = active_cals)) +\n  geom_point() +\n  geom_smooth(span = 0.3)\nggplot(data = fitness, aes(x = Start, y = active_cals)) +\n  geom_point() +\n  geom_smooth(se = FALSE, method = \"lm\")\nggplot(data = fitness, aes(x = Start, y = steps)) +\n  geom_point() + geom_smooth() + xlab(\"Date\")\nggplot(data = fitness, aes(x = Start, y = steps)) +\n  geom_line() + geom_smooth() + xlab(\"Date\")\nggplot(data = stat113_df, aes(x = Exercise, y = Pulse,\n                           colour = Year)) +\n  geom_point() +\n  geom_smooth(se = TRUE)\nggplot(data = stat113_df, aes(x = Exercise, y = Pulse)) +\n  geom_point() +\n  geom_smooth(se = TRUE) +\n  facet_wrap(~ Year)\nggplot(data = stat113_df, aes(x = Award, y = Exercise)) +\n  geom_boxplot()\nggplot(data = stat113_df, aes(x = Award, y = Exercise)) +\n  geom_violin()\nggplot(data = stat113_df, aes(x = Year, fill = SocialMedia)) +\n  geom_bar(position = \"fill\") +\n  ylab(\"Proportion\")"
  },
  {
    "path": "dplyr.html",
    "id": "dplyr",
    "chapter": " 4 Wrangling with dplyr",
    "heading": " 4 Wrangling with dplyr",
    "text": "Goals:Use mutate(), if_else(), case_when() functions create new variables.Use mutate(), if_else(), case_when() functions create new variables.Use filter() slice(), select(), arrange() functions dplyr choose certain rows keep get rid , choose certain columns keep get rid , sort data, respectively.Use filter() slice(), select(), arrange() functions dplyr choose certain rows keep get rid , choose certain columns keep get rid , sort data, respectively.Use group_by() summarise() create useful summaries data set.Use group_by() summarise() create useful summaries data set.Combine goals plotting explore babynames data set data set SLU majors.Combine goals plotting explore babynames data set data set SLU majors.Explain pipe operator |> explain can use pipe operator.Explain pipe operator |> explain can use pipe operator.Throughout chapter, use babynames data set babynames R package. begin, install babynames package typing install.packages(\"babynames\") bottom-left console winow, read data set runningand typing ?babynames bottom-left window R Studio. see data set contains baby name data provided SSA United States dating back 1880:second data set use 27 observations, one SLU’s majors contains 3 variables:Major, name major.nfemales, number female graduates major 2015 - 2019.nmales, number male graduates major 2015 - 2019.data kindly provided Dr. Ramler. Notes R Project open, can read data set withThere many interesting informative plots make either data set, require data wrangling first. chapter provide foundation wrangling skills.",
    "code": "\nlibrary(babynames)\nhead(babynames)\n#> # A tibble: 6 × 5\n#>    year sex   name          n   prop\n#>   <dbl> <chr> <chr>     <int>  <dbl>\n#> 1  1880 F     Mary       7065 0.0724\n#> 2  1880 F     Anna       2604 0.0267\n#> 3  1880 F     Emma       2003 0.0205\n#> 4  1880 F     Elizabeth  1939 0.0199\n#> 5  1880 F     Minnie     1746 0.0179\n#> 6  1880 F     Margaret   1578 0.0162\nlibrary(tidyverse)\nslumajors_df <- read_csv(\"data/SLU_Majors_15_19.csv\")\nslumajors_df\n#> # A tibble: 27 × 3\n#>    Major                        nfemales nmales\n#>    <chr>                           <dbl>  <dbl>\n#>  1 Anthropology                       34     15\n#>  2 Art & Art History                  65     11\n#>  3 Biochemistry                       14     11\n#>  4 Biology                           162     67\n#>  5 Business in the Liberal Arts      135    251\n#>  6 Chemistry                          26     14\n#>  7 Computer Science                   21     47\n#>  8 Conservation Biology               38     20\n#>  9 Economics                         128    349\n#> 10 English                           131     54\n#> # … with 17 more rows\n#> # ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "path": "dplyr.html",
    "id": "mutate-create-variables",
    "chapter": " 4 Wrangling with dplyr",
    "heading": "4.1 mutate(): Create Variables",
    "text": "Sometimes, want create new variable ’s data set, oftentimes using if_else(), case_when(), basic algebraic operations one columns already present data set.R understands following symbols:+ addition, - subtraction* multiplication, / division^ raising something power (3 ^ 2 equal 9)R also order operations usual: parentheses, exponents, multiplication division, addition subtraction.example, suppose want create variable slumajors_df total number students graduating major. can mutate():’s lot break code chunk: importantly, ’re seeing first many, many, many, many, many, many, many instances using |> pipe! |> operator approximately reads take slumajors_df “” mutate() .Piping really convenient, easy--read way build sequence commands. can read code :Take slumajors_df slumajors_df,Take slumajors_df slumajors_df,perform mutate() step create new variable called ntotal, nfemales plus nmales.perform mutate() step create new variable called ntotal, nfemales plus nmales.Since first time using mutate(), let’s also delve function . general, mutate() reads:mutate(name_of_new_variable = operations_on_old_variables).R just automatically assumes want operation every single row data set, often quite convenient!might also want create variable percentage students identifying female major:happened ntotal? still printout? ’s : created variable ntotal, didn’t actually save new data set anything. R makes prints new variable, doesn’t get saved data set. want save new data set, can use <- operator. , ’re saving new data set name old data set: slumajors_df. , ’re thing percfemale variable. won’t always want give new data set name old one: ’ll talk chapter exercises., can pipe many things together want , ’s probably easier just create variables one go. following chunk says “Take slumajors_df create new variable ntotal. new data set, create new variable called percfemale.” Finally, slumajors_df <- beginning says “save new data set data set name, slumajors_df.”",
    "code": "\nslumajors_df |> mutate(ntotal = nfemales + nmales)\n#> # A tibble: 27 × 4\n#>    Major                        nfemales nmales ntotal\n#>    <chr>                           <dbl>  <dbl>  <dbl>\n#>  1 Anthropology                       34     15     49\n#>  2 Art & Art History                  65     11     76\n#>  3 Biochemistry                       14     11     25\n#>  4 Biology                           162     67    229\n#>  5 Business in the Liberal Arts      135    251    386\n#>  6 Chemistry                          26     14     40\n#>  7 Computer Science                   21     47     68\n#>  8 Conservation Biology               38     20     58\n#>  9 Economics                         128    349    477\n#> 10 English                           131     54    185\n#> # … with 17 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\nslumajors_df |>\n  mutate(percfemale = 100 * nfemales / (nfemales + nmales))\n#> # A tibble: 27 × 4\n#>    Major                        nfemales nmales percfemale\n#>    <chr>                           <dbl>  <dbl>      <dbl>\n#>  1 Anthropology                       34     15       69.4\n#>  2 Art & Art History                  65     11       85.5\n#>  3 Biochemistry                       14     11       56  \n#>  4 Biology                           162     67       70.7\n#>  5 Business in the Liberal Arts      135    251       35.0\n#>  6 Chemistry                          26     14       65  \n#>  7 Computer Science                   21     47       30.9\n#>  8 Conservation Biology               38     20       65.5\n#>  9 Economics                         128    349       26.8\n#> 10 English                           131     54       70.8\n#> # … with 17 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\nslumajors_df <- slumajors_df |>\n  mutate(percfemale = 100 * nfemales / (nfemales + nmales))\nslumajors_df <- slumajors_df |> mutate(ntotal = nfemales + nmales)\nslumajors_df <- slumajors_df |>\n  mutate(ntotal = nfemales + nmales) |>\n  mutate(percfemale = 100 * nfemales / (nfemales + nmales))"
  },
  {
    "path": "dplyr.html",
    "id": "if_else-and-case_when",
    "chapter": " 4 Wrangling with dplyr",
    "heading": "4.1.1 if_else() and case_when()",
    "text": "Suppose want make new variable conditional another variable (one variable) data set. typically use mutate() coupled withif_else() new variable created one conditioncase_when() new variable created one conditionSuppose want create new variable tells us whether Major majority Women. , want new variable, morewomen \"Yes\" Major 50% women \"\" 50% less.mutate() statement reads: create new variable called morewomen equal \"Yes\" percfemale > 50 true equal \"\" perfemale > 0.5. first argument condition, second name new variable condition holds, third name variable condition hold.use conditions time every day life. example, New York quarantine order stating people coming 22 states July 2020 need quarantine. terms condition, read “traveling New York one 22 states, need quarantine 2 weeks. Else, , don’t need quarantine.” trick using conditions R getting used syntax code.can see set one condition, ’d need use different function (use nested if_else() statements, can nightmare read). one condition creating new variable, use case_when().example, looking output, see Biochemistry 56% female graduates. ’s “” 50/50 split, suppose want variable called large_majority “female” percent women 70 , “male” percent women 30 less, “none” percent female 30 70.case_when() function reads “percent female equal 70, assign new variable large_majority value ”female”, ’s less equal 30, assign 30 less 70, assign variable value “none” .” & boolean operator: ’ll talk later don’t worry much now.Let’s save two new variables slumajors_df:",
    "code": "\nslumajors_df |> mutate(morewomen = if_else(percfemale > 50,\n                                            true = \"Yes\",\n                                            false = \"No\"))\n#> # A tibble: 27 × 6\n#>    Major               nfema…¹ nmales percf…² ntotal morew…³\n#>    <chr>                 <dbl>  <dbl>   <dbl>  <dbl> <chr>  \n#>  1 Anthropology             34     15    69.4     49 Yes    \n#>  2 Art & Art History        65     11    85.5     76 Yes    \n#>  3 Biochemistry             14     11    56       25 Yes    \n#>  4 Biology                 162     67    70.7    229 Yes    \n#>  5 Business in the Li…     135    251    35.0    386 No     \n#>  6 Chemistry                26     14    65       40 Yes    \n#>  7 Computer Science         21     47    30.9     68 No     \n#>  8 Conservation Biolo…      38     20    65.5     58 Yes    \n#>  9 Economics               128    349    26.8    477 No     \n#> 10 English                 131     54    70.8    185 Yes    \n#> # … with 17 more rows, and abbreviated variable names\n#> #   ¹​nfemales, ²​percfemale, ³​morewomen\n#> # ℹ Use `print(n = ...)` to see more rows\nslumajors_df |> mutate(large_majority =\n                          case_when(percfemale >= 70 ~ \"female\",\n                                    percfemale <= 30 ~ \"male\",\n                                    percfemale > 30 & percfemale < 70 ~ \"none\")) \n#> # A tibble: 27 × 6\n#>    Major               nfema…¹ nmales percf…² ntotal large…³\n#>    <chr>                 <dbl>  <dbl>   <dbl>  <dbl> <chr>  \n#>  1 Anthropology             34     15    69.4     49 none   \n#>  2 Art & Art History        65     11    85.5     76 female \n#>  3 Biochemistry             14     11    56       25 none   \n#>  4 Biology                 162     67    70.7    229 female \n#>  5 Business in the Li…     135    251    35.0    386 none   \n#>  6 Chemistry                26     14    65       40 none   \n#>  7 Computer Science         21     47    30.9     68 none   \n#>  8 Conservation Biolo…      38     20    65.5     58 none   \n#>  9 Economics               128    349    26.8    477 male   \n#> 10 English                 131     54    70.8    185 female \n#> # … with 17 more rows, and abbreviated variable names\n#> #   ¹​nfemales, ²​percfemale, ³​large_majority\n#> # ℹ Use `print(n = ...)` to see more rows\nslumajors_df <- slumajors_df |>\n  mutate(morewomen = if_else(percfemale > 50,\n                             true = \"Yes\",\n                             false = \"No\")) |>\n  mutate(large_majority =\n           case_when(percfemale >= 70 ~ \"female\",\n                     percfemale <= 30 ~ \"male\",\n                     percfemale > 30 & percfemale < 70 ~ \"none\")) "
  },
  {
    "path": "dplyr.html",
    "id": "exercise-3-1",
    "chapter": " 4 Wrangling with dplyr",
    "heading": "4.1.2 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 4.7.think ethical exclude non-binary genders analyses graphs slumajors data set? ?think ethical exclude non-binary genders analyses graphs slumajors data set? ?* Create new variable called major_size “large” total number majors 100 “small” total number majors less 100.* Create new variable called major_size “large” total number majors 100 “small” total number majors less 100.Create new variable called major_size2 “large total number majors 150 ,”medium” total number majors 41 149, “small” total number majors 40 fewer.Create new variable called major_size2 “large total number majors 150 ,”medium” total number majors 41 149, “small” total number majors 40 fewer.55% SLU students identify female. , definition morewomen variable, make sense use 55% cutoff 50%?55% SLU students identify female. , definition morewomen variable, make sense use 55% cutoff 50%?* Investigate happens case_when() give overlapping conditions give conditions don’t cover observations. overlapping conditions, create variable testcase \"Yes\" percfemale greater equal 40 \"\" percfemale greater 60 conditions don’t cover observations, create variable testcase2 \"Yes\" percfemale greater equal 55 \"\" percfemale less 35.* Investigate happens case_when() give overlapping conditions give conditions don’t cover observations. overlapping conditions, create variable testcase \"Yes\" percfemale greater equal 40 \"\" percfemale greater 60 conditions don’t cover observations, create variable testcase2 \"Yes\" percfemale greater equal 55 \"\" percfemale less 35.one two newly created variables mutate(), create plot investigates question interest might data.one two newly created variables mutate(), create plot investigates question interest might data.",
    "code": ""
  },
  {
    "path": "dplyr.html",
    "id": "arrange-ordering-rows-select-choosing-columns-and-slice-and-filter-choosing-rows",
    "chapter": " 4 Wrangling with dplyr",
    "heading": "4.2 arrange() (Ordering Rows), select() (Choosing Columns), and slice() and filter() (Choosing Rows)",
    "text": "arrange() used order rows data set according variable, select() used choose columns keep (get rid ) filter() used keep (get rid ) observations (rows).",
    "code": ""
  },
  {
    "path": "dplyr.html",
    "id": "arrange-ordering-rows",
    "chapter": " 4 Wrangling with dplyr",
    "heading": "4.2.1 arrange(): Ordering Rows",
    "text": "arrange() function allows us order rows data set using one variables. function straightforward. Suppose want order rows majors lowest percfemale first:major lowest percentage female graduates?see , default, arrange() orders rows low high. order high low majors highest percfemale first, use desc() around variable ordering :major highest percentage women graduates?",
    "code": "\nslumajors_df |> arrange(percfemale)\n#> # A tibble: 27 × 7\n#>    Major       nfema…¹ nmales percf…² ntotal morew…³ large…⁴\n#>    <chr>         <dbl>  <dbl>   <dbl>  <dbl> <chr>   <chr>  \n#>  1 Economics       128    349    26.8    477 No      male   \n#>  2 Physics           6     14    30       20 No      male   \n#>  3 Computer S…      21     47    30.9     68 No      none   \n#>  4 Business i…     135    251    35.0    386 No      none   \n#>  5 Music            13     21    38.2     34 No      none   \n#>  6 Geology          28     41    40.6     69 No      none   \n#>  7 History          62     82    43.1    144 No      none   \n#>  8 Philosophy       24     29    45.3     53 No      none   \n#>  9 Mathematics      74     83    47.1    157 No      none   \n#> 10 Government      127    116    52.3    243 Yes     none   \n#> # … with 17 more rows, and abbreviated variable names\n#> #   ¹​nfemales, ²​percfemale, ³​morewomen, ⁴​large_majority\n#> # ℹ Use `print(n = ...)` to see more rows\nslumajors_df |> arrange(desc(percfemale))\n#> # A tibble: 27 × 7\n#>    Major       nfema…¹ nmales percf…² ntotal morew…³ large…⁴\n#>    <chr>         <dbl>  <dbl>   <dbl>  <dbl> <chr>   <chr>  \n#>  1 Art & Art …      65     11    85.5     76 Yes     female \n#>  2 Psychology      278     61    82.0    339 Yes     female \n#>  3 French           27      7    79.4     34 Yes     female \n#>  4 Spanish          35     10    77.8     45 Yes     female \n#>  5 Statistics       28      9    75.7     37 Yes     female \n#>  6 Global Stu…      69     27    71.9     96 Yes     female \n#>  7 Neuroscien…      61     24    71.8     85 Yes     female \n#>  8 Performanc…     144     57    71.6    201 Yes     female \n#>  9 Religious …      10      4    71.4     14 Yes     female \n#> 10 English         131     54    70.8    185 Yes     female \n#> # … with 17 more rows, and abbreviated variable names\n#> #   ¹​nfemales, ²​percfemale, ³​morewomen, ⁴​large_majority\n#> # ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "path": "dplyr.html",
    "id": "select-choose-columns",
    "chapter": " 4 Wrangling with dplyr",
    "heading": "4.2.2 select() Choose Columns",
    "text": "might also interested getting rid columns data set. One reason overwhelming (30+) columns data set, know just need . easiest way use select() just input names columns want keep. example, interested majors totals, doIf wanted use data set anything else, ’d also need name, rename, <-. probably want name something slumajors_df overwrite original data set, case want use variables later!might also want use select() get rid one two columns. case, denote column want get rid -. example, might want get rid ntotal column made get rid nmales nfemales columns:select() comes many useful helper functions, oftentimes needed. One helper functions actually often useful everything(). can, example, use using mutate() put variable just created front data set make sure weren’t unexpected issues:Verify propfemale now appears first data set. everything() tacks remaining variables propfemale. , case, ’s useful way re-order columns might interested appears first.",
    "code": "\nslumajors_df |> select(Major, ntotal)\n#> # A tibble: 27 × 2\n#>    Major                        ntotal\n#>    <chr>                         <dbl>\n#>  1 Anthropology                     49\n#>  2 Art & Art History                76\n#>  3 Biochemistry                     25\n#>  4 Biology                         229\n#>  5 Business in the Liberal Arts    386\n#>  6 Chemistry                        40\n#>  7 Computer Science                 68\n#>  8 Conservation Biology             58\n#>  9 Economics                       477\n#> 10 English                         185\n#> # … with 17 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\nslumajors_df |> select(-ntotal, -nfemales, -nmales)\n#> # A tibble: 27 × 4\n#>    Major                        percfemale morewomen large…¹\n#>    <chr>                             <dbl> <chr>     <chr>  \n#>  1 Anthropology                       69.4 Yes       none   \n#>  2 Art & Art History                  85.5 Yes       female \n#>  3 Biochemistry                       56   Yes       none   \n#>  4 Biology                            70.7 Yes       female \n#>  5 Business in the Liberal Arts       35.0 No        none   \n#>  6 Chemistry                          65   Yes       none   \n#>  7 Computer Science                   30.9 No        none   \n#>  8 Conservation Biology               65.5 Yes       none   \n#>  9 Economics                          26.8 No        male   \n#> 10 English                            70.8 Yes       female \n#> # … with 17 more rows, and abbreviated variable name\n#> #   ¹​large_majority\n#> # ℹ Use `print(n = ...)` to see more rows\nslumajors_df |> mutate(propfemale = percfemale / 100) |>\n  select(propfemale, everything())\n#> # A tibble: 27 × 8\n#>    propfemale Major    nfema…¹ nmales percf…² ntotal morew…³\n#>         <dbl> <chr>      <dbl>  <dbl>   <dbl>  <dbl> <chr>  \n#>  1      0.694 Anthrop…      34     15    69.4     49 Yes    \n#>  2      0.855 Art & A…      65     11    85.5     76 Yes    \n#>  3      0.56  Biochem…      14     11    56       25 Yes    \n#>  4      0.707 Biology      162     67    70.7    229 Yes    \n#>  5      0.350 Busines…     135    251    35.0    386 No     \n#>  6      0.65  Chemist…      26     14    65       40 Yes    \n#>  7      0.309 Compute…      21     47    30.9     68 No     \n#>  8      0.655 Conserv…      38     20    65.5     58 Yes    \n#>  9      0.268 Economi…     128    349    26.8    477 No     \n#> 10      0.708 English      131     54    70.8    185 Yes    \n#> # … with 17 more rows, 1 more variable:\n#> #   large_majority <chr>, and abbreviated variable names\n#> #   ¹​nfemales, ²​percfemale, ³​morewomen\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names"
  },
  {
    "path": "dplyr.html",
    "id": "slice-and-filter-choose-rows",
    "chapter": " 4 Wrangling with dplyr",
    "heading": "4.2.3 slice() and filter(): Choose Rows",
    "text": "Instead choosing columns keep, can also choose certain rows keep using either slice() filter().slice() allows specify row numbers corresponding rows want keep. example, suppose want keep rows five popular majors:can alternatively use slice(1:5), shorthand slice(1, 2, 3, 4, 5). slice() useful, relatively simple. ’ll come back weeks well discuss subsetting base R.filter() way keep rows specifying condition related one variables data set. ’ve already seen conditions if_else() case_when() statements, ’ll now used “filter” rows data set.can keep rows based categorical variable quantitative variable combination number categorical quantitative variables. R uses following symbols make comparisons. ’ve already using intuitive symbols (like < >):< <= less less equal , respectively> >= greater greater equal , respectively== equal (careful: equal double equal sign ==)!= equal (general, ! denotes “”)’s probably time change data set ! ’ll working babynames data set rest chapter:needed, can remind babynames data set typing ?babynames console window.following statements ? See can guess running code.things put quotes, like \"Matthew\" things aren’t, like 2000? Can make pattern?can also combine conditions multiple variables filter() using Boolean operators. ’ve already seen one case_when() statement : & means “”.Look Venn diagrams R Data Science learn various Boolean operators can use R: https://r4ds..co.nz/transform.html#logical-operators. Boolean operators can used functions R well, ’ve already seen if_else() case_when().following gives examples. See can figure line code running .",
    "code": "\nslumajors_df |> arrange(desc(ntotal)) |>\n  slice(1, 2, 3, 4, 5)\n#> # A tibble: 5 × 7\n#>   Major        nfema…¹ nmales percf…² ntotal morew…³ large…⁴\n#>   <chr>          <dbl>  <dbl>   <dbl>  <dbl> <chr>   <chr>  \n#> 1 Economics        128    349    26.8    477 No      male   \n#> 2 Business in…     135    251    35.0    386 No      none   \n#> 3 Psychology       278     61    82.0    339 Yes     female \n#> 4 Government       127    116    52.3    243 Yes     none   \n#> 5 Biology          162     67    70.7    229 Yes     female \n#> # … with abbreviated variable names ¹​nfemales, ²​percfemale,\n#> #   ³​morewomen, ⁴​large_majority\nlibrary(babynames)\nbabynames\n#> # A tibble: 1,924,665 × 5\n#>     year sex   name          n   prop\n#>    <dbl> <chr> <chr>     <int>  <dbl>\n#>  1  1880 F     Mary       7065 0.0724\n#>  2  1880 F     Anna       2604 0.0267\n#>  3  1880 F     Emma       2003 0.0205\n#>  4  1880 F     Elizabeth  1939 0.0199\n#>  5  1880 F     Minnie     1746 0.0179\n#>  6  1880 F     Margaret   1578 0.0162\n#>  7  1880 F     Ida        1472 0.0151\n#>  8  1880 F     Alice      1414 0.0145\n#>  9  1880 F     Bertha     1320 0.0135\n#> 10  1880 F     Sarah      1288 0.0132\n#> # … with 1,924,655 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\nbabynames |> filter(name == \"Matthew\")\nbabynames |> filter(year >= 2000)\nbabynames |> filter(sex != \"M\")\nbabynames |> filter(prop > 0.05)\nbabynames |> filter(year == max(year))\nbabynames |> filter(n > 20000 | prop > 0.05)\nbabynames |> filter(sex == \"F\" & name == \"Mary\")\nbabynames |> filter(sex == \"F\" & name == \"Mary\" & prop > 0.05)"
  },
  {
    "path": "dplyr.html",
    "id": "exercise-3-2",
    "chapter": " 4 Wrangling with dplyr",
    "heading": "4.2.4 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 4.7.happens arrange() one categorical variables slumajors_df data set?happens arrange() one categorical variables slumajors_df data set?* Use select() everything() put large_majority variable first column slumajors_df data set.* Use select() everything() put large_majority variable first column slumajors_df data set.* babynames data set, use filter(), mutate() rank(), arrange() print 10 popular Male babynames 2017.* babynames data set, use filter(), mutate() rank(), arrange() print 10 popular Male babynames 2017.babynames data set, use filter() keep rows name (, another name interests ) one sex (either \"M\" \"F\"). Name new data set something construct line plot looks either n prop chosen name year.babynames data set, use filter() keep rows name (, another name interests ) one sex (either \"M\" \"F\"). Name new data set something construct line plot looks either n prop chosen name year.",
    "code": ""
  },
  {
    "path": "dplyr.html",
    "id": "summarise-and-group_by-create-summaries",
    "chapter": " 4 Wrangling with dplyr",
    "heading": "4.3 summarise() and group_by(): Create Summaries",
    "text": "summarise() function useful get summaries data. example, suppose want know average major size SLU across five year span total number majors across five years. can use summarise() summary function, like mean(), sum(), median(), max(), min(), n(), etc. ’ll notice format summarise() extremely similar format mutate(). Using slumajors_df data just one quick example,",
    "code": "\nslumajors_df |>\n  summarise(meantotalmajor = mean(ntotal),\n            totalgrad = sum(ntotal))\n#> # A tibble: 1 × 2\n#>   meantotalmajor totalgrad\n#>            <dbl>     <dbl>\n#> 1           124.      3347"
  },
  {
    "path": "dplyr.html",
    "id": "group_by-groups",
    "chapter": " 4 Wrangling with dplyr",
    "heading": "4.3.1 group_by(): Groups",
    "text": "summarise() often useful paired group_by() statement. allows us get summaries across different groups.example, suppose wanted total number registered births per year babynames data set:group_by() takes grouping variable, , using summarise() computes given summary function group.summary functions intuitive ’ve intro stat. , ’re sure whether summary getting maximum maximum() max(), just try !n() function can used within summarise() obtain number observations. give total number rows, used without group_by()Note n() typically doesn’t inputs. ’s typically useful paired group_by(): allows us see number observations within year, instance:",
    "code": "\nbabynames |> group_by(year) |>\n  summarise(totalbirths = sum(n))\n#> # A tibble: 138 × 2\n#>     year totalbirths\n#>    <dbl>       <int>\n#>  1  1880      201484\n#>  2  1881      192696\n#>  3  1882      221533\n#>  4  1883      216946\n#>  5  1884      243462\n#>  6  1885      240854\n#>  7  1886      255317\n#>  8  1887      247394\n#>  9  1888      299473\n#> 10  1889      288946\n#> # … with 128 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\nbabynames |> summarise(totalobs = n())\n#> # A tibble: 1 × 1\n#>   totalobs\n#>      <int>\n#> 1  1924665\nbabynames |> group_by(year) |>\n  summarise(ngroup = n())\n#> # A tibble: 138 × 2\n#>     year ngroup\n#>    <dbl>  <int>\n#>  1  1880   2000\n#>  2  1881   1935\n#>  3  1882   2127\n#>  4  1883   2084\n#>  5  1884   2297\n#>  6  1885   2294\n#>  7  1886   2392\n#>  8  1887   2373\n#>  9  1888   2651\n#> 10  1889   2590\n#> # … with 128 more rows\n#> # ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "path": "dplyr.html",
    "id": "exercise-3-3",
    "chapter": " 4 Wrangling with dplyr",
    "heading": "4.3.2 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 4.7.Compare summarise() mutate() using following code. ’s difference two functions?Using data set group_by() n() combination,make line plot ngroup x-axis year y-axis. interpret plot?* Create data set column name column shows total number births name across years sexes.* Create data set column name column shows total number births name across years sexes.* group_by() can also used functions, including mutate(). Use group_by() mutate() rank names least popular year-sex combination.* group_by() can also used functions, including mutate(). Use group_by() mutate() rank names least popular year-sex combination.* data set 4, filter() data keep popular name year-sex combination construct summary table showing many times name appears popular name.* data set 4, filter() data keep popular name year-sex combination construct summary table showing many times name appears popular name.* Run following code. Intuitively, slice(1, 2, 3, 4, 5) grab first five rows data set, , try run , get 1380 rows. Try figure issue using Google search something like “dplyr slicing correctly using group .” find?* Run following code. Intuitively, slice(1, 2, 3, 4, 5) grab first five rows data set, , try run , get 1380 rows. Try figure issue using Google search something like “dplyr slicing correctly using group .” find?",
    "code": "\nslumajors_df |>\n  summarise(meantotalmajor = mean(ntotal),\n            totalgrad = sum(ntotal)) \nslumajors_df |>\n  mutate(meantotalmajor = mean(ntotal),\n            totalgrad = sum(ntotal)) |>\n  select(meantotalmajor, totalgrad, everything())\nbabynames |> group_by(year) |>\n  summarise(ngroup = n())\n#> # A tibble: 138 × 2\n#>     year ngroup\n#>    <dbl>  <int>\n#>  1  1880   2000\n#>  2  1881   1935\n#>  3  1882   2127\n#>  4  1883   2084\n#>  5  1884   2297\n#>  6  1885   2294\n#>  7  1886   2392\n#>  8  1887   2373\n#>  9  1888   2651\n#> 10  1889   2590\n#> # … with 128 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\nbabynames_test <- babynames |>\n  group_by(year, sex) |> mutate(ntest = n / prop)\nbabynames_test |> slice(1, 2, 3, 4, 5)\n#> # A tibble: 1,380 × 6\n#> # Groups:   year, sex [276]\n#>     year sex   name          n   prop   ntest\n#>    <dbl> <chr> <chr>     <int>  <dbl>   <dbl>\n#>  1  1880 F     Mary       7065 0.0724  97605.\n#>  2  1880 F     Anna       2604 0.0267  97605.\n#>  3  1880 F     Emma       2003 0.0205  97605.\n#>  4  1880 F     Elizabeth  1939 0.0199  97605.\n#>  5  1880 F     Minnie     1746 0.0179  97605.\n#>  6  1880 M     John       9655 0.0815 118400.\n#>  7  1880 M     William    9532 0.0805 118400.\n#>  8  1880 M     James      5927 0.0501 118400.\n#>  9  1880 M     Charles    5348 0.0452 118400.\n#> 10  1880 M     George     5126 0.0433 118400.\n#> # … with 1,370 more rows\n#> # ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "path": "dplyr.html",
    "id": "missing-values",
    "chapter": " 4 Wrangling with dplyr",
    "heading": "4.4 Missing Values",
    "text": "data sets ’ve worked nice missing values. ’ll see plenty examples data sets missing values later, examine various functions ’ve talked far tackle missing values.Missing values R denoted NA “Available.” Run following code create toy data set missing values can see various functions ’ve used far deal NA values.",
    "code": "\ntoy_df <- tibble(x = c(NA, 3, 4, 7),\n                 y = c(1, 4, 3, 2),\n                 z = c(\"A\", \"A\", \"B\", NA))\ntoy_df\n#> # A tibble: 4 × 3\n#>       x     y z    \n#>   <dbl> <dbl> <chr>\n#> 1    NA     1 A    \n#> 2     3     4 A    \n#> 3     4     3 B    \n#> 4     7     2 <NA>"
  },
  {
    "path": "dplyr.html",
    "id": "exercise-3-4",
    "chapter": " 4 Wrangling with dplyr",
    "heading": "4.4.1 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 4.7.* mutate(). Try create new variable mutate() involving x. R missing value?* mutate(). Try create new variable mutate() involving x. R missing value?arrange(). Try arranging data set x. R missing value?arrange(). Try arranging data set x. R missing value?filter(). Try filtering observations x less 5 kept. R missing value?filter(). Try filtering observations x less 5 kept. R missing value?summarise(). Try using summarise() function involving x. R return?summarise(). Try using summarise() function involving x. R return?group_by() summarise(). statement 4, add group_by(z) statement summarise(). R return now?group_by() summarise(). statement 4, add group_by(z) statement summarise(). R return now?",
    "code": ""
  },
  {
    "path": "dplyr.html",
    "id": "removing-missing-values",
    "chapter": " 4 Wrangling with dplyr",
    "heading": "4.4.2 Removing Missing Values",
    "text": "Missing values removed without carefully examination note consequences might (e.g. values missing?). toy data set meaningless, aren’t asking questions now, data set missing values!investigated missing values comfortable removing , many functions use summarise() na.rm argument can set TRUE tell summarise() remove NAs taking mean(), median(), max(), etc.want remove missing values directly, can use .na() function combination filter(). variable NA (Available) observation, .na() evaluates TRUE; , .na() evaluates FALSE. Test using mutate() create new variable whether Median missing:missingx TRUE first observation. can use advantage filter() filter data set, without going extra step actually making new variable missingx:’ll commonly see written short-hand people’s code may come across :says “keep anything missing x value” (recall ! means “”).",
    "code": "\ntoy_df |> summarise(meanx = mean(x, na.rm = TRUE))\n#> # A tibble: 1 × 1\n#>   meanx\n#>   <dbl>\n#> 1  4.67\ntoy_df |> mutate(missingx = is.na(x))\n#> # A tibble: 4 × 4\n#>       x     y z     missingx\n#>   <dbl> <dbl> <chr> <lgl>   \n#> 1    NA     1 A     TRUE    \n#> 2     3     4 A     FALSE   \n#> 3     4     3 B     FALSE   \n#> 4     7     2 <NA>  FALSE\ntoy_df |> filter(is.na(x) != TRUE)\n#> # A tibble: 3 × 3\n#>       x     y z    \n#>   <dbl> <dbl> <chr>\n#> 1     3     4 A    \n#> 2     4     3 B    \n#> 3     7     2 <NA>\ntoy_df |> filter(!is.na(x))\n#> # A tibble: 3 × 3\n#>       x     y z    \n#>   <dbl> <dbl> <chr>\n#> 1     3     4 A    \n#> 2     4     3 B    \n#> 3     7     2 <NA>"
  },
  {
    "path": "dplyr.html",
    "id": "more-about-the-pipe",
    "chapter": " 4 Wrangling with dplyr",
    "heading": "4.5 More about the Pipe",
    "text": "jumping straight using piping, want appreciation terrible life without . piping make whatever given |> pipe first argument whatever function follows |>. Sois equivalent toIt might also help use analogy thinking piping. Consider Ke$ha’s morning routine opening song Tik Tok. write morning routine terms piping,Kesha first wakes morning, Kesha woken grabs glasses, Kesha woken glasses brushes teeth, etc.pipe operator |> loaded automatically R. ’ve using pipe quite bit, let’s delve little deeper ’s actually . use videogame_clean.csv data file, contains variables video games 2004 - 2019, includinggame, name gamerelease_date, release date gamerelease_date2, second coding release dateprice, price dollars,owners, number owners (given range)median_playtime, median playtime gamemetascore, score website Metacriticprice_cat, 1 Low (less 10.00 dollars), 2 Moderate (10 29.99 dollars), 3 High (30.00 dollars)meta_cat, Metacritic’s review system, following categories: “Overwhelming Dislike”, “Generally Unfavorable”, “Mixed Reviews”, “Generally Favorable”, “Universal Acclaim”.playtime_miss, whether median play time missing (TRUE) (FALSE)Load data set withLet’s say want filter() data set include videogames metascore isn’t missing. ’ve using code likeWhat pipe putting videogame_df first argument filter() function piping statement chunk equivalent :want first filter games non-missing metascore, get rid observations median play time 0, obtain “median” median_playtime 3 price categories, typically useWe see summary , general, games tend give “bang buck”: expensive games tend larger median play time. Consecutive pipes build : can slowly build pipe step--step. Starting top, videogame_df first argument filter(!.na(metascore)) function:filter(videogame_df, !.na(metascore)) first argument filter(median_playtime > 0):filter(filter(videogame_df, !.na(metascore)), median_playtime > 0) first argument group_by(price_cat):group_by(filter(filter(videogame_df, !.na(metascore)), median_playtime > 0), price_cat) first argument summarise(avg_med_time = median(median_playtime, na.rm = TRUE)):obtain result without |> pipe. , use pipe? Compare code uses pipe operator find average median playtime code doesn’t. easier read? think easier write? example shows , purposes, pipe useful aiding readability code. ’s lot easier see ’s happening code chunk pipes previous code chunk without pipe , pipe, can read code left right top bottom. Without pipe, need read code “inside outside”, much challenging.",
    "code": "\ndf |> mutate(x = y + 4)\nmutate(df, x = y + 4)kesha |> wake_up(time = \"morning\", feels_like = \"P-Diddy\") |>\n  grab(glasses) |>\n  brush(teeth, item = \"jack\", unit = \"bottle\") |> ....\nvideogame_df <- read_csv(\"data/videogame_clean.csv\")\n#> Rows: 26688 Columns: 15\n#> ── Column specification ────────────────────────────────────\n#> Delimiter: \",\"\n#> chr  (7): game, release_date, owners, meta_cat, develope...\n#> dbl  (6): price, median_playtime, metascore, price_cat, ...\n#> lgl  (1): playtime_miss\n#> date (1): release_date2\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nhead(videogame_df)\n#> # A tibble: 6 × 15\n#>   game       relea…¹ release_…² price owners media…³ metas…⁴\n#>   <chr>      <chr>   <date>     <dbl> <chr>    <dbl>   <dbl>\n#> 1 Half-Life… Nov 16… 2004-11-16  9.99 10,00…      66      96\n#> 2 Counter-S… Nov 1,… 2004-11-01  9.99 10,00…     128      88\n#> 3 Counter-S… Mar 1,… 2004-03-01  9.99 10,00…       3      65\n#> 4 Half-Life… Nov 1,… 2004-11-01  4.99 5,000…       0      NA\n#> 5 Half-Life… Jun 1,… 2004-06-01  9.99 2,000…       0      NA\n#> 6 CS2D       Dec 24… 2004-12-24 NA    1,000…      10      NA\n#> # … with 8 more variables: price_cat <dbl>, meta_cat <chr>,\n#> #   playtime_miss <lgl>, number <dbl>, developer <chr>,\n#> #   publisher <chr>, average_playtime <dbl>,\n#> #   meta_cat_factor <chr>, and abbreviated variable names\n#> #   ¹​release_date, ²​release_date2, ³​median_playtime,\n#> #   ⁴​metascore\n#> # ℹ Use `colnames()` to see all variable names\nvideogame_df |> filter(!is.na(metascore))\nfilter(videogame_df, !is.na(metascore))\nvideogame_df |> filter(!is.na(metascore)) |>\n  filter(median_playtime > 0) |>\n  group_by(price_cat) |>\n  summarise(avg_med_time = median(median_playtime, na.rm = TRUE))\nfilter(videogame_df, !is.na(metascore))\nfilter(filter(videogame_df, !is.na(metascore)), median_playtime > 0)\ngroup_by(filter(filter(videogame_df, !is.na(metascore)),\n                median_playtime > 0), price_cat)\nsummarise(group_by(filter(filter(videogame_df, !is.na(metascore)),\n  median_playtime > 0), price_cat), \n  avg_med_time = median(median_playtime, na.rm = TRUE))"
  },
  {
    "path": "dplyr.html",
    "id": "when-you-cant-use-the-pipe",
    "chapter": " 4 Wrangling with dplyr",
    "heading": "4.5.1 When You Can’t Use the Pipe",
    "text": ", pipe convenient way put precedes pipe first argument function follows pipe. ’s important understand learn R , functions tidyverse purposefully made make good use pipe, functions R utilize pipe. functions tidyverse first argument data set (can use pipes consecutively), isn’t case R functions.example, taken STAT 213, ’ve used lm() fit many different types linear models. haven’t taken STAT 213, lm(response ~ explanatory, data = name_of_data_set) stands “linear model” can used fit simple linear regression model learned STAT 113. might expect something like work:throws us error. Typing ?lm reveals first argument formula fit model, data set. function trying runwhich doesn’t work arguments function mixed (formula appear first data set appear second).one final note pipe, note pipe operator |> relatively new. Previously, primary pipe operator used %>% came magrittr package. almost cases, two operators equivalent. However, scanning Internet help code, probably see |> used many people’s responses sites like StackOverflow.",
    "code": "\nvideogame_df |> lm(metascore ~ price)\n#> Error in as.data.frame.default(data): cannot coerce class '\"formula\"' to a data.frame\nlm(videogame_df, metascore ~ price)\n#> Error in as.data.frame.default(data): cannot coerce class '\"formula\"' to a data.frame"
  },
  {
    "path": "dplyr.html",
    "id": "exercise-3-4",
    "chapter": " 4 Wrangling with dplyr",
    "heading": "4.5.2 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 4.7.* Recode following look cleaner using pipe |>.Explain following code gives warning message returns NA. Use list Arguments ?mean explanation.",
    "code": "\nfitness_df <- read_csv(\"data/higham_fitness_clean.csv\")\n#> Rows: 993 Columns: 9\n#> ── Column specification ────────────────────────────────────\n#> Delimiter: \",\"\n#> chr  (2): month, weekday\n#> dbl  (6): active_cals, distance, flights, steps, dayofye...\n#> date (1): Start\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nsummarise(group_by(filter(fitness_df, weekday == \"Sat\" | weekday == \"Sun\"),\n                   month),\n          meanweekend = mean(distance, na.rm = TRUE)) \n#> # A tibble: 12 × 2\n#>    month meanweekend\n#>    <chr>       <dbl>\n#>  1 Apr          5.30\n#>  2 Aug          5.52\n#>  3 Dec          3.30\n#>  4 Feb          4.87\n#>  5 Jan          3.89\n#>  6 Jul          4.85\n#>  7 Jun          4.18\n#>  8 Mar          4.86\n#>  9 May          5.00\n#> 10 Nov          3.06\n#> 11 Oct          3.82\n#> 12 Sep          4.02\nfitness_df |> mean(distance, na.rm = TRUE)\n#> [1] NA"
  },
  {
    "path": "dplyr.html",
    "id": "chapexercise-3",
    "chapter": " 4 Wrangling with dplyr",
    "heading": "4.6 Chapter Exercises",
    "text": "found SLU majors data set FiveThirtyEight majors data set Statistics higher proportion women almost STEM fields. Read first two sections article. Write 2-3 sentences article’s reasoning women statistics STEM fields.found SLU majors data set FiveThirtyEight majors data set Statistics higher proportion women almost STEM fields. Read first two sections article. Write 2-3 sentences article’s reasoning women statistics STEM fields.* . Choose 5 names interest create new data set data 5 names.* . Choose 5 names interest create new data set data 5 names.Use group_by() summarise() add together number Females Males name year. Hint: can group_by() one variable!Use group_by() summarise() add together number Females Males name year. Hint: can group_by() one variable!Make line plot showing popularity 5 names time.Make line plot showing popularity 5 names time.Choose year sex interests filter data set contain observations year sex.\nChoose year sex interests filter data set contain observations year sex.Create new variable ranks names popular least popular.Create new variable ranks names popular least popular.Create bar plot shows 10 popular names well count name.Create bar plot shows 10 popular names well count name.* cases throughout chapter, ’ve renamed data sets using <- name likeIn cases, ’ve given data set new name, likeFor functions generally “safe” name data set using name using function. ?mutate()mutate()arrange()arrange()filter()filter()summarise()summarise()select()select()Pose question babynames data set answer question either graphic data summary.",
    "code": "\ntoy_df <- toy_df |> mutate(newvar = x / y)\ntoy_small <- toy_df |> filter(!is.na(x))"
  },
  {
    "path": "dplyr.html",
    "id": "solutions-3",
    "chapter": " 4 Wrangling with dplyr",
    "heading": "4.7 Exercise Solutions",
    "text": "",
    "code": ""
  },
  {
    "path": "dplyr.html",
    "id": "mutate-s",
    "chapter": " 4 Wrangling with dplyr",
    "heading": "4.7.1 mutate() S",
    "text": "* Create new variable called major_size “large” total number majors 100 “small” total number majors less 100.* Investigate happens case_when() give overlapping conditions give conditions don’t cover observations. overlapping conditions, create variable testcase \"Yes\" percfemale greater equal 40 \"\" percfemale greater 60 conditions don’t cover observations, create variable testcase2 \"Yes\" percefemale greater equal 55 \"\" percfemale less 35.overlapping cases, case_when prioritizes first case given.non-coverage, observation covered given NA.",
    "code": "\nslumajors_df |> mutate(major_size = if_else(ntotal >= 100,\n                                             true = \"large\",\n                                             false = \"small\"))\n#> # A tibble: 27 × 8\n#>    Major       nfema…¹ nmales percf…² ntotal morew…³ large…⁴\n#>    <chr>         <dbl>  <dbl>   <dbl>  <dbl> <chr>   <chr>  \n#>  1 Anthropolo…      34     15    69.4     49 Yes     none   \n#>  2 Art & Art …      65     11    85.5     76 Yes     female \n#>  3 Biochemist…      14     11    56       25 Yes     none   \n#>  4 Biology         162     67    70.7    229 Yes     female \n#>  5 Business i…     135    251    35.0    386 No      none   \n#>  6 Chemistry        26     14    65       40 Yes     none   \n#>  7 Computer S…      21     47    30.9     68 No      none   \n#>  8 Conservati…      38     20    65.5     58 Yes     none   \n#>  9 Economics       128    349    26.8    477 No      male   \n#> 10 English         131     54    70.8    185 Yes     female \n#> # … with 17 more rows, 1 more variable: major_size <chr>,\n#> #   and abbreviated variable names ¹​nfemales, ²​percfemale,\n#> #   ³​morewomen, ⁴​large_majority\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n## OR\nslumajors_df |>\n  mutate(major_size = case_when(ntotal >= 100 ~ \"large\",\n                                ntotal < 100 ~ \"small\"))\n#> # A tibble: 27 × 8\n#>    Major       nfema…¹ nmales percf…² ntotal morew…³ large…⁴\n#>    <chr>         <dbl>  <dbl>   <dbl>  <dbl> <chr>   <chr>  \n#>  1 Anthropolo…      34     15    69.4     49 Yes     none   \n#>  2 Art & Art …      65     11    85.5     76 Yes     female \n#>  3 Biochemist…      14     11    56       25 Yes     none   \n#>  4 Biology         162     67    70.7    229 Yes     female \n#>  5 Business i…     135    251    35.0    386 No      none   \n#>  6 Chemistry        26     14    65       40 Yes     none   \n#>  7 Computer S…      21     47    30.9     68 No      none   \n#>  8 Conservati…      38     20    65.5     58 Yes     none   \n#>  9 Economics       128    349    26.8    477 No      male   \n#> 10 English         131     54    70.8    185 Yes     female \n#> # … with 17 more rows, 1 more variable: major_size <chr>,\n#> #   and abbreviated variable names ¹​nfemales, ²​percfemale,\n#> #   ³​morewomen, ⁴​large_majority\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names#> # A tibble: 27 × 9\n#>    Major       nfema…¹ nmales percf…² ntotal morew…³ large…⁴\n#>    <chr>         <dbl>  <dbl>   <dbl>  <dbl> <chr>   <chr>  \n#>  1 Anthropolo…      34     15    69.4     49 Yes     none   \n#>  2 Art & Art …      65     11    85.5     76 Yes     female \n#>  3 Biochemist…      14     11    56       25 Yes     none   \n#>  4 Biology         162     67    70.7    229 Yes     female \n#>  5 Business i…     135    251    35.0    386 No      none   \n#>  6 Chemistry        26     14    65       40 Yes     none   \n#>  7 Computer S…      21     47    30.9     68 No      none   \n#>  8 Conservati…      38     20    65.5     58 Yes     none   \n#>  9 Economics       128    349    26.8    477 No      male   \n#> 10 English         131     54    70.8    185 Yes     female \n#> # … with 17 more rows, 2 more variables: testcase <chr>,\n#> #   testcase2 <chr>, and abbreviated variable names\n#> #   ¹​nfemales, ²​percfemale, ³​morewomen, ⁴​large_majority\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names"
  },
  {
    "path": "dplyr.html",
    "id": "arrange-select-.-s",
    "chapter": " 4 Wrangling with dplyr",
    "heading": "4.7.2 arrange(), select(), …. S",
    "text": "* Use select() everything() put large_majority variable first column slumajors_df data set.* babynames data set, use filter(), mutate() rank(), arrange() print 10 popular Male babynames 2017.",
    "code": "\nslumajors_df |> select(large_majority, everything())\n#> # A tibble: 27 × 7\n#>    large_major…¹ Major nfema…² nmales percf…³ ntotal morew…⁴\n#>    <chr>         <chr>   <dbl>  <dbl>   <dbl>  <dbl> <chr>  \n#>  1 none          Anth…      34     15    69.4     49 Yes    \n#>  2 female        Art …      65     11    85.5     76 Yes    \n#>  3 none          Bioc…      14     11    56       25 Yes    \n#>  4 female        Biol…     162     67    70.7    229 Yes    \n#>  5 none          Busi…     135    251    35.0    386 No     \n#>  6 none          Chem…      26     14    65       40 Yes    \n#>  7 none          Comp…      21     47    30.9     68 No     \n#>  8 none          Cons…      38     20    65.5     58 Yes    \n#>  9 male          Econ…     128    349    26.8    477 No     \n#> 10 female        Engl…     131     54    70.8    185 Yes    \n#> # … with 17 more rows, and abbreviated variable names\n#> #   ¹​large_majority, ²​nfemales, ³​percfemale, ⁴​morewomen\n#> # ℹ Use `print(n = ...)` to see more rows\nbabynames |> filter(sex == \"M\" & year == 2017) |>\n  mutate(rankname = rank(desc(n))) |>\n  filter(rankname <= 10)\n#> # A tibble: 10 × 6\n#>     year sex   name         n    prop rankname\n#>    <dbl> <chr> <chr>    <int>   <dbl>    <dbl>\n#>  1  2017 M     Liam     18728 0.00954        1\n#>  2  2017 M     Noah     18326 0.00933        2\n#>  3  2017 M     William  14904 0.00759        3\n#>  4  2017 M     James    14232 0.00725        4\n#>  5  2017 M     Logan    13974 0.00712        5\n#>  6  2017 M     Benjamin 13733 0.00699        6\n#>  7  2017 M     Mason    13502 0.00688        7\n#>  8  2017 M     Elijah   13268 0.00676        8\n#>  9  2017 M     Oliver   13141 0.00669        9\n#> 10  2017 M     Jacob    13106 0.00668       10"
  },
  {
    "path": "dplyr.html",
    "id": "summarise-and-group_by-s",
    "chapter": " 4 Wrangling with dplyr",
    "heading": "4.7.3 summarise() and group_by() S",
    "text": "* Create data set column name column shows total number births name across years sexes.* group_by() can also used functions, including mutate(). Use group_by() mutate() rank names least popular year-sex combination.* data set 4, filter() data keep popular name year-sex combination construct summary table showing many times name appears popular name.* Run following code. Intuitively, slice(1, 2, 3, 4, 5) grab first five rows data set, , try run , get 1380 rows. Try figure issue using Google search something like “dplyr slicing correctly using group .” find?Functions like slice() rank() operate defined groups data set using function like group_by() first. Sometimes feature quite convenient. , longer want slice() rank() functions account groups, need add ungroup() pipe, simply drops groups formed:",
    "code": "\nbabynames |> group_by(name) |>\n  summarise(totalbirths = sum(n))\n#> # A tibble: 97,310 × 2\n#>    name      totalbirths\n#>    <chr>           <int>\n#>  1 Aaban             107\n#>  2 Aabha              35\n#>  3 Aabid              10\n#>  4 Aabir               5\n#>  5 Aabriella          32\n#>  6 Aada                5\n#>  7 Aadam             254\n#>  8 Aadan             130\n#>  9 Aadarsh           199\n#> 10 Aaden            4658\n#> # … with 97,300 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\nranked_babynames <- babynames |> group_by(year, sex) |>\n  mutate(rankname = rank((desc(n))))\nranked_babynames |> filter(rankname == 1) |>\n  group_by(name) |>\n  summarise(nappear = n()) |>\n  arrange(desc(nappear))\n#> # A tibble: 18 × 2\n#>    name     nappear\n#>    <chr>      <int>\n#>  1 Mary          76\n#>  2 John          44\n#>  3 Michael       44\n#>  4 Robert        17\n#>  5 Jennifer      15\n#>  6 Jacob         14\n#>  7 James         13\n#>  8 Emily         12\n#>  9 Jessica        9\n#> 10 Lisa           8\n#> 11 Linda          6\n#> 12 Emma           5\n#> 13 Noah           4\n#> 14 Sophia         3\n#> 15 Ashley         2\n#> 16 Isabella       2\n#> 17 David          1\n#> 18 Liam           1\nbabynames_test <- babynames |>\n  group_by(year, sex) |> mutate(ntest = n / prop)\nbabynames_test |> slice(1, 2, 3, 4, 5)\n#> # A tibble: 1,380 × 6\n#> # Groups:   year, sex [276]\n#>     year sex   name          n   prop   ntest\n#>    <dbl> <chr> <chr>     <int>  <dbl>   <dbl>\n#>  1  1880 F     Mary       7065 0.0724  97605.\n#>  2  1880 F     Anna       2604 0.0267  97605.\n#>  3  1880 F     Emma       2003 0.0205  97605.\n#>  4  1880 F     Elizabeth  1939 0.0199  97605.\n#>  5  1880 F     Minnie     1746 0.0179  97605.\n#>  6  1880 M     John       9655 0.0815 118400.\n#>  7  1880 M     William    9532 0.0805 118400.\n#>  8  1880 M     James      5927 0.0501 118400.\n#>  9  1880 M     Charles    5348 0.0452 118400.\n#> 10  1880 M     George     5126 0.0433 118400.\n#> # … with 1,370 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\nbabynames_test |> ungroup() |> slice(1:5)\n#> # A tibble: 5 × 6\n#>    year sex   name          n   prop  ntest\n#>   <dbl> <chr> <chr>     <int>  <dbl>  <dbl>\n#> 1  1880 F     Mary       7065 0.0724 97605.\n#> 2  1880 F     Anna       2604 0.0267 97605.\n#> 3  1880 F     Emma       2003 0.0205 97605.\n#> 4  1880 F     Elizabeth  1939 0.0199 97605.\n#> 5  1880 F     Minnie     1746 0.0179 97605."
  },
  {
    "path": "dplyr.html",
    "id": "missing-values-s",
    "chapter": " 4 Wrangling with dplyr",
    "heading": "4.7.4 Missing Values S",
    "text": "* mutate(). Try create new variable mutate() involving x. R missing value?R puts another NA place x times y observation missing x.",
    "code": "\ntoy_df |> mutate(xy = x * y)\n#> # A tibble: 4 × 5\n#>       x     y z     newvar    xy\n#>   <dbl> <dbl> <chr>  <dbl> <dbl>\n#> 1    NA     1 A      NA       NA\n#> 2     3     4 A       0.75    12\n#> 3     4     3 B       1.33    12\n#> 4     7     2 <NA>    3.5     14"
  },
  {
    "path": "dplyr.html",
    "id": "piping-s",
    "chapter": " 4 Wrangling with dplyr",
    "heading": "4.7.5 Piping S",
    "text": "* Recode following look cleaner using pipe |>:",
    "code": "\nfitness_df <- read_csv(\"data/higham_fitness_clean.csv\")\n#> Rows: 993 Columns: 9\n#> ── Column specification ────────────────────────────────────\n#> Delimiter: \",\"\n#> chr  (2): month, weekday\n#> dbl  (6): active_cals, distance, flights, steps, dayofye...\n#> date (1): Start\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nsummarise(group_by(filter(fitness_df, weekday == 1 | weekday == 7),\n                   month),\n          meanweekend = mean(distance, na.rm = TRUE)) \n#> # A tibble: 0 × 2\n#> # … with 2 variables: month <chr>, meanweekend <dbl>\n#> # ℹ Use `colnames()` to see all variable names\nfitness_df |> filter(weekday == 1 | weekday == 7) |>\n  group_by(month) |>\n  summarise(meanweekend = mean(distance, na.rm = TRUE)) \n#> # A tibble: 0 × 2\n#> # … with 2 variables: month <chr>, meanweekend <dbl>\n#> # ℹ Use `colnames()` to see all variable names"
  },
  {
    "path": "dplyr.html",
    "id": "chapexercise-3-S",
    "chapter": " 4 Wrangling with dplyr",
    "heading": "4.7.6 Chapter Exercises S",
    "text": "* . Choose 5 names interest create new data set data 5 names.Use group_by() summarise() add together number Females Males name year. Hint: can group_by() one variable!Use group_by() summarise() add together number Females Males name year. Hint: can group_by() one variable!Make line plot showing popularity 5 names time.Make line plot showing popularity 5 names time.* cases throughout chapter, ’ve renamed data sets using <- name likeIn cases, ’ve given data set new name, likeFor functions generally “safe” name data set using name using function. ?mutate()Usually fine: mutating creates new variable, doesn’t change variables data set, things get messed new variable.arrange()Usually fine: ordering rows certain way won’t change plots doesn’t change underlying data.filter()Usually best practice. Naming data set name filter means permanently lose data filtered , unless re-read data set beginning.summarise()Usually best practice. , naming summarized data set original data means lose original data, unless re-read beginning. example,means now way access original data toy_df.select()can sometimes okay ’re sure variables removing won’t ever used.",
    "code": "\nbaby5 <- babynames |> filter(name == \"Matthew\" | name == \"Ivan\" |\n                                name == \"Jessica\" | name == \"Robin\" |\n                                name == \"Michael\")\nbaby5_tot <- baby5 |> group_by(year, name) |>\n  summarise(ntot = sum(n))\n#> `summarise()` has grouped output by 'year'. You can\n#> override using the `.groups` argument.\nggplot(data = baby5_tot, aes(x = year, y = ntot, colour = name)) +\n  geom_line()\ntoy_df <- toy_df |> mutate(newvar = x / y)\ntoy_small <- toy_df |> filter(!is.na(x))\ntoy_df <- toy_df |> summarise(meanx = mean(x))\ntoy_df\n#> # A tibble: 1 × 1\n#>   meanx\n#>   <dbl>\n#> 1    NA"
  },
  {
    "path": "dplyr.html",
    "id": "rcode-3",
    "chapter": " 4 Wrangling with dplyr",
    "heading": "4.8 Non-Exercise R Code",
    "text": "",
    "code": "\nlibrary(babynames)\nhead(babynames)\nlibrary(tidyverse)\nslumajors_df <- read_csv(\"data/SLU_Majors_15_19.csv\")\nslumajors_df\nslumajors_df |> mutate(ntotal = nfemales + nmales)\nslumajors_df |>\n  mutate(percfemale = 100 * nfemales / (nfemales + nmales))\nslumajors_df <- slumajors_df |>\n  mutate(percfemale = 100 * nfemales / (nfemales + nmales))\nslumajors_df <- slumajors_df |> mutate(ntotal = nfemales + nmales)\nslumajors_df <- slumajors_df |>\n  mutate(ntotal = nfemales + nmales) |>\n  mutate(percfemale = 100 * nfemales / (nfemales + nmales))\nslumajors_df |> mutate(morewomen = if_else(percfemale > 50,\n                                            true = \"Yes\",\n                                            false = \"No\"))\nslumajors_df |> mutate(large_majority =\n                          case_when(percfemale >= 70 ~ \"female\",\n                                    percfemale <= 30 ~ \"male\",\n                                    percfemale > 30 & percfemale < 70 ~ \"none\")) \nslumajors_df <- slumajors_df |>\n  mutate(morewomen = if_else(percfemale > 50,\n                             true = \"Yes\",\n                             false = \"No\")) |>\n  mutate(large_majority =\n           case_when(percfemale >= 70 ~ \"female\",\n                     percfemale <= 30 ~ \"male\",\n                     percfemale > 30 & percfemale < 70 ~ \"none\")) \nslumajors_df |> arrange(percfemale)\nslumajors_df |> arrange(desc(percfemale))\nslumajors_df |> select(Major, ntotal)\nslumajors_df |> select(-ntotal, -nfemales, -nmales)\nslumajors_df |> mutate(propfemale = percfemale / 100) |>\n  select(propfemale, everything())\nslumajors_df |> arrange(desc(ntotal)) |>\n  slice(1, 2, 3, 4, 5)\nlibrary(babynames)\nbabynames\nbabynames |> filter(name == \"Matthew\")\nbabynames |> filter(year >= 2000)\nbabynames |> filter(sex != \"M\")\nbabynames |> filter(prop > 0.05)\nbabynames |> filter(year == max(year))\nbabynames |> filter(n > 20000 | prop > 0.05)\nbabynames |> filter(sex == \"F\" & name == \"Mary\")\nbabynames |> filter(sex == \"F\" & name == \"Mary\" & prop > 0.05)\nslumajors_df |>\n  summarise(meantotalmajor = mean(ntotal),\n            totalgrad = sum(ntotal))\nbabynames |> group_by(year) |>\n  summarise(totalbirths = sum(n))\nbabynames |> summarise(totalobs = n())\nbabynames |> group_by(year) |>\n  summarise(ngroup = n())\ntoy_df <- tibble(x = c(NA, 3, 4, 7),\n                 y = c(1, 4, 3, 2),\n                 z = c(\"A\", \"A\", \"B\", NA))\ntoy_df\ntoy_df |> summarise(meanx = mean(x, na.rm = TRUE))\ntoy_df |> mutate(missingx = is.na(x))\ntoy_df |> filter(is.na(x) != TRUE)\ntoy_df |> filter(!is.na(x))\nvideogame_df |> filter(!is.na(metascore))\nfilter(videogame_df, !is.na(metascore))\nvideogame_df |> filter(!is.na(metascore)) |>\n  filter(median_playtime > 0) |>\n  group_by(price_cat) |>\n  summarise(avg_med_time = median(median_playtime, na.rm = TRUE))\nfilter(videogame_df, !is.na(metascore))\nfilter(filter(videogame_df, !is.na(metascore)), median_playtime > 0)\ngroup_by(filter(filter(videogame_df, !is.na(metascore)),\n                median_playtime > 0), price_cat)\nsummarise(group_by(filter(filter(videogame_df, !is.na(metascore)),\n  median_playtime > 0), price_cat), \n  avg_med_time = median(median_playtime, na.rm = TRUE))"
  },
  {
    "path": "communication-with-quarto.html",
    "id": "communication-with-quarto",
    "chapter": " 5 Communication with Quarto",
    "heading": " 5 Communication with Quarto",
    "text": "Special Note: Quarto .qmd files R Markdown .qmd files extremely similar. previous version section, used R Markdown. Quarto advantage .qmd files also work Python Julia, generally better -purpose data scientist. spot references R Markdown .qmd file, just mentally convert R Markdown Quarto .rmd qmd.Goals:Explain reproducibility means explain ’s important analyses reproducible.Explain reproducibility means explain ’s important analyses reproducible.Explain Quarto provides tools making analyses reproducible base R Microsoft Word Microsoft Excel.Explain Quarto provides tools making analyses reproducible base R Microsoft Word Microsoft Excel.Use Code Options Quarto Text Options modify Quarto file renders readable, professional .html file.Use Code Options Quarto Text Options modify Quarto file renders readable, professional .html file.Use titles, labels, colour scales, annotations, themes make plots easy read, including people Colour Vision Deficiency.Use titles, labels, colour scales, annotations, themes make plots easy read, including people Colour Vision Deficiency.Overall: ’re making quick plots just , things communication won’t apply. , ’re planning sharing results (usually , eventually), communication tools become much important.",
    "code": ""
  },
  {
    "path": "communication-with-quarto.html",
    "id": "reproducbility",
    "chapter": " 5 Communication with Quarto",
    "heading": "5.1 Reproducbility",
    "text": "’ve using Quarto now, yet talked features anything except insert new code chunk. end section, want able use Quarto options make nice-looking document (can implement options first mini-project).Reproducibility concept recently gained popularity sciences describing analyses another researcher able repeat. , analysis reproducible provide enough information person sitting next can obtain identical results long follow procedures. analysis reproducible isn’t case.\nQuarto makes easy make analysis reproducible couple reasons:Quarto file render unless code runs, meaning won’t accidentally give someone code doesn’t work.Quarto file render unless code runs, meaning won’t accidentally give someone code doesn’t work.Quarto combines “coding” steps “write-” steps one coherent document contains code, figures tables, explanations.Quarto combines “coding” steps “write-” steps one coherent document contains code, figures tables, explanations.",
    "code": ""
  },
  {
    "path": "communication-with-quarto.html",
    "id": "r-scripts-vs.-quarto",
    "chapter": " 5 Communication with Quarto",
    "heading": "5.1.1 R Scripts vs. Quarto",
    "text": "’ve using Quarto entirety course. , may noticed go File -> New File open new Quarto Document file, ton options. first option R Script. Go ahead open new R Script file now.file open completely blank. R Script file reads R code. text , unless text commented #. example, copy paste code inside code chunk .qmd file .R file run line line., advantages disadvantages using R Script file compared using Quarto file? Let’s start advantages Quarto. Quarto allows fully integrate text explanations code results, actual tables figures , code make tables figures one cohesive document. see, using R Scripts write-analysis Word, lot copy-pasting involved results. reason, using Quarto often results reproducible analyses.advantage R Script situation really aren’t presenting results anyone also don’t need text explanations. often occurs two situations. (1) lot data preparation steps. case, typically complete data prep steps R script write resulting clean data .csv ’d import Quarto file. (2) ’re complicated statistically. case, code much focus text creating figures ’d use R Script.“demo” reproducible analysis class.",
    "code": ""
  },
  {
    "path": "communication-with-quarto.html",
    "id": "spell-checking",
    "chapter": " 5 Communication with Quarto",
    "heading": "5.1.2 Spell-Checking",
    "text": "using Quarto communication, probably want utilize spell-check feature. Go Edit -> Check Spelling, ’ll presented spell-checker lets change spelling words may misspelled.",
    "code": ""
  },
  {
    "path": "communication-with-quarto.html",
    "id": "exercise-5-1",
    "chapter": " 5 Communication with Quarto",
    "heading": "5.1.3 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 5.5.’s difference R Quarto?’s difference R Quarto?Quarto analysis reproducible base R script analysis?Quarto analysis reproducible base R script analysis?Quarto analysis easier make reproducible analysis Excel?Quarto analysis easier make reproducible analysis Excel?friend Chaz data analysis project Excel compare average GPA student athletes average GPA non-student athletes. two variables: whether student student athlete GPA. decides two-sample t-test appropriate procedure data (recall Intro Stat procedure appropriate comparing quantitative response (GPA) across two groups). steps analysis.friend Chaz data analysis project Excel compare average GPA student athletes average GPA non-student athletes. two variables: whether student student athlete GPA. decides two-sample t-test appropriate procedure data (recall Intro Stat procedure appropriate comparing quantitative response (GPA) across two groups). steps analysis.writes null alternative hypotheses words statistical notation.writes null alternative hypotheses words statistical notation.uses Excel make set side--side boxplots. changes labels limits y-axis using Point--Click Excel operations.uses Excel make set side--side boxplots. changes labels limits y-axis using Point--Click Excel operations.boxplots, see 3 outliers non-athlete group. three students GPAs 0 suspended repeatedly refusing wear masks indoors. Chaz decides 3 students removed analysis , stayed enrolled, GPAs different 0. deletes 3 rows Excel.boxplots, see 3 outliers non-athlete group. three students GPAs 0 suspended repeatedly refusing wear masks indoors. Chaz decides 3 students removed analysis , stayed enrolled, GPAs different 0. deletes 3 rows Excel.Chaz uses t.test function Excel run test. writes degrees freedom, T-stat, p-value.Chaz uses t.test function Excel run test. writes degrees freedom, T-stat, p-value.Chaz copies graph Word writes conclusion context problem.Chaz copies graph Word writes conclusion context problem.State 2 aspects Chaz’s analysis reproducible.",
    "code": ""
  },
  {
    "path": "communication-with-quarto.html",
    "id": "quarto-files",
    "chapter": " 5 Communication with Quarto",
    "heading": "5.2 Quarto Files",
    "text": "Let’s talk bit components Quarto file used make reproducible analysis shown class.First, open new Quarto file clicking File -> New File -> Quarto Document keep new file renders HTML now.first four five lines top file make YAML (Yet Another Markup Language) header. ’ll come back end, ’s frustrating part learn.Delete code YAML header paste following code chunks clean .qmd file:cars data set built R ’s need anything read (already exists R ).",
    "code": "\nlibrary(tidyverse)\nhead(cars)\nggplot(data = cars, aes(x = speed, y = dist)) +\n  geom_point()\nsummary(cars)"
  },
  {
    "path": "communication-with-quarto.html",
    "id": "code-chunk-options",
    "chapter": " 5 Communication with Quarto",
    "heading": "5.2.1 Code Chunk Options",
    "text": "First, render new file (give name, prompted). see code, couple results tables, scatterplot.Chunk options allow control gets printed file render. example, may may want: code printed, figure printed, tables printed, tidyverse message printed, etc. ton chunk options give us control code output shown! going just focus commonly used.options common execute options Quarto.echo. set either true print code false print code. blank line ```{r}, insert following, tells Quarto print code chunk: #| echo: false. , re-render document make sure code making plot actually hidden .html output.can keep adding options new lines. options include:warning. set either true print warnings messages false print warnings messages. example, load tidyverse, message automatically prints . code chunk, add new line #| warning: false get rid message. Re-render make sure message actually gone.warning. set either true print warnings messages false print warnings messages. example, load tidyverse, message automatically prints . code chunk, add new line #| warning: false get rid message. Re-render make sure message actually gone.output. default, set true shows output tables figures. Change false print output running code. Practice adding #| output: false code chunk Quarto file summary(cars) re-render make sure output summary(cars) gone.output. default, set true shows output tables figures. Change false print output running code. Practice adding #| output: false code chunk Quarto file summary(cars) re-render make sure output summary(cars) gone.eval. eval set true code evaluated false .eval. eval set true code evaluated false .Besides execute options, also options pertaining size figures figure captions. common examples includefig-height fig-width control height width figures. default, 7, can change fig-height fig-width make figures take less space rendered .html document (fig-height: 5, example).fig-height fig-width control height width figures. default, 7, can change fig-height fig-width make figures take less space rendered .html document (fig-height: 5, example).fig-cap adds figure caption figure. Try inserting #| fig-cap: \"Figure 1: caption text blah blah blah\" chunk options chunk plot.fig-cap adds figure caption figure. Try inserting #| fig-cap: \"Figure 1: caption text blah blah blah\" chunk options chunk plot.",
    "code": ""
  },
  {
    "path": "communication-with-quarto.html",
    "id": "global-options",
    "chapter": " 5 Communication with Quarto",
    "heading": "5.2.2 Global Options",
    "text": "discussed far change code output options individual chunks code. , can pain add certain option every single chunk code want option apply . can instead change global option code /output options apply code chunks, unless specifically overwritten chunk.can change execute options (echo, warning, eval, output, ) globally adding line YAML header top Quarto file. Try addingas two new lines title format lines YAML header. tells Quarto echo code code chunk. However, note can change local code chunk #| echo: true override global setting chunk.Additional global execute options go new lines:non-execute options pertaining figure size can changed globally specifying option html part YAML header. following changes figure heights 2, unless chunk overrides global setting:Important Note: need pay particular attention spacing things YAML header. Notice, example, echo: false indented exactly two spaces. Try adding space deleting space, ’ll get error!",
    "code": "execute: \n  echo: falseexecute: \n  echo: false\n  warning: false---\ntitle: \"Quarto Test\"\nexecute: \n  echo: false\nformat: \n  html:\n    fig-height: 2\n---"
  },
  {
    "path": "communication-with-quarto.html",
    "id": "figures-and-tables",
    "chapter": " 5 Communication with Quarto",
    "heading": "5.2.3 Figures and Tables",
    "text": "’ve already seen Figures pop automatically (unless set output: false), quite convenient. Making tables look nice requires one extra step.Delete output: false option added earlier chunk summary(cars). render .qmd file now, results tables head(cars) summary(cars) look kind ugly. focus using kable() function knitr package make tables much aesthetically pleasing. Another option use pander() function pander package. pander() kable() simple functions generate tables sufficient purposes. generate complicated tables, see xtable package.use functions, simply add |> pipe name table function want use. head(cars) |> kable() make nice-looking table kable head(cars) |> pander() use pander(). using kable() knitr package, ’ll need install knitr package install.packages(\"knitr\") load library adding line library(knitr) head(cars) |> kable(). using pander(), ’ll need install pander package install.packages(\"pander\") load library adding line library(pander) head(cars) |> pander(). Try Quarto file.table like better case?plenty options making tables look presentable, discuss Exercises. Keep mind probably wouldn’t use making tables . ’re much useful ’re writing report want share others.",
    "code": ""
  },
  {
    "path": "communication-with-quarto.html",
    "id": "non-code-options",
    "chapter": " 5 Communication with Quarto",
    "heading": "5.2.4 Non-Code Options",
    "text": "Quarto combines R (code chunks, ’ve already discussed) Markdown syntax, comprises stuff outside code chunks, like ’re reading right now!many Markdown options, time, want something specific, can just Google . purpose follows just get us familiar basics things probably use often.Bullet Points Sub-bullet Points: Denoted * -, respectively. sub bullets indented 4 spaces. Note bullet points code appear code chunk.Note: Everything Markdown particular spacing. Things often precise. personally just love , can frustrating sometimes. example, indenting sub-bullet 3 spaces instead 4 spaces make sub-bullet.Numbered Lists bulleted ones, except * replaced numbers 1., 2., etc.Bold, Italics, Code. Surround text __bold text__ make text bold, _italic text_ make text Italics, backticks make text look like Code.Links: simplest way create link something web surround < > <https://www.youtube.com/watch?v=gJf_DDAfDXs>want name link something web address, use [name link](https://www.youtube.com/watch?v=gJf_DDAfDXs), show rendered document “name link” , clicked , take youtube video.Headers: Headers created ## fewer hashtags resulting bigger Header. Typing #Big Header beginning line make big header, ### Medium Header make medium header, ##### Small Header make small header. Headers important get mapped table contents.’s lot stuff explore: <href=“https://rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf” target=“blank> https://rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf ., want something basics, Google definitely help.",
    "code": "* Bullet 1\n* Bullet 2\n    - Sub bullet 1\n    - Sub bullet 2\n    - Sub bullet 3* Bullet 1\n   - Sub bullet 1"
  },
  {
    "path": "communication-with-quarto.html",
    "id": "yaml",
    "chapter": " 5 Communication with Quarto",
    "heading": "5.2.5 YAML",
    "text": "briefly discussed ’s given top every .qmd file: YAML header. YAML header frustrating part change ’s particular spacing.addition controlling global chunk options, can also use YAML header specify theme  Bootswatch.25 themes Bootswatch project. YAML header uses darkly theme. Try pasting YAML header rendering document see outputted theme.can also add table contents, create table contents based headers created ##.many options available theming, , know css, can provide .css file customize theme.",
    "code": "---\ntitle: \"Quarto Test\"\nexecute: \n  echo: false\n  warning: false\nformat: \n  html:\n    fig-height: 2\n    theme: darkly\n    self-contained: true\n------\ntitle: \"Quarto Test\"\nexecute: \n  echo: false\n  warning: false\nformat: \n  html:\n    fig-height: 2\n    theme: darkly\n    toc: true\n    self-contained: true\n---"
  },
  {
    "path": "communication-with-quarto.html",
    "id": "exercise-5-2",
    "chapter": " 5 Communication with Quarto",
    "heading": "5.2.6 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 7.5.rest section, use built-R data set mtcars, observations makes models cars. variables using :cyl, number cylinders car hasmpg, mileage car, miles per gallonBecause data set loaded every time R started , need line reads data set. can examine first observations * Create table showing mean mpg cyl group (cyl stands cylinder can 4-cylinder, 6-cylinder, 8-cylinder) kable() pander(). Hint: remember call knitr library pander library.* Create table showing mean mpg cyl group (cyl stands cylinder can 4-cylinder, 6-cylinder, 8-cylinder) kable() pander(). Hint: remember call knitr library pander library.* Type ?kable console window scroll Help file. Change rounding mean displays one number decimal. , add caption table says “First Table Caption!!”* Type ?kable console window scroll Help file. Change rounding mean displays one number decimal. , add caption table says “First Table Caption!!”* Google “Change Column Names kable” replace column names “Cylinder Numb.” “Mean Mileage”.* Google “Change Column Names kable” replace column names “Cylinder Numb.” “Mean Mileage”.Find table plan use first mini-project. Use column names, caption, digits options make table look nicer kable() function.Find table plan use first mini-project. Use column names, caption, digits options make table look nicer kable() function.Create new R chunk copy paste following new R chunk. Don’t worry factor() : cover next week!Create new R chunk copy paste following new R chunk. Don’t worry factor() : cover next week!Modify R chunk : () figure height 3, (b) code R chunk shows .html file, (c) table running head(cars) hidden .html file. Make (b) (c) local chunk option, set () global option applies R chunks.Change global options first project () hide messages loading tidyverse (b) show code.Change global options first project () hide messages loading tidyverse (b) show code.Use bullet points Introduction first mini-project explains important variables . , add header project marks Introduction.Use bullet points Introduction first mini-project explains important variables . , add header project marks Introduction.Change YAML header project Author (something like author: \"Name\" file uses theme Bootswatch default theme.Change YAML header project Author (something like author: \"Name\" file uses theme Bootswatch default theme.",
    "code": "\nhead(mtcars)\n#>                    mpg cyl disp  hp drat    wt  qsec vs am\n#> Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1\n#> Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1\n#> Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1\n#> Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0\n#> Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0\n#> Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0\n#>                   gear carb\n#> Mazda RX4            4    4\n#> Mazda RX4 Wag        4    4\n#> Datsun 710           4    1\n#> Hornet 4 Drive       3    1\n#> Hornet Sportabout    3    2\n#> Valiant              3    1\nlibrary(tidyverse)\nhead(mtcars)\nggplot(data = mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_boxplot()"
  },
  {
    "path": "communication-with-quarto.html",
    "id": "ggplot2-communication",
    "chapter": " 5 Communication with Quarto",
    "heading": "5.3 ggplot2 Communication",
    "text": "first introduced plotting, used histograms, boxplots, frequency plots, bar plots, scatterplots, line plots, help us explore data set. probably make many different plots single analysis, , exploring, ’s fine keep plots unlabeled untitled default colour scheme theme. ’re just , typically understand data variable means.However, ’ve finished exploring ’d like communicate results, graphically numerically, ’ll likely want tweak plots look aesthetically pleasing. certainly wouldn’t presenting every exploratory plot made tweaking needs done plots. might consider:changing x-axis y-axis labels, changing legend title, adding title, adding subtitle, adding caption + labs()changing x-axis y-axis labels, changing legend title, adding title, adding subtitle, adding caption + labs()changing limits x-axis y-axis + xlim() + ylim()changing limits x-axis y-axis + xlim() + ylim()changing colour scheme visually appealing easy see people colour-vision-deficiency (CVD)changing colour scheme visually appealing easy see people colour-vision-deficiency (CVD)labeling certain points lines + geom_label() + geom_text()labeling certain points lines + geom_label() + geom_text()changing default theme + theme_<name_of_theme>()changing default theme + theme_<name_of_theme>()bullet labeling certain points data set one reason second ggplot2 section now, opposed immediately first ggplot2 section. see, ’ll make use combining ’ve learned dplyr help us label interesting observations plots.DataThe Happy Planet Index (HPI) measure efficiently country uses ecological resources give citizens long “happy” lives. can read data :  ., basic idea HPI metric computes happy healthy country’s citizens , adjusts country’s ecological footprint (much “damage” country planet Earth). data set obtained  https://github.com/aepoetry/happy_planet_index_2016. Variables data set :HPIRank, rank country’s Happy Planet Index (lower better)Country, name countryLifeExpectancy, average life expectancy citizen (years)Wellbeing, average well score (scale 1 - 10). See ladder question documentation calculated.HappyLifeYears, combination LifeExpectancy WellbeingFootprint, ecological footprint per person (higher footprint means average person country less ecologically friendly)Read data set withLet’s look relationship HappyLifeYears Footprint countries different Regions world.region seems variability Ecological Footprint?",
    "code": "\nlibrary(tidyverse)\nhpi_df <- read_csv(\"data/hpi-tidy.csv\")\nhead(hpi_df)\n#> # A tibble: 6 × 11\n#>   HPIRank Country    LifeE…¹ Wellb…² Happy…³ Footp…⁴ Happy…⁵\n#>     <dbl> <chr>        <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n#> 1     109 Afghanist…    48.7    4.76    29.0   0.540    36.8\n#> 2      18 Albania       76.9    5.27    48.8   1.81     54.1\n#> 3      26 Algeria       73.1    5.24    46.2   1.65     52.2\n#> 4     127 Angola        51.1    4.21    28.2   0.891    33.2\n#> 5      17 Argentina     75.9    6.44    55.0   2.71     54.1\n#> 6      53 Armenia       74.2    4.37    41.9   1.73     46.0\n#> # … with 4 more variables: Population <dbl>,\n#> #   GDPcapita <dbl>, GovernanceRank <chr>, Region <chr>,\n#> #   and abbreviated variable names ¹​LifeExpectancy,\n#> #   ²​Wellbeing, ³​HappyLifeYears, ⁴​Footprint,\n#> #   ⁵​HappyPlanetIndex\n#> # ℹ Use `colnames()` to see all variable names\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point()"
  },
  {
    "path": "communication-with-quarto.html",
    "id": "change-labels-and-titles",
    "chapter": " 5 Communication with Quarto",
    "heading": "5.3.1 Change Labels and Titles",
    "text": "can add + labs() change various labels titles throughout plot:aes() use plot gets label can changed name_of_aethetic = \"Label\". example , changed three aes() labels: x, y, colour.text plot aren’t able change labs()?",
    "code": "\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  labs(title = \"Countries with a Higher Ecological Footprint Tend to Have Citizens with Longer, Happier Lives\", \n       ## add title\n       subtitle = \"HappyLifeYears is a Combination of Life Expectancy and Citizen Well-Being\", \n       ## add subtitle (smaller text size than the title)\n       caption = \"Data Source: http://happyplanetindex.org/countries\", \n       ## add caption to the bottom of the figure\n       x = \"Ecological Footprint\", ## change x axis label\n       y = \"Happy Life Years\", ## change y axis label\n       colour = \"World Region\") ## change label of colour legend"
  },
  {
    "path": "communication-with-quarto.html",
    "id": "changing-x-and-y-axis-limits",
    "chapter": " 5 Communication with Quarto",
    "heading": "5.3.2 Changing x and y axis Limits",
    "text": "can also change x-axis limits y-axis limits , example, start 0 y-axis:case, makes points plot bit harder see. can also change often tick marks appear x y-axes. special things like , think ’s best just resort Google (“ggplot change x-axis breaks tick marks” help).",
    "code": "\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  ylim(c(0, 70))"
  },
  {
    "path": "communication-with-quarto.html",
    "id": "changing-a-colour-scale",
    "chapter": " 5 Communication with Quarto",
    "heading": "5.3.3 Changing A Colour Scale",
    "text": "want use graphics communicate others clearly possible. also want inclusive possible communications. means , choose use colour, graphics made colour-vision-deficient (CVD) person can read graphs. 4.5% people colour vision deficient, ’s actually quite likely CVD person view graphics make (depending many people share ) Information CVD.colour scales R Colour Brewer readable common types CVD. list scales can found .typically use top scales variable colouring ordered sequentially (called seq sequential, like grades course: , B, C, D, F), bottom scales variable diverging (called div diverging, like Republican / Democrat lean middle colourless), middle set scales variable unordered categorical (called qual qualitative like names different treatment drugs medical experiment).3 situations World Region graph?want use one colour scales, just need add scale_colour_brewer() name scale want use.Try changing palette something else besides \"Accent\". like new palette better worse?One option easily change colour scale use viridis package. base viridis functions automatically load ggplot2 ’s need call package library(viridis). viridis colour scales made aesthetically pleasing CVD-friendly.drawback viridis package yellow can really hard see (least ).Read examples section Help file ?scale_colour_viridis_d. ’s difference scale_colour_viridis_d(), ?scale_colour_viridis_c(), scale_colour_viridis_b()?like better: Colour Brewer scale Viridis scale?",
    "code": "\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Accent\")\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_viridis_d(option = \"plasma\")"
  },
  {
    "path": "communication-with-quarto.html",
    "id": "labeling-points-or-lines-of-interest",
    "chapter": " 5 Communication with Quarto",
    "heading": "5.3.4 Labeling Points or Lines of Interest",
    "text": "One goal might communication highlighting particular points data set show something interesting. example, might want label points graph corresponding countries highest HPI region: countries best terms using resources efficiently maximize citizen happiness. , might want highlight “bad” example countries least efficient region. , might want label country graph.can done geom_label(). Let’s start labeling points. geom_label() needs one aesthetic called label name column data set labels want use.Yikes! ’s quite uncommon want label points. Let’s see can instead label country best HPI country’s region. , first need use dplyr skills create new data set 7 “best” countries. used group_by(), typically used summarise() afterward. , group_by() works filter() well!code previous chunk ?Now new data set, can use within geom_label(). Recall data = argument ggplot() carries geoms unless specify otherwise. Now chance “specify otherwise” including another data = argument within geom_label():think colour legend changed showing letter “” region?code chunk change “”’s back points?common issue, even labels, labels overlap. ggrepel package solves problem including geom_label_repel() geom automatically repels overlapping labels:final issue plot ’s always clear point plot labeled. trick used R Data Science book surround points labeled open circle using extra geom_point() function:code , shape = 1 says new point open circle size = 3 makes point bigger, ensuring goes around original point. show.legend = FALSE ensures larger open circles don’t become part legend.can use strategy label specific countries. ’m interested United States America falls graph ’m U.S. ’m also interested Denmark falls ’s country ’m interested visiting. Feel free replace countries ’re interested !",
    "code": "\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_label(aes(label = Country))\nplot_df <- hpi_df |> group_by(Region) |>\n  filter(HPIRank == min(HPIRank))\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_label(data = plot_df, aes(label = Country))\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point(aes(colour = Region)) +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_label(data = plot_df, aes(label = Country), show.legend = FALSE)\nlibrary(ggrepel)\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_label_repel(data = plot_df, aes(label = Country),\n                   show.legend = FALSE) \nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears, colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_label_repel(data = plot_df, aes(label = Country), show.legend = FALSE) +\n  geom_point(data = plot_df, size = 3, shape = 1, show.legend = FALSE) \nplot_df_us <- hpi_df |>\n  filter(Country == \"United States of America\" | Country == \"Denmark\")\n\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_point(data = plot_df_us, size = 3, shape = 1,\n             show.legend = FALSE) +\n  geom_label_repel(data = plot_df_us, aes(label = Country),\n                   show.legend = FALSE)"
  },
  {
    "path": "communication-with-quarto.html",
    "id": "plot-themes",
    "chapter": " 5 Communication with Quarto",
    "heading": "5.3.5 Plot Themes",
    "text": "Plot themes easy way change many aspects plot overall theme someone developed. default theme ggplot2 graphs theme_grey(), graph grey background ’ve using entirety class. 7 themes given R Data Science Figure 28.3.However, many choices ggthemes package. Load package library(ggthemes) check https://yutannihilation.github.io/allYourFigureAreBelongToUs/ggthemes/ themes package. personal favorites, given , theme_solarized(), theme_fivethirtyeight(), theme_economist(), choosing theme mostly matter personal taste.’s still much can ggplot2. fact, entire books . , specializations, can usually use Google help !",
    "code": "\nlibrary(ggthemes)\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_point(data = plot_df_us, size = 3, shape = 1, show.legend = FALSE) +\n  geom_label_repel(data = plot_df_us, aes(label = Country), show.legend = FALSE) +\n  theme_solarized()\n\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_point(data = plot_df_us, size = 3, shape = 1, show.legend = FALSE) +\n  geom_label_repel(data = plot_df_us, aes(label = Country), show.legend = FALSE) +\n  theme_fivethirtyeight()\n\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_point(data = plot_df_us, size = 3, shape = 1, show.legend = FALSE) +\n  geom_label_repel(data = plot_df_us, aes(label = Country), show.legend = FALSE) +\n  theme_economist()"
  },
  {
    "path": "communication-with-quarto.html",
    "id": "exercise-5-3",
    "chapter": " 5 Communication with Quarto",
    "heading": "5.3.6 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 5.5.theme() function way really specialise plot. explore exercise .Using options theme() options change colours, shapes, sizes, etc., create ugliest possible ggplot2 graph can make. may change underlying data graph, goal investigate options given theme(). list theme options given link.practice communicating plots chapter exercises.",
    "code": "\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point()"
  },
  {
    "path": "communication-with-quarto.html",
    "id": "chapexercise-5",
    "chapter": " 5 Communication with Quarto",
    "heading": "5.4 Chapter Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 5.5.data sets exist within specific R packages. example, Jenny Bryan, quite famous stats/data science community, put together gapminder package users R access specific data set countries throughout world. https://github.com/jennybc/gapminder.load data set within specific R package, first need install package install.packages(\"gapminder\") load package :, name data set something. case, name data set gapminder, ’s always name package . name data set country_df.Explore data set head(country_df) ?gapminder proceeding following exercises.* Make line graph shows relationship lifeExp year countries data set, faceting graph continent also colouring continent (though redundant). Add x-axis label, y-axis label, legend label, title graph.* Make line graph shows relationship lifeExp year countries data set, faceting graph continent also colouring continent (though redundant). Add x-axis label, y-axis label, legend label, title graph.* Change colour palette CVD-friendly using either scale_colour_brewer() scale_colour_viridis_d().* Change colour palette CVD-friendly using either scale_colour_brewer() scale_colour_viridis_d().* can see couple interesting trends life expectancy. one country Africa one country Asia sees sharp decline life expectancy one point. Europe, one country substantially lower life expectancy rest 1950s catches European countries 2000s. Use filter() create data set 3 countries. , use geom_label() label three countries plot.* can see couple interesting trends life expectancy. one country Africa one country Asia sees sharp decline life expectancy one point. Europe, one country substantially lower life expectancy rest 1950s catches European countries 2000s. Use filter() create data set 3 countries. , use geom_label() label three countries plot.Google history countries Africa Asia just labeled. Add short description country experienced dip life expectancy caption graph.Google history countries Africa Asia just labeled. Add short description country experienced dip life expectancy caption graph.Read help file ?annotate. different geom_label(). one allows finer tuning? one takes code use? One functions (annotate() geom_label()) becomes pain use many labels. one becomes harder use ?Read help file ?annotate. different geom_label(). one allows finer tuning? one takes code use? One functions (annotate() geom_label()) becomes pain use many labels. one becomes harder use ?Suppose want legend appear bottom graph. Without using entirely different theme, use Google figure move legend right-hand side bottom.Suppose want legend appear bottom graph. Without using entirely different theme, use Google figure move legend right-hand side bottom.lot overlapping points overlapping lines, can use alpha control transparency lines. Google “change transparency lines ggplot” change alpha lines transparent.lot overlapping points overlapping lines, can use alpha control transparency lines. Google “change transparency lines ggplot” change alpha lines transparent.Change theme plot theme ggthemes package. , change order two commands change legend position change overall theme. happens?Change theme plot theme ggthemes package. , change order two commands change legend position change overall theme. happens?Modify .qmd file :Modify .qmd file :figure made Exercise 8 prints .html file. (Hint: use global options help ).figure made Exercise 8 prints .html file. (Hint: use global options help ).none code gets printed.none code gets printed.warnings/messages R prints default hidden code chunks.warnings/messages R prints default hidden code chunks.figure height 5 instead default 7.figure height 5 instead default 7.Read following “can software tools make research reproducible?” https://ropensci.github.io/reproducibility-guide/sections/introduction/. discussed article related Quarto?",
    "code": "\nlibrary(gapminder)\ncountry_df <- gapminder"
  },
  {
    "path": "communication-with-quarto.html",
    "id": "solutions-5",
    "chapter": " 5 Communication with Quarto",
    "heading": "5.5 Exercise Solutions",
    "text": "",
    "code": ""
  },
  {
    "path": "communication-with-quarto.html",
    "id": "reproducibility-s",
    "chapter": " 5 Communication with Quarto",
    "heading": "5.5.1 Reproducibility S",
    "text": "",
    "code": ""
  },
  {
    "path": "communication-with-quarto.html",
    "id": "quarto-files-s",
    "chapter": " 5 Communication with Quarto",
    "heading": "5.5.2 Quarto Files S",
    "text": "* Create table showing mean mpg cyl group (cyl stands cylinder can 4-cylinder, 6-cylinder, 8-cylinder) kable() pander(). Hint: remember call knitr library pander library.* Type ?kable console window scroll Help file. Change rounding mean displays one number decimal. , add caption table says “First Table Caption!!”Table 5.1: First Table Caption!!* Google “Change Column Names kable” replace column names “Cylinder Numb.” “Mean Mileage”.Table 5.2: First Table Caption!!",
    "code": "\nlibrary(knitr)\nlibrary(pander)\nlibrary(tidyverse)\nmpg_df <- mtcars |> group_by(cyl) |>\n  summarise(meanmpg = mean(mpg))\nmpg_df |> kable()\nmpg_df |> pander()\nmpg_df |> kable(digits = 1, caption = \"My First Table Caption!!\")\nmpg_df |> kable(digits = 1, caption = \"My First Table Caption!!\",\n  col.names = c(\"Cylinder Numb.\", \"Mean Mileage\"))"
  },
  {
    "path": "communication-with-quarto.html",
    "id": "ggplot2-communication-s",
    "chapter": " 5 Communication with Quarto",
    "heading": "5.5.3 ggplot2 Communication S",
    "text": "",
    "code": ""
  },
  {
    "path": "communication-with-quarto.html",
    "id": "chapexercise-5-S",
    "chapter": " 5 Communication with Quarto",
    "heading": "5.5.4 Chapter Exercises S",
    "text": "* Make line graph shows relationship lifeExp year countries data set, faceting graph continent also colouring continent (though redundant). Add x-axis label, y-axis label, legend label, title graph.* Make line graph shows relationship lifeExp year countries data set, faceting graph continent also colouring continent (though redundant). Add x-axis label, y-axis label, legend label, title graph.* Change colour palette CVD-friendly using either scale_colour_brewer() scale_colour_viridis_d().* Change colour palette CVD-friendly using either scale_colour_brewer() scale_colour_viridis_d().* can see couple interesting trends life expectancy. one country Africa one country Asia sees sharp decline life expxectancy one point. Europe, one country substantially lower life expectancy rest 1950s catches European countries 2000s. Use filter() create data set 3 countries. , use geom_label() label three countries plot.* can see couple interesting trends life expectancy. one country Africa one country Asia sees sharp decline life expxectancy one point. Europe, one country substantially lower life expectancy rest 1950s catches European countries 2000s. Use filter() create data set 3 countries. , use geom_label() label three countries plot.",
    "code": "\ninterest_countries <- country_df |> filter((year == 1952 & continent == \"Europe\" &\n    lifeExp < 50) | (year == 1992 & continent == \"Africa\" &\n    lifeExp < 30) | (year == 1977 & continent == \"Asia\" & \n    lifeExp < 35))\nggplot(data = country_df, aes(x = year, y = lifeExp, group = country,\n  colour = continent)) +\n  geom_line() +\n  facet_wrap( ~ continent) +\n  scale_colour_brewer(palette = \"Set2\") +\n  labs(x = \"Year\", y = \"Life Expectancy (Years)\", colour = \"Continent\",\n    title = \"Life Expectancy Increases Across time for nearly every country\") +\n  geom_label(data = interest_countries, aes(label = country),\n    nudge_x = 7)"
  },
  {
    "path": "communication-with-quarto.html",
    "id": "rcode-5",
    "chapter": " 5 Communication with Quarto",
    "heading": "5.6 Non-Exercise R Code",
    "text": "",
    "code": "\nlibrary(tidyverse)\nhpi_df <- read_csv(\"data/hpi-tidy.csv\")\nhead(hpi_df)\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point()\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  labs(title = \"Countries with a Higher Ecological Footprint Tend to Have Citizens with Longer, Happier Lives\", \n       ## add title\n       subtitle = \"HappyLifeYears is a Combination of Life Expectancy and Citizen Well-Being\", \n       ## add subtitle (smaller text size than the title)\n       caption = \"Data Source: http://happyplanetindex.org/countries\", \n       ## add caption to the bottom of the figure\n       x = \"Ecological Footprint\", ## change x axis label\n       y = \"Happy Life Years\", ## change y axis label\n       colour = \"World Region\") ## change label of colour legend\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  ylim(c(0, 70))\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Accent\")\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_viridis_d(option = \"plasma\")\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_label(aes(label = Country))\nplot_df <- hpi_df |> group_by(Region) |>\n  filter(HPIRank == min(HPIRank))\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_label(data = plot_df, aes(label = Country))\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point(aes(colour = Region)) +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_label(data = plot_df, aes(label = Country), show.legend = FALSE)\nlibrary(ggrepel)\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_label_repel(data = plot_df, aes(label = Country),\n                   show.legend = FALSE) \nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears, colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_label_repel(data = plot_df, aes(label = Country), show.legend = FALSE) +\n  geom_point(data = plot_df, size = 3, shape = 1, show.legend = FALSE) \nplot_df_us <- hpi_df |>\n  filter(Country == \"United States of America\" | Country == \"Denmark\")\n\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_point(data = plot_df_us, size = 3, shape = 1,\n             show.legend = FALSE) +\n  geom_label_repel(data = plot_df_us, aes(label = Country),\n                   show.legend = FALSE)\nlibrary(ggthemes)\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_point(data = plot_df_us, size = 3, shape = 1, show.legend = FALSE) +\n  geom_label_repel(data = plot_df_us, aes(label = Country), show.legend = FALSE) +\n  theme_solarized()\n\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_point(data = plot_df_us, size = 3, shape = 1, show.legend = FALSE) +\n  geom_label_repel(data = plot_df_us, aes(label = Country), show.legend = FALSE) +\n  theme_fivethirtyeight()\n\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_point(data = plot_df_us, size = 3, shape = 1, show.legend = FALSE) +\n  geom_label_repel(data = plot_df_us, aes(label = Country), show.legend = FALSE) +\n  theme_economist()\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point()"
  },
  {
    "path": "workflow.html",
    "id": "workflow",
    "chapter": " 6 Workflow and Other Skills",
    "heading": " 6 Workflow and Other Skills",
    "text": "Goals:Describe files organized R project, describe advantage common working directory R Project, use package assist accessing reading files.Describe files organized R project, describe advantage common working directory R Project, use package assist accessing reading files.Use strategies debug code working correctly.Use strategies debug code working correctly.Explain context data set comes informs analysis data.Explain context data set comes informs analysis data.Find missing values outliers, explain might affect conclusions data analysis.Find missing values outliers, explain might affect conclusions data analysis.Use tibble create data sets R, describe benefits reprexes.Use tibble create data sets R, describe benefits reprexes.",
    "code": ""
  },
  {
    "path": "workflow.html",
    "id": "r-projects-and-file-organization",
    "chapter": " 6 Workflow and Other Skills",
    "heading": "6.1 R Projects and File Organization",
    "text": "R Projects convenient way keep related code, data sets, analyses together. Read short introduction R Data Science : https://r4ds..co.nz/workflow-projects.html#paths--directories https://r4ds..co.nz/workflow-projects.html#rstudio-projects.rarely use absolute directory?Look top bottom-left terminal window. ’ve made R project (!), see file path current folder ’re working . R Studio look files default.package can help keep track files reading files. Install package install.packages(\"\"). , load package run () function withhere() prints directory current R project . reading data set read_csv(), useR can read data set successfully: starts path given console printed (), looks folder called data path looks file called athletesdata.csv data folder., zipped project sent someone else, ’d able open read data file without needing change directory code!() function package can used just printing current working directory. see usefulness, suppose , folder current R project, want make folder called Quizzes .qmd files quizzes class. Make folder, create new .qmd file, paste R chunk reads athletesdata.csv data set, save file, try render file.get error athletesdata.csv data file found. rendering .qmd file, R looks data/ folder within folder contains .qmd file. Since data/ folder folder R Project, folder .qmd file, R can’t find .fix issue, specify entire file path data/ file. , better fix use () function, tells R start looking folders files folder R Project:allows us Quarto files within folders R project.",
    "code": "\nlibrary(here)\n#> here() starts at /Users/highamm/Desktop/datascience234\nhere()\nlibrary(tidyverse)\n#> ── Attaching packages ─────────────────── tidyverse 1.3.2 ──\n#> ✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n#> ✔ tibble  3.1.8     ✔ dplyr   1.0.9\n#> ✔ tidyr   1.2.0     ✔ stringr 1.4.0\n#> ✔ readr   2.1.2     ✔ forcats 0.5.1\n#> ── Conflicts ────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\nathletes_df <- read_csv(\"data/athletesdata.csv\")\n#> New names:\n#> Rows: 100 Columns: 9\n#> ── Column specification\n#> ──────────────────────────────────── Delimiter: \",\" chr\n#> (3): Name, Sport, Gender dbl (6): ...1, Rank, endorsements,\n#> totalpay, salary, age\n#> ℹ Use `spec()` to retrieve the full column specification\n#> for this data. ℹ Specify the column types or set\n#> `show_col_types = FALSE` to quiet this message.\n#> • `` -> `...1`\nathletes_test_read <- read_csv(here(\"data/athletesdata.csv\"))\n#> New names:\n#> Rows: 100 Columns: 9\n#> ── Column specification\n#> ──────────────────────────────────── Delimiter: \",\" chr\n#> (3): Name, Sport, Gender dbl (6): ...1, Rank, endorsements,\n#> totalpay, salary, age\n#> ℹ Use `spec()` to retrieve the full column specification\n#> for this data. ℹ Specify the column types or set\n#> `show_col_types = FALSE` to quiet this message.\n#> • `` -> `...1`"
  },
  {
    "path": "workflow.html",
    "id": "exercise-14-1",
    "chapter": " 6 Workflow and Other Skills",
    "heading": "6.1.1 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 6.7.Take time modify files course creating folders help keep things bit organized. might consider making Quizzes folder Projects folder, example. Move relevant files folders modify file load package use () function read relevant data sets. ’ve using () function quite bit already may already !Take time modify files course creating folders help keep things bit organized. might consider making Quizzes folder Projects folder, example. Move relevant files folders modify file load package use () function read relevant data sets. ’ve using () function quite bit already may already !Click “Packages” button lower-right hand window bring packages menu. Instead using library(name_of_package), can click check-box package name load R. Try un-checking re-checking tidyverse. Explain, reproducibility perspective, loading packages way good practice.Click “Packages” button lower-right hand window bring packages menu. Instead using library(name_of_package), can click check-box package name load R. Try un-checking re-checking tidyverse. Explain, reproducibility perspective, loading packages way good practice.",
    "code": ""
  },
  {
    "path": "workflow.html",
    "id": "code-style",
    "chapter": " 6 Workflow and Other Skills",
    "heading": "6.2 Code Style",
    "text": "Writing code “readable” helpful others also , especially project working long-term. constitutes “readable” code varies bit, general principles widely accepted “good” code. Much coding “style” seen far imposed : style writing code naturally, use style code write course materials.",
    "code": ""
  },
  {
    "path": "workflow.html",
    "id": "names",
    "chapter": " 6 Workflow and Other Skills",
    "heading": "6.2.1 Names",
    "text": "Names objects create descriptive yet short. Sometimes, thinking name makes sense can challenging! examples “bad” names objects include names generic won’t able distinguish later:Better names data frames cyl4_df, cyl6_df, cyl8_df, respectively, names tell us data frame.“bad” names objects names long:Long names descriptive can pain type read.may noticed coding “style” separate words names _: cyl4_df. Others may choose separate words names .: cyl4.df others may use capitalization second word: cyl4Df. important thing consistent choice. words, using _ instead . isn’t necessarily better, poor practice mix naming notation.mixed, always keep track whether object named _ . capitalization.Finally, may noticed data frames named suffix _df. worked coding style like keeping track dataframe (tibble) isn’t. generally helpful encounter different types objects (model output, lists, matrices, vectors, etc.).",
    "code": "\ndf1 <- mtcars |> filter(cyl == 4)\ndf2 <- mtcars |> filter(cyl == 6)\ndf3 <- mtcars |> filter(cyl == 8)\ncars_with_4_cylinders_data_set <- mtcars |> filter(cyl == 4)\ncyl4_df <- mtcars |> filter(cyl == 4)\ncyl6.df <- mtcars |> filter(cyl == 6)"
  },
  {
    "path": "workflow.html",
    "id": "code-readability",
    "chapter": " 6 Workflow and Other Skills",
    "heading": "6.2.2 Code Readability",
    "text": "can also follow general practices make code “readable.” already employing practices throughout semester: R Studio generally makes code readable indenting appropriately.Appropriately using spacing can make code much readable. Consider following ggplot() code. example, following code chunk executes scatterplot fitted regression line ’s generally tough read.couple conventions can help: (1) spaces around equal sign, plus sign, comma (2) putting code plus sign different line.Indenting subsequent lines ggplot2 code dplyr pipeline shows subsequent lines “go ” first line:concepts using multiple lines holds piping statement well. general,easier read ",
    "code": "\nggplot(data=mtcars,aes(x=wt,y=drat))+geom_point()+geom_smooth(method=\"lm\",se=FALSE)\n#> `geom_smooth()` using formula 'y ~ x'\nggplot(data = mtcars, aes(x = wt, y = drat)) +\ngeom_point() +\ngeom_smooth(method = \"lm\", se = FALSE)\n#> `geom_smooth()` using formula 'y ~ x'\nggplot(data = mtcars, aes(x = wt, y = drat)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n#> `geom_smooth()` using formula 'y ~ x'\nmtcars |> filter(cyl == 4) |>\n  group_by(vs) |>\n  summarise(mean_mpg = mean(mpg, na.rm = TRUE))\n#> # A tibble: 2 × 2\n#>      vs mean_mpg\n#>   <dbl>    <dbl>\n#> 1     0     26  \n#> 2     1     26.7\nmtcars |> filter(cyl == 4) |> group_by(vs) |> summarise(mean_mpg = mean(mpg, na.rm = TRUE))\n#> # A tibble: 2 × 2\n#>      vs mean_mpg\n#>   <dbl>    <dbl>\n#> 1     0     26  \n#> 2     1     26.7"
  },
  {
    "path": "workflow.html",
    "id": "exercise-14-2",
    "chapter": " 6 Workflow and Other Skills",
    "heading": "6.2.3 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 6.7.Change following object names “better:”Change style following code make code readable.",
    "code": "\ncars_where_wt_is_larger_than_3_tons <- mtcars |> filter(wt > 3)\ndataset <- mtcars |> group_by(am) |>\n  summarise(mean_disp = mean(disp),\n            med_disp = median(disp),\n            sd_disp = sd(disp))\nggplot(data=mtcars,aes(x = mpg))+geom_histogram(colour=\"black\",fill=\"white\",bins=15) + facet_wrap(~cyl, ncol=1)"
  },
  {
    "path": "workflow.html",
    "id": "debugging-code",
    "chapter": " 6 Workflow and Other Skills",
    "heading": "6.3 Debugging Code",
    "text": "previous section code readability can seen one step helping code debugging: code easier read code easier spot errors . Additionally, strategies can take code working figure issue .",
    "code": ""
  },
  {
    "path": "workflow.html",
    "id": "identify-the-problem",
    "chapter": " 6 Workflow and Other Skills",
    "heading": "6.3.1 Identify the Problem",
    "text": "run R code data analyses “top bottom,” makes bit easier identify problem code occurring. can run code top .qmd file, line line, see red Error message.Often Error message occur ggplot statement piping statement. case, strategy run ggplot statement + sign + sign run piping statement pipe pipe isolate error. example, take following ggplot code, generates somewhat cryptic error.case, error message help us locate issue, always case. sure error , can runto see get error. don’t, move code next + sign:still don’t get error move code next + sign:error. now, instead isolating error particular chunk code, isolated error particular line code: know issue something using geom_smooth(). (missing aes() refer variable disp).strategy can used piping. following code, used figure average bill length bill depth ratio Adelie penguins, give error instead outputs something might expect: tibble NaN (Number) value (note must install palmerpenguins package install.packages(\"palmerpenguins\") loading palmerpenguins library.can troubleshoot running code “pipe pipe,” starting code first filter() pipe:Right away, see problem: get tibble data misspelled Adelie:correcting issue, can continue pipes:doesn’t seem issues mutate() statement can go next pipe.get NA value, isolated issue something summarise(), , possibly something mutate() set something quite right summarise(). Can figure issue?addition isolating coding issue, couple basic strategies trying fix problematic code use search engine like google see anyone else similar error message one may restart R make sure working clean slate.“restart R” strategy can particularly helpful code run .qmd file render. can happen , example, created data set use later chunk code since deleted code created data set. example, suppose create cyl4_df make plot:, later delete line creating cyl4_df. plot still work cyl4_df already environment file render missing crucial line code. Restarting R can help us identify issue plot longer work get sensible error message like cyl4_df found.",
    "code": "\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(colour = disp) +\n  facet_wrap(~ cyl) \nggplot(data = mtcars, aes(x = wt, y = mpg))\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point()\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(colour = disp)\nlibrary(palmerpenguins)\npenguins |> filter(species == \"Adeie\") |>\n  mutate(bill_ratio = bill_length_mm / bill_depth_mm) |>\n  summarise(mean_ratio = mean(bill_ratio))\npenguins |> filter(species == \"Adeie\")\npenguins |> filter(species == \"Adelie\")\npenguins |> filter(species == \"Adelie\") |>\n  mutate(bill_ratio = bill_length_mm / bill_depth_mm)\npenguins |> filter(species == \"Adelie\") |>\n  mutate(bill_ratio = bill_length_mm / bill_depth_mm) |>\n  summarise(mean_ratio = mean(bill_ratio))\ncyl4_df <- mtcars |> filter(cyl == 4)\n\nggplot(data = cyl4_df, aes(x = mpg)) +\n  geom_histogram()\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`."
  },
  {
    "path": "workflow.html",
    "id": "exercise-14-3",
    "chapter": " 6 Workflow and Other Skills",
    "heading": "6.3.2 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 6.7.Find error following code chunk running code “+ sign + sign).Find error following code chunk running code “pipe pipe.”Find error following code chunk running code “pipe pipe.”",
    "code": "\nggplot(data = mtcars, aes(x = hp, y = drat)) +\n  geom_point(aes(colour = factor(gear))) +\n  facet_wrap(cyl) +\n  geom_smooth()\npenguins |> mutate(flipper_ratio = flipper_length_mm / body_mass_g) |>\n  group_by(species, island) |>\n  summarise(mean_flipper = mean(flipper_ratio, na.rm = TRUE)) |>\n  arrange(flipper_ratio) |>\n  pivot_wider(names_from = c(\"species\"), values_from = \"mean_flipper\")\npenguins |> mutate(flipper_ratio = flipper_length_mm / body_mass_g) |>\n  filter(flipper_ratio > median(flipper_ratio)) |>\n  group_by(species) |>\n  summarise(count_var = n())"
  },
  {
    "path": "workflow.html",
    "id": "context-outliers-and-missing-values",
    "chapter": " 6 Workflow and Other Skills",
    "heading": "6.4 Context, Outliers, and Missing Values",
    "text": "primary purpose section explore always think critically data set analyzing opposed simply making summary tables without thinking interpreted. words, need examine data set see things like missing values outliers affect interpretation well consider context data set comes .",
    "code": ""
  },
  {
    "path": "workflow.html",
    "id": "context",
    "chapter": " 6 Workflow and Other Skills",
    "heading": "6.4.1 Context",
    "text": "Considering context includes thinking questions like:data set come ? collected ?data set come ? collected ?missing values coded NAs data set. affect analysis missing “random.” Missing values coded NA referred explicitly missing values.missing values coded NAs data set. affect analysis missing “random.” Missing values coded NA referred explicitly missing values.missing values data set actually observations ? implicitly missing. example might collecting data students attending class. Students present time data collected implicitly missing.missing values data set actually observations ? implicitly missing. example might collecting data students attending class. Students present time data collected implicitly missing.data come observational study experiment?data come observational study experiment?many questions ask pertaining context, many questions depend particular data collected. example, consider data set majors SLU 2015 2020. now, can ignore extra code given read data: pivoting functions variable types topics learn upcoming weeks.data, n_majors variable represents number students graduating particular major particular year. example, year 2005, just 2 students graduating Biochemistry major.Suppose interested trends among majors Estudios Hispanicos (Spanish). United States, many people speak Spanish might expect somewhat popular major. particular, want see increase decrease number majors since 2005. can make line chart :conclude based plot?topic subsection context data set arises . , might guess conclusion one make based line graph (spanish major SLU seems decline) tell full story. fact, decade ago, International Economics Combined major introduced, students complete courses Economics well foreign language studies. popular choice foreign language Spanish.can make graph number International Economics Combined majors:new contextual information International Economics major influence conclusions popularity Spanish studies SLU?find throughout semester data sets topics familiar easier analyze data sets topics familiar . large part reasoning much contextual information data topics prior knowledge . extra contextual information generally allows us pose deeper questions, identify potentially erroneous data, write subtle conclusions. discuss context throughout semester also another focus context discuss data ethics.",
    "code": "\nmajors_df <- read_csv(here(\"data/majors.csv\")) |>\n  pivot_longer(-1, names_to = \"year\", values_to = \"n_majors\") |>\n  mutate(year = as.numeric(year)) |>\n  rename(major = `...1`)\n#> New names:\n#> Rows: 63 Columns: 17\n#> ── Column specification\n#> ──────────────────────────────────── Delimiter: \",\" chr\n#> (1): ...1 dbl (16): 2005, 2006, 2007, 2008, 2009, 2010,\n#> 2011, 2012...\n#> ℹ Use `spec()` to retrieve the full column specification\n#> for this data. ℹ Specify the column types or set\n#> `show_col_types = FALSE` to quiet this message.\n#> • `` -> `...1`\nhead(majors_df)\n#> # A tibble: 6 × 3\n#>   major         year n_majors\n#>   <chr>        <dbl>    <dbl>\n#> 1 Biochemistry  2005        2\n#> 2 Biochemistry  2006        6\n#> 3 Biochemistry  2007        5\n#> 4 Biochemistry  2008        8\n#> 5 Biochemistry  2009        3\n#> 6 Biochemistry  2010        7\nspanish_df <- majors_df |> filter(major == \"Estudios Hispanicos (Spanish)\")\nggplot(data = spanish_df, aes(x = year, y = n_majors)) +\n  geom_line() +\n  geom_smooth(se = FALSE)\n#> `geom_smooth()` using method = 'loess' and formula 'y ~ x'\nint_econ_df <- majors_df |> filter(major == \"Int'l Economics (Combined)\")\nggplot(data = int_econ_df, aes(x = year, y = n_majors)) +\n  geom_line() +\n  geom_smooth(se = FALSE)\n#> `geom_smooth()` using method = 'loess' and formula 'y ~ x'"
  },
  {
    "path": "workflow.html",
    "id": "outliers-and-missing-values",
    "chapter": " 6 Workflow and Other Skills",
    "heading": "6.4.2 Outliers and Missing Values",
    "text": "Outliers data analysis can affect certain summary statistics, like mean standard deviation (learned STAT 113). also observations warrant investigation interested particular point outlier.Missing values can also cause us reach potentially misleading conclusion carefully consider values missing.talk consequences outliers missing values next, first, discuss determine outliers missing values data set. easy function use purpose skim() function skimr package. Install skimr package use skim() function thevideogame_clean.csv file, contains variables video games 2004 - 2019, includinggame, name gamerelease_date, release date gamerelease_date2, second coding release dateprice, price dollars,owners, number owners (given range)median_playtime, median playtime gamemetascore, score website Metacriticprice_cat, 1 Low (less 10.00 dollars), 2 Moderate (10 29.99 dollars), 3 High (30.00 dollars)meta_cat, Metacritic’s review system, following categories: “Overwhelming Dislike”, “Generally Unfavorable”, “Mixed Reviews”, “Generally Favorable”, “Universal Acclaim”.playtime_miss, whether median play time missing (TRUE) (FALSE)data set modified https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-07-30.See can find output following:number rows data set number columnsthe number missing values variablethe number unique values character variablethe completion rate (proportion values non-missing).particular, number missing values given nmissing complete_rate gives proportion values non-missing. give us idea missing values exist certain variables, , , many exist variable.Also, bottom output, see tiny histograms numeric variable summary statistics. Looking min, max, histograms variable can inform us whether variable outliers. example, see histograms price, median_playtime, average_playtime look extremely skewed right outlier(s) upper end., now know outliers missing values certain variables videogame data set. might affect tables graphs make?First, let’s focus metascore variable, gives Metacritic’s overall aggregated review score videogames. Note complete_rate metascore variable 0.107: almost 90% videogames metascore., suppose interested exploring “typical” metascore . can figure average metascore median metascore non-missing videogames :Ignoring missing values, say , average, videogames receive metascores around 72 points. question need ask : “reasonable assume missing games receive similar reviews non-missing games can thin 71.9 average review score games?”answer might depend understand videogames review process. argue missing games reviewed worse non-missing games. Major games usually get reviews also usually funding many minor games little funding, get reviewed, , get reviewed, may get lower rating.can certainly make different argument: don’t know argument correct without data. important thing least think make clear possible limitations conclusions data analysis.second example, consider exploration relationship median_playtime game metascore. can make scatterplot relationship, ignoring missing values, withWe see clear outliers, talk next, missing values metascore affect conclusions draw graph? answer “yes” think videogames missing metascores follow different overall trend non-missing metascores “” think , videogames missing metascores rated, follow similar trend already graph.question, make argument games follow similar trend. , assumption need make need explicit .also mentioned idea implicit missing values. videogames appear data set . words, set videogames sample videogames ever published United States? sample, selected, , convenience sample, types games left ?Outliers can also pose interesting challenges data analysis. example, consider graph median_playtime vs. metascore. focus outliers now, ignore missing values metascore.see clear outliers median_playtime: games median playtime thousands hours. , knowledge videogames can help us determine outliers.important thing dealing outliers explicit , analyst, choose keep graph summary table choose remove. choose remove values, give reasoning, , space, can also give second graph data without removing outliers.example, median playtime 3000+ hours seems bit excessive, ’s challenging determine reasonable cutoff “excessive” . reasonable game median playtime 1000 hours? aobut 2000 hours? 500 hours? Choosing points keep affect fit smoother. may learned STAT 113 STAT 213, observations high control fit smoother regression line influential.",
    "code": "\nlibrary(skimr)\nlibrary(here)\nvideogame_df <- read_csv(here(\"data/videogame_clean.csv\"))\n#> Rows: 26688 Columns: 15\n#> ── Column specification ────────────────────────────────────\n#> Delimiter: \",\"\n#> chr  (7): game, release_date, owners, meta_cat, develope...\n#> dbl  (6): price, median_playtime, metascore, price_cat, ...\n#> lgl  (1): playtime_miss\n#> date (1): release_date2\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n## skim(videogame_df)\nvideogame_df |> summarise(mean_meta = mean(metascore, na.rm = TRUE),\n                          med_meta = median(metascore, na.rm = TRUE))\n#> # A tibble: 1 × 2\n#>   mean_meta med_meta\n#>       <dbl>    <dbl>\n#> 1      71.9       73\nggplot(data = videogame_df, aes(x = metascore, y = median_playtime)) +\n  geom_point() +\n  geom_smooth()\n#> `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\nggplot(data = videogame_df, aes(x = metascore, y = median_playtime)) +\n  geom_point() +\n  geom_smooth()\n#> `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'"
  },
  {
    "path": "workflow.html",
    "id": "exercise-14-4",
    "chapter": " 6 Workflow and Other Skills",
    "heading": "6.4.3 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 6.7.STAT 113 survey data set contains responses 397 STAT 113 students survey students take beginning semester. 5 categorical variables 7 numeric variables. categorical variables, many variables 0 missing values? numeric variables, many variables 0 missing values?Choose variable missing values feel comfortable ignoring missing values table graph. Give one two sentence reason.Choose variable missing values feel comfortable ignoring missing values table graph. Give one two sentence reason.Choose variable missing values feel comfortable ignoring missing values table graph. Give one two sentence reason.Choose variable missing values feel comfortable ignoring missing values table graph. Give one two sentence reason.Find mean median median_playtime videogames metacritic videogame data set. , remove games median_playtime 1000 hours. Compute mean median median_playtime data set without games. measure, mean median affected outliers present?Find mean median median_playtime videogames metacritic videogame data set. , remove games median_playtime 1000 hours. Compute mean median median_playtime data set without games. measure, mean median affected outliers present?",
    "code": "\nlibrary(tidyverse)\nstat113_df <- read_csv(here(\"data/stat113.csv\"))\n#> Rows: 397 Columns: 12\n#> ── Column specification ────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (5): Year, Sex, Sport, Award, SocialMedia\n#> dbl (7): Hgt, Wgt, Haircut, GPA, Exercise, TV, Pulse\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "path": "workflow.html",
    "id": "reprexes-and-tibble",
    "chapter": " 6 Workflow and Other Skills",
    "heading": "6.5 Reprexes and tibble",
    "text": "reproducible example, reprex, chunk code can give someone else runs without outside data. used often StackExchange. can create data set directly within R tibble() function tibble package. useful want make small reproducible example someone else may help code.following code chunk reprex people necessarily data set parsedf.csv.Suppose want post StackExchange someone ask friend help us convert variable character vector units numeric vector without units. want able give possible helpers small example data set work isolate problem question . , can create tiny data set tibble():library(tidyverse) necessary code chunk reprex?can copy paste code chunk question: ’s code anyone can run long tidyverse package installed, really encourages people help.second example, might question find mean many numeric variables. example, stat113.csv file, many numeric variables. can compute mean numeric variable writing separate summarise() statement variable. also may interested quicker way. , since helper might stat113.csv file, can create reprex problem:Note included categorical variables reprex data set. want code work even categorical variables data set, must include reprex example general possible.reference, across() function can used answer question (though ’s point section). code reads summarise() across() variables numeric (.numeric) summary measure mean.",
    "code": "\n## Hello! How do I get rid of the units from the values in\n## my variable `x`? Thanks!\nlibrary(tidyverse)\ntest_df <- read_csv(here(\"data/parsedf.csv\"))\nhead(test_df)\n#> # A tibble: 3 × 2\n#>   x                   y\n#>   <chr>           <dbl>\n#> 1 20,000 dollars      1\n#> 2 40 dollars          2\n#> 3 only 13 dollars     3\n## Hello! How do I get rid of the units from the values in\n## my variable `xvar`? Thanks!\nlibrary(tidyverse)\ntest_df2 <- tibble(xvar = c(\"20,000 dollars\", \"40 dollars\"),\n                   yvar = c(1, 2))\ntest_df2\n#> # A tibble: 2 × 2\n#>   xvar            yvar\n#>   <chr>          <dbl>\n#> 1 20,000 dollars     1\n#> 2 40 dollars         2\n## is there a way to get a summary measure, like the mean, for \n## all numeric variables in a data set without writing a separate\n## summarise() statement for each variable?\n\nlibrary(tidyverse)\nsum_df <- tibble(xvar = c(\"A\", \"B\"), yvar = c(1, 4), zvar = c(-1, 4),\n                 svar = c(\"G\", \"g\"), tvar = c(99, 100000))\nsum_df |> summarise(across(where(is.numeric), mean))\n#> # A tibble: 1 × 3\n#>    yvar  zvar   tvar\n#>   <dbl> <dbl>  <dbl>\n#> 1   2.5   1.5 50050."
  },
  {
    "path": "workflow.html",
    "id": "exercise-14-5",
    "chapter": " 6 Workflow and Other Skills",
    "heading": "6.5.1 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 6.7.Project 2, work course evaluation data professor SLU. Overall, ’ll answer questions professor can improve courses SLU looking course evaluation data. variables data set described detail project description.* Suppose can’t figure create semester variable year variable Term evals_prof_S21.csv. (want split Term variable two variables: Semester levels F S Year levels 19, 20, 21).Put together reprex using tibble() someone able run help figure question.",
    "code": "\nlibrary(tidyverse)\nevals_df <- read_csv(here(\"data/evals_prof_S21.csv\"))\nhead(evals_df)\n#> # A tibble: 6 × 10\n#>   Term  Course Quest…¹ Agree…² Agree Agree…³ Neutral Disag…⁴\n#>   <chr> <chr>  <chr>     <dbl> <dbl>   <dbl>   <dbl>   <dbl>\n#> 1 F19   113-02 1. Cou…       9     9       1       5       0\n#> 2 F19   113-02 2. Eff…      12     8       1       2       1\n#> 3 F19   113-02 3. Env…      11     8       2       3       0\n#> 4 F19   113-02 5a. Fa…       5    13       3       1       1\n#> 5 F19   113-02 5b. Ti…       8    12       1       2       1\n#> 6 F19   113-02 5c. Co…       5     8       4       6       1\n#> # … with 2 more variables: Disagree <dbl>,\n#> #   `Disagree Strongly` <dbl>, and abbreviated variable\n#> #   names ¹​Question, ²​`Agree strongly`, ³​`Agree Somewhat`,\n#> #   ⁴​`Disagree Somewhat`\n#> # ℹ Use `colnames()` to see all variable names"
  },
  {
    "path": "workflow.html",
    "id": "chapexercise-14",
    "chapter": " 6 Workflow and Other Skills",
    "heading": "6.6 Chapter Exercises",
    "text": "chapter exercises section workflow.",
    "code": ""
  },
  {
    "path": "workflow.html",
    "id": "solutions-14",
    "chapter": " 6 Workflow and Other Skills",
    "heading": "6.7 Exercise Solutions",
    "text": "",
    "code": ""
  },
  {
    "path": "workflow.html",
    "id": "r-projects-and-file-organization-s",
    "chapter": " 6 Workflow and Other Skills",
    "heading": "6.7.1 R Projects and File Organization S",
    "text": "",
    "code": ""
  },
  {
    "path": "workflow.html",
    "id": "code-style-s",
    "chapter": " 6 Workflow and Other Skills",
    "heading": "6.7.2 Code Style S",
    "text": "",
    "code": ""
  },
  {
    "path": "workflow.html",
    "id": "debugging-code-s",
    "chapter": " 6 Workflow and Other Skills",
    "heading": "6.7.3 Debugging Code S",
    "text": "",
    "code": ""
  },
  {
    "path": "workflow.html",
    "id": "context-outliers-and-missing-values-s",
    "chapter": " 6 Workflow and Other Skills",
    "heading": "6.7.4 Context, Outliers, and Missing Values S",
    "text": "",
    "code": ""
  },
  {
    "path": "workflow.html",
    "id": "reprexes-and-tibble-s",
    "chapter": " 6 Workflow and Other Skills",
    "heading": "6.7.5 Reprexes and tibble S",
    "text": "* Suppose can’t figure create semester variable year variable Term evals_prof_S21.csv. (want split Term variable two variables: Semester levels F S Year levels 19, 20, 21).Put together reprex using tibble() someone able run help figure question.",
    "code": "\nlibrary(tidyverse)\nevals_df <- read_csv(here(\"data/evals_prof_S21.csv\"))\nhead(evals_df)\n#> # A tibble: 6 × 10\n#>   Term  Course Quest…¹ Agree…² Agree Agree…³ Neutral Disag…⁴\n#>   <chr> <chr>  <chr>     <dbl> <dbl>   <dbl>   <dbl>   <dbl>\n#> 1 F19   113-02 1. Cou…       9     9       1       5       0\n#> 2 F19   113-02 2. Eff…      12     8       1       2       1\n#> 3 F19   113-02 3. Env…      11     8       2       3       0\n#> 4 F19   113-02 5a. Fa…       5    13       3       1       1\n#> 5 F19   113-02 5b. Ti…       8    12       1       2       1\n#> 6 F19   113-02 5c. Co…       5     8       4       6       1\n#> # … with 2 more variables: Disagree <dbl>,\n#> #   `Disagree Strongly` <dbl>, and abbreviated variable\n#> #   names ¹​Question, ²​`Agree strongly`, ³​`Agree Somewhat`,\n#> #   ⁴​`Disagree Somewhat`\n#> # ℹ Use `colnames()` to see all variable names\nlibrary(tidyverse)\ndf <- tibble(Term = c(\"F19\", \"S20\"), x = c(1, 2))\n## Hello! I need help creating a variable that has F/S and \n## a separate year variable that has 19 and 20 from the data set above.\n## Thanks!"
  },
  {
    "path": "workflow.html",
    "id": "rcode-14",
    "chapter": " 6 Workflow and Other Skills",
    "heading": "6.8 Non-Exercise R Code",
    "text": "",
    "code": "\nlibrary(here)\nhere()\nlibrary(tidyverse)\nathletes_df <- read_csv(\"data/athletesdata.csv\")\nathletes_test_read <- read_csv(here(\"data/athletesdata.csv\"))\ndf1 <- mtcars |> filter(cyl == 4)\ndf2 <- mtcars |> filter(cyl == 6)\ndf3 <- mtcars |> filter(cyl == 8)\ncars_with_4_cylinders_data_set <- mtcars |> filter(cyl == 4)\ncyl4_df <- mtcars |> filter(cyl == 4)\ncyl6.df <- mtcars |> filter(cyl == 6)\nggplot(data=mtcars,aes(x=wt,y=drat))+geom_point()+geom_smooth(method=\"lm\",se=FALSE)\nggplot(data = mtcars, aes(x = wt, y = drat)) +\ngeom_point() +\ngeom_smooth(method = \"lm\", se = FALSE)\nggplot(data = mtcars, aes(x = wt, y = drat)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\nmtcars |> filter(cyl == 4) |>\n  group_by(vs) |>\n  summarise(mean_mpg = mean(mpg, na.rm = TRUE))\nmtcars |> filter(cyl == 4) |> group_by(vs) |> summarise(mean_mpg = mean(mpg, na.rm = TRUE))\ncyl4_df <- mtcars |> filter(cyl == 4)\n\nggplot(data = cyl4_df, aes(x = mpg)) +\n  geom_histogram()\nmajors_df <- read_csv(here(\"data/majors.csv\")) |>\n  pivot_longer(-1, names_to = \"year\", values_to = \"n_majors\") |>\n  mutate(year = as.numeric(year)) |>\n  rename(major = `...1`)\nhead(majors_df)\nspanish_df <- majors_df |> filter(major == \"Estudios Hispanicos (Spanish)\")\nggplot(data = spanish_df, aes(x = year, y = n_majors)) +\n  geom_line() +\n  geom_smooth(se = FALSE)\nint_econ_df <- majors_df |> filter(major == \"Int'l Economics (Combined)\")\nggplot(data = int_econ_df, aes(x = year, y = n_majors)) +\n  geom_line() +\n  geom_smooth(se = FALSE)\nlibrary(skimr)\nlibrary(here)\nvideogame_df <- read_csv(here(\"data/videogame_clean.csv\"))\n## skim(videogame_df)\nvideogame_df |> summarise(mean_meta = mean(metascore, na.rm = TRUE),\n                          med_meta = median(metascore, na.rm = TRUE))\nggplot(data = videogame_df, aes(x = metascore, y = median_playtime)) +\n  geom_point() +\n  geom_smooth()\nggplot(data = videogame_df, aes(x = metascore, y = median_playtime)) +\n  geom_point() +\n  geom_smooth()\n## Hello! How do I get rid of the units from the values in\n## my variable `x`? Thanks!\nlibrary(tidyverse)\ntest_df <- read_csv(here(\"data/parsedf.csv\"))\nhead(test_df)\n## Hello! How do I get rid of the units from the values in\n## my variable `xvar`? Thanks!\nlibrary(tidyverse)\ntest_df2 <- tibble(xvar = c(\"20,000 dollars\", \"40 dollars\"),\n                   yvar = c(1, 2))\ntest_df2"
  },
  {
    "path": "tidying-with-tidyr.html",
    "id": "tidying-with-tidyr",
    "chapter": " 7 Tidying with tidyr",
    "heading": " 7 Tidying with tidyr",
    "text": "Goals:describe means data set tidy.describe means data set tidy.use separate() unite() transform data set tidy form.use separate() unite() transform data set tidy form.use pivot_longer() pivot_wider() transform data set tidy form.use pivot_longer() pivot_wider() transform data set tidy form.combine tidyr functions dplyr ggplot2 functions form complete workflow.combine tidyr functions dplyr ggplot2 functions form complete workflow.Data: first use polling data set contains variables collected different polls July 2016 U.S. presidential election. data set scraped RealClear politics https://www.realclearpolitics.com/epolls/latest_polls/president/ Dr. Ramler. variables :Poll, name pollDate, date range poll conductedSample, contains sample size poll whether poll Likely Voters Registered VotersMoE, margin error poll (recall term IntroStat)Clinton (D), percentage people poll voting ClintonTrump (R), percentage people poll voting TrumpJohnson (L), percentage people poll voting JohnsonSteing (G), percentage people poll voting Stein",
    "code": ""
  },
  {
    "path": "tidying-with-tidyr.html",
    "id": "what-is-tidy-data",
    "chapter": " 7 Tidying with tidyr",
    "heading": "7.1 What is Tidy Data?",
    "text": "R usually (always) works best data tidy form. tidy data set characteristics. Note already quite familiar tidy data , point, data sets used class (probably data sets see STAT 113 data sets may seen STAT 213) tidy. definition tidy data taken R Data Science:every variable data set stored columnevery case data set stored roweach value variable stored one cellvalues data set contain unitsthere table headers footnotesWe begin focusing first characteristic: every variable data set stored column (correspondingly, number 3: value variable stored one cell).",
    "code": ""
  },
  {
    "path": "tidying-with-tidyr.html",
    "id": "separate-and-unite-columns",
    "chapter": " 7 Tidying with tidyr",
    "heading": "7.2 separate() and unite() Columns",
    "text": "fresh .qmd file (File -> New File -> Quarto) Notes project, copy paste following code R chunk:Suppose wanted know average sample size polls . Using dplyr functions,warning get? ?get similar warning (sometimes error) time want try use Sample size plotting summaries. issue Sample column actually contains two variables data set tidy.",
    "code": "\nlibrary(tidyverse)\nlibrary(here)\npolls <- read_csv(here(\"data/rcp-polls.csv\"), na = \"--\")\npolls\n#> # A tibble: 7 × 8\n#>   Poll    Date  Sample   MoE Clint…¹ Trump…² Johns…³ Stein…⁴\n#>   <chr>   <chr> <chr>  <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n#> 1 Monmou… 7/14… 688 LV   3.7      45      43       5       1\n#> 2 CNN/ORC 7/13… 872 RV   3.5      42      37      13       5\n#> 3 ABC Ne… 7/11… 816 RV   4        42      38       8       5\n#> 4 NBC Ne… 7/9 … 1000 …   3.1      41      35      11       6\n#> 5 Econom… 7/9 … 932 RV   4.5      40      37       5       2\n#> 6 Associ… 7/7 … 837 RV  NA        40      36       6       2\n#> 7 McClat… 7/5 … 1053 …   3        40      35      10       5\n#> # … with abbreviated variable names ¹​`Clinton (D)`,\n#> #   ²​`Trump (R)`, ³​`Johnson (L)`, ⁴​`Stein (G)`\npolls |> summarise(meansample = mean(Sample))"
  },
  {
    "path": "tidying-with-tidyr.html",
    "id": "separate-a-column",
    "chapter": " 7 Tidying with tidyr",
    "heading": "7.2.1 separate() a Column",
    "text": "Let’s separate() two variables Sample_size Sample_type:arguments separate() fairly easy learn:col name column data set want separate.col name column data set want separate.name new columns. anything want, entered vector (c() separate names)name new columns. anything want, entered vector (c() separate names)sep character want separate column . case, sample size sample type separated whitespace, sep = \" \", white space.sep character want separate column . case, sample size sample type separated whitespace, sep = \" \", white space.sep argument newest piece information . Note even using sep = \"\" produce error (space now, R doesn’t know separate ).Similarly, like Date column separated poll start date poll end date:use \" - \" separator instead \"-\"? Try using \"-\" aren’t sure: shouldn’t get error something look .happened Sample? back un-separated form?",
    "code": "\npolls |>\n  separate(col = Sample, into = c(\"Sample_size\", \"Sample_type\"), \n           sep = \" \")\n#> # A tibble: 7 × 9\n#>   Poll   Date  Sampl…¹ Sampl…²   MoE Clint…³ Trump…⁴ Johns…⁵\n#>   <chr>  <chr> <chr>   <chr>   <dbl>   <dbl>   <dbl>   <dbl>\n#> 1 Monmo… 7/14… 688     LV        3.7      45      43       5\n#> 2 CNN/O… 7/13… 872     RV        3.5      42      37      13\n#> 3 ABC N… 7/11… 816     RV        4        42      38       8\n#> 4 NBC N… 7/9 … 1000    RV        3.1      41      35      11\n#> 5 Econo… 7/9 … 932     RV        4.5      40      37       5\n#> 6 Assoc… 7/7 … 837     RV       NA        40      36       6\n#> 7 McCla… 7/5 … 1053    RV        3        40      35      10\n#> # … with 1 more variable: `Stein (G)` <dbl>, and\n#> #   abbreviated variable names ¹​Sample_size, ²​Sample_type,\n#> #   ³​`Clinton (D)`, ⁴​`Trump (R)`, ⁵​`Johnson (L)`\n#> # ℹ Use `colnames()` to see all variable names\npolls |>\n  separate(col = Sample, into = c(\"Sample_size\", \"Sample_type\"), \n           sep = \"\")\npolls_sep <- polls |>\n  separate(col = Date, into = c(\"Start\", \"End\"),\n           sep = \" - \")"
  },
  {
    "path": "tidying-with-tidyr.html",
    "id": "unite-columns",
    "chapter": " 7 Tidying with tidyr",
    "heading": "7.2.2 unite() Columns",
    "text": "unite() “opposite” separate(): use one variable stored across multiple columns, row still represents single case. need use unite() less common separate(). current data set, need use . , sake seeing example, let’s separate Start date month day use unite() re-unite columns:situation occur practice: date variable multiple columns: one month one day (multiple years, third year). use unite() combine two columns single Date, called New_start_date:Note unite() just switches around first two arguments separate(). Argument 1 now name new column Argument 2 names columns data set want combine.also used c() function separate() unite(). c() general R function isn’t specific tidy data, first time ’re seeing course. c() officially stands concatenate, , simpler terms, c() combines two “things”, separated comma.useful function argument expects two “things”: example, separate(), argument requires two column names example. column names must specified combining names together c().",
    "code": "\npolls_sillytest <- polls_sep |>\n  separate(col = Start, into = c(\"Start_month\", \"Start_day\"), \n           sep = \"/\")\npolls_sillytest\n#> # A tibble: 7 × 10\n#>   Poll    Start…¹ Start…² End   Sample   MoE Clint…³ Trump…⁴\n#>   <chr>   <chr>   <chr>   <chr> <chr>  <dbl>   <dbl>   <dbl>\n#> 1 Monmou… 7       14      7/16  688 LV   3.7      45      43\n#> 2 CNN/ORC 7       13      7/16  872 RV   3.5      42      37\n#> 3 ABC Ne… 7       11      7/14  816 RV   4        42      38\n#> 4 NBC Ne… 7       9       7/13  1000 …   3.1      41      35\n#> 5 Econom… 7       9       7/11  932 RV   4.5      40      37\n#> 6 Associ… 7       7       7/11  837 RV  NA        40      36\n#> 7 McClat… 7       5       7/9   1053 …   3        40      35\n#> # … with 2 more variables: `Johnson (L)` <dbl>,\n#> #   `Stein (G)` <dbl>, and abbreviated variable names\n#> #   ¹​Start_month, ²​Start_day, ³​`Clinton (D)`, ⁴​`Trump (R)`\n#> # ℹ Use `colnames()` to see all variable names\npolls_sillytest |>\n  unite(\"New_start_date\", c(Start_month, Start_day),\n        sep = \"/\")\n#> # A tibble: 7 × 9\n#>   Poll    New_s…¹ End   Sample   MoE Clint…² Trump…³ Johns…⁴\n#>   <chr>   <chr>   <chr> <chr>  <dbl>   <dbl>   <dbl>   <dbl>\n#> 1 Monmou… 7/14    7/16  688 LV   3.7      45      43       5\n#> 2 CNN/ORC 7/13    7/16  872 RV   3.5      42      37      13\n#> 3 ABC Ne… 7/11    7/14  816 RV   4        42      38       8\n#> 4 NBC Ne… 7/9     7/13  1000 …   3.1      41      35      11\n#> 5 Econom… 7/9     7/11  932 RV   4.5      40      37       5\n#> 6 Associ… 7/7     7/11  837 RV  NA        40      36       6\n#> 7 McClat… 7/5     7/9   1053 …   3        40      35      10\n#> # … with 1 more variable: `Stein (G)` <dbl>, and\n#> #   abbreviated variable names ¹​New_start_date,\n#> #   ²​`Clinton (D)`, ³​`Trump (R)`, ⁴​`Johnson (L)`\n#> # ℹ Use `colnames()` to see all variable names\nc(1, 4, 2)\n#> [1] 1 4 2\nc(\"A\", \"A\", \"D\")\n#> [1] \"A\" \"A\" \"D\""
  },
  {
    "path": "tidying-with-tidyr.html",
    "id": "column-names-and-rename",
    "chapter": " 7 Tidying with tidyr",
    "heading": "7.2.3 Column Names and rename()",
    "text": "might noticed columns percentage votes Clinton, Trump, etc. surrounded backticks ` ` print polls polls_sep:happens column names space (also occur columns started number odd special characters ). , time want reference variable, need include backticks:variable names spaces doesn’t technically violate principle tidy data, can quite annoying. Always using backticks can huge pain. can rename variables easily rename(), just takes series new_name = old_name arguments.rename() can also useful variable names long type . rename() actually dplyr, tidyr, didn’t need dplyr data sets.",
    "code": "\npolls_sep\n#> # A tibble: 7 × 9\n#>   Poll      Start End   Sample   MoE Clint…¹ Trump…² Johns…³\n#>   <chr>     <chr> <chr> <chr>  <dbl>   <dbl>   <dbl>   <dbl>\n#> 1 Monmouth  7/14  7/16  688 LV   3.7      45      43       5\n#> 2 CNN/ORC   7/13  7/16  872 RV   3.5      42      37      13\n#> 3 ABC News… 7/11  7/14  816 RV   4        42      38       8\n#> 4 NBC News… 7/9   7/13  1000 …   3.1      41      35      11\n#> 5 Economis… 7/9   7/11  932 RV   4.5      40      37       5\n#> 6 Associat… 7/7   7/11  837 RV  NA        40      36       6\n#> 7 McClatch… 7/5   7/9   1053 …   3        40      35      10\n#> # … with 1 more variable: `Stein (G)` <dbl>, and\n#> #   abbreviated variable names ¹​`Clinton (D)`,\n#> #   ²​`Trump (R)`, ³​`Johnson (L)`\n#> # ℹ Use `colnames()` to see all variable names\npolls_sep |>\n  summarise(meanclinton = mean(Clinton (D))) ## throws an error\npolls_sep |>\n  summarise(meanclinton = mean(`Clinton (D)`)) ## backticks save the day!\npolls_new <- polls_sep |>\n  rename(Clinton_D = `Clinton (D)`, Trump_R = `Trump (R)`,\n         Johnson_L = `Johnson (L)`, Stein_G = `Stein (G)`)\npolls_new\n#> # A tibble: 7 × 9\n#>   Poll      Start End   Sample   MoE Clint…¹ Trump_R Johns…²\n#>   <chr>     <chr> <chr> <chr>  <dbl>   <dbl>   <dbl>   <dbl>\n#> 1 Monmouth  7/14  7/16  688 LV   3.7      45      43       5\n#> 2 CNN/ORC   7/13  7/16  872 RV   3.5      42      37      13\n#> 3 ABC News… 7/11  7/14  816 RV   4        42      38       8\n#> 4 NBC News… 7/9   7/13  1000 …   3.1      41      35      11\n#> 5 Economis… 7/9   7/11  932 RV   4.5      40      37       5\n#> 6 Associat… 7/7   7/11  837 RV  NA        40      36       6\n#> 7 McClatch… 7/5   7/9   1053 …   3        40      35      10\n#> # … with 1 more variable: Stein_G <dbl>, and abbreviated\n#> #   variable names ¹​Clinton_D, ²​Johnson_L\n#> # ℹ Use `colnames()` to see all variable names"
  },
  {
    "path": "tidying-with-tidyr.html",
    "id": "exercise-4-1",
    "chapter": " 7 Tidying with tidyr",
    "heading": "7.2.4 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 7.5.MLB salary data set contains salaries 862 players Major League Baseball 2016. data set obtained http://www.usatoday.com/sports/mlb/salaries/2016/player//Read data using following code chunk write sentence two explains data set tidy.* Tidy data set just thatDuration salary contract (currently given Year column) columnthe year range (also currently given Year column) split variable called Start variable called End year give start end years contract. can still special characters now (like ( )) start end year.received warning message. message mean? See can figure typing View(baseball_df) console window scrolling rows warning mentions: 48, 59, 60, etc.received warning message. message mean? See can figure typing View(baseball_df) console window scrolling rows warning mentions: 48, 59, 60, etc.won’t learn parse_number() readr, function straightforward enough mention . ’s useful extra characters values numeric variable (like $ (), just want grab actual number:won’t learn parse_number() readr, function straightforward enough mention . ’s useful extra characters values numeric variable (like $ (), just want grab actual number:Run code parsing saved baseball_df.* Using function dplyr. fix End variable created , example, first observation 2020 instead just 20.* Using function dplyr. fix End variable created , example, first observation 2020 instead just 20.* tidyr extremely useful, ’s glamorous. end data set ggplot2 dplyr can use cool things. , let’s something tidy data set make tidying little worth moving . Make graphic investigates player Salary compares different POS.* tidyr extremely useful, ’s glamorous. end data set ggplot2 dplyr can use cool things. , let’s something tidy data set make tidying little worth moving . Make graphic investigates player Salary compares different POS.* State reason making plot worked tidied data set.* State reason making plot worked tidied data set.",
    "code": "\nlibrary(tidyverse)\nlibrary(here)\nbaseball_df <- read_csv(here(\"data/mlb2016.csv\"))\nhead(baseball_df)\n#> # A tibble: 6 × 7\n#>   Name             Team  POS   Salary  Years Total…¹ Avg.A…²\n#>   <chr>            <chr> <chr> <chr>   <chr> <chr>   <chr>  \n#> 1 Clayton Kershaw  LAD   SP    $ 33,0… 7 (2… $ 215,… $ 30,7…\n#> 2 Zack Greinke     ARI   SP    $ 31,7… 6 (2… $ 206,… $ 34,4…\n#> 3 David Price      BOS   SP    $ 30,0… 7 (2… $ 217,… $ 31,0…\n#> 4 Miguel Cabrera   DET   1B    $ 28,0… 10 (… $ 292,… $ 29,2…\n#> 5 Justin Verlander DET   SP    $ 28,0… 7 (2… $ 180,… $ 25,7…\n#> 6 Yoenis Cespedes  NYM   CF    $ 27,3… 3 (2… $ 75,0… $ 25,0…\n#> # … with abbreviated variable names ¹​Total.Value,\n#> #   ²​Avg.Annual\nbaseball_df <- baseball_df |>\n  mutate(Salary = parse_number(Salary),\n         Total.Value = parse_number(Total.Value),\n         Avg.Annual = parse_number(Avg.Annual),\n         Start = parse_number(Start),\n         End = parse_number(End))"
  },
  {
    "path": "tidying-with-tidyr.html",
    "id": "reshaping-with-pivot_",
    "chapter": " 7 Tidying with tidyr",
    "heading": "7.3 Reshaping with pivot_()",
    "text": "continue use polling data set introduce pivoting functions data reshaping. make sure working data set, run following line code:data set polls_clean still isn’t tidy!! candidate variable spread 4 different columns values 4 columns actually represent 1 variable: poll percentage.Thinking data “tidyness” using definitions can sometimes little bit confusing. practice, oftentimes usually realize data set untidy go something super simple something turns super simple data current form.example, one thing might want make plot poll Start time x-axis, polling numbers y-axis, candidates represented different colours. small data set, might see trends time, imagine graph quite useful polling numbers June, July, August, September, etc.Take moment think make graph ggplot2: x-axis variable? variable specifying y-axis? colours?first attempt making graph :’re stuck. ’s certainly impossible make graph data current form (keep adding geom_point() re-specifying aesthetics, manually specify colours, manually specify legend), ’s definitely huge pain.pivot_longer() can help! https://www.youtube.com/watch?v=8w3wmQAMoxQ",
    "code": "\npolls_clean <- polls |>\n  separate(col = Sample, into = c(\"Sample_size\", \"Sample_type\"), \n           sep = \" \")  |>\n  separate(col = Date, into = c(\"Start\", \"End\"),\n           sep = \" - \") |> \n  rename(Clinton_D = `Clinton (D)`, Trump_R = `Trump (R)`,\n         Johnson_L = `Johnson (L)`, Stein_G = `Stein (G)`)\npolls_clean\n#> # A tibble: 7 × 10\n#>   Poll     Start End   Sampl…¹ Sampl…²   MoE Clint…³ Trump_R\n#>   <chr>    <chr> <chr> <chr>   <chr>   <dbl>   <dbl>   <dbl>\n#> 1 Monmouth 7/14  7/16  688     LV        3.7      45      43\n#> 2 CNN/ORC  7/13  7/16  872     RV        3.5      42      37\n#> 3 ABC New… 7/11  7/14  816     RV        4        42      38\n#> 4 NBC New… 7/9   7/13  1000    RV        3.1      41      35\n#> 5 Economi… 7/9   7/11  932     RV        4.5      40      37\n#> 6 Associa… 7/7   7/11  837     RV       NA        40      36\n#> 7 McClatc… 7/5   7/9   1053    RV        3        40      35\n#> # … with 2 more variables: Johnson_L <dbl>, Stein_G <dbl>,\n#> #   and abbreviated variable names ¹​Sample_size,\n#> #   ²​Sample_type, ³​Clinton_D\n#> # ℹ Use `colnames()` to see all variable namesggplot(data = polls_clean, aes(x = Start, y = Clinton_D)) + \n  geom_point(aes(colour = ....??????????))"
  },
  {
    "path": "tidying-with-tidyr.html",
    "id": "pivot_longer-to-gather-columns",
    "chapter": " 7 Tidying with tidyr",
    "heading": "7.3.1 pivot_longer() to Gather Columns",
    "text": "pivot_longer() “pivots” data set rows (hence “longer”) collapsing multiple columns two columns. One new column “key” column, new variable containing old data set’s column names. second new column “value” column, new variable containing old data set’s values old data set’s column names. ’s easier see example. know plotting exercise ’d really like candidate variable colour poll_percent variable y-axis plot. , can use pivot_longer() make two columns:pivot_longer() three important arguments:cols, names columns want PIVOT!names_to, name new variable old column names (anything want !)values_to, name new variable old column values (anything want !)happens omit names_to values_to arguments? Give try!Now can make plot using Week 1 ggplot functions. don’t forget give name new “long” data set first!",
    "code": "\npolls_clean |>\n  pivot_longer(cols = c(Clinton_D, Trump_R, Johnson_L, Stein_G),\n               names_to = \"candidate\", values_to = \"poll_percent\")\n#> # A tibble: 28 × 8\n#>    Poll    Start End   Sampl…¹ Sampl…²   MoE candi…³ poll_…⁴\n#>    <chr>   <chr> <chr> <chr>   <chr>   <dbl> <chr>     <dbl>\n#>  1 Monmou… 7/14  7/16  688     LV        3.7 Clinto…      45\n#>  2 Monmou… 7/14  7/16  688     LV        3.7 Trump_R      43\n#>  3 Monmou… 7/14  7/16  688     LV        3.7 Johnso…       5\n#>  4 Monmou… 7/14  7/16  688     LV        3.7 Stein_G       1\n#>  5 CNN/ORC 7/13  7/16  872     RV        3.5 Clinto…      42\n#>  6 CNN/ORC 7/13  7/16  872     RV        3.5 Trump_R      37\n#>  7 CNN/ORC 7/13  7/16  872     RV        3.5 Johnso…      13\n#>  8 CNN/ORC 7/13  7/16  872     RV        3.5 Stein_G       5\n#>  9 ABC Ne… 7/11  7/14  816     RV        4   Clinto…      42\n#> 10 ABC Ne… 7/11  7/14  816     RV        4   Trump_R      38\n#> # … with 18 more rows, and abbreviated variable names\n#> #   ¹​Sample_size, ²​Sample_type, ³​candidate, ⁴​poll_percent\n#> # ℹ Use `print(n = ...)` to see more rows\npolls_long <- polls_clean |>\n  pivot_longer(cols = c(Clinton_D, Trump_R, Johnson_L, Stein_G),\n               names_to = \"candidate\", values_to = \"poll_percent\")\n\n## ignore as.Date for now....we will get to dates later!\nggplot(data = polls_long,\n       aes(x = as.Date(Start, \"%m/%d\"), y = poll_percent,\n           colour = candidate)) +\n  geom_point() + xlab(\"Poll Start Date\")"
  },
  {
    "path": "tidying-with-tidyr.html",
    "id": "pivot_wider-to-spread-to-multiple-columns",
    "chapter": " 7 Tidying with tidyr",
    "heading": "7.3.2 pivot_wider() to Spread to Multiple Columns",
    "text": "“opposite” pivot_longer() pivot_wider(). need use pivot_wider() one case actually spread across multiple rows. , typically realize issue untidy data go something simple ’s .Let’s examine airline safety data fivethirtyeight used Travelers Avoid Flying Airlines Crashes Past? story: https://fivethirtyeight.com/features/-travelers-avoid-flying-airlines----crashes---past/\n. raw data can found .data set contains following columns:airline, name airlineavail_seat_km_per_week, available seat kilometers flown weekincidents 1985_1999, number incidents 1985 1999fatal_accidents 1985_1999, number fatal accidents 1985 1999fatalities 1985_1999, number fatalities 1985 1999incidents 2000_2014fatal_accidents 2000_2014fatalities 2000_2014There’s whole lot mess data set: really want variable year two values (1985-1999 2000-2014). Sometimes ’s tough know even start, one strategy draw sketch data frame ’d like end . example, think want data set following columns: airline, available seat km, years, incidents, fatal accidents, fatalities. , sketch might look something like:etc.Let’s start pivot_longer() see can get year variable (know year variable, want, make rows pivot_longer() seems like good place start):Instead giving pivot_longer() names variables, gave column numbers instead. c(3, 4, 5, 6, 7, 8) corresponds 3rd, 4th, …., 8th columns data set. didn’t quite give us year variable, excited see opportunity take advantage separate():format want data set ? Depending task, . , also might want accident types variable. , might want collapse data set variable incidents, variable fatal_accidents, variable fatalities. , want add columns data set, need use pivot_wider().pivot_wider() two main arguments:names_from, column old data set provide names new columns andnames_from, column old data set provide names new columns andvalues_from, column old data set provide values fill new columnsvalues_from, column old data set provide values fill new columnsWe see examples pivot_wider() pivot_longer() Exercises. Note tidy data isn’t necessarily always better: might find cases need “untidy” data using pivot_longer() pivot_wider(). However, functions R (languages) work best tidy data.topics discuss tidying data. yet discussed 4th 5th characteristics tidy data (cells contain units headers footers), usually dealt read data. Therefore, issues covered discuss readr.",
    "code": "\nlibrary(here)\nairlines <- read_csv(here(\"data/airline-safety.csv\"))\nhead(airlines)\n#> # A tibble: 6 × 8\n#>   airline    avail…¹ incid…² fatal…³ fatal…⁴ incid…⁵ fatal…⁶\n#>   <chr>        <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n#> 1 Aer Lingus  3.21e8       2       0       0       0       0\n#> 2 Aeroflot*   1.20e9      76      14     128       6       1\n#> 3 Aerolinea…  3.86e8       6       0       0       1       0\n#> 4 Aeromexic…  5.97e8       3       1      64       5       0\n#> 5 Air Canada  1.87e9       2       0       0       2       0\n#> 6 Air France  3.00e9      14       4      79       6       2\n#> # … with 1 more variable: `fatalities 2000_2014` <dbl>, and\n#> #   abbreviated variable names ¹​avail_seat_km_per_week,\n#> #   ²​`incidents 1985_1999`, ³​`fatal_accidents 1985_1999`,\n#> #   ⁴​`fatalities 1985_1999`, ⁵​`incidents 2000_2014`,\n#> #   ⁶​`fatal_accidents 2000_2014`\n#> # ℹ Use `colnames()` to see all variable names\nairlines |>\n  pivot_longer(c(3, 4, 5, 6, 7, 8), names_to = \"type_year\",\n  values_to = \"total_num\") \n#> # A tibble: 336 × 4\n#>    airline    avail_seat_km_per_week type_year       total…¹\n#>    <chr>                       <dbl> <chr>             <dbl>\n#>  1 Aer Lingus              320906734 incidents 1985…       2\n#>  2 Aer Lingus              320906734 fatal_accident…       0\n#>  3 Aer Lingus              320906734 fatalities 198…       0\n#>  4 Aer Lingus              320906734 incidents 2000…       0\n#>  5 Aer Lingus              320906734 fatal_accident…       0\n#>  6 Aer Lingus              320906734 fatalities 200…       0\n#>  7 Aeroflot*              1197672318 incidents 1985…      76\n#>  8 Aeroflot*              1197672318 fatal_accident…      14\n#>  9 Aeroflot*              1197672318 fatalities 198…     128\n#> 10 Aeroflot*              1197672318 incidents 2000…       6\n#> # … with 326 more rows, and abbreviated variable name\n#> #   ¹​total_num\n#> # ℹ Use `print(n = ...)` to see more rows\nairlines |> pivot_longer(c(3, 4, 5, 6, 7, 8), names_to = \"type_year\",\n                          values_to = \"total_num\") |>\n  separate(type_year, into = c(\"type\", \"year\"), sep = \" \")\n#> # A tibble: 336 × 5\n#>    airline    avail_seat_km_per_week type      year  total…¹\n#>    <chr>                       <dbl> <chr>     <chr>   <dbl>\n#>  1 Aer Lingus              320906734 incidents 1985…       2\n#>  2 Aer Lingus              320906734 fatal_ac… 1985…       0\n#>  3 Aer Lingus              320906734 fataliti… 1985…       0\n#>  4 Aer Lingus              320906734 incidents 2000…       0\n#>  5 Aer Lingus              320906734 fatal_ac… 2000…       0\n#>  6 Aer Lingus              320906734 fataliti… 2000…       0\n#>  7 Aeroflot*              1197672318 incidents 1985…      76\n#>  8 Aeroflot*              1197672318 fatal_ac… 1985…      14\n#>  9 Aeroflot*              1197672318 fataliti… 1985…     128\n#> 10 Aeroflot*              1197672318 incidents 2000…       6\n#> # … with 326 more rows, and abbreviated variable name\n#> #   ¹​total_num\n#> # ℹ Use `print(n = ...)` to see more rows\n## name the long data set\nairlines_long <- airlines |>\n  pivot_longer(c(3, 4, 5, 6, 7, 8), names_to = \"type_year\",\n               values_to = \"total_num\") |>\n  separate(type_year, into = c(\"type\", \"year\"), sep = \" \")\n\n## use pivot_wider() to create variables for incidents, fatalities, and\n## fatal_accidents:\nairlines_long |> pivot_wider(names_from = type,\n                              values_from = total_num)\n#> # A tibble: 112 × 6\n#>    airline             avail…¹ year  incid…² fatal…³ fatal…⁴\n#>    <chr>                 <dbl> <chr>   <dbl>   <dbl>   <dbl>\n#>  1 Aer Lingus           3.21e8 1985…       2       0       0\n#>  2 Aer Lingus           3.21e8 2000…       0       0       0\n#>  3 Aeroflot*            1.20e9 1985…      76      14     128\n#>  4 Aeroflot*            1.20e9 2000…       6       1      88\n#>  5 Aerolineas Argenti…  3.86e8 1985…       6       0       0\n#>  6 Aerolineas Argenti…  3.86e8 2000…       1       0       0\n#>  7 Aeromexico*          5.97e8 1985…       3       1      64\n#>  8 Aeromexico*          5.97e8 2000…       5       0       0\n#>  9 Air Canada           1.87e9 1985…       2       0       0\n#> 10 Air Canada           1.87e9 2000…       2       0       0\n#> # … with 102 more rows, and abbreviated variable names\n#> #   ¹​avail_seat_km_per_week, ²​incidents, ³​fatal_accidents,\n#> #   ⁴​fatalities\n#> # ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "path": "tidying-with-tidyr.html",
    "id": "exercise-4-2",
    "chapter": " 7 Tidying with tidyr",
    "heading": "7.3.3 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 7.5.handle data science terminology, ’s difficult transfer ’ve learned different language. example, students computer science might familiar Python. Google something like “pivot wide long python” find help achieving equivalent pivot_longer() Python.UBSprices2 data set contains information prices common commodities cities throughout world years 2003 2009. three commodities data set Rice (1 kg worth), Bread (1 kg worth), Big Mac https://media1.giphy.com/media/Fw5LicDKem6nC/source.gif* Convert data set tidier form year variable commodity variable 3 values: \"bigmac\", \"bread\", \"rice\"Hint: point, need separate commodity year , example, bread2009. , ’ll notice different uses separate() “-” ” ” “/” use separating character. Look help separate() scroll sep argument see can figure issue. first code chunk shows solution particular issue case get stuck part second code chunk shows entire solution.* Convert data set previous exercise commodity split 3 variables: bigmac price, rice price bread price.* Convert data set previous exercise commodity split 3 variables: bigmac price, rice price bread price.data set easiest make line plot year x-axis price rice y-axis lines city? data set easiest make line chart 3 lines, one type commodity, city Amsterdam?data set easiest make line plot year x-axis price rice y-axis lines city? data set easiest make line chart 3 lines, one type commodity, city Amsterdam?time, make plots!New Data:under5mortality.csv file contains data mortality people age 5 countries around world (mortality deaths per 1000 people). data come https://www.gapminder.org/data/. data set extremely wide current form, column year data set. Read data set * Notice 217 columns (top print header, 217 second number). use tidyr, aren’t going want type c(2, 3, 4, 5, .....) way 217! R short-hand notation can use :. example, type 4:9 console window. Use notation tidy mortality_df data set.Note: ’ll need add something pivot_longer() function convert variable Year numeric. haven’t talked much variable types yet , values_to = \"Mortality\" statement, add , names_transform = list(Year = .numeric), making sure second ) close pivot_longer() function.Make line plot look overall 5 mortality trends country.Make line plot look overall 5 mortality trends country.overall trend 5 mortality? every single country follow trend? looks strange plot, specifically data collected 1900?overall trend 5 mortality? every single country follow trend? looks strange plot, specifically data collected 1900?Write two short paragraphs article found https://www.r-bloggers.com/. important thing exercise pick article interests . many choose , multiple posts put day. purposes assignment though, find article author actually provides code (“big-picture” views certain topics).Write two short paragraphs article found https://www.r-bloggers.com/. important thing exercise pick article interests . many choose , multiple posts put day. purposes assignment though, find article author actually provides code (“big-picture” views certain topics).first paragraph, answer following: () main purpose blog post? (b) data /author(s) using? (c) main findings? (d) important author data main findings?second paragraph, discuss () code see explicitly seen class, (b) code see explicitly seen class, (c) anything else find interesting article. , copy paste URL.",
    "code": "\nprices_df <- read_csv(here(\"data/UBSprices2.csv\"))\nseparate(name_of_variable, into = c(\"newname1\", \"newname2\"), sep = -4)\nmortality_df <- read_csv(here(\"data/under5mortality.csv\"))\nhead(mortality_df)\n#> # A tibble: 6 × 217\n#>   Under f…¹ `1800` `1801` `1802` `1803` `1804` `1805` `1806`\n#>   <chr>      <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n#> 1 Abkhazia     NA     NA     NA     NA     NA     NA     NA \n#> 2 Afghanis…   469.   469.   469.   469.   469.   469.   470.\n#> 3 Akrotiri…    NA     NA     NA     NA     NA     NA     NA \n#> 4 Albania     375.   375.   375.   375.   375.   375.   375.\n#> 5 Algeria     460.   460.   460.   460.   460.   460.   460.\n#> 6 American…    NA     NA     NA     NA     NA     NA     NA \n#> # … with 209 more variables: `1807` <dbl>, `1808` <dbl>,\n#> #   `1809` <dbl>, `1810` <dbl>, `1811` <dbl>, `1812` <dbl>,\n#> #   `1813` <dbl>, `1814` <dbl>, `1815` <dbl>, `1816` <dbl>,\n#> #   `1817` <dbl>, `1818` <dbl>, `1819` <dbl>, `1820` <dbl>,\n#> #   `1821` <dbl>, `1822` <dbl>, `1823` <dbl>, `1824` <dbl>,\n#> #   `1825` <dbl>, `1826` <dbl>, `1827` <dbl>, `1828` <dbl>,\n#> #   `1829` <dbl>, `1830` <dbl>, `1831` <dbl>, …\n#> # ℹ Use `colnames()` to see all variable names"
  },
  {
    "path": "tidying-with-tidyr.html",
    "id": "chapexercise-4",
    "chapter": " 7 Tidying with tidyr",
    "heading": "7.4 Chapter Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 7.5.use nfl salary data obtained FiveThirtyEight originally obtained Spotrac.com.data set top 100 paid players year position 2011 2018, broken player position. unfamiliar American football, positions data set Quarterback, Running Back, Wide Receiver, Tight End, Offensive Lineman offense, Cornerback, Defensive Lineman, Linebacker, Safety Defense, separate category Special Teams players includes punters kickers. can review summary player positions  .interested salaries compare top 100 players position salaries changed time position.Read data set withUse head() functions look data, explain data set tidy form.Use head() functions look data, explain data set tidy form.* Use function tidyr make data tidy, give tidy data set new name.* Use function tidyr make data tidy, give tidy data set new name.* data set previous exercise, add ranking variable ranks salaries within player position highest paid players position receive 1, second highest paid players receive 2, etc. Compare results default way R uses break ties two salaries using ties.method = \"first\".* data set previous exercise, add ranking variable ranks salaries within player position highest paid players position receive 1, second highest paid players receive 2, etc. Compare results default way R uses break ties two salaries using ties.method = \"first\".Hint: See Exercise 4 4.3.2 another example .* Find maximum salary player position year. , create two different line graphs shows maximum salary changed 2011 2018 position. one line graph, make colours lines different position. second line graph, facet position. graph like better?* Find maximum salary player position year. , create two different line graphs shows maximum salary changed 2011 2018 position. one line graph, make colours lines different position. second line graph, facet position. graph like better?* maximum salary dependent one specific player. Make graph, plot average salary top 20 players position year. notice? interesting patterns positions? ’re fan football, provide guess one positions salary plateau recent years.* maximum salary dependent one specific player. Make graph, plot average salary top 20 players position year. notice? interesting patterns positions? ’re fan football, provide guess one positions salary plateau recent years.* Sometimes graphs involving cost salary, want take account inflation rate. Google inflation rate 2011 2018 (Google something like “inflation rate 2011 2018” able find something). Adjust 2011 salaries inflation comparable 2018 salaries. , make similar line plot ignore years 2012 2017 (line plot just 2 points per position).* Sometimes graphs involving cost salary, want take account inflation rate. Google inflation rate 2011 2018 (Google something like “inflation rate 2011 2018” able find something). Adjust 2011 salaries inflation comparable 2018 salaries. , make similar line plot ignore years 2012 2017 (line plot just 2 points per position).adjusting inflation, many positions average higher salaries top 20 players position?Construct graph shows much salary decreases moving higher ranked players lower ranked players position year 2018. think depreciation large Quarterbacks?",
    "code": "\nnfl_df <- read_csv(here(\"data/nfl_salary.csv\"))"
  },
  {
    "path": "tidying-with-tidyr.html",
    "id": "solutions-4",
    "chapter": " 7 Tidying with tidyr",
    "heading": "7.5 Exercise Solutions",
    "text": "",
    "code": ""
  },
  {
    "path": "tidying-with-tidyr.html",
    "id": "what-is-tidy-data-s",
    "chapter": " 7 Tidying with tidyr",
    "heading": "7.5.1 What is Tidy Data? S",
    "text": "",
    "code": ""
  },
  {
    "path": "tidying-with-tidyr.html",
    "id": "separate-and-unite-s",
    "chapter": " 7 Tidying with tidyr",
    "heading": "7.5.2 separate() and unite() S",
    "text": "* Tidy data set just thatDuration salary contract (currently given Year column) columnthe year range (also currently given Year column) split variable called Start variable called End year give start end years contract. can still special characters now (like ( )) start end year.* Using function dplyr. fix End variable created , example, first observation 2020 instead just 20.somewhat lazy way get trouble (one years 1990s?) , ’s safe particular data set.* tidyr extremely useful, ’s glamorous. end data set ggplot2 dplyr can use cool things. , let’s something tidy data set make tidying little worth moving . Make graphic investigates player Salary compares different POS.* State reason making plot worked tidied data set.Salary variable dollar sign ggplot known plot .",
    "code": "\nbaseball_df <- baseball_df |>\n  separate(Years, into = c(\"Duration\", \"Range\"), sep = \" \") |>\n  separate(Range, into = c(\"Start\", \"End\"), sep = \"-\")\nbaseball_df <- baseball_df |> mutate(End = End + 2000)\nggplot(data = baseball_df, aes(x = POS, y = Salary)) +\n  geom_boxplot()\nggplot(data = baseball_df, aes(x = Salary, colour = POS)) + \n  geom_freqpoly() ## boxplots look better in this case"
  },
  {
    "path": "tidying-with-tidyr.html",
    "id": "pivot_-s",
    "chapter": " 7 Tidying with tidyr",
    "heading": "7.5.3 pivot_() S",
    "text": "* Convert data set tidier form year variable commodity variable 3 values: \"bigmac\", \"bread\", \"rice\"Hint: point, need separate commodity year , example, bread2009. , ’ll notice different uses separate() “-” ” ” “/” use separating character. Look help separate() scroll sep argument see can figure issue. first code chunk shows solution particular issue case get stuck part second code chunk shows entire solution.* Convert data set previous exercise commodity split 3 variables: bigmac price, rice price bread price.* Notice 217 columns (top print header, 217 second number). use tidyr, aren’t going want type c(2, 3, 4, 5, .....) way 217! R short-hand notation can use :. example, type 4:9 console window. Use notation tidy mortality_df data set.’ll need add something pivot_longer() function convert variable Year numeric. haven’t talked much variable types yet , values_to = \"Mortality\" statement, add , names_transform = list(Year = .numeric), making sure second ) close pivot_longer() function.",
    "code": "\nseparate(name_of_variable, into = c(\"newname1\", \"newname2\"), sep = -4)\nprices_long <- prices_df |> pivot_longer(cols = c(2, 3, 4, 5, 6, 7),\n  names_to = \"commod_year\", values_to = \"price\") |>\n  separate(col = \"commod_year\", into = c(\"commodity\", \"year\"), sep = -4)\nhead(prices_long)\n#> # A tibble: 6 × 4\n#>   city      commodity year  price\n#>   <chr>     <chr>     <chr> <dbl>\n#> 1 Amsterdam bigmac    2009     19\n#> 2 Amsterdam bread     2009     10\n#> 3 Amsterdam rice      2009     11\n#> 4 Amsterdam bigmac    2003     16\n#> 5 Amsterdam bread     2003      9\n#> 6 Amsterdam rice      2003      9\nprices_wide <- prices_long |>\n  pivot_wider(names_from = commodity, values_from = price)\nhead(prices_wide)\n#> # A tibble: 6 × 5\n#>   city      year  bigmac bread  rice\n#>   <chr>     <chr>  <dbl> <dbl> <dbl>\n#> 1 Amsterdam 2009      19    10    11\n#> 2 Amsterdam 2003      16     9     9\n#> 3 Athens    2009      30    13    27\n#> 4 Athens    2003      21    12    19\n#> 5 Auckland  2009      19    19    13\n#> 6 Auckland  2003      19    19     9\nmortality_long <- mortality_df |>\n  pivot_longer(cols = 2:217, names_to = \"Year\",\n               values_to = \"Mortality\",\n               names_transform = list(Year = as.numeric))"
  },
  {
    "path": "tidying-with-tidyr.html",
    "id": "chapexercise-4-S",
    "chapter": " 7 Tidying with tidyr",
    "heading": "7.5.4 Chapter Exercises S",
    "text": "* Use function tidyr make data tidy, give tidy data set new name.* data set previous exercise, add ranking variable ranks salaries within player position highest paid players position receive 1, second highest paid players receive 2, etc. Compare results default way R uses break ties two salaries using ties.method = \"first\".Hint: See Exercise 4 4.3.2 another example .first ranking code allows observations ranking get rankings averaged together (e.g. two observations tied 5th get ranking (5 + 6) / 2 = 5.5).second ranking method, first observation data set gets “higher” rank.* Find maximum salary player position year. , create two different line graphs shows maximum salary changed 2011 2018 position. one line graph, make colours lines different position. second line graph, facet position. graph like better?number levels, personally prefer faceted graph cleaner look.* maximum salary dependent one specific player. Make graph, plot average salary top 20 players position year. notice? interesting patterns positions? ’re fan football, provide guess one positions salary plateau recent years.Running backs haven’t much salary increase whereas offensive positions large salary increase. many plausible explanations case. One NFL much “passing league” now decades ago.* Sometimes graphs involving cost salary, want take account inflation rate. Google inflation rate 2011 2018 (Google something like “inflation rate 2011 2018” able find something). Adjust 2011 salaries inflation comparable 2018 salaries. , make similar line plot ignore years 2012 2017 (line plot just 2 points per position).adjusting inflation, many positions average higher salaries top 20 players position?positions higher salaries, even adjusting inflation, except perhaps running backs (’s hard tell graph).",
    "code": "\nnfl_long <- nfl_df |>\n  pivot_longer(c(2, 3, 4, 5, 6, 7, 8, 9, 10, 11),\n               names_to = \"position\", values_to = \"salary\")\nnfl_long\n#> # A tibble: 8,000 × 3\n#>     year position            salary\n#>    <dbl> <chr>                <dbl>\n#>  1  2011 Cornerback        11265916\n#>  2  2011 Defensive Lineman 17818000\n#>  3  2011 Linebacker        16420000\n#>  4  2011 Offensive Lineman 15960000\n#>  5  2011 Quarterback       17228125\n#>  6  2011 Running Back      12955000\n#>  7  2011 Safety             8871428\n#>  8  2011 Special Teamer     4300000\n#>  9  2011 Tight End          8734375\n#> 10  2011 Wide Receiver     16250000\n#> # … with 7,990 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\nnfl_long_default <- nfl_long |> group_by(position, year) |>\n  mutate(rank = rank(desc(salary)))\n\nnfl_long <- nfl_long |> group_by(position, year) |>\n  mutate(rank = rank(desc(salary), ties.method = \"first\"))\nnfl_max <- nfl_long |> group_by(position, year) |>\n  summarise(maxsal = max(salary, na.rm = TRUE))\n#> `summarise()` has grouped output by 'position'. You can\n#> override using the `.groups` argument.\n\nggplot(data = nfl_max,\n  aes(x = year, y = maxsal, group = position, colour = position)) +\n  geom_line()\n\nggplot(data = nfl_max, aes(x = year, y = maxsal)) +\n  geom_line() +\n  facet_wrap( ~ position)\nnfl_rank <- nfl_long |> filter(rank <= 20) |>\n  group_by(position, year) |>\n  summarise(mean20 = mean(salary, na.rm = TRUE))\n#> `summarise()` has grouped output by 'position'. You can\n#> override using the `.groups` argument.\n\nggplot(data = nfl_rank, aes(x = year, y = mean20)) +\n  geom_line() + \n  facet_wrap( ~ position)\n## 11.6% from 2011 to 2018.\nnfl_inf <- nfl_long |>\n  mutate(salary_inf = if_else(year == 2011,\n                              true = salary * 1.116,\n                              false = salary)) |> \n  filter(year == 2011 | year == 2018) |> \n  filter(rank <= 20) |> group_by(position, year) |>\n  summarise(mean20 = mean(salary, na.rm = TRUE)) \n#> `summarise()` has grouped output by 'position'. You can\n#> override using the `.groups` argument.\n\nggplot(data = nfl_inf, aes(x = year, y = mean20)) +\n  geom_line() +\n  geom_point() +\n  facet_wrap( ~ position)"
  },
  {
    "path": "tidying-with-tidyr.html",
    "id": "rcode-4",
    "chapter": " 7 Tidying with tidyr",
    "heading": "7.6 Non-Exercise R Code",
    "text": "",
    "code": "\nlibrary(tidyverse)\nlibrary(here)\npolls <- read_csv(here(\"data/rcp-polls.csv\"), na = \"--\")\npolls\npolls |> summarise(meansample = mean(Sample))\npolls |>\n  separate(col = Sample, into = c(\"Sample_size\", \"Sample_type\"), \n           sep = \" \")\npolls_sep <- polls |>\n  separate(col = Date, into = c(\"Start\", \"End\"),\n           sep = \" - \")\npolls_sillytest <- polls_sep |>\n  separate(col = Start, into = c(\"Start_month\", \"Start_day\"), \n           sep = \"/\")\npolls_sillytest\npolls_sillytest |>\n  unite(\"New_start_date\", c(Start_month, Start_day),\n        sep = \"/\")\nc(1, 4, 2)\nc(\"A\", \"A\", \"D\")\npolls_sep\npolls_new <- polls_sep |>\n  rename(Clinton_D = `Clinton (D)`, Trump_R = `Trump (R)`,\n         Johnson_L = `Johnson (L)`, Stein_G = `Stein (G)`)\npolls_new\npolls_clean <- polls |>\n  separate(col = Sample, into = c(\"Sample_size\", \"Sample_type\"), \n           sep = \" \")  |>\n  separate(col = Date, into = c(\"Start\", \"End\"),\n           sep = \" - \") |> \n  rename(Clinton_D = `Clinton (D)`, Trump_R = `Trump (R)`,\n         Johnson_L = `Johnson (L)`, Stein_G = `Stein (G)`)\npolls_clean\npolls_clean |>\n  pivot_longer(cols = c(Clinton_D, Trump_R, Johnson_L, Stein_G),\n               names_to = \"candidate\", values_to = \"poll_percent\")\npolls_long <- polls_clean |>\n  pivot_longer(cols = c(Clinton_D, Trump_R, Johnson_L, Stein_G),\n               names_to = \"candidate\", values_to = \"poll_percent\")\n\n## ignore as.Date for now....we will get to dates later!\nggplot(data = polls_long,\n       aes(x = as.Date(Start, \"%m/%d\"), y = poll_percent,\n           colour = candidate)) +\n  geom_point() + xlab(\"Poll Start Date\")\nlibrary(here)\nairlines <- read_csv(here(\"data/airline-safety.csv\"))\nhead(airlines)\nairlines |>\n  pivot_longer(c(3, 4, 5, 6, 7, 8), names_to = \"type_year\",\n  values_to = \"total_num\") \nairlines |> pivot_longer(c(3, 4, 5, 6, 7, 8), names_to = \"type_year\",\n                          values_to = \"total_num\") |>\n  separate(type_year, into = c(\"type\", \"year\"), sep = \" \")\n## name the long data set\nairlines_long <- airlines |>\n  pivot_longer(c(3, 4, 5, 6, 7, 8), names_to = \"type_year\",\n               values_to = \"total_num\") |>\n  separate(type_year, into = c(\"type\", \"year\"), sep = \" \")\n\n## use pivot_wider() to create variables for incidents, fatalities, and\n## fatal_accidents:\nairlines_long |> pivot_wider(names_from = type,\n                              values_from = total_num)"
  },
  {
    "path": "coding-in-base-r.html",
    "id": "coding-in-base-r",
    "chapter": " 8 Coding in Base R",
    "heading": " 8 Coding in Base R",
    "text": "Goals:describe common classes variables data set.explain R errors come class misspecifications.use indexing reference rows, columns, specific observations tibble data set.Motivation“Base R” generally refers R code can use without loading outside packages (code tidyverse family packages). chapter R basics first chapter discuss? certainly advantages things way, also advantages starting something like “classes variables R.”First, ’s inherently interesting thing look . ’s lot fun make plots wrangle data. long someone makes sure variables already “correct” class, ’s need talk .Second, much discuss make sense, previous four chapters belt. ’ll able see misspecified variable classes cause issues certain summaries plots already know make plots get summaries.",
    "code": ""
  },
  {
    "path": "coding-in-base-r.html",
    "id": "variable-classes-in-r",
    "chapter": " 8 Coding in Base R",
    "heading": "8.1 Variable Classes in R",
    "text": "R different classes variables take, including numeric, factor, character Date, logical. delve specifics classes mean, let’s try make plots illustrate care classes mean.videogame_clean.csv file contains variables video games 2004 - 2019, includinggame, name gamerelease_date, release date gamerelease_date2, second coding release dateprice, price dollars,owners, number owners (given range)median_playtime, median playtime gamemetascore, score website Metacriticprice_cat, 1 Low (less 10.00 dollars), 2 Moderate (10 29.99 dollars), 3 High (30.00 dollars)meta_cat, Metacritic’s review system, following categories: “Overwhelming Dislike”, “Generally Unfavorable”, “Mixed Reviews”, “Generally Favorable”, “Universal Acclaim”.playtime_miss, whether median play time missing (TRUE) (FALSE)data set modified https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-07-30.Run code following R chunk read data.data frame tibble holds variables allowed different classes. variable different class expect, ’ll get strange errors results trying wrangle data make graphics.Run following lines code. cases, using first 100 observations videogame_small. Otherwise, code take long time run.first plot, release_date isn’t ordered according expect (date). Instead, R orders alphabetically.second plot, expect get plot 3 different colours, one level price_cat. Instead, get continuous colour scale, doesn’t make sense, given price_cat can 1, 2, 3.plots rendered correctly variable classes correct underlying data set. point, data provided almost always correct variable classes, default, won’t always case!’ve actually seen issues well (Date issue exercise data continuous colour scale cars data), , instances, code provided “fix” problem. section, ’ll tools fix many class issues !examine output following line codeyou’ll see , top output, right variable names, R provides classes variables tibble.<chr> character, used strings text.<fct> used variables factors, typically used character variables finite number possible values variable can take .<date> used dates.<dbl> stands double used numeric class.<int> numbers integers. practice, much difference class class dbl.<lgl> logical, variables either TRUE FALSE.",
    "code": "\nlibrary(tidyverse)\nlibrary(here)\nvideogame_df <- read_csv(here(\"data/videogame_clean.csv\"))\nhead(videogame_df)\n#> # A tibble: 6 × 15\n#>   game       relea…¹ release_…² price owners media…³ metas…⁴\n#>   <chr>      <chr>   <date>     <dbl> <chr>    <dbl>   <dbl>\n#> 1 Half-Life… Nov 16… 2004-11-16  9.99 10,00…      66      96\n#> 2 Counter-S… Nov 1,… 2004-11-01  9.99 10,00…     128      88\n#> 3 Counter-S… Mar 1,… 2004-03-01  9.99 10,00…       3      65\n#> 4 Half-Life… Nov 1,… 2004-11-01  4.99 5,000…       0      NA\n#> 5 Half-Life… Jun 1,… 2004-06-01  9.99 2,000…       0      NA\n#> 6 CS2D       Dec 24… 2004-12-24 NA    1,000…      10      NA\n#> # … with 8 more variables: price_cat <dbl>, meta_cat <chr>,\n#> #   playtime_miss <lgl>, number <dbl>, developer <chr>,\n#> #   publisher <chr>, average_playtime <dbl>,\n#> #   meta_cat_factor <chr>, and abbreviated variable names\n#> #   ¹​release_date, ²​release_date2, ³​median_playtime,\n#> #   ⁴​metascore\n#> # ℹ Use `colnames()` to see all variable names\nvideogame_small <- videogame_df |> slice(1:100)\nggplot(data = videogame_small, aes(x = release_date, y = price)) +\n  geom_point() \n\nggplot(data = videogame_small, aes(x = release_date2, y = metascore)) +\n  geom_point(aes(colour = price_cat))\nhead(videogame_df)\n#> # A tibble: 6 × 15\n#>   game       relea…¹ release_…² price owners media…³ metas…⁴\n#>   <chr>      <chr>   <date>     <dbl> <chr>    <dbl>   <dbl>\n#> 1 Half-Life… Nov 16… 2004-11-16  9.99 10,00…      66      96\n#> 2 Counter-S… Nov 1,… 2004-11-01  9.99 10,00…     128      88\n#> 3 Counter-S… Mar 1,… 2004-03-01  9.99 10,00…       3      65\n#> 4 Half-Life… Nov 1,… 2004-11-01  4.99 5,000…       0      NA\n#> 5 Half-Life… Jun 1,… 2004-06-01  9.99 2,000…       0      NA\n#> 6 CS2D       Dec 24… 2004-12-24 NA    1,000…      10      NA\n#> # … with 8 more variables: price_cat <dbl>, meta_cat <chr>,\n#> #   playtime_miss <lgl>, number <dbl>, developer <chr>,\n#> #   publisher <chr>, average_playtime <dbl>,\n#> #   meta_cat_factor <chr>, and abbreviated variable names\n#> #   ¹​release_date, ²​release_date2, ³​median_playtime,\n#> #   ⁴​metascore\n#> # ℹ Use `colnames()` to see all variable names"
  },
  {
    "path": "coding-in-base-r.html",
    "id": "referencing-variables-and-using-str",
    "chapter": " 8 Coding in Base R",
    "heading": "8.1.1 Referencing Variables and Using str()",
    "text": "can use name_of_dataset$name_of_variable look specific variable data set:prints first thousand entries variable game. ways get class variable: way use often str(), stands “structure”, gives class variable, number observations (26688), well first couple observations:can also get variable’s class directly class()",
    "code": "\nvideogame_df$game\nstr(videogame_df$game)\n#>  chr [1:26688] \"Half-Life 2\" \"Counter-Strike: Source\" ...\nclass(videogame_df$game)\n#> [1] \"character\""
  },
  {
    "path": "coding-in-base-r.html",
    "id": "classes-in-detail",
    "chapter": " 8 Coding in Base R",
    "heading": "8.2 Classes in Detail",
    "text": "following gives summary information class variables R:",
    "code": ""
  },
  {
    "path": "coding-in-base-r.html",
    "id": "chr-and-fct-class",
    "chapter": " 8 Coding in Base R",
    "heading": "8.2.1 <chr> and <fct> Class",
    "text": "character class, R give warning /missing value try perform numerical operations:also can’t convert character class numeric. can, however, convert character class <fct> class, using .factor(). <fct> class useful discuss forcats package, isn’t particularly useful now.general, ._____ lets convert classes. Note, however, aren’t saving converted variable anywhere. wanted conversion factor saved data set, can use mutate():R functions, won’t matter whether variable class character class factor. general, though, character classes variables ton different levels, like name videogame, whereas factors reserved categorical variables finite number levels.",
    "code": "\nmean(videogame_df$game)\n#> [1] NA\nvideogame_df |> summarise(maxgame = max(game))\n#> # A tibble: 1 × 1\n#>   maxgame\n#>   <chr>  \n#> 1 <NA>\nclass(videogame_df$meta_cat)\n#> [1] \"character\"\nclass(as.factor(videogame_df$meta_cat))\n#> [1] \"factor\"\nvideogame_df <- videogame_df |>\n  mutate(meta_cat_factor = as.factor(meta_cat))\nstr(videogame_df$meta_cat_factor)\n#>  Factor w/ 4 levels \"Generally Favorable\",..: 4 1 3 NA NA NA 4 1 3 NA ..."
  },
  {
    "path": "coding-in-base-r.html",
    "id": "date-class",
    "chapter": " 8 Coding in Base R",
    "heading": "8.2.2 <date> Class",
    "text": "<date> class used dates, <datetime> class used Dates times. R requires specific format dates times. Note , human eye, following variables contain dates, one class <date>:release_date class character, issue odd ordering dates earlier. can try converting using .Date, function doesn’t always work:Dates times can pretty complicated. fact, spend entire week covering using lubridate package.variables Date format, like release_date2, can use numerical operations:think taking median taking mean date class means?",
    "code": "\nstr(videogame_df$release_date)\n#>  chr [1:26688] \"Nov 16, 2004\" \"Nov 1, 2004\" ...\nstr(videogame_df$release_date2)\n#>  Date[1:26688], format: \"2004-11-16\" \"2004-11-01\" \"2004-03-01\" ...\nas.Date(videogame_df$release_date)\n#> Error in charToDate(x): character string is not in a standard unambiguous format\nmedian(videogame_df$release_date2, na.rm = TRUE)\n#> [1] \"2017-06-09\"\nmean(videogame_df$release_date2, na.rm = TRUE)\n#> [1] \"2016-09-15\""
  },
  {
    "path": "coding-in-base-r.html",
    "id": "dbl-and-int-class",
    "chapter": " 8 Coding in Base R",
    "heading": "8.2.3 <dbl> and <int> Class",
    "text": "Class <dbl> <int> probably self-explanatory classes. <dbl>, numeric class, just variables numbers <int> integers (…, -2, -1, 0, 1, 2, ….). can numerical operations either classes (’ve throughout semester). purposes, two classes interchangeable.Problems arise numeric variables coded something non-numeric, non-numeric variables coded numeric. example, examine:price_cat categorical coded 1 cheap games, 2 moderately priced games, 3 expensive games. Therefore, R thinks variable numeric, , ’s actually factor.cause odd colour scale encountered earlier can fixed converting price_cat factor:",
    "code": "\nstr(videogame_df$price)\n#>  num [1:26688] 9.99 9.99 9.99 4.99 9.99 ...\nstr(videogame_df$price_cat)\n#>  num [1:26688] 1 1 1 1 1 NA 2 1 1 1 ...\nstr(as.factor(videogame_df$price_cat))\n#>  Factor w/ 3 levels \"1\",\"2\",\"3\": 1 1 1 1 1 NA 2 1 1 1 ...\nvideogame_df <- videogame_df |>\n  mutate(price_factor = as.factor(price_cat)) \nggplot(data = videogame_df, aes(x = release_date2, y = metascore)) +\n  geom_point(aes(colour = price_factor))"
  },
  {
    "path": "coding-in-base-r.html",
    "id": "lgl-class",
    "chapter": " 8 Coding in Base R",
    "heading": "8.2.4 <lgl> Class",
    "text": "Finally, class variables called logical. variables can take 2 values: TRUE FALSE. example, playtime_miss, variable whether median_playtime variable missing , logical:’s little strange first, R can perform numeric operations logical classes. R treat every TRUE 1 every FALSE 0. Therefore, sum() gives total number TRUEs mean() gives proportion TRUEs. , can find number proportion games missing median_playtime :’s lot games missing information!’ve actually used ideas logical variables quite time now, particularly statements involving if_else(), case_when(), filter(), mutate().primary purpose section able identify variable classes able convert different variable types mutate() “fix” variables incorrect class.",
    "code": "\nstr(videogame_df$playtime_miss)\n#>  logi [1:26688] FALSE FALSE FALSE TRUE TRUE FALSE ...\nsum(videogame_df$playtime_miss)\n#> [1] 25837\nmean(videogame_df$playtime_miss)\n#> [1] 0.968113"
  },
  {
    "path": "coding-in-base-r.html",
    "id": "exercise-6-2",
    "chapter": " 8 Coding in Base R",
    "heading": "8.2.5 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 8.6.use fitness data set set exercises, data set issues variable class discussed. However, week 1, work work fix issues already done saw data. Now, ’ll get fix couple issues! Read data set :* issue following plot? figure issue, use mutate() create new variable fixes issue reconstruct graph.* another variable data set incorrect class?* another variable data set incorrect class?Create new variable, called step_goal 1 TRUE least 10000 steps walked 0 FALSE fewer 10000 steps walked. Using variable, find total number days goal met proportion days goal met.Create new variable, called step_goal 1 TRUE least 10000 steps walked 0 FALSE fewer 10000 steps walked. Using variable, find total number days goal met proportion days goal met.",
    "code": "\nlibrary(tidyverse)\nfitness_df <- read_csv(here(\"data/higham_fitness_notclean.csv\"))\nggplot(data = fitness_df, aes(x = active_cals)) +\n  geom_freqpoly(aes(group = weekday, colour = weekday))\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`."
  },
  {
    "path": "coding-in-base-r.html",
    "id": "object-types-and-subsetting",
    "chapter": " 8 Coding in Base R",
    "heading": "8.3 Object Types and Subsetting",
    "text": "Variables different classes can stored variety different objects R. almost exclusively used tibble object type. tidy tibbleis “rectangular” specific number rows columns.columns variableseach column must elements class, different columns can different classes. allows us character numeric variables tibble.",
    "code": ""
  },
  {
    "path": "coding-in-base-r.html",
    "id": "tibble-and-data.frame",
    "chapter": " 8 Coding in Base R",
    "heading": "8.3.1 tibble and data.frame",
    "text": "tibble object similar data.frame object. can also check type object ’re working using str() command:small section tibbles coming weeks won’t focus . , take note , reference specific element tibble, called indexing, can use [# , #]. , example, videogame_df[5, 3] grabs value fifth row third column:often, ’d want grab entire row (range rows) entire column. can leaving row number blank (grab entire column) leaving column number blank (grab entire row):can also grab range columns rows using : operator:can grab different columns rows using c():get rid entire row column, use -: videogame_df[ ,-c(1, 2)] drops first second columns videogame_df[-c(1, 2), ] drops first second rows.",
    "code": "\nstr(videogame_df) ## look at the beginning to see \"tibble\"\nvideogame_df[5, 3]\n#> # A tibble: 1 × 1\n#>   release_date2\n#>   <date>       \n#> 1 2004-06-01\nvideogame_df[ ,3] ## grab the third column\n\nvideogame_df[5, ] ## grab the fifth row\n3:7\n\nvideogame_df[ ,3:7] ## grab columns 3 through 7\n\nvideogame_df[3:7, ] ## grab rows 3 through 7\nvideogame_df[ ,c(1, 3, 4)] ## grab columns 1, 3, and 4\n\nvideogame_df[c(1, 3, 4), ] ## grab rows 1, 3, and 4"
  },
  {
    "path": "coding-in-base-r.html",
    "id": "vectors",
    "chapter": " 8 Coding in Base R",
    "heading": "8.3.2 Vectors",
    "text": "vector object holds “things”, elements, class. can create vector R using c() function, stands “concatenate”. ’ve used c() function bind things together; just hadn’t yet discussed context creating vector.Notice vec2 character class. R requires elements vector one class; since R knows b can’t numeric, makes numbers characters well.Using dataset$variable draws vector tibble data.frame:wanted make vector “hand”, ’d need lot patience: c(96, 88, 65, NA, NA, NA, 93, .........)Just like tibbles, can save vectors something later use:get mean metascore using dplyr functions?Vectors one-dimensional: want grab 100th element vector just use name_of_vector[100]:aware , ’re coming math perspective, “vector” R doesn’t correspond “vector” mathematics physics.",
    "code": "\nvec1 <- c(1, 3, 2)\nvec2 <- c(\"b\", 1, 2)\nvec3 <- c(FALSE, FALSE, TRUE)\nstr(vec1); str(vec2); str(vec3)\n#>  num [1:3] 1 3 2\n#>  chr [1:3] \"b\" \"1\" \"2\"\n#>  logi [1:3] FALSE FALSE TRUE\nvideogame_df$metascore\nmetavec <- videogame_df$metascore\nmean(metavec, na.rm = TRUE)\n#> [1] 71.89544\nmetavec[100] ## 100th element is missing\n#> [1] NA"
  },
  {
    "path": "coding-in-base-r.html",
    "id": "lists",
    "chapter": " 8 Coding in Base R",
    "heading": "8.3.3 Lists",
    "text": "Lists one flexible objects R: can put objects different classes list lists aren’t required rectangular (like tibbles ). Lists extremely useful flexibility, , won’t use much class. Therefore, just see example list moving :testlist four elements: single character \"\", single number 4, vector 1, 4, 2, 6, tibble couple variables. Lists can therefore used store complex information wouldn’t easily stored vector tibble.",
    "code": "\ntestlist <- list(\"a\", 4, c(1, 4, 2, 6),\n                 tibble(x = c(1, 2), y = c(3, 2)))\ntestlist\n#> [[1]]\n#> [1] \"a\"\n#> \n#> [[2]]\n#> [1] 4\n#> \n#> [[3]]\n#> [1] 1 4 2 6\n#> \n#> [[4]]\n#> # A tibble: 2 × 2\n#>       x     y\n#>   <dbl> <dbl>\n#> 1     1     3\n#> 2     2     2"
  },
  {
    "path": "coding-in-base-r.html",
    "id": "exercise-6-3",
    "chapter": " 8 Coding in Base R",
    "heading": "8.3.4 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 8.6.* Look subsetting commands [ , ]. dplyr functions can use thing?* Look subsetting commands [ , ]. dplyr functions can use thing?Create tibble called last100 last 100 days data set using (1) indexing [ , ] (2) dplyr function.Create tibble called last100 last 100 days data set using (1) indexing [ , ] (2) dplyr function.Create tibble doesn’t flights variable using (1) indexing [ , ] (2) dplyr function.Create tibble doesn’t flights variable using (1) indexing [ , ] (2) dplyr function.* Use following steps create new variable weekend_ind, “weekend” day week Saturday Sunday “weekday” day week day. current weekday variable coded 1 represents Sunday, 2 represents Monday, …., 7 represents Saturday.* Use following steps create new variable weekend_ind, “weekend” day week Saturday Sunday “weekday” day week day. current weekday variable coded 1 represents Sunday, 2 represents Monday, …., 7 represents Saturday.Create vector numbers corresponding two weekend days. Name vector create second vector numbers corresponding five weekday days.Create vector numbers corresponding two weekend days. Name vector create second vector numbers corresponding five weekday days.Use dplyr functions %% operator create new weekend_ind variable. can use following code chunk help %% :Use dplyr functions %% operator create new weekend_ind variable. can use following code chunk help %% :",
    "code": "\n1 %in% c(1, 2, 3, 4)\n2 %in% c(1, 2, 3, 4)\n\n2 %in% c(3, 4, 5, 6)"
  },
  {
    "path": "coding-in-base-r.html",
    "id": "other-useful-base-r-functions",
    "chapter": " 8 Coding in Base R",
    "heading": "8.4 Other Useful Base R Functions",
    "text": "addition functions like %% previous exercise, many useful base R functions. following give functions think useful data science.Generating Data: rnorm(), sample(), set.seed()rnorm() can used generate certain number normal random variables given mean standard deviation. three arguments: sample size, mean, standard deviation.sample() can used obtain sample vector, either without replacement: two required arguments: vector want sample size, size sample.set.seed() can used fix R’s random seed. can set , example, person class can get random sample long set seed.can combined quickly generate toy data. example, generating two quantitative variables (normally distributed) two categorical variables:Tables: can use table() function $ operator quickly generate tables categorical variables:Others: quite useful base R functions. nrow() can used data frame quickly look number rows data frame summary() can used get quick summary vector:also useful functions viewing data frame. View() function can used console window data frame: View(toy_df) pull spreadsheet-like view data set different window within R Studio.Options print() allow us view rows columns console printout:stop , surely encounter base R functions run different types problems.",
    "code": "\nset.seed(15125141)\ntoy_df <- tibble(xvar = rnorm(100, 3, 4),\n                 yvar = rnorm(100, -5, 10),\n                 group1 = sample(c(\"A\", \"B\", \"C\"), size = 100, replace = TRUE),\n                 group2 = sample(c(\"Place1\", \"Place2\", \"Place3\"), size = 100,\n                                 replace = TRUE))\ntoy_df\n#> # A tibble: 100 × 4\n#>      xvar   yvar group1 group2\n#>     <dbl>  <dbl> <chr>  <chr> \n#>  1  0.516 -13.5  B      Place2\n#>  2 -0.891 -13.3  A      Place2\n#>  3  5.58  -14.3  B      Place2\n#>  4  2.42   -4.91 C      Place1\n#>  5  1.43   -5.86 B      Place2\n#>  6  6.61   12.7  B      Place2\n#>  7 -2.04   -9.28 A      Place1\n#>  8  7.56    1.89 A      Place3\n#>  9 -0.425 -30.1  C      Place1\n#> 10  4.14    2.65 C      Place2\n#> # … with 90 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\ntable(toy_df$group1)\n#> \n#>  A  B  C \n#> 27 39 34\n\ntable(toy_df$group1, toy_df$group2)\n#>    \n#>     Place1 Place2 Place3\n#>   A     10      8      9\n#>   B      9     20     10\n#>   C     10     10     14\nnrow(toy_df)\n#> [1] 100\nsummary(toy_df$yvar)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> -30.123 -12.938  -5.380  -6.630  -1.298  13.858\ntoy_df |>\n  print(n = 60) ## print out 60 rows\n#> # A tibble: 100 × 4\n#>      xvar    yvar group1 group2\n#>     <dbl>   <dbl> <chr>  <chr> \n#>  1  0.516 -13.5   B      Place2\n#>  2 -0.891 -13.3   A      Place2\n#>  3  5.58  -14.3   B      Place2\n#>  4  2.42   -4.91  C      Place1\n#>  5  1.43   -5.86  B      Place2\n#>  6  6.61   12.7   B      Place2\n#>  7 -2.04   -9.28  A      Place1\n#>  8  7.56    1.89  A      Place3\n#>  9 -0.425 -30.1   C      Place1\n#> 10  4.14    2.65  C      Place2\n#> 11  5.03   -8.82  C      Place1\n#> 12  2.98  -22.7   C      Place1\n#> 13  5.97   -2.67  B      Place3\n#> 14  0.882   1.59  A      Place1\n#> 15  2.14   -5.63  B      Place3\n#> 16  7.74    5.79  C      Place3\n#> 17  5.20   -3.17  B      Place2\n#> 18  2.89   -0.697 A      Place1\n#> 19  2.71    1.09  B      Place2\n#> 20  5.87   -4.87  C      Place1\n#> 21  5.65  -10.3   B      Place2\n#> 22 -0.520  -4.77  B      Place3\n#> 23 -0.130 -18.3   B      Place3\n#> 24 -0.174 -18.9   A      Place3\n#> 25  4.33    4.63  A      Place3\n#> 26  0.462 -12.8   A      Place3\n#> 27  5.53   -3.36  C      Place1\n#> 28  1.66   -5.34  A      Place1\n#> 29 -0.469 -13.2   C      Place2\n#> 30  7.51  -13.4   B      Place1\n#> 31 -1.82   -6.47  C      Place3\n#> 32 -2.44   -2.17  C      Place3\n#> 33  1.52  -12.6   C      Place2\n#> 34  4.60   -6.69  A      Place2\n#> 35  3.10  -25.5   A      Place2\n#> 36 -0.682 -20.4   A      Place1\n#> 37 -5.72    2.65  B      Place3\n#> 38  0.976 -12.1   B      Place3\n#> 39  1.39    2.78  B      Place2\n#> 40  6.67  -14.6   A      Place1\n#> 41  3.09  -10.4   B      Place2\n#> 42 -1.98  -12.8   A      Place3\n#> 43  0.225  13.9   C      Place1\n#> 44  5.71   -3.50  A      Place3\n#> 45  5.57   -2.02  B      Place3\n#> 46  8.96   -1.86  B      Place2\n#> 47  3.80  -11.3   C      Place1\n#> 48  7.40   -1.32  C      Place2\n#> 49  0.988   4.04  B      Place2\n#> 50  1.93   -5.24  A      Place2\n#> 51  5.23   13.2   C      Place2\n#> 52 -2.13  -19.6   A      Place2\n#> 53  6.05   -4.42  A      Place3\n#> 54  0.865  -9.47  A      Place1\n#> 55  4.16  -16.4   B      Place1\n#> 56 -2.73   -4.09  B      Place2\n#> 57 -0.532   7.70  C      Place3\n#> 58 -2.96  -11.6   C      Place3\n#> 59  4.34   -5.99  B      Place2\n#> 60  6.72  -13.6   B      Place2\n#> # … with 40 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\ntoy_df |>\n  print(width = Inf) ## print out all of the columns\n#> # A tibble: 100 × 4\n#>      xvar   yvar group1 group2\n#>     <dbl>  <dbl> <chr>  <chr> \n#>  1  0.516 -13.5  B      Place2\n#>  2 -0.891 -13.3  A      Place2\n#>  3  5.58  -14.3  B      Place2\n#>  4  2.42   -4.91 C      Place1\n#>  5  1.43   -5.86 B      Place2\n#>  6  6.61   12.7  B      Place2\n#>  7 -2.04   -9.28 A      Place1\n#>  8  7.56    1.89 A      Place3\n#>  9 -0.425 -30.1  C      Place1\n#> 10  4.14    2.65 C      Place2\n#> # … with 90 more rows\n#> # ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "path": "coding-in-base-r.html",
    "id": "exercise-6-4",
    "chapter": " 8 Coding in Base R",
    "heading": "8.4.1 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 8.6.Use dplyr tidyr functions re-create tables generated ",
    "code": "\ntable(toy_df$group1)\n#> \n#>  A  B  C \n#> 27 39 34\n\ntable(toy_df$group1, toy_df$group2)\n#>    \n#>     Place1 Place2 Place3\n#>   A     10      8      9\n#>   B      9     20     10\n#>   C     10     10     14"
  },
  {
    "path": "coding-in-base-r.html",
    "id": "chapexercise-6",
    "chapter": " 8 Coding in Base R",
    "heading": "8.5 Chapter Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 8.6.Work following exercises pertaining video game data set.* Read data set use filter() remove rows missing metascores, missing median playtime, median playtime 0 hours.Note: usually don’t want remove missing values without valid reason. case, missing metascore means game wasn’t “major” enough get enough critic reviews, missing 0 hour median playtime means weren’t enough users uploaded playtime database. Therefore, analyses constructed games popular enough get enough reviews metacritic enough users upload median playtimes.* Make scatterplot median_playtime y-axis metascore x-axis filtered data set.* Make scatterplot median_playtime y-axis metascore x-axis filtered data set.* Something may notice many points directly overlap one another. common least one variables scatterplot discrete: metascore can take integer values case. Change geom_point() previous plot geom_jitter(). , use help write sentence geom_jitter() .* Something may notice many points directly overlap one another. common least one variables scatterplot discrete: metascore can take integer values case. Change geom_point() previous plot geom_jitter(). , use help write sentence geom_jitter() .* Another option control point transparency alpha. geom_jitter() statement, change alpha can still see points, can tell plot lot points overlapping.* Another option control point transparency alpha. geom_jitter() statement, change alpha can still see points, can tell plot lot points overlapping.* Label points median playtimes 1500 hours. may want use ggrepel package labels don’t overlap.* Label points median playtimes 1500 hours. may want use ggrepel package labels don’t overlap.Choose one games got labeled Google game’s median, possibly average, play time. vicinity median_playtime recorded data set?Choose one games got labeled Google game’s median, possibly average, play time. vicinity median_playtime recorded data set?done outliers? discuss investigate issue class.done outliers? discuss investigate issue class.",
    "code": "\nvideogame_df <- read_csv(here(\"data/videogame_clean.csv\"))"
  },
  {
    "path": "coding-in-base-r.html",
    "id": "solutions-6",
    "chapter": " 8 Coding in Base R",
    "heading": "8.6 Exercise Solutions",
    "text": "",
    "code": ""
  },
  {
    "path": "coding-in-base-r.html",
    "id": "variable-classes-s",
    "chapter": " 8 Coding in Base R",
    "heading": "8.6.1 Variable Classes S",
    "text": "",
    "code": ""
  },
  {
    "path": "coding-in-base-r.html",
    "id": "classes-in-detail-s",
    "chapter": " 8 Coding in Base R",
    "heading": "8.6.2 Classes in Detail S",
    "text": "* issue following plot? figure issue, use mutate() create new variable fixes issue reconstruct graph.issue weekday factor, numeric.* another variable data set incorrect class?Month ordered factor, numeric.",
    "code": "\nggplot(data = fitness_df, aes(x = active_cals)) +\n  geom_freqpoly(aes(group = weekday, colour = weekday))\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\nfitness_df <- fitness_df |> mutate(weekday_cat = as.factor(weekday))\nggplot(data = fitness_df, aes(x = active_cals)) +\n  geom_freqpoly(aes(group = weekday_cat, colour = weekday_cat)) +\n  scale_colour_viridis_d()\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`."
  },
  {
    "path": "coding-in-base-r.html",
    "id": "object-types-s",
    "chapter": " 8 Coding in Base R",
    "heading": "8.6.3 Object Types S",
    "text": "* Look subsetting commands [ , ]. dplyr functions can use thing?slice() can used row indexing select() can used column indexing.* Use following steps create new variable weekend_ind, “weekend” day week Saturday Sunday “weekday” day week day. current weekday variable coded 1 represents Sunday, 2 represents Monday, …., 7 represents Saturday.Create vector numbers corresponding two weekend days. Name vector create second vector numbers corresponding five weekday days.Use dplyr functions %% operator create new weekend_ind variable. can use following code chunk help %% :",
    "code": "\nvecweekend <- c(1, 7)\nvecweekday <- 2:6 ## or c(2, 3, 4, 5, 6)\n1 %in% c(1, 2, 3, 4)\n#> [1] TRUE\n2 %in% c(1, 2, 3, 4)\n#> [1] TRUE\n\n2 %in% c(3, 4, 5, 6)\n#> [1] FALSE\nfitness_df |>\n  mutate(weekend_ind = case_when(weekday %in% vecweekend ~ \"weekend\",\n                                 weekday %in% vecweekday ~ \"weekday\")) |>\n  select(weekend_ind, everything())\n#> # A tibble: 993 × 11\n#>    weekend…¹ Start      activ…² dista…³ flights  steps month\n#>    <chr>     <date>       <dbl>   <dbl>   <dbl>  <dbl> <dbl>\n#>  1 weekday   2018-11-28    57.8   0.930       0  1885.    11\n#>  2 weekday   2018-11-29   509.    4.64       18  8953.    11\n#>  3 weekday   2018-11-30   599.    6.05       12 11665     11\n#>  4 weekend   2018-12-01   661.    6.80        6 12117     12\n#>  5 weekend   2018-12-02   527.    4.61        1  8925.    12\n#>  6 weekday   2018-12-03   550.    3.96        2  7205     12\n#>  7 weekday   2018-12-04   670.    6.60        5 12483.    12\n#>  8 weekday   2018-12-05   557.    4.91        6  9258.    12\n#>  9 weekday   2018-12-06   997.    7.50       13 14208     12\n#> 10 weekday   2018-12-07   533.    4.27        8  8269.    12\n#> # … with 983 more rows, 4 more variables: weekday <dbl>,\n#> #   dayofyear <dbl>, stepgoal <dbl>, weekday_cat <fct>, and\n#> #   abbreviated variable names ¹​weekend_ind, ²​active_cals,\n#> #   ³​distance\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n## can also use if_else, which is actually a little simpler in this case:\nfitness_df |> mutate(weekend_ind = if_else(weekday %in% vecweekend,\n  true = \"weekend\", false = \"weekday\")) |>\n  select(weekend_ind, everything())\n#> # A tibble: 993 × 11\n#>    weekend…¹ Start      activ…² dista…³ flights  steps month\n#>    <chr>     <date>       <dbl>   <dbl>   <dbl>  <dbl> <dbl>\n#>  1 weekday   2018-11-28    57.8   0.930       0  1885.    11\n#>  2 weekday   2018-11-29   509.    4.64       18  8953.    11\n#>  3 weekday   2018-11-30   599.    6.05       12 11665     11\n#>  4 weekend   2018-12-01   661.    6.80        6 12117     12\n#>  5 weekend   2018-12-02   527.    4.61        1  8925.    12\n#>  6 weekday   2018-12-03   550.    3.96        2  7205     12\n#>  7 weekday   2018-12-04   670.    6.60        5 12483.    12\n#>  8 weekday   2018-12-05   557.    4.91        6  9258.    12\n#>  9 weekday   2018-12-06   997.    7.50       13 14208     12\n#> 10 weekday   2018-12-07   533.    4.27        8  8269.    12\n#> # … with 983 more rows, 4 more variables: weekday <dbl>,\n#> #   dayofyear <dbl>, stepgoal <dbl>, weekday_cat <fct>, and\n#> #   abbreviated variable names ¹​weekend_ind, ²​active_cals,\n#> #   ³​distance\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names"
  },
  {
    "path": "coding-in-base-r.html",
    "id": "chapexercise-6-S",
    "chapter": " 8 Coding in Base R",
    "heading": "8.6.4 Chapter Exercises S",
    "text": "* Read data set use filter() remove rows missing metascores, missing median playtime, median playtime 0 hours.Note: usually don’t want remove missing values without valid reason. case, missing metascore means game wasn’t “major” enough get enough critic reviews, missing 0 hour median playtime means weren’t enough users uploaded playtime database. Therefore, analyses constructed games popular enough get enough reviews metacritic enough users upload median playtimes.* Make scatterplot median_playtime y-axis metascore x-axis filtered data set.* Something may notice many points directly overlap one another. common least one variables scatterplot discrete: metascore can take integer values case. Change geom_point() previous plot geom_jitter(). , use help write sentence geom_jitter() .geom_jitter() adds small amount “noise” data point points don’t overlap quite much.* Another option control point transparency alpha. geom_jitter() statement, change alpha can still see points, can tell plot lot points overlapping.* Label points median playtimes 1500 hours. may want use ggrepel package labels don’t overlap.",
    "code": "\nvideogame_df <- read_csv(here(\"data/videogame_clean.csv\"))\nvideogame_nomiss <- videogame_df |>\n  filter(!is.na(median_playtime) &\n           !is.na(metascore) &\n           median_playtime != 0)\nggplot(data = videogame_nomiss, aes(x = metascore,\n                                    y = median_playtime)) + \n  geom_point()\nggplot(data = videogame_nomiss, aes(x = metascore,\n                                    y = median_playtime)) + \n  geom_jitter()\nggplot(data = videogame_nomiss, aes(x = metascore,\n                                    y = median_playtime)) + \n  geom_jitter(alpha = 0.4)\n## can see a lot of ponits have median playtimes close to 0\nlibrary(ggrepel)\nvideogame_long <- videogame_nomiss |> filter(median_playtime > 1500)\nggplot(data = videogame_nomiss,\n       aes(x = metascore, y = median_playtime)) + \n  geom_jitter(alpha = 0.4) +\n  geom_label_repel(data = videogame_long, aes(label = game))"
  },
  {
    "path": "coding-in-base-r.html",
    "id": "rcode-6",
    "chapter": " 8 Coding in Base R",
    "heading": "8.7 Non-Exercise R Code",
    "text": "",
    "code": "\nlibrary(tidyverse)\nlibrary(here)\nvideogame_df <- read_csv(here(\"data/videogame_clean.csv\"))\nhead(videogame_df)\nvideogame_small <- videogame_df |> slice(1:100)\nggplot(data = videogame_small, aes(x = release_date, y = price)) +\n  geom_point() \n\nggplot(data = videogame_small, aes(x = release_date2, y = metascore)) +\n  geom_point(aes(colour = price_cat))\nhead(videogame_df)\nvideogame_df$game\nstr(videogame_df$game)\nclass(videogame_df$game)\nmean(videogame_df$game)\nvideogame_df |> summarise(maxgame = max(game))\nclass(videogame_df$meta_cat)\nclass(as.factor(videogame_df$meta_cat))\nvideogame_df <- videogame_df |>\n  mutate(meta_cat_factor = as.factor(meta_cat))\nstr(videogame_df$meta_cat_factor)\nstr(videogame_df$release_date)\nstr(videogame_df$release_date2)\nmedian(videogame_df$release_date2, na.rm = TRUE)\nmean(videogame_df$release_date2, na.rm = TRUE)\nstr(videogame_df$price)\nstr(videogame_df$price_cat)\nstr(as.factor(videogame_df$price_cat))\nvideogame_df <- videogame_df |>\n  mutate(price_factor = as.factor(price_cat)) \nggplot(data = videogame_df, aes(x = release_date2, y = metascore)) +\n  geom_point(aes(colour = price_factor))\nstr(videogame_df$playtime_miss)\nsum(videogame_df$playtime_miss)\nmean(videogame_df$playtime_miss)\nstr(videogame_df) ## look at the beginning to see \"tibble\"\nvideogame_df[5, 3]\nvideogame_df[ ,3] ## grab the third column\n\nvideogame_df[5, ] ## grab the fifth row\n3:7\n\nvideogame_df[ ,3:7] ## grab columns 3 through 7\n\nvideogame_df[3:7, ] ## grab rows 3 through 7\nvideogame_df[ ,c(1, 3, 4)] ## grab columns 1, 3, and 4\n\nvideogame_df[c(1, 3, 4), ] ## grab rows 1, 3, and 4\nvec1 <- c(1, 3, 2)\nvec2 <- c(\"b\", 1, 2)\nvec3 <- c(FALSE, FALSE, TRUE)\nstr(vec1); str(vec2); str(vec3)\nvideogame_df$metascore\nmetavec <- videogame_df$metascore\nmean(metavec, na.rm = TRUE)\nmetavec[100] ## 100th element is missing\ntestlist <- list(\"a\", 4, c(1, 4, 2, 6),\n                 tibble(x = c(1, 2), y = c(3, 2)))\ntestlist\nset.seed(15125141)\ntoy_df <- tibble(xvar = rnorm(100, 3, 4),\n                 yvar = rnorm(100, -5, 10),\n                 group1 = sample(c(\"A\", \"B\", \"C\"), size = 100, replace = TRUE),\n                 group2 = sample(c(\"Place1\", \"Place2\", \"Place3\"), size = 100,\n                                 replace = TRUE))\ntoy_df\ntable(toy_df$group1)\n\ntable(toy_df$group1, toy_df$group2)\nnrow(toy_df)\nsummary(toy_df$yvar)\ntoy_df |>\n  print(n = 60) ## print out 60 rows\ntoy_df |>\n  print(width = Inf) ## print out all of the columns"
  },
  {
    "path": "factors-with-forcats.html",
    "id": "factors-with-forcats",
    "chapter": " 9 Factors with forcats",
    "heading": " 9 Factors with forcats",
    "text": "Goals:Use forcats package change levels factors, re-order levels factors way makes tables graphs easier read.",
    "code": ""
  },
  {
    "path": "factors-with-forcats.html",
    "id": "change-factor-levels",
    "chapter": " 9 Factors with forcats",
    "heading": "9.1 Change Factor Levels",
    "text": "Data: pokemon_allgen.csv data set contains observations Pokemon first 6 Generations (first 6 games). 20 variable data set, , particular interest chapter areType 1, first Type characteristic Pokemon (factor 13 levels)Type 2, second Type characteristic Pokemon (factor 13 levels, NA Pokemon one type)Generation, generation Pokemon first appeared (factor 6 levels)Read data set read_csv(). , use mutate() statement make Generation_cat variable factor.One easy way get quick summary factor variable use group_by() n() within summarise() statement:",
    "code": "\nlibrary(tidyverse)\nlibrary(here)\npokemon_df <- read_csv(here(\"data/pokemon_allgen.csv\")) |>\n  mutate(Generation_cat = factor(Generation))\npokemon_df |> group_by(`Type 1`) |>\n  summarise(counttype = n())\n#> # A tibble: 18 × 2\n#>    `Type 1` counttype\n#>    <chr>        <int>\n#>  1 Bug             75\n#>  2 Dark            31\n#>  3 Dragon          41\n#>  4 Electric        90\n#>  5 Fairy           18\n#>  6 Fighting        27\n#>  7 Fire            56\n#>  8 Flying           6\n#>  9 Ghost           58\n#> 10 Grass           73\n#> 11 Ground          42\n#> 12 Ice             24\n#> 13 Normal         108\n#> 14 Poison          30\n#> 15 Psychic         73\n#> 16 Rock            47\n#> 17 Steel           29\n#> 18 Water          119"
  },
  {
    "path": "factors-with-forcats.html",
    "id": "fct_recode-to-rename-levels",
    "chapter": " 9 Factors with forcats",
    "heading": "9.1.1 fct_recode() to Rename Levels",
    "text": "Now, let’s make bar plot examines many Legendary Pokemon first appear generation, using dplyr commands ’ve used simple geom_col():’ve discussed change many aspects ggplot2 graphs, haven’t discussed rename labels levels categorical variable, whether appear x-axis separate legend. easiest way rename levels factor using fct_recode(). Suppose, example, want relabel Generation number actual region corresponding game (Kanto, Johto, Hoenn, Sinnoh, Unova, Kalos). function fct_recode() takes name factor already present data set first argument series renaming schemes (new_name = “old_name”) remaining arguments.",
    "code": "\npokemon_legend <- pokemon_df |> filter(Legendary == TRUE) |>\n  group_by(Generation_cat) |>\n  summarise(nlegend = n())\nggplot(data = pokemon_legend, aes(x = Generation_cat, y = nlegend)) +\n  geom_col()\npokemon_legend <- pokemon_legend |>\n  mutate(Generation_cat2 = fct_recode(Generation_cat, Kanto = \"1\",\n                                      Johto = \"2\", Hoenn = \"3\",\n                                      Sinnoh = \"4\", Unova = \"5\",\n                                      Kalos = \"6\")) |>\n  select(Generation_cat2, everything())\nhead(pokemon_legend)\n#> # A tibble: 6 × 3\n#>   Generation_cat2 Generation_cat nlegend\n#>   <fct>           <fct>            <int>\n#> 1 Kanto           1                    6\n#> 2 Johto           2                    5\n#> 3 Hoenn           3                   34\n#> 4 Sinnoh          4                   17\n#> 5 Unova           5                   27\n#> 6 Kalos           6                   13\nggplot(data = pokemon_legend,\n       aes(x = Generation_cat2, y = nlegend)) +\n  geom_col()"
  },
  {
    "path": "factors-with-forcats.html",
    "id": "collapsing-many-levels-into-fewer-levels-with-fct_collapse",
    "chapter": " 9 Factors with forcats",
    "heading": "9.1.2 Collapsing Many Levels Into Fewer Levels with fct_collapse()",
    "text": "Sometimes, might want collapse levels two factors single level. Pokemon data set, isn’t example really makes sense, , exercises, ’ll see good use function social survey data set. practice, can collapse Ice Dark type Pokemon new level called Coolest can collapse Poison, Fighting, Fire type Pokemon new level called Least_Cool.happens levels aren’t re-specified?",
    "code": "\npokemon_long <- pokemon_df |> pivot_longer(c(`Type 1`, `Type 2`),\n                            names_to = \"Number\",\n                            values_to = \"Type\")\npokemon_long |>\n  mutate(new_type = fct_collapse(Type, Coolest = c(\"Ice\", \"Dark\"),\n                                 Least_Cool = c(\"Fire\", \"Fighting\", \"Poison\"))) |>\n  select(new_type, Type, everything())\n#> # A tibble: 1,894 × 22\n#>    new_type   Type     `#` Name   Total    HP Attack Defense\n#>    <fct>      <chr>  <dbl> <chr>  <dbl> <dbl>  <dbl>   <dbl>\n#>  1 Grass      Grass      1 Bulba…   318    45     49      49\n#>  2 Least_Cool Poison     1 Bulba…   318    45     49      49\n#>  3 Grass      Grass      2 Ivysa…   405    60     62      63\n#>  4 Least_Cool Poison     2 Ivysa…   405    60     62      63\n#>  5 Grass      Grass      3 Venus…   525    80     82      83\n#>  6 Least_Cool Poison     3 Venus…   525    80     82      83\n#>  7 Grass      Grass      3 Venus…   525    80     82      83\n#>  8 Least_Cool Poison     3 Venus…   525    80     82      83\n#>  9 Least_Cool Fire       4 Charm…   309    39     52      43\n#> 10 <NA>       <NA>       4 Charm…   309    39     52      43\n#> # … with 1,884 more rows, and 14 more variables:\n#> #   `Sp. Atk` <dbl>, `Sp. Def` <dbl>, Speed <dbl>,\n#> #   Generation <dbl>, Legendary <lgl>, id <chr>,\n#> #   identifier <chr>, height <dbl>, weight <dbl>,\n#> #   base_experience <dbl>, order <dbl>, is_default <dbl>,\n#> #   Generation_cat <fct>, Number <chr>\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names"
  },
  {
    "path": "factors-with-forcats.html",
    "id": "exercise-7-1",
    "chapter": " 9 Factors with forcats",
    "heading": "9.1.3 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 9.4.dplyr function(s) also use create new levels created fct_collapse()? might little easier use fct_collapse()?dplyr function(s) also use create new levels created fct_collapse()? might little easier use fct_collapse()?* properly explore data set making graphs , , fact, double counting Pokemon data set (another example familiar data set ’re working advantageous: people familiar Pokemon know fewer 947 Pokemon Generations 1 6).* properly explore data set making graphs , , fact, double counting Pokemon data set (another example familiar data set ’re working advantageous: people familiar Pokemon know fewer 947 Pokemon Generations 1 6).Figure Pokemon double counted. , create new data set keeps one observation per Pokemon #.Create bar plot non-duplicated data set. results significantly changed?",
    "code": ""
  },
  {
    "path": "factors-with-forcats.html",
    "id": "reorder-factor-levels",
    "chapter": " 9 Factors with forcats",
    "heading": "9.2 Reorder Factor Levels",
    "text": "",
    "code": ""
  },
  {
    "path": "factors-with-forcats.html",
    "id": "change-the-order-of-levels-by-a-quantitative-variable-with-fct_reorder",
    "chapter": " 9 Factors with forcats",
    "heading": "9.2.1 Change the Order of Levels by a Quantitative Variable with fct_reorder()",
    "text": "might also interested re-ordering x y-axis particular graph order factors correspond , example, median quantitative variable level. reason want easiest see example. example, suppose want look common Pokemon types across first 6 generations. use non-duplicated data set previous section’s exercises, pivot data type one column, remove observations missing Type, correspond second Type Pokemon single Type:R order levels Type factor, default? might like ordered make graph readable?following code creates new factor variable called Type_ordered orders type count_type variable. fct_reorder() takes factor first argument numeric variable re-order factor second argument. bar plot reconstructed new variable.",
    "code": "\npokemon_nodup <- pokemon_df |> group_by(`#`) |> slice(1) |>\n  ungroup()\npokemon_long <- pokemon_nodup |>\n  pivot_longer(c(`Type 1`, `Type 2`),\n               names_to = \"Number\",\n               values_to = \"Type\")\npokemon_sum <- pokemon_long |>\n  group_by(Type) |>\n  summarise(count_type = n()) |>\n  filter(!is.na(Type))\nggplot(data = pokemon_sum, aes(x = Type,\n                               y = count_type)) +\n  geom_col() +\n  coord_flip()  ## flips the x and y axes\npokemon_sum <- pokemon_sum |> \n  mutate(Type_ordered = fct_reorder(.f = Type, .x = count_type))\nggplot(data = pokemon_sum, aes(x = Type_ordered,\n                               y = count_type)) +\n  geom_col() +\n  coord_flip()"
  },
  {
    "path": "factors-with-forcats.html",
    "id": "lollipop-plots",
    "chapter": " 9 Factors with forcats",
    "heading": "9.2.2 Lollipop Plots",
    "text": "Lollipop plots popular alternative bar plots often look cleaner less ink. make lollipop plot R, specify two different geoms: geom_segment() form stick lollipop geom_point() form pop part lollipop. geom_segment() requires 4 aesthetics: x, xend, y, yend.fct_reorder() also works boxplots simple point plots show, example, median response level factor. following set plots investigate Defense stat changes different Pokemon typesThe following code makes point plot shows median defense type instead boxplots.Finally, can make lollipop plot median defense.preference boxplot graph, point plot, lollipop plot?New Data. gun_violence_us.csv data set obtained https://www.openintro.org/book/statdata/index.php?data=gun_violence_us contains following variables gun violence 2014:state, name U.S. statemortality_rate, number deaths gun violence per 100,000 peopleownership_rate, proportion adults gunregion, region U.S. (South, West, NE, MW)",
    "code": "\nggplot(data = pokemon_sum, aes(x = Type_ordered,\n                               y = count_type)) +\n  geom_segment(aes(x = Type_ordered, xend = Type_ordered,\n                   y = 0, yend = count_type)) +\n  geom_point() +\n  coord_flip()\npokemon_long <- pokemon_long |>\n  filter(!is.na(Type)) |>\n  mutate(Type_Deford = fct_reorder(.f = Type, .x = Defense,\n                                   .fun = median))\nggplot(data = pokemon_long, aes(x = Type_Deford,\n                               y = Defense)) +\n  geom_boxplot() + \n  coord_flip()\npokemon_med <- pokemon_long |> group_by(Type_Deford) |>\n  summarise(med_def = median(Defense)) |>\n  mutate(Type_Deford = fct_reorder(.f = Type_Deford, .x = med_def,\n                                   .fun = median))\n\nggplot(data = pokemon_med, aes(x = med_def, y = Type_Deford)) +\n  geom_point()\nggplot(data = pokemon_med, aes(x = Type_Deford, y = med_def)) +\n  geom_segment(aes(xend = Type_Deford, y = 0, yend = med_def)) +\n  geom_point() +\n  coord_flip()\nmortality_df <- read_csv(here(\"data/gun_violence_us.csv\")) |>\n  mutate(region = factor(region))"
  },
  {
    "path": "factors-with-forcats.html",
    "id": "re-leveling-by-two-quantitative-variables-with-fct_reorder2",
    "chapter": " 9 Factors with forcats",
    "heading": "9.2.3 Re-Leveling By Two Quantitative Variables with fct_reorder2()",
    "text": "Suppose want investigate relationship mortality_rate ownership_rate using data set. Run following code create scatterplot mortality_rate vs. ownership_rate fitted linear regression lines region United States:Notice order levels legend. people prefer order actually match lines plot end, order alphabetical. achieve , can use fct_reorder2() change order factor levels:change order levels expect? fct_reorder2() actually looks points, lines, determining ordering. want levels match exactly, ’ll reorder levels manually fct_relevel():",
    "code": "\nggplot(data = mortality_df,\n       aes(x = ownership_rate, y = mortality_rate, colour = region)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\nmortality_df <- mortality_df |>\n  mutate(region_2 = fct_reorder2(region,\n                                 .x = ownership_rate,\n                                 .y = mortality_rate))\nggplot(data = mortality_df,\n       aes(x = ownership_rate, y = mortality_rate, colour = region_2)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")"
  },
  {
    "path": "factors-with-forcats.html",
    "id": "reordering-levels-manually-with-fct_relevel",
    "chapter": " 9 Factors with forcats",
    "heading": "9.2.4 Reordering Levels Manually with fct_relevel()",
    "text": "Factors ordered alphabetically default. want precise control order levels factor, can use fct_relevel(), takes factor vector new levels inputs:Reordering levels factor manually might also useful fitting linear models. Recall , default, R makes reference group linear model first level alphabetically. ’d like different reference group, can reorder levels factor:",
    "code": "\nmortality_df <- mortality_df |>\n  mutate(region_3 = fct_relevel(region, c(\"South\", \"West\", \"MW\", \"NE\")))\nggplot(data = mortality_df,\n       aes(x = ownership_rate, y = mortality_rate, colour = region_3)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\nmod <- lm(mortality_rate ~ ownership_rate + region, data = mortality_df)\nmod2 <- lm(mortality_rate ~ ownership_rate + region_2, data = mortality_df)\nmod3 <- lm(mortality_rate ~ ownership_rate + region_3, data = mortality_df)\nsummary(mod)\nsummary(mod2)\nsummary(mod3)"
  },
  {
    "path": "factors-with-forcats.html",
    "id": "exercise-7-2",
    "chapter": " 9 Factors with forcats",
    "heading": "9.2.5 Exercises",
    "text": "Make side--side boxplots pokemon data use ungroup() running following code.aren’t types ordered median defense anymore?.fun argument fct_reorder() controls Type factor ordered. Change specify ordering mean, max, min. ordering makes sense? ?",
    "code": "\npokemon_nodup <- pokemon_df |> group_by(`#`) |> slice(1) ## |>\n  ## ungroup()\npokemon_long <- pokemon_nodup |>\n  pivot_longer(c(`Type 1`, `Type 2`),\n               names_to = \"Number\",\n               values_to = \"Type\")\n\npokemon_long <- pokemon_long |>\n  filter(!is.na(Type)) |>\n  mutate(Type_Deford = fct_reorder(.f = Type, .x = Defense,\n                                   .fun = median))\nggplot(data = pokemon_long, aes(x = Type_Deford,\n                               y = Defense)) +\n  geom_boxplot() + \n  coord_flip()"
  },
  {
    "path": "factors-with-forcats.html",
    "id": "chapexercise-7",
    "chapter": " 9 Factors with forcats",
    "heading": "9.3 Chapter Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 9.4.use general social survey data set, forcats library R. Wikipedia page better understand data comes Wikipedia.variables self-explanatory, couple aren’t :partyid, political leaning anddenom, religious denomination (unfamiliar , can think “specific” subset particular religion).Note exercises R Data Science textbook.Load data set * Using forcats function, change name level str republican Weak republican change name level str democrat Weak democrat. names closely match levels Strong republican Strong democrat. , create table counts shows number respondents political party partyid.Note: Levels aren’t specified forcats function change.Note 2: naming something Weak republican, ’ll need use backticks since space level name.* Use forcats function partyid just 4 categories: (corresponding answer, Don’t know, party), Ind (corresponding Ind,near rep, Independent, Ind, near dem), Rep (corresponding Strong republican str republican), Dem (corresponding str democrat Strong democrat).* Use forcats function partyid just 4 categories: (corresponding answer, Don’t know, party), Ind (corresponding Ind,near rep, Independent, Ind, near dem), Rep (corresponding Strong republican str republican), Dem (corresponding str democrat Strong democrat).* Run code create following plot shows average number hours television people watch various religions.* Run code create following plot shows average number hours television people watch various religions., use forcats function create new variable data set reorders religion factor levels make lollipop plot religion watches television, average, top, religion watches least television, average, bottom.* Run code make following line plot shows age x-axis, proportion y-axis, coloured various marital statuses (married, divorced, widowed, etc.):, use forcats function make plot legend labels line better different coloured marital status lines (e.g. label widowed first appears legend, label married second, etc.).haven’t talked much creating two-way tables (contingency tables). generally quite difficult make tidyverse functions, can use base R table() prop.table() functions make .Using data year 2014, run following code make 4 two-way tables party_small variable constructed earlier race:Use help ?prop.table figure three tables constructed.table think informative? conclusions help draw?",
    "code": "\nlibrary(tidyverse)\ngss_cat\nrelig_summary <- gss_cat |>\n  group_by(relig) |>\n  summarise(\n    age = mean(age, na.rm = TRUE),\n    tvhours = mean(tvhours, na.rm = TRUE),\n    n = n()\n  )\n\nggplot(data = relig_summary, aes(tvhours, relig)) +\n  geom_point()\nby_age <- gss_cat |>\n  filter(!is.na(age)) |>\n  count(age, marital) |>\n  group_by(age) |>\n  mutate(prop = n / sum(n))\n\nggplot(by_age, aes(age, prop,\n                  colour = marital)) +\n  geom_line(na.rm = TRUE) +\n  labs(colour = \"marital\")\ngss_cat <- gss_cat |> mutate(party_small = fct_collapse(partyid,\n                                              Other = c(\"No answer\", \"Don't know\", \"Other party\"),\n                                              Ind = c(\"Ind,near rep\", \"Independent\", \"Ind,near dem\"),\n                                              Rep = c(\"Strong republican\", \"Not str republican\"),\n                                              Dem = c(\"Not str democrat\", \"Strong democrat\")))\n\ngss_recent <- gss_cat |> filter(year == 2014)\n\ntab1 <- table(gss_recent$party_small, gss_recent$race)\ntab1\n#>        \n#>         Other Black White Not applicable\n#>   Other     8    12    68              0\n#>   Rep      22    17   498              0\n#>   Ind     152   108   828              0\n#>   Dem      80   249   496              0\nprop.table(tab1)\n#>        \n#>               Other       Black       White Not applicable\n#>   Other 0.003152088 0.004728132 0.026792750    0.000000000\n#>   Rep   0.008668243 0.006698188 0.196217494    0.000000000\n#>   Ind   0.059889677 0.042553191 0.326241135    0.000000000\n#>   Dem   0.031520883 0.098108747 0.195429472    0.000000000\nprop.table(tab1, margin = 1)\n#>        \n#>              Other      Black      White Not applicable\n#>   Other 0.09090909 0.13636364 0.77272727     0.00000000\n#>   Rep   0.04096834 0.03165736 0.92737430     0.00000000\n#>   Ind   0.13970588 0.09926471 0.76102941     0.00000000\n#>   Dem   0.09696970 0.30181818 0.60121212     0.00000000\nprop.table(tab1, margin = 2)\n#>        \n#>              Other      Black      White Not applicable\n#>   Other 0.03053435 0.03108808 0.03597884               \n#>   Rep   0.08396947 0.04404145 0.26349206               \n#>   Ind   0.58015267 0.27979275 0.43809524               \n#>   Dem   0.30534351 0.64507772 0.26243386"
  },
  {
    "path": "factors-with-forcats.html",
    "id": "solutions-7",
    "chapter": " 9 Factors with forcats",
    "heading": "9.4 Exercise Solutions",
    "text": "",
    "code": ""
  },
  {
    "path": "factors-with-forcats.html",
    "id": "change-factor-levels-s",
    "chapter": " 9 Factors with forcats",
    "heading": "9.4.1 Change Factor Levels S",
    "text": "* properly explore data set making graphs , , fact, double counting Pokemon data set (another example familiar data set ’re working advantageous: people familiar Pokemon know fewer 947 Pokemon Generations 1 6).Figure Pokemon double counted. , create new data set keeps one observation per Pokemon #.",
    "code": "\npokemon_nodup <- pokemon_df |> group_by(`#`) |> slice(1) |>\n  ungroup()"
  },
  {
    "path": "factors-with-forcats.html",
    "id": "reorder-factor-levels-s",
    "chapter": " 9 Factors with forcats",
    "heading": "9.4.2 Reorder Factor Levels S",
    "text": "",
    "code": ""
  },
  {
    "path": "factors-with-forcats.html",
    "id": "chapexercise-7-S",
    "chapter": " 9 Factors with forcats",
    "heading": "9.4.3 Chapter Exercises S",
    "text": "* Using forcats function, change name level str republican Weak republican change name level str democrat Weak democrat. names closely match levels Strong republican Strong democrat. , create table counts shows number respondents political party partyid.Note: Levels aren’t specified forcats function change.Note 2: naming something Weak republican, ’ll need use backticks since space level name.* Use forcats function partyid just 4 categories: (corresponding answer, Don’t know, party), Ind (corresponding Ind,near rep, Independent, Ind, near dem), Rep (corresponding Strong republican str republican), Dem (corresponding str democrat Strong democrat).* Run code create following plot shows average number hours television people watch various religions., use forcats function create new variable data set reorders religion factor levels remake barplot religion watches television, average, top, religion watches least television, average, bottom.* Run code make following line plot shows age x-axis, proportion y-axis, coloured various marital statuses (married, divorced, widowed, etc.):, use forcats function make plot legend labels line better different coloured marital status lines (e.g. label widowed first appears legend, label married second, etc.).",
    "code": "\ngss_cat |>\n  mutate(partyid_new = fct_recode(partyid,\n                                  `Weak republican` = \"Not str republican\",\n                                  `Weak democrat` = \"Not str democrat\")) |> group_by(partyid_new) |>\n  summarise(ncount = n())\ngss_cat <- gss_cat |> mutate(party_small = fct_collapse(partyid,\n                                              Other = c(\"No answer\", \"Don't know\", \"Other party\"),\n                                              Ind = c(\"Ind,near rep\", \"Independent\", \"Ind,near dem\"),\n                                              Rep = c(\"Strong republican\", \"Not str republican\"),\n                                              Dem = c(\"Not str democrat\", \"Strong democrat\")))\nrelig_summary <- gss_cat |>\n  group_by(relig) |>\n  summarise(\n    age = mean(age, na.rm = TRUE),\n    tvhours = mean(tvhours, na.rm = TRUE),\n    n = n()\n  )\n\nggplot(data = relig_summary, aes(tvhours, relig)) +\n  geom_point()\nrelig_summary <- relig_summary |>\n  mutate(relig = fct_reorder(relig, tvhours))\nggplot(data = relig_summary, aes(x = relig, y = tvhours)) +\n  geom_segment(aes(x = relig, xend = relig, y = 0, yend = tvhours)) +\n  geom_point() +\n  coord_flip()\nby_age <- gss_cat |>\n  filter(!is.na(age)) |>\n  count(age, marital) |>\n  group_by(age) |>\n  mutate(prop = n / sum(n))\n\nggplot(by_age, aes(age, prop,\n                  colour = marital)) +\n  geom_line(na.rm = TRUE) +\n  labs(colour = \"marital\")\nby_age2 <- by_age |> ungroup() |>\n  mutate(marital2 = fct_reorder2(marital, .x = age, .y = prop))\nggplot(by_age2, aes(age, prop,\n                  colour = marital2)) +\n  geom_line(na.rm = TRUE) +\n  labs(colour = \"marital\") +\n  scale_colour_viridis_d()"
  },
  {
    "path": "factors-with-forcats.html",
    "id": "rcode-7",
    "chapter": " 9 Factors with forcats",
    "heading": "9.5 Non-Exercise R Code",
    "text": "",
    "code": "\nlibrary(tidyverse)\nlibrary(here)\npokemon_df <- read_csv(here(\"data/pokemon_allgen.csv\")) |>\n  mutate(Generation_cat = factor(Generation))\npokemon_df |> group_by(`Type 1`) |>\n  summarise(counttype = n())\npokemon_legend <- pokemon_df |> filter(Legendary == TRUE) |>\n  group_by(Generation_cat) |>\n  summarise(nlegend = n())\nggplot(data = pokemon_legend, aes(x = Generation_cat, y = nlegend)) +\n  geom_col()\npokemon_legend <- pokemon_legend |>\n  mutate(Generation_cat2 = fct_recode(Generation_cat, Kanto = \"1\",\n                                      Johto = \"2\", Hoenn = \"3\",\n                                      Sinnoh = \"4\", Unova = \"5\",\n                                      Kalos = \"6\")) |>\n  select(Generation_cat2, everything())\nhead(pokemon_legend)\nggplot(data = pokemon_legend,\n       aes(x = Generation_cat2, y = nlegend)) +\n  geom_col()\npokemon_long <- pokemon_df |> pivot_longer(c(`Type 1`, `Type 2`),\n                            names_to = \"Number\",\n                            values_to = \"Type\")\npokemon_long |>\n  mutate(new_type = fct_collapse(Type, Coolest = c(\"Ice\", \"Dark\"),\n                                 Least_Cool = c(\"Fire\", \"Fighting\", \"Poison\"))) |>\n  select(new_type, Type, everything())\npokemon_nodup <- pokemon_df |> group_by(`#`) |> slice(1) |>\n  ungroup()\npokemon_long <- pokemon_nodup |>\n  pivot_longer(c(`Type 1`, `Type 2`),\n               names_to = \"Number\",\n               values_to = \"Type\")\npokemon_sum <- pokemon_long |>\n  group_by(Type) |>\n  summarise(count_type = n()) |>\n  filter(!is.na(Type))\nggplot(data = pokemon_sum, aes(x = Type,\n                               y = count_type)) +\n  geom_col() +\n  coord_flip()  ## flips the x and y axes\npokemon_sum <- pokemon_sum |> \n  mutate(Type_ordered = fct_reorder(.f = Type, .x = count_type))\nggplot(data = pokemon_sum, aes(x = Type_ordered,\n                               y = count_type)) +\n  geom_col() +\n  coord_flip()\npokemon_long <- pokemon_long |>\n  filter(!is.na(Type)) |>\n  mutate(Type_Deford = fct_reorder(.f = Type, .x = Defense,\n                                   .fun = median))\nggplot(data = pokemon_long, aes(x = Type_Deford,\n                               y = Defense)) +\n  geom_boxplot() + \n  coord_flip()\npokemon_med <- pokemon_long |> group_by(Type_Deford) |>\n  summarise(med_def = median(Defense)) |>\n  mutate(Type_Deford = fct_reorder(.f = Type_Deford, .x = med_def,\n                                   .fun = median))\n\nggplot(data = pokemon_med, aes(x = med_def, y = Type_Deford)) +\n  geom_point()\nmortality_df <- read_csv(here(\"data/gun_violence_us.csv\")) |>\n  mutate(region = factor(region))\nggplot(data = mortality_df,\n       aes(x = ownership_rate, y = mortality_rate, colour = region)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\nmortality_df <- mortality_df |>\n  mutate(region_2 = fct_reorder2(region,\n                                 .x = ownership_rate,\n                                 .y = mortality_rate))\nggplot(data = mortality_df,\n       aes(x = ownership_rate, y = mortality_rate, colour = region_2)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\nmortality_df <- mortality_df |>\n  mutate(region_3 = fct_relevel(region, c(\"South\", \"West\", \"MW\", \"NE\")))\nggplot(data = mortality_df,\n       aes(x = ownership_rate, y = mortality_rate, colour = region_3)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\nmod <- lm(mortality_rate ~ ownership_rate + region, data = mortality_df)\nmod2 <- lm(mortality_rate ~ ownership_rate + region_2, data = mortality_df)\nmod3 <- lm(mortality_rate ~ ownership_rate + region_3, data = mortality_df)\nsummary(mod)\nsummary(mod2)\nsummary(mod3)"
  },
  {
    "path": "data-ethics.html",
    "id": "data-ethics",
    "chapter": " 10 Data Ethics",
    "heading": " 10 Data Ethics",
    "text": "Goals:explain data ethics important issue data science using couple examples.describe issues data privacy explain , just data doesn’t individual’s name doesn’t necessarily make data truly anonymous.explain difference hypothesis confirmation hypothesis exploration distinction matters.",
    "code": ""
  },
  {
    "path": "data-ethics.html",
    "id": "ethical-examples",
    "chapter": " 10 Data Ethics",
    "heading": "10.1 Ethical Examples",
    "text": "’ve tried interweave issues ethics throughout many examples used already course, purpose section put data ethics direct focus.questions consider data collected, especially data collected human subjects:gets use data purposes?gets use data purposes?collected data organization conflicts interest?collected data organization conflicts interest?presentation analysis harmful particular person group people? benefits analysis?presentation analysis harmful particular person group people? benefits analysis?subjects data collection procedure treated respectfully given consent information collected?\nconsent needed ? example, looked data professional athletes. need provide consent consent inherent spotlight?\n’ve also scraped data SLU’s athletics website look data pertaining ! ethical? line wouldn’t cross pertaining data collected named, individual people?\nsubjects data collection procedure treated respectfully given consent information collected?consent needed ? example, looked data professional athletes. need provide consent consent inherent spotlight?consent needed ? example, looked data professional athletes. need provide consent consent inherent spotlight?’ve also scraped data SLU’s athletics website look data pertaining ! ethical? line wouldn’t cross pertaining data collected named, individual people?’ve also scraped data SLU’s athletics website look data pertaining ! ethical? line wouldn’t cross pertaining data collected named, individual people?",
    "code": ""
  },
  {
    "path": "data-ethics.html",
    "id": "exercise-9-1",
    "chapter": " 10 Data Ethics",
    "heading": "10.1.1 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 10.5.Read Sections 8.1 - 8.3 Modern Data Science R. , write one paragraph summary reading might pertain way use interpret data.Read Sections 8.1 - 8.3 Modern Data Science R. , write one paragraph summary reading might pertain way use interpret data.Data Feminism related data ethics, though two terms certainly synonymous. Recently, Catherine D’Ignazio Lauren F. Klein published book called Data Feminism https://datafeminism.io/ Data Feminism related data ethics, though two terms certainly synonymous. Recently, Catherine D’Ignazio Lauren F. Klein published book called Data Feminism https://datafeminism.io/ Read following blog post Data Feminism, focusing section Missing Data. https://teachdatascience.com/datafem/ .Pick one example bulleted list write 2 sentence explanation explains might important acknowledge missing data analysis.Choose 1 following two articles readhttps://www.theguardian.com/world/2017/sep/08/ai-gay-gaydar-algorithm-facial-recognition-criticism-stanford  use data LGBTQIA+ communityhttps://towardsdatascience.com/5-steps--take---antiracist-data-scientist-89712877c214  anti-racist data practices.LGBTQIA+ article, write two sentence summary side argument research facial recognition software identify members LGBTQ+ community occur, even viewpoint isn’t ., write two sentence summary side argument research facial recognition software identify members LGBTQ+ community okay long results used responsibly, even viewpoint isn’t .anti-racist data science article, Step 2, pick News Article read first paragraphs. Describe, 2-3 sentences, article’s example bias incidence bias matters.",
    "code": ""
  },
  {
    "path": "data-ethics.html",
    "id": "data-privacy",
    "chapter": " 10 Data Ethics",
    "heading": "10.2 Data Privacy",
    "text": "Related data ethics idea data privacy.data private data public? examples, may seem obvious, others (e.g. data government agency collects data people), answer might clear cut.anonymous data truly anonymous?type consent provided collecting data someone?explore issues following exercises.",
    "code": ""
  },
  {
    "path": "data-ethics.html",
    "id": "exercise-9-2",
    "chapter": " 10 Data Ethics",
    "heading": "10.2.1 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 10.5.Recall course evaluations data set, used Mini-Project 2. might obvious, course evaluations course evaluations last year, felt ethically okay share . , data privacy always cut--dry issue. Consider following course evaluation formats, think whether consider ethically okay share evaluation information :gave course averages, also give PDFs student’s written responses. PDFs anonymous, student’s sex, year, whether took course Major, Minor, Distribution Requirement, etc. Assume also can obtain class roster class.gave course averages, also give PDFs student’s written responses. PDFs anonymous, student’s sex, year, whether took course Major, Minor, Distribution Requirement, etc. Assume also can obtain class roster class.gave course averages PDFs (), also give grade student received course PDF list responses (still anonymous can still obtain class roster).gave course averages PDFs (), also give grade student received course PDF list responses (still anonymous can still obtain class roster).Another professor SLU posts evaluation averages table personal website. scrape data table give , along professor’s name courses. don’t ask permission, website tables public.Another professor SLU posts evaluation averages table personal website. scrape data table give , along professor’s name courses. don’t ask permission, website tables public.Suppose collect data students Data Science class. setting () (d), suppose give data set following variables collected student class. option, , ethically okay share data students class.current grade time spent R Studio servercurrent grade time spent R Studio servercurrent grade, class year, whether student stat majorcurrent grade, class year, whether student stat majorfavorite R package, whether student took STAT 213, whether student took CS 140, Majorfavorite R package, whether student took STAT 213, whether student took CS 140, Majorfavorite R package, whether student took STAT 213, whether student took CS 140, current grade coursefavorite R package, whether student took STAT 213, whether student took CS 140, current grade courseHow anonymous SLU’s course evaluations? -class activity investigate .",
    "code": ""
  },
  {
    "path": "data-ethics.html",
    "id": "hypothesis-generation-vs.-confirmation",
    "chapter": " 10 Data Ethics",
    "heading": "10.3 Hypothesis Generation vs. Confirmation",
    "text": "focused hypothesis generation data sets particular course. Read following two articles explain difference hypothesis generation hypothesis confirmation:Read following two short articles, one textbook one another source:https://r4ds..co.nz/model-intro.html#hypothesis-generation-vs.-hypothesis-confirmationhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC6718169/",
    "code": ""
  },
  {
    "path": "data-ethics.html",
    "id": "exercise-9-3",
    "chapter": " 10 Data Ethics",
    "heading": "10.3.1 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 10.5.Explain difference hypothesis generation hypothesis confirmation.Explain difference hypothesis generation hypothesis confirmation.many times can use single observation hypothesis generation? hypothesis confirmation?many times can use single observation hypothesis generation? hypothesis confirmation?following questions, pertaining someone’s fitness, sound suitable answered Hypothesis Exploration? Hypothesis Confirmation?following questions, pertaining someone’s fitness, sound suitable answered Hypothesis Exploration? Hypothesis Confirmation?want know , average, person exercises weekends weekdays, questions interest.want know , average, person exercises weekends weekdays, questions interest.want look general trends person’s step count try determine various events influenced step count.want look general trends person’s step count try determine various events influenced step count.want know person exercises winter summer, also like investigate seasonal trends.want know person exercises winter summer, also like investigate seasonal trends.Note: Prediction different hypothesis confirmation, typically don’t really care variables associated response. want model gives “best” predictions. , goal prediction, typically lot freedom many times can “use” single observation. talk little prediction later semester.",
    "code": ""
  },
  {
    "path": "data-ethics.html",
    "id": "chapexercise-9",
    "chapter": " 10 Data Ethics",
    "heading": "10.4 Chapter Exercises",
    "text": "chapter exercises chapter.",
    "code": ""
  },
  {
    "path": "data-ethics.html",
    "id": "solutions-9",
    "chapter": " 10 Data Ethics",
    "heading": "10.5 Exercise Solutions",
    "text": "exercise solutions chapter.",
    "code": ""
  },
  {
    "path": "data-import.html",
    "id": "data-import",
    "chapter": " 11 Data Import",
    "heading": " 11 Data Import",
    "text": "Goals:Use readr read data R .csv, .txt, .tsv files.Use readr read data R .csv, .txt, .tsv files.Use rvest scrape data public websites.Use rvest scrape data public websites.Use jsonlite read data JSON (Java Script Object Notation) files.Use jsonlite read data JSON (Java Script Object Notation) files.",
    "code": ""
  },
  {
    "path": "data-import.html",
    "id": "readr-to-read-in-data",
    "chapter": " 11 Data Import",
    "heading": "11.1 readr to Read in Data",
    "text": "now, mostly worked data “R Ready”: meaning nice .csv file read R easily read_csv() readr package. begin looking options read_csv() function move formats .csv data commonly stored .",
    "code": ""
  },
  {
    "path": "data-import.html",
    "id": "read_csv-options",
    "chapter": " 11 Data Import",
    "heading": "11.1.1 read_csv() Options",
    "text": "mtcarsex.csv observations different car models variables include things like gas mileage, number cylinders, etc. Read mtcarsex.csv data set following code. , examine data set head().notice data set seems odd? Open .csv file Excel program examine data set outside R.Type ?read_csv bottom-left window look options read_csv(). particular, use na skip arguments fix reading.Let’s start skip aren’t reading first two rows data set:looks better, still couple problems. notice?Go help read na argument. Let’s add option fix missing value issue.Now look classes variable. classes look like incorrect?’ve talked re-specify classes variables using mutate() .factor() .Date() .numeric() functions, sometimes ’s easier just respecify class reading data. Notice , use read_csv(), R gives us message column types. actually argument read_csv() called col_types. can add |> spec() piping statement read_csv() statement tell R print col_types ’s easy us copy paste read_csv() change classes.example, notice cyl = col_double() changed cyl = col_factor() code chunk :Finally, two rows missing values. aren’t providing anything useful can slice() :many possible file formats data storage. example, data set called oscars.tsv, tab-separated file. can read read_tsv() instead read_csv().’ll able work .txt files Excel files Exercises. Check https://rawgit.com/rstudio/cheatsheets/master/data-import.pdf data import cheatsheet.final issue discuss section occurs data set units within cells. Consider earlier example used reprex section:parse_number() function really useful just want number (commas, units, etc.). function often paired mutate() since creating new variable:",
    "code": "\nlibrary(tidyverse)\nlibrary(here)\ncars_df <- read_csv(here(\"data/mtcarsex.csv\"))\nhead(cars_df)\n#> # A tibble: 6 × 11\n#>   This is …¹ ...2  ...3  ...4  ...5  ...6  ...7  ...8  ...9 \n#>   <chr>      <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr>\n#> 1 \"I'm a na… <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \n#> 2 \"mpg\"      cyl   disp  hp    drat  wt    qsec  vs    am   \n#> 3  <NA>      <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \n#> 4  <NA>      <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \n#> 5 \"-999\"     6     160   110   3.9   2.62  16.46 0     1    \n#> 6 \"21\"       6     160   110   3.9   2.875 17.02 0     1    \n#> # … with 2 more variables: ...10 <chr>, ...11 <chr>, and\n#> #   abbreviated variable name\n#> #   ¹​`This is a data set about cars.`\n#> # ℹ Use `colnames()` to see all variable names\ncars_df <- read_csv(here(\"data/mtcarsex.csv\"), skip = 2)\n## first two lines will be skipped\nhead(cars_df)\n#> # A tibble: 6 × 11\n#>      mpg   cyl  disp    hp  drat    wt  qsec    vs    am\n#>    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1   NA      NA    NA    NA NA    NA     NA      NA    NA\n#> 2   NA      NA    NA    NA NA    NA     NA      NA    NA\n#> 3 -999       6   160   110  3.9   2.62  16.5     0     1\n#> 4   21       6   160   110  3.9   2.88  17.0     0     1\n#> 5   22.8     4   108    93  3.85  2.32  18.6     1     1\n#> 6   21.4     6   258   110  3.08  3.22  19.4     1     0\n#> # … with 2 more variables: gear <dbl>, carb <dbl>\n#> # ℹ Use `colnames()` to see all variable names\ncars_df <- read_csv(here(\"data/mtcarsex.csv\"), na = c(NA, \"-999\"), skip = 2)\nhead(cars_df)\n#> # A tibble: 6 × 11\n#>   mpg   cyl   disp  hp    drat  wt    qsec  vs    am   \n#>   <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr>\n#> 1 NA    NA    NA    NA    NA    NA    NA    NA    NA   \n#> 2 NA    NA    NA    NA    NA    NA    NA    NA    NA   \n#> 3 <NA>  6     160   110   3.9   2.62  16.46 0     1    \n#> 4 21    6     160   110   3.9   2.875 17.02 0     1    \n#> 5 22.8  4     108   93    3.85  2.32  18.61 1     1    \n#> 6 21.4  6     258   110   3.08  3.215 19.44 1     0    \n#> # … with 2 more variables: gear <chr>, carb <chr>\n#> # ℹ Use `colnames()` to see all variable names\nread_csv(here(\"data/mtcarsex.csv\"), na = c(NA, \"-999\"), skip = 2) |>\n  spec()\n#> Rows: 34 Columns: 11\n#> ── Column specification ────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (11): mpg, cyl, disp, hp, drat, wt, qsec, vs, am, ge...\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n#> cols(\n#>   mpg = col_character(),\n#>   cyl = col_character(),\n#>   disp = col_character(),\n#>   hp = col_character(),\n#>   drat = col_character(),\n#>   wt = col_character(),\n#>   qsec = col_character(),\n#>   vs = col_character(),\n#>   am = col_character(),\n#>   gear = col_character(),\n#>   carb = col_character()\n#> )\ncars_df <- read_csv(here(\"data/mtcarsex.csv\"), na = c(NA, \"-999\"), skip = 2,\n  col_types = cols(\n  mpg = col_double(),\n  cyl = col_factor(),\n  disp = col_double(),\n  hp = col_double(),\n  drat = col_double(),\n  wt = col_double(),\n  qsec = col_double(),\n  vs = col_factor(),\n  am = col_double(),\n  gear = col_double(),\n  carb = col_double()\n))\ncars_df <- read_csv(here(\"data/mtcarsex.csv\"), na = c(NA, \"-999\"), skip = 2,\n  col_types = cols(\n  mpg = col_double(),\n  cyl = col_factor(),\n  disp = col_double(),\n  hp = col_double(),\n  drat = col_double(),\n  wt = col_double(),\n  qsec = col_double(),\n  vs = col_factor(),\n  am = col_double(),\n  gear = col_double(),\n  carb = col_double()\n)) |>\n  slice(-(1:2))\nhead(cars_df)\n#> # A tibble: 6 × 11\n#>     mpg cyl    disp    hp  drat    wt  qsec vs       am\n#>   <dbl> <fct> <dbl> <dbl> <dbl> <dbl> <dbl> <fct> <dbl>\n#> 1  NA   6       160   110  3.9   2.62  16.5 0         1\n#> 2  21   6       160   110  3.9   2.88  17.0 0         1\n#> 3  22.8 4       108    93  3.85  2.32  18.6 1         1\n#> 4  21.4 6       258   110  3.08  3.22  19.4 1         0\n#> 5  NA   8       360   175  3.15  3.44  17.0 0         0\n#> 6  18.1 6       225   105  2.76  3.46  20.2 1         0\n#> # … with 2 more variables: gear <dbl>, carb <dbl>\n#> # ℹ Use `colnames()` to see all variable names\noscars_df <- read_tsv(here(\"data/oscars.tsv\"))\nhead(oscars_df)\n#> # A tibble: 6 × 51\n#>   FilmName    Oscar…¹ Durat…² Rating Direc…³ Direc…⁴ Oscar…⁵\n#>   <chr>         <dbl>   <dbl>  <dbl> <chr>     <dbl>   <dbl>\n#> 1 Crash          2006     113      4 Haggis        0       1\n#> 2 Brokeback …    2006     134      4 Lee           0       0\n#> 3 Capote         2006     114      4 Miller        0       0\n#> 4 Good Night…    2006      93      2 Clooney       0       0\n#> 5 Munich         2006     164      4 Spielb…       0       0\n#> 6 The Depart…    2007     151      4 Scorse…       0       1\n#> # … with 44 more variables: GenreName <chr>,\n#> #   Genre_Drama <dbl>, Genre_Bio <dbl>, CountryName <chr>,\n#> #   ForeignandUSA <dbl>, ProductionName <chr>,\n#> #   ProductionCompany <dbl>, BudgetRevised <chr>,\n#> #   Budget <chr>, DomesticBoxOffice <dbl>,\n#> #   WorldwideRevised <dbl>, WorldwideBoxOffice <dbl>,\n#> #   DomesticPercent <dbl>, LimitedOpeningWnd <dbl>, …\n#> # ℹ Use `colnames()` to see all variable names\ntest_df <- read_csv(here(\"data/parsedf.csv\"))\nhead(test_df)\n#> # A tibble: 3 × 2\n#>   x                   y\n#>   <chr>           <dbl>\n#> 1 20,000 dollars      1\n#> 2 40 dollars          2\n#> 3 only 13 dollars     3\ntest_df |> mutate(x2 = parse_number(x))\n#> # A tibble: 3 × 3\n#>   x                   y    x2\n#>   <chr>           <dbl> <dbl>\n#> 1 20,000 dollars      1 20000\n#> 2 40 dollars          2    40\n#> 3 only 13 dollars     3    13"
  },
  {
    "path": "data-import.html",
    "id": "exercise-8-2",
    "chapter": " 11 Data Import",
    "heading": "11.1.2 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 11.5.* birthdays.txt file information birthdays various animals Animal Crossing island. also columns Animal’s Name, Animal Type, long animal lived island (weeks). Click file open look format data.Start following code chunk use options read_delim() read data (?read_delim). delim argument ’s already provided specifies delimiter (separator) ’ll use -, opposed , example, , .csv file. Arguments may need change includeskipcol_namesnatrim_wscol_types* Another common format data stored Excel file. Often, ’s easiest just save Excel file .csv file read using read_csv(). , sometimes route can difficult (example, Excel file thousands sheets). read directly Excel, ’ll need install readxl install.packages(\"readxl\"). installed, load package library(readxl), read first sheet evals_prof.xlsx data set, data set used Project 2, read_excel() function.* Another common format data stored Excel file. Often, ’s easiest just save Excel file .csv file read using read_csv(). , sometimes route can difficult (example, Excel file thousands sheets). read directly Excel, ’ll need install readxl install.packages(\"readxl\"). installed, load package library(readxl), read first sheet evals_prof.xlsx data set, data set used Project 2, read_excel() function.* Now, read second sheet Excel file, using help file ?read_excel change one arguments.* Now, read second sheet Excel file, using help file ?read_excel change one arguments.",
    "code": "\nlibrary(tidyverse)\ndf <- read_delim(here(\"data/birthdays.txt\"), delim = \" - \")\nhead(df)"
  },
  {
    "path": "data-import.html",
    "id": "data-scraping-with-rvest",
    "chapter": " 11 Data Import",
    "heading": "11.2 Data Scraping with rvest",
    "text": "Sometimes, might want data public website isn’t provided file format. obtain data, ’ll need use web scraping, term just means “getting data website.” easiest way R rvest package. Note spend entire semester talking web scraping, focus websites scraping data “easy” won’t give us major errors.Go following website suppose wanted table gun violence statistics R: https://en.wikipedia.org/wiki/Gun_violence_in_the_United_States_by_state. try copy-pasting table Excel reading data set read_excel(). Depending format table, strategy may work may . Another way scrape directly rvest. Additionally, website continually updates (standings sports league, enrollment data school, best-selling products company, etc.), scraping much convenient, don’t need continually copy-paste updated data.following code chunk, read_html() reads entire html file url provided html_nodes() extracts tables website.’ll see , example, 3 tables provided. tables stored list can reference first table using [[1]], second table using [[2]], etc. purposes class, figure 3 tables one actually want using trial error.html_table() function converts table data.frame object.3 tables one want use analysis gun violence United States?another example, consider scraping data SLU’s athletics page. particular, suppose want analysis SLU’s baseball team.Go following website look table data want scrape: https://saintsathletics.com/sports/baseball/stats/2021.looking website, use following code scrape data set.’s now 72 different tables! See can figure first tables coming website.",
    "code": "\nlibrary(tidyverse)\nlibrary(rvest)\n\n## provide the URL and name it something (in this case, url).\nurl <- \"https://en.wikipedia.org/wiki/Gun_violence_in_the_United_States_by_state\"\n\n## convert the html code into something R can read\nh <- read_html(url)\n\n## grabs the tables\ntab <- h |> html_nodes(\"table\")\ntest1 <- tab[[1]] |> html_table()\ntest2 <- tab[[2]] |> html_table()\ntest3 <- tab[[3]] |> html_table()\n\nhead(test1)\nhead(test2)\nhead(test3)\nurl <- \"https://saintsathletics.com/sports/baseball/stats/2021\"\nh <- read_html(url)\ntab <- h |> html_nodes(\"table\")\ntab\nobj <- tab[[1]] |> html_table(fill = TRUE)\nhead(obj)\ntail(obj)\nobj2 <- tab[[2]] |> html_table(fill = TRUE)\nhead(obj2)\ntail(obj2)"
  },
  {
    "path": "data-import.html",
    "id": "exercise-8-3",
    "chapter": " 11 Data Import",
    "heading": "11.2.1 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 11.5.Choose topic/person/place/etc. interests tables Wikipedia scrape table related topic.Choose topic/person/place/etc. interests tables Wikipedia scrape table related topic.* SLU keeps track diversity faculty time makes data public following website: https://www.stlawu.edu/ir/diversity/faculty. Use rvest scrape data tables R.* SLU keeps track diversity faculty time makes data public following website: https://www.stlawu.edu/ir/diversity/faculty. Use rvest scrape data tables R.Hint: may need use extra argument html_table() like fill.",
    "code": ""
  },
  {
    "path": "data-import.html",
    "id": "json-files-with-jsonlite",
    "chapter": " 11 Data Import",
    "heading": "11.3 JSON Files with jsonlite",
    "text": "final common data format discuss JSON (JavaScript Object Notation). cover basics JSON data use jsonlite package R read .json files. JSON files read R list object.",
    "code": ""
  },
  {
    "path": "data-import.html",
    "id": "everything-working-well",
    "chapter": " 11 Data Import",
    "heading": "11.3.1 Everything Working Well",
    "text": "First, consider data mobile game Clash Royale. Install jsonlite package use read json file function fromJSON():get warning message, investigate class.Next, type View(cr_cards) console (bottom-left) window look data. See can pull data set clicking things View() window.following give couple ways grab data using code. as_tibble() function converts rectangular object familiar tibble.first option specifies name table ’s JSON file (case, name \"cards\"):second method uses flatten() function purrr package, package core tidyverse talk detail class. also different flatten() function jsonlite package. code , specify want use flatten() purrr purrr::flatten(). wanted use flatten() jsonlite, ’d use jsonlite::flatten()methods give tibble can use usual tidyverse tools ggplot2, dplyr, tidyr, etc. .",
    "code": "\n## install.packages(\"jsonlite\")\nlibrary(jsonlite)\ncr_cards <- fromJSON(here(\"data/clash_royale_card_info.json\"))\nlibrary(tidyverse)\ncr_cards_flat <- cr_cards[[\"cards\"]]\ncr_cards_df <- as_tibble(cr_cards_flat)\nhead(cr_cards_df)\n#> # A tibble: 6 × 8\n#>   key     name      elixir type  rarity arena descr…¹     id\n#>   <chr>   <chr>      <int> <chr> <chr>  <int> <chr>    <int>\n#> 1 knight  Knight         3 Troop Common     0 A toug… 2.6 e7\n#> 2 archers Archers        3 Troop Common     0 A pair… 2.60e7\n#> 3 goblins Goblins        2 Troop Common     1 Three … 2.60e7\n#> 4 giant   Giant          5 Troop Rare       0 Slow b… 2.60e7\n#> 5 pekka   P.E.K.K.A      7 Troop Epic       4 A heav… 2.60e7\n#> 6 minions Minions        3 Troop Common     0 Three … 2.60e7\n#> # … with abbreviated variable name ¹​description\ncr_cards_flat2 <- purrr::flatten(cr_cards)\ncr_cards_df2 <- as_tibble(cr_cards_flat2)\nhead(cr_cards_df2)\n#> # A tibble: 6 × 8\n#>   key     name      elixir type  rarity arena descr…¹     id\n#>   <chr>   <chr>      <int> <chr> <chr>  <int> <chr>    <int>\n#> 1 knight  Knight         3 Troop Common     0 A toug… 2.6 e7\n#> 2 archers Archers        3 Troop Common     0 A pair… 2.60e7\n#> 3 goblins Goblins        2 Troop Common     1 Three … 2.60e7\n#> 4 giant   Giant          5 Troop Rare       0 Slow b… 2.60e7\n#> 5 pekka   P.E.K.K.A      7 Troop Epic       4 A heav… 2.60e7\n#> 6 minions Minions        3 Troop Common     0 Three … 2.60e7\n#> # … with abbreviated variable name ¹​description"
  },
  {
    "path": "data-import.html",
    "id": "things-arent-always-so-easy",
    "chapter": " 11 Data Import",
    "heading": "11.3.2 Things Aren’t Always So Easy",
    "text": "Now let’s try look animal crossing data obtained https://github.com/jefflomacy/villagerdb. first just want look data one individual villager (ace) file ace.json.Things now….complicated. example just show ’s always easy working JSON data. Lists can nested creates problems trying convert deeply nested list “rectangular” format ’s easy work .’s also added problem reading .json files villagers time loop mapping function purrr download read JSON files villagers. won’t delve deeply , ’s lot file formats ’ve discussed week, particularly web scraping .json files.",
    "code": "\nacedata <- fromJSON(here(\"data/ace.json\"))\naceflat <- purrr::flatten(acedata)\nhead(aceflat)\n#> $gender\n#> [1] \"male\"\n#> \n#> $species\n#> [1] \"bird\"\n#> \n#> $birthday\n#> [1] \"3-13\"\n#> \n#> $ac\n#> $ac$personality\n#> [1] \"jock\"\n#> \n#> $ac$clothes\n#> [1] \"spade-shirt\"\n#> \n#> $ac$song\n#> [1] \"K.K. Parade\"\n#> \n#> $ac$phrase\n#> [1] \"ace\"\n#> \n#> \n#> $`afe+`\n#> $`afe+`$personality\n#> [1] \"jock\"\n#> \n#> $`afe+`$clothes\n#> [1] \"spade-shirt\"\n#> \n#> $`afe+`$song\n#> [1] \"K.K. Parade\"\n#> \n#> \n#> $name\n#> [1] \"Ace\""
  },
  {
    "path": "data-import.html",
    "id": "exercise-8-4",
    "chapter": " 11 Data Import",
    "heading": "11.3.3 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 11.5.* Read pokedex.json file, data set information 151 original Pokemon. , use flatten() function purrr flatten list.* Read pokedex.json file, data set information 151 original Pokemon. , use flatten() function purrr flatten list.* Use as_tibble() convert flattened list tibble.* Use as_tibble() convert flattened list tibble.* Use parse_number() mutate() tidy two variables data set.* Use parse_number() mutate() tidy two variables data set.* Look type variable. looks odd ? happens try use , either plot, using dplyr function?* Look type variable. looks odd ? happens try use , either plot, using dplyr function?can unnest() Type variable unnest() function tidyr. didn’t discuss function feel free read ?unnestThere 6 pokemon spawn_chance 0. Figure 6 pokemon .6 pokemon spawn_chance 0. Figure 6 pokemon .Figure 5 common Pokemon types first generation (’ll need use unnest()-ed data set : ?).Figure 5 common Pokemon types first generation (’ll need use unnest()-ed data set : ?).",
    "code": "\npokemon_unnest <- unnest(pokemon_df, cols = c(type))"
  },
  {
    "path": "data-import.html",
    "id": "chapexercise-8",
    "chapter": " 11 Data Import",
    "heading": "11.4 Chapter Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 11.5.Choose sports team SLU, go team’s website (simply googling SLU name_of_sport). Scrape data tables “Results” “Statistics” section sport. scrape data, tidy data set. , choose one following options (different options might make /less sense different sports)(). Summarise different team statistics, either numerically graphically. Perhaps make graphs showing different statistics time.(b). Summarise different individual statistics, either numerically graphically.(c). Ask answer questions make sense particular sport looking !Note: sports (men’s women’s golf, example), give results PDF format. PDF format generally horrible way record share data, ’s difficult read almost program. Therefore, avoid sports PDF results purposes exercise.",
    "code": ""
  },
  {
    "path": "data-import.html",
    "id": "solutions-8",
    "chapter": " 11 Data Import",
    "heading": "11.5 Exercise Solutions",
    "text": "",
    "code": ""
  },
  {
    "path": "data-import.html",
    "id": "readr-s",
    "chapter": " 11 Data Import",
    "heading": "11.5.1 readr S",
    "text": "* birthdays.txt file information birthdays various animals Animal Crossing island. also columns Animal’s Name, Animal Type, long animal lived island (weeks). Click file open look format data.Start following code chunk use options read_delim() read data (?read_delim). delim argument ’s already provided specifies delimiter (separator) ’ll use -, opposed , example, , .csv file. Arguments may change includeskipcol_namesnatrim_wscol_types* Another common format data stored Excel file. Often, ’s easiest just save Excel file .csv file read using read_csv(). , sometimes route can difficult (example, Excel file thousands sheets). read directly Excel, ’ll need install readxl install.packages(\"readxl\"). installed, load package library(readxl), read first sheet evals_prof.xlsx data set, data set used Project 2, read_excel() function.* Now, read second sheet, using help file ?read_excel change one arguments.",
    "code": "\nlibrary(tidyverse)\ndf <- read_delim(here(\"data/birthdays.txt\"), delim = \" - \")\nhead(df)\nread_delim(here(\"data/birthdays.txt\"), delim = \"-\", skip = 4,\n  col_names = c(\"Birthday\", \"Name\",\n    \"Animal\", \"Island\"),\n  trim_ws = TRUE,\n  col_types = list(\n    col_character(), col_character(), col_character(), col_number()\n  ), na = c(\"N/A\", \"?\"))\n## install.packages(\"readxl\")\nlibrary(readxl)\nread_excel(here(\"data/evals_prof.xlsx\"))\nread_excel(here(\"data/evals_prof.xlsx\"), sheet = 2)"
  },
  {
    "path": "data-import.html",
    "id": "rvest-and-data-scraping-s",
    "chapter": " 11 Data Import",
    "heading": "11.5.2 rvest and Data Scraping S",
    "text": "* SLU keeps track diversity faculty time makes data public following website: https://www.stlawu.edu/ir/diversity/faculty. Use rvest scrape data tables R.",
    "code": "\nurl <- \"https://www.stlawu.edu/ir/diversity/faculty\"\nh <- read_html(url)\ntab <- h |> html_nodes(\"table\")\nobj <- tab[[1]] |> html_table(fill = TRUE)\nobj"
  },
  {
    "path": "data-import.html",
    "id": "json-with-jsonlite-s",
    "chapter": " 11 Data Import",
    "heading": "11.5.3 JSON with jsonlite S",
    "text": "* Read pokedex.json file, data set information 151 original Pokemon. , use flatten() function purrr flatten list.* Use as_tibble() convert flattened list tibble.* Use parse_number() mutate() tidy two variables data set.* Look type variable. looks odd ? happens try use , either plot, using dplyr function?can unnest() Type variable unnest() function tidyr. didn’t discuss function feel free read ?unnest",
    "code": "\nlibrary(jsonlite)\npokedex <- fromJSON(here(\"data/pokedex.json\"))\ndf <- purrr::flatten(pokedex)\npokemon_df <- as_tibble(df)\npokemon_df <- pokemon_df |> mutate(height = parse_number(height),\n                                    weight = parse_number(weight))\n## it's a variable of lists....this is happening because some \n## pokemon have more than one type.\n\n## most ggplot() and dplyr() functions won't work, or\n## won't work as you'd expect\npokemon_unnest <- unnest(pokemon_df, cols = c(type))"
  },
  {
    "path": "data-import.html",
    "id": "chapexercise-8-S",
    "chapter": " 11 Data Import",
    "heading": "11.5.4 Chapter Exercises S",
    "text": "",
    "code": ""
  },
  {
    "path": "data-import.html",
    "id": "rcode-8",
    "chapter": " 11 Data Import",
    "heading": "11.6 Non-Exercise R Code",
    "text": "",
    "code": "\nlibrary(tidyverse)\nlibrary(here)\ncars_df <- read_csv(here(\"data/mtcarsex.csv\"))\nhead(cars_df)\ncars_df <- read_csv(here(\"data/mtcarsex.csv\"), skip = 2)\n## first two lines will be skipped\nhead(cars_df)\ncars_df <- read_csv(here(\"data/mtcarsex.csv\"), na = c(NA, \"-999\"), skip = 2)\nhead(cars_df)\ncars_df <- read_csv(here(\"data/mtcarsex.csv\"), na = c(NA, \"-999\"), skip = 2,\n  col_types = cols(\n  mpg = col_double(),\n  cyl = col_factor(),\n  disp = col_double(),\n  hp = col_double(),\n  drat = col_double(),\n  wt = col_double(),\n  qsec = col_double(),\n  vs = col_factor(),\n  am = col_double(),\n  gear = col_double(),\n  carb = col_double()\n))\ncars_df <- read_csv(here(\"data/mtcarsex.csv\"), na = c(NA, \"-999\"), skip = 2,\n  col_types = cols(\n  mpg = col_double(),\n  cyl = col_factor(),\n  disp = col_double(),\n  hp = col_double(),\n  drat = col_double(),\n  wt = col_double(),\n  qsec = col_double(),\n  vs = col_factor(),\n  am = col_double(),\n  gear = col_double(),\n  carb = col_double()\n)) |>\n  slice(-(1:2))\nhead(cars_df)\noscars_df <- read_tsv(here(\"data/oscars.tsv\"))\nhead(oscars_df)\ntest_df <- read_csv(here(\"data/parsedf.csv\"))\nhead(test_df)\ntest_df |> mutate(x2 = parse_number(x))\nlibrary(tidyverse)\nlibrary(rvest)\n\n## provide the URL and name it something (in this case, url).\nurl <- \"https://en.wikipedia.org/wiki/Gun_violence_in_the_United_States_by_state\"\n\n## convert the html code into something R can read\nh <- read_html(url)\n\n## grabs the tables\ntab <- h |> html_nodes(\"table\")\ntest1 <- tab[[1]] |> html_table()\ntest2 <- tab[[2]] |> html_table()\ntest3 <- tab[[3]] |> html_table()\n\nhead(test1)\nhead(test2)\nhead(test3)\nurl <- \"https://saintsathletics.com/sports/baseball/stats/2021\"\nh <- read_html(url)\ntab <- h |> html_nodes(\"table\")\ntab\nobj <- tab[[1]] |> html_table(fill = TRUE)\nhead(obj)\ntail(obj)\nobj2 <- tab[[2]] |> html_table(fill = TRUE)\nhead(obj2)\ntail(obj2)\n## install.packages(\"jsonlite\")\nlibrary(jsonlite)\ncr_cards <- fromJSON(here(\"data/clash_royale_card_info.json\"))\nlibrary(tidyverse)\ncr_cards_flat <- cr_cards[[\"cards\"]]\ncr_cards_df <- as_tibble(cr_cards_flat)\nhead(cr_cards_df)\ncr_cards_flat2 <- purrr::flatten(cr_cards)\ncr_cards_df2 <- as_tibble(cr_cards_flat2)\nhead(cr_cards_df2)\nacedata <- fromJSON(here(\"data/ace.json\"))\naceflat <- purrr::flatten(acedata)\nhead(aceflat)"
  },
  {
    "path": "merging-with-dplyr.html",
    "id": "merging-with-dplyr",
    "chapter": " 12 Merging with dplyr",
    "heading": " 12 Merging with dplyr",
    "text": "Goals:use bind_rows() stack two data sets bind_cols() merge two data sets.use bind_rows() stack two data sets bind_cols() merge two data sets.identify keys two related data sets.identify keys two related data sets.use mutating join functions dplyr merge two data sets key.use mutating join functions dplyr merge two data sets key.use filtering join functions dplyr filter one data set values another data set.use filtering join functions dplyr filter one data set values another data set.apply appropriate join() function given problem context.apply appropriate join() function given problem context.",
    "code": ""
  },
  {
    "path": "merging-with-dplyr.html",
    "id": "stacking-rows-and-appending-columns",
    "chapter": " 12 Merging with dplyr",
    "heading": "12.1 Stacking Rows and Appending Columns",
    "text": "",
    "code": ""
  },
  {
    "path": "merging-with-dplyr.html",
    "id": "stacking-with-bind_rows",
    "chapter": " 12 Merging with dplyr",
    "heading": "12.1.1 Stacking with bind_rows()",
    "text": "First, talk combining two data sets “stacking” top form one new data set. bind_rows() function can used purpose two data sets identical column names.common instance useful two data sets come source different locations years, exact column names.example, examine following website notice .csv files given year matches ATP (Association (men’s) Tennis Professionals). https://github.com/JeffSackmann/tennis_atp., read data sets, look many columns .combine results data sets,happens? Can fix error? Hint: runto get full column specifications use readr knowledge change couple column types. also discuss , , using col_type argument read_csv(), don’t need specify column types. Just specifying ones want change works . following code forces seed variables 2018 data set characters.can try combining data sets now.quick check make sure number rows atp_2018 plus number rows atp_2019 equals number rows atp_df.might seem little annoying, , default bind_rows() combine two data sets stacking rows data sets identical column names identical column classes, saw previous example.Now run following look output.behavior expect?",
    "code": "\nlibrary(tidyverse)\nlibrary(here)\natp_2019 <- read_csv(here(\"data/atp_matches_2019.csv\"))\natp_2018 <- read_csv(here(\"data/atp_matches_2018.csv\"))\nhead(atp_2019) \nhead(atp_2018)\natp_df <- bind_rows(atp_2018, atp_2019)\n#> Error in `bind_rows()`:\n#> ! Can't combine `winner_seed` <double> and `winner_seed` <character>.\nspec(atp_2018)\natp_2018 <- read_csv(here(\"data/atp_matches_2018.csv\"),\n                     col_types = cols(winner_seed = col_character(),\n                                      loser_seed = col_character()))\natp_df <- bind_rows(atp_2018, atp_2019)\natp_df\ndf_test2a <- tibble(xvar = c(1, 2))\ndf_test2b <- tibble(xvar = c(1, 2), y = c(5, 1))\nbind_rows(df_test2a, df_test2b)\n#> # A tibble: 4 × 2\n#>    xvar     y\n#>   <dbl> <dbl>\n#> 1     1    NA\n#> 2     2    NA\n#> 3     1     5\n#> 4     2     1"
  },
  {
    "path": "merging-with-dplyr.html",
    "id": "binding-columns-with-bind_cols",
    "chapter": " 12 Merging with dplyr",
    "heading": "12.1.2 Binding Columns with bind_cols()",
    "text": "won’t spend much time talking bind together columns ’s generally little dangerous.use couple test data sets, df_test1a df_test1b, see action:larger data set, might dangerous way combine data? must sure way data collected order combine data way?",
    "code": "\ndf_test1a <- tibble(xvar = c(1, 2), yvar = c(5, 1))\ndf_test1b <- tibble(x = c(1, 2), y = c(5, 1))\nbind_cols(df_test1a, df_test1b)\n#> # A tibble: 2 × 4\n#>    xvar  yvar     x     y\n#>   <dbl> <dbl> <dbl> <dbl>\n#> 1     1     5     1     5\n#> 2     2     1     2     1"
  },
  {
    "path": "merging-with-dplyr.html",
    "id": "exercise-10-1",
    "chapter": " 12 Merging with dplyr",
    "heading": "12.1.3 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 12.5.* Run following explain R simply stack rows. , fix issue rename() function.",
    "code": "\ndf_test1a <- tibble(xvar = c(1, 2), yvar = c(5, 1))\ndf_test1b <- tibble(x = c(1, 2), y = c(5, 1))\nbind_rows(df_test1a, df_test1b)\n#> # A tibble: 4 × 4\n#>    xvar  yvar     x     y\n#>   <dbl> <dbl> <dbl> <dbl>\n#> 1     1     5    NA    NA\n#> 2     2     1    NA    NA\n#> 3    NA    NA     1     5\n#> 4    NA    NA     2     1"
  },
  {
    "path": "merging-with-dplyr.html",
    "id": "mutating-joins",
    "chapter": " 12 Merging with dplyr",
    "heading": "12.2 Mutating Joins",
    "text": "goal combine two data sets using common variable(s) data sets , need different tools simply stacking rows appending columns. merging together two data sets, need matching identification variable data set. variable commonly called key. key can identification number, name, date, etc, must present data sets.simple first example, considerOur goal combine two data sets people’s favorite sports favorite colours one data set.Identify key example . can longer use bind_cols() ?",
    "code": "\nlibrary(tidyverse)\ndf1 <- tibble(name = c(\"Emily\", \"Miguel\", \"Tonya\"), fav_sport = c(\"Swimming\", \"Football\", \"Tennis\"))\ndf2 <- tibble(name = c(\"Tonya\", \"Miguel\", \"Emily\"),\n              fav_colour = c(\"Robin's Egg Blue\", \"Tickle Me Pink\", \"Goldenrod\"))"
  },
  {
    "path": "merging-with-dplyr.html",
    "id": "keep-all-rows-of-data-set-1-with-left_join",
    "chapter": " 12 Merging with dplyr",
    "heading": "12.2.1 Keep All Rows of Data Set 1 with left_join()",
    "text": "Consider babynames R package, following data sets:lifetables: cohort life tables different sex different year variables, starting year 1900.births: number births United States year, since 1909babynames: popularity different baby names per year sex since year 1880.Read data set ?babynames, ?births ?lifetables.Suppose want combine births data set babynames data set, row babynames now total number births year. first need identify key data set use joining. case, data set year variable, can use left_join() keep observations babynames_df, even years births_df data set.births missing head(combined_left) tail(combined_left)?",
    "code": "\n##install.packages(\"babynames\")\nlibrary(babynames)\nlife_df <- babynames::lifetables\nbirth_df <- babynames::births\nbabynames_df <- babynames::babynames\nhead(babynames)\nhead(births)\nhead(lifetables)\ncombined_left <- left_join(babynames_df, birth_df, by = c(\"year\" = \"year\"))\nhead(combined_left)\n#> # A tibble: 6 × 6\n#>    year sex   name          n   prop births\n#>   <dbl> <chr> <chr>     <int>  <dbl>  <int>\n#> 1  1880 F     Mary       7065 0.0724     NA\n#> 2  1880 F     Anna       2604 0.0267     NA\n#> 3  1880 F     Emma       2003 0.0205     NA\n#> 4  1880 F     Elizabeth  1939 0.0199     NA\n#> 5  1880 F     Minnie     1746 0.0179     NA\n#> 6  1880 F     Margaret   1578 0.0162     NA\ntail(combined_left)\n#> # A tibble: 6 × 6\n#>    year sex   name       n       prop  births\n#>   <dbl> <chr> <chr>  <int>      <dbl>   <int>\n#> 1  2017 M     Zyhier     5 0.00000255 3855500\n#> 2  2017 M     Zykai      5 0.00000255 3855500\n#> 3  2017 M     Zykeem     5 0.00000255 3855500\n#> 4  2017 M     Zylin      5 0.00000255 3855500\n#> 5  2017 M     Zylis      5 0.00000255 3855500\n#> 6  2017 M     Zyrie      5 0.00000255 3855500"
  },
  {
    "path": "merging-with-dplyr.html",
    "id": "keep-all-rows-of-data-set-2-with-right_join",
    "chapter": " 12 Merging with dplyr",
    "heading": "12.2.2 Keep All Rows of Data Set 2 with right_join()",
    "text": "Recall accompanying handout need ever use right_join() using left_join() first two data set arguments switched:Therefore, ’s usually easier just always use left_join() ignore right_join() completely.",
    "code": "\n## these will always do the same exact thing\nright_join(babynames_df, birth_df, by = c(\"year\" = \"year\"))\n#> # A tibble: 1,839,952 × 6\n#>     year sex   name          n   prop  births\n#>    <dbl> <chr> <chr>     <int>  <dbl>   <int>\n#>  1  1909 F     Mary      19259 0.0523 2718000\n#>  2  1909 F     Helen      9250 0.0251 2718000\n#>  3  1909 F     Margaret   7359 0.0200 2718000\n#>  4  1909 F     Ruth       6509 0.0177 2718000\n#>  5  1909 F     Dorothy    6253 0.0170 2718000\n#>  6  1909 F     Anna       5804 0.0158 2718000\n#>  7  1909 F     Elizabeth  5176 0.0141 2718000\n#>  8  1909 F     Mildred    5054 0.0137 2718000\n#>  9  1909 F     Marie      4301 0.0117 2718000\n#> 10  1909 F     Alice      4170 0.0113 2718000\n#> # … with 1,839,942 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\nleft_join(birth_df, babynames_df, by = c(\"year\" = \"year\"))\n#> # A tibble: 1,839,952 × 6\n#>     year  births sex   name          n   prop\n#>    <dbl>   <int> <chr> <chr>     <int>  <dbl>\n#>  1  1909 2718000 F     Mary      19259 0.0523\n#>  2  1909 2718000 F     Helen      9250 0.0251\n#>  3  1909 2718000 F     Margaret   7359 0.0200\n#>  4  1909 2718000 F     Ruth       6509 0.0177\n#>  5  1909 2718000 F     Dorothy    6253 0.0170\n#>  6  1909 2718000 F     Anna       5804 0.0158\n#>  7  1909 2718000 F     Elizabeth  5176 0.0141\n#>  8  1909 2718000 F     Mildred    5054 0.0137\n#>  9  1909 2718000 F     Marie      4301 0.0117\n#> 10  1909 2718000 F     Alice      4170 0.0113\n#> # … with 1,839,942 more rows\n#> # ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "path": "merging-with-dplyr.html",
    "id": "keep-all-rows-of-both-data-sets-with-full_join",
    "chapter": " 12 Merging with dplyr",
    "heading": "12.2.3 Keep All Rows of Both Data Sets with full_join()",
    "text": "full_join() keep rows data set 1 don’t matching key data set 2, also keep rows data set 2 don’t matching key data set 1, filling NA missing values necessary. example merging babynames_df birth_df,",
    "code": "\nfull_join(babynames_df, birth_df, by = c(\"year\" = \"year\"))"
  },
  {
    "path": "merging-with-dplyr.html",
    "id": "keep-only-rows-with-matching-keys-with-inner_join",
    "chapter": " 12 Merging with dplyr",
    "heading": "12.2.4 Keep Only Rows with Matching Keys with inner_join()",
    "text": "can also keep rows matching keys inner_join(). join, row data set 1 without matching key data set 2 dropped, row data set 2 without matching key data set 1 also dropped.",
    "code": "\ninner_join(babynames_df, birth_df, by = c(\"year\" = \"year\"))\n#> # A tibble: 1,839,952 × 6\n#>     year sex   name          n   prop  births\n#>    <dbl> <chr> <chr>     <int>  <dbl>   <int>\n#>  1  1909 F     Mary      19259 0.0523 2718000\n#>  2  1909 F     Helen      9250 0.0251 2718000\n#>  3  1909 F     Margaret   7359 0.0200 2718000\n#>  4  1909 F     Ruth       6509 0.0177 2718000\n#>  5  1909 F     Dorothy    6253 0.0170 2718000\n#>  6  1909 F     Anna       5804 0.0158 2718000\n#>  7  1909 F     Elizabeth  5176 0.0141 2718000\n#>  8  1909 F     Mildred    5054 0.0137 2718000\n#>  9  1909 F     Marie      4301 0.0117 2718000\n#> 10  1909 F     Alice      4170 0.0113 2718000\n#> # … with 1,839,942 more rows\n#> # ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "path": "merging-with-dplyr.html",
    "id": "which-xxxx_join",
    "chapter": " 12 Merging with dplyr",
    "heading": "12.2.5 Which xxxx_join()?",
    "text": "join function use depend context data questions answering analysis. importantly, ’re using left_join(), right_join() inner_join(), ’re potentially cutting data. ’s important aware data ’re omitting. example, babynames births data, want keep note left_join() removed observations 1909 joined data set.",
    "code": ""
  },
  {
    "path": "merging-with-dplyr.html",
    "id": "the-importance-of-a-good-key",
    "chapter": " 12 Merging with dplyr",
    "heading": "12.2.6 The Importance of a Good Key",
    "text": "key variable important joining always available “perfect” form. Recall college majors data sets , called slumajors_df, information majors SLU. Another data set, collegemajors_df, different statistics college majors nationwide. ’s lots interesting variables data sets, ’ll focus Major variable . Read examine two data sets :logical key joining two data sets Major, joining data sets won’t actually work. following attempt using Major key.collegemajors_df give NA values tried merge major?example underscores importance key matches exactly. , , issues involved joining two data sets can solved functions stringr package (discussed weeks). example, capitalization issue can solved str_to_title() function, converts -caps majors collegemajors_df majors first letter word capitalized:can see, solves issue majors others still different naming conventions two data sets.",
    "code": "\nslumajors_df <- read_csv(here(\"data/SLU_Majors_15_19.csv\"))\ncollegemajors_df <- read_csv(here(\"data/college-majors.csv\"))\nhead(slumajors_df)\n#> # A tibble: 6 × 3\n#>   Major                        nfemales nmales\n#>   <chr>                           <dbl>  <dbl>\n#> 1 Anthropology                       34     15\n#> 2 Art & Art History                  65     11\n#> 3 Biochemistry                       14     11\n#> 4 Biology                           162     67\n#> 5 Business in the Liberal Arts      135    251\n#> 6 Chemistry                          26     14\nhead(collegemajors_df)\n#> # A tibble: 6 × 12\n#>   Major    Total   Men Women Major…¹ Emplo…² Full_…³ Part_…⁴\n#>   <chr>    <dbl> <dbl> <dbl> <chr>     <dbl>   <dbl>   <dbl>\n#> 1 PETROLE…  2339  2057   282 Engine…    1976    1849     270\n#> 2 MINING …   756   679    77 Engine…     640     556     170\n#> 3 METALLU…   856   725   131 Engine…     648     558     133\n#> 4 NAVAL A…  1258  1123   135 Engine…     758    1069     150\n#> 5 CHEMICA… 32260 21239 11021 Engine…   25694   23170    5180\n#> 6 NUCLEAR…  2573  2200   373 Engine…    1857    2038     264\n#> # … with 4 more variables: Unemployed <dbl>, Median <dbl>,\n#> #   P25th <dbl>, P75th <dbl>, and abbreviated variable\n#> #   names ¹​Major_category, ²​Employed, ³​Full_time,\n#> #   ⁴​Part_time\n#> # ℹ Use `colnames()` to see all variable names\nleft_join(slumajors_df, collegemajors_df, by = c(\"Major\" = \"Major\"))\n#> # A tibble: 27 × 14\n#>    Major    nfema…¹ nmales Total   Men Women Major…² Emplo…³\n#>    <chr>      <dbl>  <dbl> <dbl> <dbl> <dbl> <chr>     <dbl>\n#>  1 Anthrop…      34     15    NA    NA    NA <NA>         NA\n#>  2 Art & A…      65     11    NA    NA    NA <NA>         NA\n#>  3 Biochem…      14     11    NA    NA    NA <NA>         NA\n#>  4 Biology      162     67    NA    NA    NA <NA>         NA\n#>  5 Busines…     135    251    NA    NA    NA <NA>         NA\n#>  6 Chemist…      26     14    NA    NA    NA <NA>         NA\n#>  7 Compute…      21     47    NA    NA    NA <NA>         NA\n#>  8 Conserv…      38     20    NA    NA    NA <NA>         NA\n#>  9 Economi…     128    349    NA    NA    NA <NA>         NA\n#> 10 English      131     54    NA    NA    NA <NA>         NA\n#> # … with 17 more rows, 6 more variables: Full_time <dbl>,\n#> #   Part_time <dbl>, Unemployed <dbl>, Median <dbl>,\n#> #   P25th <dbl>, P75th <dbl>, and abbreviated variable\n#> #   names ¹​nfemales, ²​Major_category, ³​Employed\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\ncollegemajors_df <- collegemajors_df |>\n  mutate(Major = str_to_title(Major))\nleft_join(slumajors_df, collegemajors_df)\n#> Joining, by = \"Major\"\n#> # A tibble: 27 × 14\n#>    Major nfema…¹ nmales  Total    Men  Women Major…² Emplo…³\n#>    <chr>   <dbl>  <dbl>  <dbl>  <dbl>  <dbl> <chr>     <dbl>\n#>  1 Anth…      34     15     NA     NA     NA <NA>         NA\n#>  2 Art …      65     11     NA     NA     NA <NA>         NA\n#>  3 Bioc…      14     11     NA     NA     NA <NA>         NA\n#>  4 Biol…     162     67 280709 111762 168947 Biolog…  182295\n#>  5 Busi…     135    251     NA     NA     NA <NA>         NA\n#>  6 Chem…      26     14  66530  32923  33607 Physic…   48535\n#>  7 Comp…      21     47 128319  99743  28576 Comput…  102087\n#>  8 Cons…      38     20     NA     NA     NA <NA>         NA\n#>  9 Econ…     128    349 139247  89749  49498 Social…  104117\n#> 10 Engl…     131     54     NA     NA     NA <NA>         NA\n#> # … with 17 more rows, 6 more variables: Full_time <dbl>,\n#> #   Part_time <dbl>, Unemployed <dbl>, Median <dbl>,\n#> #   P25th <dbl>, P75th <dbl>, and abbreviated variable\n#> #   names ¹​nfemales, ²​Major_category, ³​Employed\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names"
  },
  {
    "path": "merging-with-dplyr.html",
    "id": "exercise-10-2",
    "chapter": " 12 Merging with dplyr",
    "heading": "12.2.7 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 12.5.Examine following two joins ’ve done, explain one resulting data set fewer observations (rows) .Evaluate whether following statement true false: inner_join() always result data set fewer rows full_join().Evaluate whether following statement true false: inner_join() always result data set fewer rows full_join().Evaluate whether following statement true false: inner_join() always result data set fewer rows left_join().Evaluate whether following statement true false: inner_join() always result data set fewer rows left_join().",
    "code": "\nleft_join(babynames_df, birth_df, by = c(\"year\" = \"year\"))\n#> # A tibble: 1,924,665 × 6\n#>     year sex   name          n   prop births\n#>    <dbl> <chr> <chr>     <int>  <dbl>  <int>\n#>  1  1880 F     Mary       7065 0.0724     NA\n#>  2  1880 F     Anna       2604 0.0267     NA\n#>  3  1880 F     Emma       2003 0.0205     NA\n#>  4  1880 F     Elizabeth  1939 0.0199     NA\n#>  5  1880 F     Minnie     1746 0.0179     NA\n#>  6  1880 F     Margaret   1578 0.0162     NA\n#>  7  1880 F     Ida        1472 0.0151     NA\n#>  8  1880 F     Alice      1414 0.0145     NA\n#>  9  1880 F     Bertha     1320 0.0135     NA\n#> 10  1880 F     Sarah      1288 0.0132     NA\n#> # … with 1,924,655 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\nleft_join(birth_df, babynames_df, by = c(\"year\" = \"year\"))\n#> # A tibble: 1,839,952 × 6\n#>     year  births sex   name          n   prop\n#>    <dbl>   <int> <chr> <chr>     <int>  <dbl>\n#>  1  1909 2718000 F     Mary      19259 0.0523\n#>  2  1909 2718000 F     Helen      9250 0.0251\n#>  3  1909 2718000 F     Margaret   7359 0.0200\n#>  4  1909 2718000 F     Ruth       6509 0.0177\n#>  5  1909 2718000 F     Dorothy    6253 0.0170\n#>  6  1909 2718000 F     Anna       5804 0.0158\n#>  7  1909 2718000 F     Elizabeth  5176 0.0141\n#>  8  1909 2718000 F     Mildred    5054 0.0137\n#>  9  1909 2718000 F     Marie      4301 0.0117\n#> 10  1909 2718000 F     Alice      4170 0.0113\n#> # … with 1,839,942 more rows\n#> # ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "path": "merging-with-dplyr.html",
    "id": "filtering-joins",
    "chapter": " 12 Merging with dplyr",
    "heading": "12.3 Filtering Joins",
    "text": "Filtering joins (semi_join() anti_join()) useful like keep variables one data set, want filter observations variable second data set.Consider two data sets men’s tennis matches 2018 2019.",
    "code": "\natp_2019 <- read_csv(here(\"data/atp_matches_2019.csv\"))\natp_2018 <- read_csv(here(\"data/atp_matches_2018.csv\"))\natp_2019\n#> # A tibble: 2,781 × 49\n#>    tourney…¹ tourn…² surface draw_…³ tourn…⁴ tourn…⁵ match…⁶\n#>    <chr>     <chr>   <chr>     <dbl> <chr>     <dbl>   <dbl>\n#>  1 2019-M020 Brisba… Hard         32 A        2.02e7     300\n#>  2 2019-M020 Brisba… Hard         32 A        2.02e7     299\n#>  3 2019-M020 Brisba… Hard         32 A        2.02e7     298\n#>  4 2019-M020 Brisba… Hard         32 A        2.02e7     297\n#>  5 2019-M020 Brisba… Hard         32 A        2.02e7     296\n#>  6 2019-M020 Brisba… Hard         32 A        2.02e7     295\n#>  7 2019-M020 Brisba… Hard         32 A        2.02e7     294\n#>  8 2019-M020 Brisba… Hard         32 A        2.02e7     293\n#>  9 2019-M020 Brisba… Hard         32 A        2.02e7     292\n#> 10 2019-M020 Brisba… Hard         32 A        2.02e7     291\n#> # … with 2,771 more rows, 42 more variables:\n#> #   winner_id <dbl>, winner_seed <chr>, winner_entry <chr>,\n#> #   winner_name <chr>, winner_hand <chr>, winner_ht <dbl>,\n#> #   winner_ioc <chr>, winner_age <dbl>, loser_id <dbl>,\n#> #   loser_seed <chr>, loser_entry <chr>, loser_name <chr>,\n#> #   loser_hand <chr>, loser_ht <dbl>, loser_ioc <chr>,\n#> #   loser_age <dbl>, score <chr>, best_of <dbl>, …\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\natp_2018\n#> # A tibble: 2,889 × 49\n#>    tourney…¹ tourn…² surface draw_…³ tourn…⁴ tourn…⁵ match…⁶\n#>    <chr>     <chr>   <chr>     <dbl> <chr>     <dbl>   <dbl>\n#>  1 2018-M020 Brisba… Hard         32 A        2.02e7     271\n#>  2 2018-M020 Brisba… Hard         32 A        2.02e7     272\n#>  3 2018-M020 Brisba… Hard         32 A        2.02e7     273\n#>  4 2018-M020 Brisba… Hard         32 A        2.02e7     275\n#>  5 2018-M020 Brisba… Hard         32 A        2.02e7     276\n#>  6 2018-M020 Brisba… Hard         32 A        2.02e7     277\n#>  7 2018-M020 Brisba… Hard         32 A        2.02e7     278\n#>  8 2018-M020 Brisba… Hard         32 A        2.02e7     279\n#>  9 2018-M020 Brisba… Hard         32 A        2.02e7     280\n#> 10 2018-M020 Brisba… Hard         32 A        2.02e7     282\n#> # … with 2,879 more rows, 42 more variables:\n#> #   winner_id <dbl>, winner_seed <dbl>, winner_entry <chr>,\n#> #   winner_name <chr>, winner_hand <chr>, winner_ht <dbl>,\n#> #   winner_ioc <chr>, winner_age <dbl>, loser_id <dbl>,\n#> #   loser_seed <dbl>, loser_entry <chr>, loser_name <chr>,\n#> #   loser_hand <chr>, loser_ht <dbl>, loser_ioc <chr>,\n#> #   loser_age <dbl>, score <chr>, best_of <dbl>, …\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names"
  },
  {
    "path": "merging-with-dplyr.html",
    "id": "filtering-with-semi_join",
    "chapter": " 12 Merging with dplyr",
    "heading": "12.3.1 Filtering with semi_join()",
    "text": "Suppose want keep matches 2019 winning player 10 wins 2018. might useful want consider players 2018 played couple matches, perhaps got injured perhaps received special wildcard draw one event.accomplish , can first create data set names players won 10 matches 2018, using functions learned dplyr earlier semester:Next, apply semi_join(), takes names two data sets (second one contains information first “filtered”). third argument gives name key (winner_name) case.Note keeps matches 2019 winner 10 match wins 2018. drops matches loser lost someone 10 match wins 2018. isn’t yet perfect take little thought matches actually want keep particular analysis.",
    "code": "\nwin10 <- atp_2018 |> group_by(winner_name) |>\n  summarise(nwin = n()) |> \n  filter(nwin >= 10)\nwin10\n#> # A tibble: 93 × 2\n#>    winner_name       nwin\n#>    <chr>            <int>\n#>  1 Adrian Mannarino    26\n#>  2 Albert Ramos        21\n#>  3 Alex De Minaur      29\n#>  4 Alexander Zverev    58\n#>  5 Aljaz Bedene        19\n#>  6 Andreas Seppi       24\n#>  7 Andrey Rublev       20\n#>  8 Benoit Paire        27\n#>  9 Borna Coric         40\n#> 10 Cameron Norrie      19\n#> # … with 83 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\ntennis_2019_10 <- semi_join(atp_2019, win10,\n                            by = c(\"winner_name\" = \"winner_name\"))\ntennis_2019_10$winner_name"
  },
  {
    "path": "merging-with-dplyr.html",
    "id": "filtering-with-anti_join",
    "chapter": " 12 Merging with dplyr",
    "heading": "12.3.2 Filtering with anti_join()",
    "text": "Now suppose want keep matches 2019 winning player wins 2018. might think players “emerging players” 2019, players coming back injury, etc.. , can use anti_join(), keeps rows first data set match second data set.can examine many wins “new” (perhaps previously injured) players 2019:filtering join functions useful want filter observations criterion different data set.",
    "code": "\nnew_winners <- anti_join(atp_2019, atp_2018,\n                         by = c(\"winner_name\" = \"winner_name\")) \nnew_winners$winner_name\nnew_winners |> group_by(winner_name) |>\n  summarise(nwin = n()) |>\n  arrange(desc(nwin))\n#> # A tibble: 59 × 2\n#>    winner_name           nwin\n#>    <chr>                <int>\n#>  1 Christian Garin         32\n#>  2 Juan Ignacio Londero    22\n#>  3 Miomir Kecmanovic       22\n#>  4 Hugo Dellien            12\n#>  5 Attila Balazs            7\n#>  6 Cedrik Marcel Stebe      7\n#>  7 Janko Tipsarevic         7\n#>  8 Jannik Sinner            7\n#>  9 Soon Woo Kwon            7\n#> 10 Gregoire Barrere         6\n#> # … with 49 more rows\n#> # ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "path": "merging-with-dplyr.html",
    "id": "exercise-10-3",
    "chapter": " 12 Merging with dplyr",
    "heading": "12.3.3 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 12.5.Take semi_join() tennis example, now suppose want keep matches 2019 either winning player losing player 10 match wins 2018. can modify code achieve goal?* Take semi_join() tennis example, now suppose want keep matches 2019 winning player losing player 10 match wins 2018. can modify code achieve goal?",
    "code": "\ntennis_2019_10 <- semi_join(atp_2019, win10,\n                            by = c(\"winner_name\" = \"winner_name\"))\ntennis_2019_10$winner_name\ntennis_2019_10 <- semi_join(atp_2019, win10,\n                            by = c(\"winner_name\" = \"winner_name\"))\ntennis_2019_10$winner_name"
  },
  {
    "path": "merging-with-dplyr.html",
    "id": "chapexercise-10",
    "chapter": " 12 Merging with dplyr",
    "heading": "12.4 Chapter Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 12.5.* Read gun violence data set, suppose want add row data set statistics gun ownership mortality rate District Columbia (Washington D.C., NE region, 16.7 deaths per 100,000 people, gun ownership rate 8.7%). , create tibble() single row representing D.C. combine new tibble overall gun violence data set. Name new data set all_df.Explain attempt combining D.C. data overall data doesn’t work incorrect.Examine following data sets R’s base library demographic statistics U.S. states state abbreviations:Combine two data sets bind_cols(). assuming data sets order use function?* Combine columns states data set made Exercise 3 mortality data set without Washington D.C.* Combine columns states data set made Exercise 3 mortality data set without Washington D.C.* Use join function combine mortality data set (all_df) D.C. states data set Exercise 3 (states_df). exercise, keep row Washington D.C., take NA values variable observed states data.* Use join function combine mortality data set (all_df) D.C. states data set Exercise 3 (states_df). exercise, keep row Washington D.C., take NA values variable observed states data.* Repeat Exercise 5, now drop Washington D.C. merging process. Practice join function (opposed slice()-ing explicitly).* Repeat Exercise 5, now drop Washington D.C. merging process. Practice join function (opposed slice()-ing explicitly).* Use semi_join() create subset states_df NE region. Hint: need filter all_df first contain states NE region.* Use semi_join() create subset states_df NE region. Hint: need filter all_df first contain states NE region.* thing Exercise 7, time, use anti_join(). Hint: ’ll need filter all_df different way achieve .* thing Exercise 7, time, use anti_join(). Hint: ’ll need filter all_df different way achieve .Examine following data sets (first df1 second df2) , without running code, answer following questions.Examine following data sets (first df1 second df2) , without running code, answer following questions.many rows data set left_join(df1, df2, = c(\"id\" = \"id\"))?many rows data set left_join(df1, df2, = c(\"id\" = \"id\"))?many rows data set left_join(df2, df1, = c(\"id\" = \"id\"))?many rows data set left_join(df2, df1, = c(\"id\" = \"id\"))?many rows data set full_join(df1, df2, = c(\"id\" = \"id\"))?many rows data set full_join(df1, df2, = c(\"id\" = \"id\"))?many rows data set inner_join(df1, df2, = c(\"id\" = \"id\"))?many rows data set inner_join(df1, df2, = c(\"id\" = \"id\"))?many rows data set semi_join(df1, df2, = c(\"id\" = \"id\"))?many rows data set semi_join(df1, df2, = c(\"id\" = \"id\"))?many rows data set anti_join(df1, df2, = c(\"id\" = \"id\"))?many rows data set anti_join(df1, df2, = c(\"id\" = \"id\"))?",
    "code": "\nlibrary(tidyverse)\nmortality_df <- read_csv(here(\"data/gun_violence_us.csv\"))\ntest1 <- tibble(state = \"Washington D.C.\", mortality_rate = 16.7,\n                ownership_rate = 8.7, region = \"NE\")\nbind_rows(mortality_df, test1)\n\ntest2 <- tibble(state = \"Washington D.C.\", mortality_rate = 16.7,\n       ownership_rate = 0.087, region = NE)\n#> Error in eval_tidy(xs[[j]], mask): object 'NE' not found\nbind_rows(mortality_df, test2)\n#> Error in list2(...): object 'test2' not found\n\ntest3 <- tibble(state = \"Washington D.C.\", mortality_rate = \"16.7\",\n       ownership_rate = \"0.087\", region = \"NE\")\nbind_rows(mortality_df, test3)\n#> Error in `bind_rows()`:\n#> ! Can't combine `mortality_rate` <double> and `mortality_rate` <character>.\ndf1 <- as_tibble(state.x77)\ndf2 <- as_tibble(state.abb)\ndf1\n#> # A tibble: 50 × 8\n#>    Population Income Illiteracy Life …¹ Murder HS Gr…² Frost\n#>         <dbl>  <dbl>      <dbl>   <dbl>  <dbl>   <dbl> <dbl>\n#>  1       3615   3624        2.1    69.0   15.1    41.3    20\n#>  2        365   6315        1.5    69.3   11.3    66.7   152\n#>  3       2212   4530        1.8    70.6    7.8    58.1    15\n#>  4       2110   3378        1.9    70.7   10.1    39.9    65\n#>  5      21198   5114        1.1    71.7   10.3    62.6    20\n#>  6       2541   4884        0.7    72.1    6.8    63.9   166\n#>  7       3100   5348        1.1    72.5    3.1    56     139\n#>  8        579   4809        0.9    70.1    6.2    54.6   103\n#>  9       8277   4815        1.3    70.7   10.7    52.6    11\n#> 10       4931   4091        2      68.5   13.9    40.6    60\n#> # … with 40 more rows, 1 more variable: Area <dbl>, and\n#> #   abbreviated variable names ¹​`Life Exp`, ²​`HS Grad`\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\ndf2\n#> # A tibble: 50 × 1\n#>    value\n#>    <chr>\n#>  1 AL   \n#>  2 AK   \n#>  3 AZ   \n#>  4 AR   \n#>  5 CA   \n#>  6 CO   \n#>  7 CT   \n#>  8 DE   \n#>  9 FL   \n#> 10 GA   \n#> # … with 40 more rows\n#> # ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "path": "merging-with-dplyr.html",
    "id": "solutions-10",
    "chapter": " 12 Merging with dplyr",
    "heading": "12.5 Exercise Solutions",
    "text": "",
    "code": ""
  },
  {
    "path": "merging-with-dplyr.html",
    "id": "bind_rows-and-bind_cols-s",
    "chapter": " 12 Merging with dplyr",
    "heading": "12.5.1 bind_rows() and bind_cols() S",
    "text": "* Run following explain R simply stack rows. , fix issue rename() function.",
    "code": "\ndf_test1a <- tibble(xvar = c(1, 2), yvar = c(5, 1))\ndf_test1b <- tibble(x = c(1, 2), y = c(5, 1))\nbind_rows(df_test1a, df_test1b)\n#> # A tibble: 4 × 4\n#>    xvar  yvar     x     y\n#>   <dbl> <dbl> <dbl> <dbl>\n#> 1     1     5    NA    NA\n#> 2     2     1    NA    NA\n#> 3    NA    NA     1     5\n#> 4    NA    NA     2     1\n## This doesn't stack rows because the columns are named differently\n## in the two data sets. If xvar is the same variable as x and \n## yvar is the same variable as y, then we can rename the columns in\n## one of the data sets:\n\ndf_test1a <- df_test1a |> rename(x = \"xvar\", y = \"yvar\")\nbind_rows(df_test1a, df_test1b)\n#> # A tibble: 4 × 2\n#>       x     y\n#>   <dbl> <dbl>\n#> 1     1     5\n#> 2     2     1\n#> 3     1     5\n#> 4     2     1"
  },
  {
    "path": "merging-with-dplyr.html",
    "id": "mutating-joins-s",
    "chapter": " 12 Merging with dplyr",
    "heading": "12.5.2 Mutating Joins S",
    "text": "",
    "code": ""
  },
  {
    "path": "merging-with-dplyr.html",
    "id": "filtering-joins-s",
    "chapter": " 12 Merging with dplyr",
    "heading": "12.5.3 Filtering Joins S",
    "text": "* Take semi_join() tennis example, now suppose want keep matches 2019 winning player losing player 10 match wins 2018. can modify code achieve goal?",
    "code": "\ntennis_2019_10 <- semi_join(atp_2019, win10,\n                            by = c(\"winner_name\" = \"winner_name\"))\ntennis_2019_10$winner_name\n## There are many ways to do this, and this solution gives just one way\n## A first step would be to create a data set that keeps only\n## the katches with losing players with 10 or more wins in 2018\ntennis_2019_10_lose <- semi_join(atp_2019, win10,\n                            by = c(\"loser_name\" = \"winner_name\"))\n\n## Using `bind_rows()` would result in many duplicate matches. A way\n## to avoid duplicates with joining functions is\n\ntennis_temp <- anti_join(tennis_2019_10_lose, tennis_2019_10)\n#> Joining, by = c(\"tourney_id\", \"tourney_name\", \"surface\",\n#> \"draw_size\", \"tourney_level\", \"tourney_date\", \"match_num\",\n#> \"winner_id\", \"winner_seed\", \"winner_entry\", \"winner_name\",\n#> \"winner_hand\", \"winner_ht\", \"winner_ioc\", \"winner_age\",\n#> \"loser_id\", \"loser_seed\", \"loser_entry\", \"loser_name\",\n#> \"loser_hand\", \"loser_ht\", \"loser_ioc\", \"loser_age\",\n#> \"score\", \"best_of\", \"round\", \"minutes\", \"w_ace\", \"w_df\",\n#> \"w_svpt\", \"w_1stIn\", \"w_1stWon\", \"w_2ndWon\", \"w_SvGms\",\n#> \"w_bpSaved\", \"w_bpFaced\", \"l_ace\", \"l_df\", \"l_svpt\",\n#> \"l_1stIn\", \"l_1stWon\", \"l_2ndWon\", \"l_SvGms\", \"l_bpSaved\",\n#> \"l_bpFaced\", \"winner_rank\", \"winner_rank_points\",\n#> \"loser_rank\", \"loser_rank_points\")\ntennis_temp\n## there are 383 matches in the lose data set that aren't in the \n## win data set. Now, we can bind_rows():\n\nfinal_df <- bind_rows(tennis_2019_10, tennis_temp)"
  },
  {
    "path": "merging-with-dplyr.html",
    "id": "chapexercise-10-S",
    "chapter": " 12 Merging with dplyr",
    "heading": "12.5.4 Chapter Exercises S",
    "text": "* Read gun violence data set, suppose want add row data set statistics gun ownership mortality rate District Columbia (Washington D.C., NE region, 16.7 deaths per 100,000 people, gun ownership rate 8.7%). , create tibble() single row representing D.C. combine new tibble overall gun violence data set. Name new data set all_df.* Combine columns states data set made Section Exercise 3 mortality data set without Washington D.C.* Use join function combine mortality data set D.C. states data set Exercise 3. exercise, keep row Washington D.C., take NA values variable observed states data.* Repeat Exercise 5, now drop Washington D.C. merging process. Practice join function (opposed slice() ing explictly).* Use semi_join() create subset states_df NE region. Hint: need filter all_df first contain states NE region.* thing Exercise 7, time, use anti_join(). Hint: ’ll need filter all_df different way achieve .",
    "code": "\nlibrary(tidyverse)\nmortality_df <- read_csv(here(\"data/gun_violence_us.csv\"))\n#> Rows: 50 Columns: 4\n#> ── Column specification ────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (2): state, region\n#> dbl (2): mortality_rate, ownership_rate\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\ndc_df <- tibble(state = \"Washington D.C.\", mortality_rate = 16.7,\n       ownership_rate = 0.087, region = \"NE\")\nall_df <- bind_rows(mortality_df, dc_df)\nbind_cols(mortality_df, states_df)\nleft_join(all_df, states_df, by = c(\"state\" = \"value\"))\n#> # A tibble: 51 × 12\n#>    state mortality_r…¹ owner…² region Popul…³ Income Illit…⁴\n#>    <chr>         <dbl>   <dbl> <chr>    <dbl>  <dbl>   <dbl>\n#>  1 AL             16.7   0.489 South     3615   3624     2.1\n#>  2 AK             18.8   0.617 West       365   6315     1.5\n#>  3 AZ             13.4   0.323 West      2212   4530     1.8\n#>  4 AR             16.4   0.579 South     2110   3378     1.9\n#>  5 CA              7.4   0.201 West     21198   5114     1.1\n#>  6 CO             12.1   0.343 West      2541   4884     0.7\n#>  7 CT              4.9   0.166 NE        3100   5348     1.1\n#>  8 DE             11.1   0.052 NE         579   4809     0.9\n#>  9 FL             11.5   0.325 South     8277   4815     1.3\n#> 10 GA             13.7   0.316 South     4931   4091     2  \n#> # … with 41 more rows, 5 more variables: `Life Exp` <dbl>,\n#> #   Murder <dbl>, `HS Grad` <dbl>, Frost <dbl>, Area <dbl>,\n#> #   and abbreviated variable names ¹​mortality_rate,\n#> #   ²​ownership_rate, ³​Population, ⁴​Illiteracy\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n## or\nfull_join(all_df, states_df, by = c(\"state\" = \"value\"))\n#> # A tibble: 51 × 12\n#>    state mortality_r…¹ owner…² region Popul…³ Income Illit…⁴\n#>    <chr>         <dbl>   <dbl> <chr>    <dbl>  <dbl>   <dbl>\n#>  1 AL             16.7   0.489 South     3615   3624     2.1\n#>  2 AK             18.8   0.617 West       365   6315     1.5\n#>  3 AZ             13.4   0.323 West      2212   4530     1.8\n#>  4 AR             16.4   0.579 South     2110   3378     1.9\n#>  5 CA              7.4   0.201 West     21198   5114     1.1\n#>  6 CO             12.1   0.343 West      2541   4884     0.7\n#>  7 CT              4.9   0.166 NE        3100   5348     1.1\n#>  8 DE             11.1   0.052 NE         579   4809     0.9\n#>  9 FL             11.5   0.325 South     8277   4815     1.3\n#> 10 GA             13.7   0.316 South     4931   4091     2  \n#> # … with 41 more rows, 5 more variables: `Life Exp` <dbl>,\n#> #   Murder <dbl>, `HS Grad` <dbl>, Frost <dbl>, Area <dbl>,\n#> #   and abbreviated variable names ¹​mortality_rate,\n#> #   ²​ownership_rate, ³​Population, ⁴​Illiteracy\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\ninner_join(all_df, states_df, by = c(\"state\" = \"value\"))\n#> # A tibble: 50 × 12\n#>    state mortality_r…¹ owner…² region Popul…³ Income Illit…⁴\n#>    <chr>         <dbl>   <dbl> <chr>    <dbl>  <dbl>   <dbl>\n#>  1 AL             16.7   0.489 South     3615   3624     2.1\n#>  2 AK             18.8   0.617 West       365   6315     1.5\n#>  3 AZ             13.4   0.323 West      2212   4530     1.8\n#>  4 AR             16.4   0.579 South     2110   3378     1.9\n#>  5 CA              7.4   0.201 West     21198   5114     1.1\n#>  6 CO             12.1   0.343 West      2541   4884     0.7\n#>  7 CT              4.9   0.166 NE        3100   5348     1.1\n#>  8 DE             11.1   0.052 NE         579   4809     0.9\n#>  9 FL             11.5   0.325 South     8277   4815     1.3\n#> 10 GA             13.7   0.316 South     4931   4091     2  \n#> # … with 40 more rows, 5 more variables: `Life Exp` <dbl>,\n#> #   Murder <dbl>, `HS Grad` <dbl>, Frost <dbl>, Area <dbl>,\n#> #   and abbreviated variable names ¹​mortality_rate,\n#> #   ²​ownership_rate, ³​Population, ⁴​Illiteracy\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n## or\nleft_join(states_df, all_df, by = c(\"value\" = \"state\"))\n#> # A tibble: 50 × 12\n#>    Population Income Illiteracy Life …¹ Murder HS Gr…² Frost\n#>         <dbl>  <dbl>      <dbl>   <dbl>  <dbl>   <dbl> <dbl>\n#>  1       3615   3624        2.1    69.0   15.1    41.3    20\n#>  2        365   6315        1.5    69.3   11.3    66.7   152\n#>  3       2212   4530        1.8    70.6    7.8    58.1    15\n#>  4       2110   3378        1.9    70.7   10.1    39.9    65\n#>  5      21198   5114        1.1    71.7   10.3    62.6    20\n#>  6       2541   4884        0.7    72.1    6.8    63.9   166\n#>  7       3100   5348        1.1    72.5    3.1    56     139\n#>  8        579   4809        0.9    70.1    6.2    54.6   103\n#>  9       8277   4815        1.3    70.7   10.7    52.6    11\n#> 10       4931   4091        2      68.5   13.9    40.6    60\n#> # … with 40 more rows, 5 more variables: Area <dbl>,\n#> #   value <chr>, mortality_rate <dbl>,\n#> #   ownership_rate <dbl>, region <chr>, and abbreviated\n#> #   variable names ¹​`Life Exp`, ²​`HS Grad`\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\nne_df <- all_df |> filter(region == \"NE\")\nsemi_join(states_df, ne_df, by = c(\"value\" = \"state\"))\n#> # A tibble: 10 × 9\n#>    Popul…¹ Income Illit…² Life …³ Murder HS Gr…⁴ Frost  Area\n#>      <dbl>  <dbl>   <dbl>   <dbl>  <dbl>   <dbl> <dbl> <dbl>\n#>  1    3100   5348     1.1    72.5    3.1    56     139  4862\n#>  2     579   4809     0.9    70.1    6.2    54.6   103  1982\n#>  3    1058   3694     0.7    70.4    2.7    54.7   161 30920\n#>  4    4122   5299     0.9    70.2    8.5    52.3   101  9891\n#>  5    5814   4755     1.1    71.8    3.3    58.5   103  7826\n#>  6     812   4281     0.7    71.2    3.3    57.6   174  9027\n#>  7    7333   5237     1.1    70.9    5.2    52.5   115  7521\n#>  8   18076   4903     1.4    70.6   10.9    52.7    82 47831\n#>  9     931   4558     1.3    71.9    2.4    46.4   127  1049\n#> 10     472   3907     0.6    71.6    5.5    57.1   168  9267\n#> # … with 1 more variable: value <chr>, and abbreviated\n#> #   variable names ¹​Population, ²​Illiteracy, ³​`Life Exp`,\n#> #   ⁴​`HS Grad`\n#> # ℹ Use `colnames()` to see all variable names\nnotne_df <- all_df |> filter(region != \"NE\")\nanti_join(states_df, notne_df, by = c(\"value\" = \"state\"))\n#> # A tibble: 10 × 9\n#>    Popul…¹ Income Illit…² Life …³ Murder HS Gr…⁴ Frost  Area\n#>      <dbl>  <dbl>   <dbl>   <dbl>  <dbl>   <dbl> <dbl> <dbl>\n#>  1    3100   5348     1.1    72.5    3.1    56     139  4862\n#>  2     579   4809     0.9    70.1    6.2    54.6   103  1982\n#>  3    1058   3694     0.7    70.4    2.7    54.7   161 30920\n#>  4    4122   5299     0.9    70.2    8.5    52.3   101  9891\n#>  5    5814   4755     1.1    71.8    3.3    58.5   103  7826\n#>  6     812   4281     0.7    71.2    3.3    57.6   174  9027\n#>  7    7333   5237     1.1    70.9    5.2    52.5   115  7521\n#>  8   18076   4903     1.4    70.6   10.9    52.7    82 47831\n#>  9     931   4558     1.3    71.9    2.4    46.4   127  1049\n#> 10     472   3907     0.6    71.6    5.5    57.1   168  9267\n#> # … with 1 more variable: value <chr>, and abbreviated\n#> #   variable names ¹​Population, ²​Illiteracy, ³​`Life Exp`,\n#> #   ⁴​`HS Grad`\n#> # ℹ Use `colnames()` to see all variable names"
  },
  {
    "path": "merging-with-dplyr.html",
    "id": "rcode-10",
    "chapter": " 12 Merging with dplyr",
    "heading": "12.6 Non-Exercise R Code",
    "text": "",
    "code": "\nlibrary(tidyverse)\nlibrary(here)\natp_2019 <- read_csv(here(\"data/atp_matches_2019.csv\"))\natp_2018 <- read_csv(here(\"data/atp_matches_2018.csv\"))\nhead(atp_2019) \nhead(atp_2018)\nspec(atp_2018)\natp_2018 <- read_csv(here(\"data/atp_matches_2018.csv\"),\n                     col_types = cols(winner_seed = col_character(),\n                                      loser_seed = col_character()))\natp_df <- bind_rows(atp_2018, atp_2019)\natp_df\ndf_test2a <- tibble(xvar = c(1, 2))\ndf_test2b <- tibble(xvar = c(1, 2), y = c(5, 1))\nbind_rows(df_test2a, df_test2b)\ndf_test1a <- tibble(xvar = c(1, 2), yvar = c(5, 1))\ndf_test1b <- tibble(x = c(1, 2), y = c(5, 1))\nbind_cols(df_test1a, df_test1b)\nlibrary(tidyverse)\ndf1 <- tibble(name = c(\"Emily\", \"Miguel\", \"Tonya\"), fav_sport = c(\"Swimming\", \"Football\", \"Tennis\"))\ndf2 <- tibble(name = c(\"Tonya\", \"Miguel\", \"Emily\"),\n              fav_colour = c(\"Robin's Egg Blue\", \"Tickle Me Pink\", \"Goldenrod\"))\n##install.packages(\"babynames\")\nlibrary(babynames)\nlife_df <- babynames::lifetables\nbirth_df <- babynames::births\nbabynames_df <- babynames::babynames\nhead(babynames)\nhead(births)\nhead(lifetables)\ncombined_left <- left_join(babynames_df, birth_df, by = c(\"year\" = \"year\"))\nhead(combined_left)\ntail(combined_left)\n## these will always do the same exact thing\nright_join(babynames_df, birth_df, by = c(\"year\" = \"year\"))\nleft_join(birth_df, babynames_df, by = c(\"year\" = \"year\"))\nfull_join(babynames_df, birth_df, by = c(\"year\" = \"year\"))\ninner_join(babynames_df, birth_df, by = c(\"year\" = \"year\"))\nslumajors_df <- read_csv(here(\"data/SLU_Majors_15_19.csv\"))\ncollegemajors_df <- read_csv(here(\"data/college-majors.csv\"))\nhead(slumajors_df)\nhead(collegemajors_df)\nleft_join(slumajors_df, collegemajors_df, by = c(\"Major\" = \"Major\"))\ncollegemajors_df <- collegemajors_df |>\n  mutate(Major = str_to_title(Major))\nleft_join(slumajors_df, collegemajors_df)\natp_2019 <- read_csv(here(\"data/atp_matches_2019.csv\"))\natp_2018 <- read_csv(here(\"data/atp_matches_2018.csv\"))\natp_2019\natp_2018\nwin10 <- atp_2018 |> group_by(winner_name) |>\n  summarise(nwin = n()) |> \n  filter(nwin >= 10)\nwin10\ntennis_2019_10 <- semi_join(atp_2019, win10,\n                            by = c(\"winner_name\" = \"winner_name\"))\ntennis_2019_10$winner_name\nnew_winners <- anti_join(atp_2019, atp_2018,\n                         by = c(\"winner_name\" = \"winner_name\")) \nnew_winners$winner_name\nnew_winners |> group_by(winner_name) |>\n  summarise(nwin = n()) |>\n  arrange(desc(nwin))"
  },
  {
    "path": "dates-with-lubridate.html",
    "id": "dates-with-lubridate",
    "chapter": " 13 Dates with lubridate",
    "heading": " 13 Dates with lubridate",
    "text": "Goals:use lubridate functions convert character variable <date> variable.use lubridate functions extract useful information <date> variable, including year, month, day week, day year.",
    "code": ""
  },
  {
    "path": "dates-with-lubridate.html",
    "id": "converting-variables-to-date",
    "chapter": " 13 Dates with lubridate",
    "heading": "13.1 Converting Variables to <date>",
    "text": "lubridate package built easily work Date objects DateTime objects. R actually class stores Time objects (unless install separate package). Dates tend much common Times, , primarily focus Dates, functions see easy extensions Times.begin, install lubridate package, load package library(). today() function prints today’s date now() prints today’s date time. can sometimes useful contexts, just run code see R stores dates date-times.first section deal convert variable R Date. use data set holidays Animal Crossing January April. columns data set :Holiday, name holiday andvarious columns different date formatsRead data set withWhich columns specified Dates? example, none columns <date> specification: date columns read character variables.",
    "code": "\nlibrary(tidyverse)\nlibrary(lubridate)\ntoday()\n#> [1] \"2022-09-14\"\nnow()\n#> [1] \"2022-09-14 22:08:19 EDT\"\nlibrary(here)\nholiday_df <- read_csv(here(\"data/animal_crossing_holidays.csv\"))\nholiday_df\n#> # A tibble: 6 × 10\n#>   Holiday    Date1 Date2 Date3 Date4 Date5 Month  Year   Day\n#>   <chr>      <chr> <chr> <chr> <chr> <chr> <dbl> <dbl> <dbl>\n#> 1 New Year'… 1-Ja… Jan-… 1/1/… 1/1/… 2020…     1  2020     1\n#> 2 Groundhog… 2-Fe… Feb-… 2/2/… 2/2/… 2020…     2  2020     2\n#> 3 Valentine… 14-F… Feb-… 2/14… 2020… 2020…     2  2020    14\n#> 4 Shamrock … 17-M… Mar-… 3/17… 2020… 2020…     3  2020    17\n#> 5 Bunny Day  12-A… Apr-… 4/12… 12/4… 2020…     4  2020    12\n#> 6 Earth Day  22-A… Apr-… 4/22… 2020… 2020…     4  2020    22\n#> # … with 1 more variable: Month2 <chr>\n#> # ℹ Use `colnames()` to see all variable names"
  },
  {
    "path": "dates-with-lubridate.html",
    "id": "from-chr-to-date",
    "chapter": " 13 Dates with lubridate",
    "heading": "13.1.1 From <chr> to <date>",
    "text": "use dmy() series functions lubridate convert character variables dates. typically pair new function mutate() statement: much like forcats functions, almost always creating new variable.series dmy()-type variables, corresponding different Day-Month-Year order.dmy() used parse date character vector day first, month second, year last.ymd() used parse date year first, month second, date lastydm() used parse date year first, day second, month last,….dym(), mdy(), myd() work similarly. lubridate usually “smart” picks dates kinds different formats (e.g. can pick specifying October month Oct month 10 month).Let’s try Date1 Date2:Reminder: <date> objects even matter? Compare following two plots: one made date <chr> form date appropriate <date> form.plot ordering x-axis make sense?",
    "code": "\nholiday_df |> mutate(Date_test = dmy(Date1)) |>\n  select(Date_test, everything())\n#> # A tibble: 6 × 11\n#>   Date_test  Holiday     Date1 Date2 Date3 Date4 Date5 Month\n#>   <date>     <chr>       <chr> <chr> <chr> <chr> <chr> <dbl>\n#> 1 2020-01-01 New Year's… 1-Ja… Jan-… 1/1/… 1/1/… 2020…     1\n#> 2 2020-02-02 Groundhog … 2-Fe… Feb-… 2/2/… 2/2/… 2020…     2\n#> 3 2020-02-14 Valentine'… 14-F… Feb-… 2/14… 2020… 2020…     2\n#> 4 2020-03-17 Shamrock D… 17-M… Mar-… 3/17… 2020… 2020…     3\n#> 5 2020-04-12 Bunny Day   12-A… Apr-… 4/12… 12/4… 2020…     4\n#> 6 2020-04-22 Earth Day   22-A… Apr-… 4/22… 2020… 2020…     4\n#> # … with 3 more variables: Year <dbl>, Day <dbl>,\n#> #   Month2 <chr>\n#> # ℹ Use `colnames()` to see all variable names\nholiday_df |> mutate(Date_test = mdy(Date2)) |>\n  select(Date_test, everything())\n#> # A tibble: 6 × 11\n#>   Date_test  Holiday     Date1 Date2 Date3 Date4 Date5 Month\n#>   <date>     <chr>       <chr> <chr> <chr> <chr> <chr> <dbl>\n#> 1 2020-01-01 New Year's… 1-Ja… Jan-… 1/1/… 1/1/… 2020…     1\n#> 2 2020-02-02 Groundhog … 2-Fe… Feb-… 2/2/… 2/2/… 2020…     2\n#> 3 2020-02-14 Valentine'… 14-F… Feb-… 2/14… 2020… 2020…     2\n#> 4 2020-03-17 Shamrock D… 17-M… Mar-… 3/17… 2020… 2020…     3\n#> 5 2020-04-12 Bunny Day   12-A… Apr-… 4/12… 12/4… 2020…     4\n#> 6 2020-04-22 Earth Day   22-A… Apr-… 4/22… 2020… 2020…     4\n#> # … with 3 more variables: Year <dbl>, Day <dbl>,\n#> #   Month2 <chr>\n#> # ℹ Use `colnames()` to see all variable names\nggplot(data = holiday_df, aes(x = Date1, y = Holiday)) +\n  geom_point()\nholiday_df <- holiday_df |> mutate(Date_test_plot = dmy(Date1)) |>\n  select(Date_test_plot, everything())\nggplot(data = holiday_df, aes(x = Date_test_plot, y = Holiday)) +\n  geom_point()"
  },
  {
    "path": "dates-with-lubridate.html",
    "id": "making-a-date-variable-from-date-components",
    "chapter": " 13 Dates with lubridate",
    "heading": "13.1.2 Making a <date> variable from Date Components",
    "text": "Another way create Date object assemble make_date() month, day, year components, stored separate column:, Month stored character (e.g. February) instead number (e.g. 2), problems arise make_date() function:make_date() function requires specific format year, month, day columns. may take little pre-processing put particular data set format.",
    "code": "\nholiday_df |> mutate(Date_test2 = make_date(year = Year,\n                                             month = Month,\n                                             day = Day)) |>\n  select(Date_test2, everything())\n#> # A tibble: 6 × 12\n#>   Date_test2 Date_test_plot Holiday  Date1 Date2 Date3 Date4\n#>   <date>     <date>         <chr>    <chr> <chr> <chr> <chr>\n#> 1 2020-01-01 2020-01-01     New Yea… 1-Ja… Jan-… 1/1/… 1/1/…\n#> 2 2020-02-02 2020-02-02     Groundh… 2-Fe… Feb-… 2/2/… 2/2/…\n#> 3 2020-02-14 2020-02-14     Valenti… 14-F… Feb-… 2/14… 2020…\n#> 4 2020-03-17 2020-03-17     Shamroc… 17-M… Mar-… 3/17… 2020…\n#> 5 2020-04-12 2020-04-12     Bunny D… 12-A… Apr-… 4/12… 12/4…\n#> 6 2020-04-22 2020-04-22     Earth D… 22-A… Apr-… 4/22… 2020…\n#> # … with 5 more variables: Date5 <chr>, Month <dbl>,\n#> #   Year <dbl>, Day <dbl>, Month2 <chr>\n#> # ℹ Use `colnames()` to see all variable names\nholiday_df |> mutate(Date_test2 = make_date(year = Year,\n                                             month = Month2,\n                                             day = Day)) |>\n  select(Date_test2, everything())\n#> # A tibble: 6 × 12\n#>   Date_test2 Date_test_plot Holiday  Date1 Date2 Date3 Date4\n#>   <date>     <date>         <chr>    <chr> <chr> <chr> <chr>\n#> 1 NA         2020-01-01     New Yea… 1-Ja… Jan-… 1/1/… 1/1/…\n#> 2 NA         2020-02-02     Groundh… 2-Fe… Feb-… 2/2/… 2/2/…\n#> 3 NA         2020-02-14     Valenti… 14-F… Feb-… 2/14… 2020…\n#> 4 NA         2020-03-17     Shamroc… 17-M… Mar-… 3/17… 2020…\n#> 5 NA         2020-04-12     Bunny D… 12-A… Apr-… 4/12… 12/4…\n#> 6 NA         2020-04-22     Earth D… 22-A… Apr-… 4/22… 2020…\n#> # … with 5 more variables: Date5 <chr>, Month <dbl>,\n#> #   Year <dbl>, Day <dbl>, Month2 <chr>\n#> # ℹ Use `colnames()` to see all variable names"
  },
  {
    "path": "dates-with-lubridate.html",
    "id": "exercise-11-1",
    "chapter": " 13 Dates with lubridate",
    "heading": "13.1.3 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 13.4.* ’s issue trying convert Date4 <date> form? may want investigate Date4 answer question.* Practice converting Date3 Date5 <date> variables lubridate functions.",
    "code": "\nholiday_df |> mutate(Date_test = ymd(Date4)) |>\n  select(Date_test, everything())\n#> # A tibble: 6 × 12\n#>   Date_test  Date_test_plot Holiday  Date1 Date2 Date3 Date4\n#>   <date>     <date>         <chr>    <chr> <chr> <chr> <chr>\n#> 1 2001-01-20 2020-01-01     New Yea… 1-Ja… Jan-… 1/1/… 1/1/…\n#> 2 2002-02-20 2020-02-02     Groundh… 2-Fe… Feb-… 2/2/… 2/2/…\n#> 3 NA         2020-02-14     Valenti… 14-F… Feb-… 2/14… 2020…\n#> 4 NA         2020-03-17     Shamroc… 17-M… Mar-… 3/17… 2020…\n#> 5 2012-04-20 2020-04-12     Bunny D… 12-A… Apr-… 4/12… 12/4…\n#> 6 NA         2020-04-22     Earth D… 22-A… Apr-… 4/22… 2020…\n#> # … with 5 more variables: Date5 <chr>, Month <dbl>,\n#> #   Year <dbl>, Day <dbl>, Month2 <chr>\n#> # ℹ Use `colnames()` to see all variable names"
  },
  {
    "path": "dates-with-lubridate.html",
    "id": "functions-for-date-variables",
    "chapter": " 13 Dates with lubridate",
    "heading": "13.2 Functions for <date> Variables",
    "text": "object <date> format, special functions lubridate can used date variable. investigate functions, pull stock market data Yahoo using quantmod package. Install package, run following code, gets stock market price data Apple, Nintendo, Chipotle, S & P 500 Index 2011 now. Note ability understand code , skip code now focus new information section (information date functions).’ll chance Exercises choose stocks investigate. now, ’ve made data set three variables:start_date, opening date stock marketStock_Type, factor 4 levels: Apple, Nintendo, Chipotle, S & P 500Price, price stock?First, let’s make line plot shows S & P 500 changed time:, ’s information can get start_date variable. might interested things like day week, monthly trends, yearly trends. extract variables like “weekday” “month” <date> variable, series functions fairly straightforward use. discuss year() month(), mday(), yday(), wday() functions.",
    "code": "\n## install.packages(\"quantmod\")\nlibrary(quantmod)\n\nstart <- ymd(\"2011-01-01\")\nend <- ymd(\"2021-5-19\")\ngetSymbols(c(\"AAPL\", \"NTDOY\", \"CMG\", \"SPY\"), src = \"yahoo\",\n           from = start, to = end)\n#> [1] \"AAPL\"  \"NTDOY\" \"CMG\"   \"SPY\"\n\ndate_tib <- as_tibble(index(AAPL)) |>\n  rename(start_date = value)\napp_tib <- as_tibble(AAPL)\nnint_tib <- as_tibble(NTDOY)\nchip_tib <- as_tibble(CMG)\nspy_tib <- as_tibble(SPY)\nall_stocks <- bind_cols(date_tib, app_tib, nint_tib, chip_tib, spy_tib)\n\nstocks_long <- all_stocks |>\n  select(start_date, AAPL.Adjusted, NTDOY.Adjusted,\n                      CMG.Adjusted, SPY.Adjusted) |>\n  pivot_longer(2:5, names_to = \"Stock_Type\", values_to = \"Price\") |>\n  mutate(Stock_Type = fct_recode(Stock_Type,\n                                 Apple = \"AAPL.Adjusted\",\n                                 Nintendo = \"NTDOY.Adjusted\",\n                                 Chipotle = \"CMG.Adjusted\",\n                                 `S & P 500` = \"SPY.Adjusted\"\n                                 ))\ntail(stocks_long)\n#> # A tibble: 6 × 3\n#>   start_date Stock_Type  Price\n#>   <date>     <fct>       <dbl>\n#> 1 2021-05-17 Chipotle   1332. \n#> 2 2021-05-17 S & P 500   408. \n#> 3 2021-05-18 Apple       124. \n#> 4 2021-05-18 Nintendo     70.4\n#> 5 2021-05-18 Chipotle   1325. \n#> 6 2021-05-18 S & P 500   405.\nstocks_sp <- stocks_long |> filter(Stock_Type == \"S & P 500\")\nggplot(data = stocks_sp, aes(x = start_date, y = Price)) +\n  geom_line()"
  },
  {
    "path": "dates-with-lubridate.html",
    "id": "year-month-and-mday",
    "chapter": " 13 Dates with lubridate",
    "heading": "13.2.1 year(), month(), and mday()",
    "text": "functions year(), month(), mday() can grab year, month, day month, respectively, <date> variable. Like forcats functions, almost always paired mutate() statement create new variable:",
    "code": "\nstocks_long |> mutate(year_stock = year(start_date))\nstocks_long |> mutate(month_stock = month(start_date))\nstocks_long |> mutate(day_stock = mday(start_date))"
  },
  {
    "path": "dates-with-lubridate.html",
    "id": "yday-and-wday",
    "chapter": " 13 Dates with lubridate",
    "heading": "13.2.2 yday() and wday()",
    "text": "yday() function grabs day year <date> object. example,returns 309, indicating November 4th 309th day year 2020. Using function mutate() statement creates new variable yday observation:Finally, function wday() grabs day week <date>. default, wday() puts day week numeric, find confusing, can’t ever remember whether 1 means Sunday 1 means Monday. Adding, label = TRUE creates weekday variable Sunday, Monday, Tuesday, etc.:Possible uses functions :want look differences years (year())want look differences years (year())want look differences months (month())want look differences months (month())want look differences days week (wday())want look differences days week (wday())want see whether yearly trends within years (yday())want see whether yearly trends within years (yday())Note: Working times extremely similar working dates. Instead ymd(), mdy(), etc., tack extra letters specify order hour, minute, seconds appear variable: ymd_hms() converts character vector order year, month, day, hour, minute, second <datetime>.Additionally, functions hour(), minute(), second() grab hour, minute, second <datetime> variable.Note Complications: Things can get complicated, especially start consider things like time duration. reason time system inherently confusing. Consider following might affect analysis involving time duration:time zonesleap years (years number days)differing number days given monthdaylight saving time (days number hours)",
    "code": "\ntest <- mdy(\"November 4, 2020\")\nyday(test)\n#> [1] 309\nstocks_long |> mutate(day_in_year = yday(start_date))\n#> # A tibble: 10,444 × 4\n#>    start_date Stock_Type Price day_in_year\n#>    <date>     <fct>      <dbl>       <dbl>\n#>  1 2011-01-03 Apple       10.0           3\n#>  2 2011-01-03 Nintendo    36.7           3\n#>  3 2011-01-03 Chipotle   224.            3\n#>  4 2011-01-03 S & P 500  102.            3\n#>  5 2011-01-04 Apple       10.1           4\n#>  6 2011-01-04 Nintendo    35.5           4\n#>  7 2011-01-04 Chipotle   222.            4\n#>  8 2011-01-04 S & P 500  102.            4\n#>  9 2011-01-05 Apple       10.2           5\n#> 10 2011-01-05 Nintendo    34.6           5\n#> # … with 10,434 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\nstocks_long |> mutate(day_of_week = wday(start_date))\nstocks_long |> mutate(day_of_week = wday(start_date,\n                                          label = TRUE, abbr = FALSE))"
  },
  {
    "path": "dates-with-lubridate.html",
    "id": "exercise-11-2",
    "chapter": " 13 Dates with lubridate",
    "heading": "13.2.3 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 13.4.month() function gives numbers corresponding month default. Type ?month figure argument need change get names (January, February, etc.) instead month numbers. abbreviations (Jan, Feb, etc.) month instead month numbers? Try making changes mutate() statement .",
    "code": "\nstocks_long |> mutate(month_stock = month(start_date))"
  },
  {
    "path": "dates-with-lubridate.html",
    "id": "chapexercise-11",
    "chapter": " 13 Dates with lubridate",
    "heading": "13.3 Chapter Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 13.4.truncated argument ymd(), dmy(), mdy(), etc. allow R parse dates aren’t actually complete. example,parses 2019 January 1, 2019 month day missing. 2 means last two parts date (case, month day) allowed missing. Similarly,truncates year (given 0000). truncate function usually useful context first example truncated month /day.Examine ds_google.csv, containsMonth, year month 2004 nowData_Science, relative popularity data science (Google keeps calculates “popularity” somewhat mystery likely based number times people search term “Data Science”)* Use lubridate function truncated option convert Month variable <date> format.* Use lubridate function truncated option convert Month variable <date> format.* Make plot popularity Data Science Time. Add smoother plot. patterns notice?* Make plot popularity Data Science Time. Add smoother plot. patterns notice?data obtained Google Trends: Google Trends. Google Trends incredibly cool explore, even without R.* Google Trends, Enter search term, change Time dropdown menu 2004-present. , enter second search term want compare. can also change country want (, can keep country United States).search terms “super smash” “animal crossing”, something interests !top-right window graph, click arrow download data set. Delete first two rows data set (either Excel R), read data set, change date variable ’s Date format.* Make plot Popularity variables time. Hint: data set need tidied first?* Make plot Popularity variables time. Hint: data set need tidied first?* Using data set explored variable two 2004 now, make table average popularity year. Hint: ’ll need lubridate function extract year variable date object.* Using data set explored variable two 2004 now, make table average popularity year. Hint: ’ll need lubridate function extract year variable date object.* Clear search now enter search term ’d like investigate past 90 days. Mine “Pittsburgh Steelers” , , something interests .* Clear search now enter search term ’d like investigate past 90 days. Mine “Pittsburgh Steelers” , , something interests ., click download button read data R. Convert date variable <date> format.* Make plot popularity variable time, adding smoother.* Make plot popularity variable time, adding smoother.Using data set explored variable past 90 days, construct table compares average popularity day week (Monday Saturday).Using data set explored variable past 90 days, construct table compares average popularity day week (Monday Saturday).Examine ds_df data set , data set data science Google Trends, suppose observation day every year (just one observation per month). want look whether data science popular certain days week. Explain following strategy wouldn’t really work well.Examine ds_df data set , data set data science Google Trends, suppose observation day every year (just one observation per month). want look whether data science popular certain days week. Explain following strategy wouldn’t really work well.create weekday variable wday()use summarise() group_by() find average popularity day weekUse code tutorial section Stocks data get data frame stock prices couple different stocks interest . start end date use completely .Explore stock data chose, constructing line plot price time, well graphs summaries show interesting patterns across years, months, days, days week, etc.Use lag() function create new variable previous day’s stock price. Can predict current stock price based previous day’s stock price accurately? ? Use either graphical numerical evidence.",
    "code": "\nlibrary(lubridate)\nymd(\"2019\", truncated = 2)\n#> [1] \"2019-01-01\"\ndmy(\"19-10\", truncated = 1)\n#> [1] \"0000-10-19\"\nlibrary(tidyverse)\nlibrary(lubridate)\nds_df <- read_csv(here(\"data/ds_google.csv\"))\nds_df\n#> # A tibble: 202 × 2\n#>    Month   Data_Science\n#>    <chr>          <dbl>\n#>  1 2004-01           14\n#>  2 2004-02            8\n#>  3 2004-03           16\n#>  4 2004-04           11\n#>  5 2004-05            5\n#>  6 2004-06            8\n#>  7 2004-07            7\n#>  8 2004-08            9\n#>  9 2004-09           13\n#> 10 2004-10           11\n#> # … with 192 more rows\n#> # ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "path": "dates-with-lubridate.html",
    "id": "solutions-11",
    "chapter": " 13 Dates with lubridate",
    "heading": "13.4 Exercise Solutions",
    "text": "",
    "code": ""
  },
  {
    "path": "dates-with-lubridate.html",
    "id": "converting-variables-to-date-s",
    "chapter": " 13 Dates with lubridate",
    "heading": "13.4.1 Converting Variables to <date> S",
    "text": "* ’s issue trying convert Date4 <date> form?* Practice converting Date3 Date5 date objects lubridate functions.",
    "code": "\nholiday_df |> mutate(Date_test = ymd(Date4)) |>\n  select(Date_test, everything())\n#> # A tibble: 6 × 12\n#>   Date_test  Date_test_plot Holiday  Date1 Date2 Date3 Date4\n#>   <date>     <date>         <chr>    <chr> <chr> <chr> <chr>\n#> 1 2001-01-20 2020-01-01     New Yea… 1-Ja… Jan-… 1/1/… 1/1/…\n#> 2 2002-02-20 2020-02-02     Groundh… 2-Fe… Feb-… 2/2/… 2/2/…\n#> 3 NA         2020-02-14     Valenti… 14-F… Feb-… 2/14… 2020…\n#> 4 NA         2020-03-17     Shamroc… 17-M… Mar-… 3/17… 2020…\n#> 5 2012-04-20 2020-04-12     Bunny D… 12-A… Apr-… 4/12… 12/4…\n#> 6 NA         2020-04-22     Earth D… 22-A… Apr-… 4/22… 2020…\n#> # … with 5 more variables: Date5 <chr>, Month <dbl>,\n#> #   Year <dbl>, Day <dbl>, Month2 <chr>\n#> # ℹ Use `colnames()` to see all variable names\n## Date4 has two __different__ formats, \n## which creates problems for `lubridate` functions\nholiday_df |> mutate(Date_test = mdy(Date3)) |>\n  select(Date_test, everything())\n#> # A tibble: 6 × 12\n#>   Date_test  Date_test_plot Holiday  Date1 Date2 Date3 Date4\n#>   <date>     <date>         <chr>    <chr> <chr> <chr> <chr>\n#> 1 2020-01-01 2020-01-01     New Yea… 1-Ja… Jan-… 1/1/… 1/1/…\n#> 2 2020-02-02 2020-02-02     Groundh… 2-Fe… Feb-… 2/2/… 2/2/…\n#> 3 2020-02-14 2020-02-14     Valenti… 14-F… Feb-… 2/14… 2020…\n#> 4 2020-03-17 2020-03-17     Shamroc… 17-M… Mar-… 3/17… 2020…\n#> 5 2020-04-12 2020-04-12     Bunny D… 12-A… Apr-… 4/12… 12/4…\n#> 6 2020-04-22 2020-04-22     Earth D… 22-A… Apr-… 4/22… 2020…\n#> # … with 5 more variables: Date5 <chr>, Month <dbl>,\n#> #   Year <dbl>, Day <dbl>, Month2 <chr>\n#> # ℹ Use `colnames()` to see all variable names\nholiday_df |> mutate(Date_test = ymd(Date5)) |>\n  select(Date_test, everything())\n#> # A tibble: 6 × 12\n#>   Date_test  Date_test_plot Holiday  Date1 Date2 Date3 Date4\n#>   <date>     <date>         <chr>    <chr> <chr> <chr> <chr>\n#> 1 2020-01-01 2020-01-01     New Yea… 1-Ja… Jan-… 1/1/… 1/1/…\n#> 2 2020-02-02 2020-02-02     Groundh… 2-Fe… Feb-… 2/2/… 2/2/…\n#> 3 2020-02-14 2020-02-14     Valenti… 14-F… Feb-… 2/14… 2020…\n#> 4 2020-03-17 2020-03-17     Shamroc… 17-M… Mar-… 3/17… 2020…\n#> 5 2020-04-12 2020-04-12     Bunny D… 12-A… Apr-… 4/12… 12/4…\n#> 6 2020-04-22 2020-04-22     Earth D… 22-A… Apr-… 4/22… 2020…\n#> # … with 5 more variables: Date5 <chr>, Month <dbl>,\n#> #   Year <dbl>, Day <dbl>, Month2 <chr>\n#> # ℹ Use `colnames()` to see all variable names"
  },
  {
    "path": "dates-with-lubridate.html",
    "id": "functions-for-date-variables-s",
    "chapter": " 13 Dates with lubridate",
    "heading": "13.4.2 Functions for <date> Variables S",
    "text": "",
    "code": ""
  },
  {
    "path": "dates-with-lubridate.html",
    "id": "chapexercise-11-S",
    "chapter": " 13 Dates with lubridate",
    "heading": "13.4.3 Chapter Exercises S",
    "text": "* Use lubridate function truncated option convert Month variable <date> format.* Make plot popularity Data Science Time. Add smoother plot. patterns notice?* Google Trends, Enter search term, change Time dropdown menu 2004-present. , enter second search term want compare. can also change country want (, can keep country United States).search terms “super smash” “animal crossing”, something interests !top-right window graph, click arrow download data set. Delete first two rows data set (either Excel R), read data set, change date variable ’s Date format.* Make plot Popularity variables time. Hint: data set need tidied first?* Using data set explored variable two 2004 now, make table average popularity year. Hint: ’ll need lubridate function extract year variable date object.* Using data set explored variable two 2004 now, make table average popularity year. Hint: ’ll need lubridate function extract year variable date object.* Clear search now enter search term ’d like investigate past 90 days. Mine “pittsburgh steelers” , , something interests .* Clear search now enter search term ’d like investigate past 90 days. Mine “pittsburgh steelers” , , something interests ., click download button read data R. Convert date variable <date> format.* Make plot popularity variable time, adding smoother.",
    "code": "\nds_df <- ds_df |> mutate(Month = ymd(Month, truncated = 1))\nds_df\nggplot(data = ds_df, aes(x = Month, y = Data_Science)) +\n  geom_line() +\n  geom_smooth(se = FALSE)\n#> `geom_smooth()` using method = 'loess' and formula 'y ~ x'\n## it's like super popular!!!!\nvideogame_df <- read_csv(here(\"data/smash_animal_crossing.csv\"))\n#> Rows: 203 Columns: 3\n#> ── Column specification ────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (1): Month\n#> dbl (2): super_smash, animal_crossing\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nvideogame_df <- videogame_df |> mutate(date = ymd(Month, truncated = 1))\nvideogame_long <- videogame_df |>\n  pivot_longer(cols = c(\"super_smash\", \"animal_crossing\"),\n                              names_to = \"game\",\n                              values_to = \"popularity\")\nggplot(data = videogame_long, aes(x = date, \n                                  y = popularity,\n                                  colour = game)) +\n  geom_line() +\n  scale_colour_viridis_d(begin = 0, end = 0.9)\nsteelers_df <- read_csv(here(\"data/steelers.csv\"))\n#> Rows: 91 Columns: 2\n#> ── Column specification ────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (1): Day\n#> dbl (1): Steelers\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nsteelers_df <- steelers_df |> mutate(day_var = mdy(Day))\nggplot(data = steelers_df, aes(x = day_var, y = Steelers)) +\n  geom_smooth() + \n  geom_line() +\n  labs(y = \"Popularity\")\n#> `geom_smooth()` using method = 'loess' and formula 'y ~ x'"
  },
  {
    "path": "dates-with-lubridate.html",
    "id": "rcode-11",
    "chapter": " 13 Dates with lubridate",
    "heading": "13.5 Non-Exercise R Code",
    "text": "",
    "code": "\nlibrary(tidyverse)\nlibrary(lubridate)\ntoday()\nnow()\nlibrary(here)\nholiday_df <- read_csv(here(\"data/animal_crossing_holidays.csv\"))\nholiday_df\nholiday_df |> mutate(Date_test = dmy(Date1)) |>\n  select(Date_test, everything())\nholiday_df |> mutate(Date_test = mdy(Date2)) |>\n  select(Date_test, everything())\nggplot(data = holiday_df, aes(x = Date1, y = Holiday)) +\n  geom_point()\nholiday_df <- holiday_df |> mutate(Date_test_plot = dmy(Date1)) |>\n  select(Date_test_plot, everything())\nggplot(data = holiday_df, aes(x = Date_test_plot, y = Holiday)) +\n  geom_point()\nholiday_df |> mutate(Date_test2 = make_date(year = Year,\n                                             month = Month,\n                                             day = Day)) |>\n  select(Date_test2, everything())\nholiday_df |> mutate(Date_test2 = make_date(year = Year,\n                                             month = Month2,\n                                             day = Day)) |>\n  select(Date_test2, everything())\n## install.packages(\"quantmod\")\nlibrary(quantmod)\n\nstart <- ymd(\"2011-01-01\")\nend <- ymd(\"2021-5-19\")\ngetSymbols(c(\"AAPL\", \"NTDOY\", \"CMG\", \"SPY\"), src = \"yahoo\",\n           from = start, to = end)\n\ndate_tib <- as_tibble(index(AAPL)) |>\n  rename(start_date = value)\napp_tib <- as_tibble(AAPL)\nnint_tib <- as_tibble(NTDOY)\nchip_tib <- as_tibble(CMG)\nspy_tib <- as_tibble(SPY)\nall_stocks <- bind_cols(date_tib, app_tib, nint_tib, chip_tib, spy_tib)\n\nstocks_long <- all_stocks |>\n  select(start_date, AAPL.Adjusted, NTDOY.Adjusted,\n                      CMG.Adjusted, SPY.Adjusted) |>\n  pivot_longer(2:5, names_to = \"Stock_Type\", values_to = \"Price\") |>\n  mutate(Stock_Type = fct_recode(Stock_Type,\n                                 Apple = \"AAPL.Adjusted\",\n                                 Nintendo = \"NTDOY.Adjusted\",\n                                 Chipotle = \"CMG.Adjusted\",\n                                 `S & P 500` = \"SPY.Adjusted\"\n                                 ))\ntail(stocks_long)\nstocks_sp <- stocks_long |> filter(Stock_Type == \"S & P 500\")\nggplot(data = stocks_sp, aes(x = start_date, y = Price)) +\n  geom_line()\nstocks_long |> mutate(year_stock = year(start_date))\nstocks_long |> mutate(month_stock = month(start_date))\nstocks_long |> mutate(day_stock = mday(start_date))\ntest <- mdy(\"November 4, 2020\")\nyday(test)\nstocks_long |> mutate(day_in_year = yday(start_date))\nstocks_long |> mutate(day_of_week = wday(start_date))\nstocks_long |> mutate(day_of_week = wday(start_date,\n                                          label = TRUE, abbr = FALSE))"
  },
  {
    "path": "text-data-with-tidytext-and-stringr.html",
    "id": "text-data-with-tidytext-and-stringr",
    "chapter": " 14 Text Data with tidytext and stringr",
    "heading": " 14 Text Data with tidytext and stringr",
    "text": "Goals:use functions stringr package tidytext package analyze text data.introduce issues manipulating strings don’t pertain numeric factor data.perform basic sentiment analysis.",
    "code": ""
  },
  {
    "path": "text-data-with-tidytext-and-stringr.html",
    "id": "text-analysis",
    "chapter": " 14 Text Data with tidytext and stringr",
    "heading": "14.1 Text Analysis",
    "text": "Beyonce legend. example, work text analysis lyrics songs Beyonce’s albums, utilizing functions stringr parse strings tidytext convert text data tidy format. begin, read data set Beyonce’s lyrics:focused line variable, value variable contains line Beyonce song. ’s variables present well, song_name artist_name (data set originally came data set artists Beyonce).can look first 4 values line withOur end goal construct plot shows popular words Beyonce’s albums. much challenging sounds deal nuances working text data.tidytext package makes lot easier work text data many regards. Let’s use unnest_tokens() functions tidytext separate lines individual words. ’ll name new data set beyonce_unnest:’ll want make sure either words capitalized words capitalized, consistency (remember R case-sensitive). end, ’ll modify word variable use stringr’s str_to_lower() change letters lower-case:Let’s try counting Beyonce’s popular words data set just made:’s issue ?remedy , can use called stop words: words common carry little meaningful information. example , , , etc. stop words. need eliminate data set continue . Luckily, tidytext package also provides data set common stop words data set named stop_words:Let’s join Beyonce lyrics data set stop words data set elminate stop words:, can re-make table stop words removed:Looking list, still stop words picked stop_words data set. address , well make plot, exercises.",
    "code": "\nlibrary(tidyverse)\nlibrary(here)\nbeyonce <- read_csv(here(\"data/beyonce_lyrics.csv\"))\nhead(beyonce)\nbeyonce$line[1:4]\n#> [1] \"If I ain't got nothing, I got you\"                       \n#> [2] \"If I ain't got something, I don't give a damn\"           \n#> [3] \"'Cause I got it with you\"                                \n#> [4] \"I don't know much about algebra, but I know 1+1 equals 2\"\nlibrary(tidytext)\nbeyonce_unnest <- beyonce |> unnest_tokens(output = \"word\", input = \"line\")\nbeyonce_unnest\n#> # A tibble: 164,740 × 6\n#>    song_id song_name artist_id artist_name song_line word   \n#>      <dbl> <chr>         <dbl> <chr>           <dbl> <chr>  \n#>  1   50396 1+1             498 Beyoncé             1 if     \n#>  2   50396 1+1             498 Beyoncé             1 i      \n#>  3   50396 1+1             498 Beyoncé             1 ain't  \n#>  4   50396 1+1             498 Beyoncé             1 got    \n#>  5   50396 1+1             498 Beyoncé             1 nothing\n#>  6   50396 1+1             498 Beyoncé             1 i      \n#>  7   50396 1+1             498 Beyoncé             1 got    \n#>  8   50396 1+1             498 Beyoncé             1 you    \n#>  9   50396 1+1             498 Beyoncé             2 if     \n#> 10   50396 1+1             498 Beyoncé             2 i      \n#> # … with 164,730 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\nbeyonce_unnest <- beyonce_unnest |> mutate(word = str_to_lower(word))\nbeyonce_unnest |> group_by(word) |>\n  summarise(n = n()) |>\n  arrange(desc(n))\n#> # A tibble: 6,469 × 2\n#>    word      n\n#>    <chr> <int>\n#>  1 you    7693\n#>  2 i      6669\n#>  3 the    4719\n#>  4 me     3774\n#>  5 to     3070\n#>  6 it     2999\n#>  7 a      2798\n#>  8 my     2676\n#>  9 and    2385\n#> 10 on     2344\n#> # … with 6,459 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\nhead(stop_words)\n#> # A tibble: 6 × 2\n#>   word      lexicon\n#>   <chr>     <chr>  \n#> 1 a         SMART  \n#> 2 a's       SMART  \n#> 3 able      SMART  \n#> 4 about     SMART  \n#> 5 above     SMART  \n#> 6 according SMART\nbeyonce_stop <- anti_join(beyonce_unnest, stop_words, by = c(\"word\" = \"word\"))\nbeyonce_sum <- beyonce_stop |> group_by(word) |>\n  summarise(n = n()) |>\n  arrange(desc(n)) |>\n  print(n = 25)\n#> # A tibble: 5,937 × 2\n#>    word        n\n#>    <chr>   <int>\n#>  1 love     1362\n#>  2 baby     1024\n#>  3 girl      592\n#>  4 wanna     564\n#>  5 hey       499\n#>  6 boy       494\n#>  7 yeah      491\n#>  8 feel      488\n#>  9 time      452\n#> 10 uh        408\n#> 11 halo      383\n#> 12 check     366\n#> 13 tonight   342\n#> 14 girls     341\n#> 15 ya        327\n#> 16 run       325\n#> 17 crazy     308\n#> 18 world     301\n#> 19 body      287\n#> 20 ooh       281\n#> 21 ladies    269\n#> 22 top       241\n#> 23 gotta     240\n#> 24 beyoncé   238\n#> 25 night     213\n#> # … with 5,912 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\nbeyonce_sum\n#> # A tibble: 5,937 × 2\n#>    word      n\n#>    <chr> <int>\n#>  1 love   1362\n#>  2 baby   1024\n#>  3 girl    592\n#>  4 wanna   564\n#>  5 hey     499\n#>  6 boy     494\n#>  7 yeah    491\n#>  8 feel    488\n#>  9 time    452\n#> 10 uh      408\n#> # … with 5,927 more rows\n#> # ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "path": "text-data-with-tidytext-and-stringr.html",
    "id": "exercise-12-1",
    "chapter": " 14 Text Data with tidytext and stringr",
    "heading": "14.1.1 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 14.5.Look remaining words. look like stop words missed stop words tidytext package? Create tibble remaining stop words (like ooh, gotta, ya, uh, yeah) picked tidytext package, use join function drop words data set.Look remaining words. look like stop words missed stop words tidytext package? Create tibble remaining stop words (like ooh, gotta, ya, uh, yeah) picked tidytext package, use join function drop words data set.new data set, construct point plot bar plot shows 20 common words Beyonce uses, well number times word used.new data set, construct point plot bar plot shows 20 common words Beyonce uses, well number times word used.Use wordcloud() function wordcloud library code make wordcloud Beyonce’s words.Use wordcloud() function wordcloud library code make wordcloud Beyonce’s words., use ?wordcloud read various arguments like random.order, scale, random.color .want delve text data , ’ll need learn regular expressions , regexes. interested, can read R4DS textbook. Starting bad, learning escaping special characters R can much challenging!analyzed short text data set, , can imagine extending type analysis things like:song lyrics, lyrics songs artist https://rpubs.com/RosieB/taylorswiftlyricanalysisbook analysis, text entire book series bookstv analysis, scripts episodes tv showIf one analyses, lots cool functions tidytext help ! one example, time looking Donald Trump’s twitter account 2016.",
    "code": "\n## install.packages(\"wordcloud\")\nlibrary(wordcloud)\n#> Loading required package: RColorBrewer\nbeyonce_small <- beyonce_sum |> filter(n > 50)\nwordcloud(beyonce_small$word, beyonce_small$n, \n          colors = brewer.pal(8, \"Dark2\"), scale = c(5, .2),\n          random.order = FALSE, random.color = FALSE)"
  },
  {
    "path": "text-data-with-tidytext-and-stringr.html",
    "id": "basic-sentiment-analysis",
    "chapter": " 14 Text Data with tidytext and stringr",
    "heading": "14.2 Basic Sentiment Analysis",
    "text": "use provided .qmd file replicate sentiment analysis Trump’s twitter account 2016. analysis used conjunction major news story hypothesized Trump wrote tweets Android device campaign staff wrote tweets iPhone device. investigate properties tweets led author believe ..qmd file used posted Canvas. see uses stringr particular analysis. entire section, able follow along understand line code . However, unlike previous sections, expected sentiment analysis .",
    "code": ""
  },
  {
    "path": "text-data-with-tidytext-and-stringr.html",
    "id": "introduction-to-stringr",
    "chapter": " 14 Text Data with tidytext and stringr",
    "heading": "14.3 Introduction to stringr",
    "text": "previous examples, string data consisted primarily words. tools tidytext make working data consisting words painful. However, data exists strings words. non-trivial example, consider data sets obtained https://github.com/JeffSackmann/tennis_MatchChartingProject, repository professional tennis match charting put together Jeff Sackmann. following code modified project completed James Wolpe data visualization course.repository, put together data set one particular tennis match make bit easier us get started. match chosen 2021 U.S. Open Final Daniil Medvedev Novak Djokovic. match? arguably important match Djokovic’s career: won, win four grand slams calendar year. don’t like Djokovic lost looking back match brings joy. Read data set :observations data set correspond points played (one row per point). ton variables data set, important variable first variable, point, contains string information types shots played point. coding point variable includes:4 serve wide, 5 serve body, 6 serve “t (center)”.f forehand stroke, b backhand stroke.1 right-hander’s forehand side, 2 middle court, 3 right-hander’s backhand side.d ball hit deep, w ball hit wide, n ball hit net@ symbol end point ended unforced errorand ’s lots numbers symbols correspond things (volleys, return depths, hitting top net, etc.)example, Djokovic served 7th point match, point value 4f18f1f2b3b2f1w@. reads that4: Djokovic served wide,f18: Medvedev hit forehand cross-court Djokovic’s forehand sidef1: Djokovic hit forehand cross-court Medvedev’s forehand sidef2: Medvedev hit forehand center courtb3: Djokovic hit backhand Medvedev’s backhand sideb2: Medvedev hit backhand center courtf1w@: Djokovic hit forehand Medvedev’s forehand side, shot landed wide recorded unforced error.Clearly, lot data encoded point variable. going introduce stringr answering relatively simple question: serving patterns Medvedev Djokovic match?",
    "code": "\nlibrary(here)\nlibrary(tidyverse)\nmed_djok_df <- read_csv(here(\"data/med_djok.csv\"))\n#> Rows: 182 Columns: 46\n#> ── Column specification ────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (19): point, Serving, match_id, Pts, Gm#, 1st, 2nd, ...\n#> dbl (19): Pt, Set1, Set2, Gm1, Gm2, TbSet, TB?, Svr, Ret...\n#> lgl  (8): TBpt, isAce, isUnret, isRallyWinner, isForced,...\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nhead(med_djok_df)\n#> # A tibble: 6 × 46\n#>   point  Serving match…¹    Pt  Set1  Set2   Gm1   Gm2 Pts  \n#>   <chr>  <chr>   <chr>   <dbl> <dbl> <dbl> <dbl> <dbl> <chr>\n#> 1 4f2d@  ND      202109…     1     0     0     0     0 0-0  \n#> 2 6d     ND      202109…     2     0     0     0     0 15-0 \n#> 3 6b29f… ND      202109…     3     0     0     0     0 15-15\n#> 4 4b28f… ND      202109…     4     0     0     0     0 30-15\n#> 5 5b37b… ND      202109…     5     0     0     0     0 40-15\n#> 6 6f28f… ND      202109…     6     0     0     0     0 40-30\n#> # … with 37 more variables: `Gm#` <chr>, TbSet <dbl>,\n#> #   `TB?` <dbl>, TBpt <lgl>, Svr <dbl>, Ret <dbl>,\n#> #   `1st` <chr>, `2nd` <chr>, Notes <chr>, `1stSV` <dbl>,\n#> #   `2ndSV` <dbl>, `1stIn` <dbl>, `2ndIn` <dbl>,\n#> #   isAce <lgl>, isUnret <lgl>, isRallyWinner <lgl>,\n#> #   isForced <lgl>, isUnforced <lgl>, isDouble <lgl>,\n#> #   PtWinner <dbl>, isSvrWinner <dbl>, rallyCount <dbl>, …\n#> # ℹ Use `colnames()` to see all variable names"
  },
  {
    "path": "text-data-with-tidytext-and-stringr.html",
    "id": "regular-expressions",
    "chapter": " 14 Text Data with tidytext and stringr",
    "heading": "14.3.1 Regular Expressions",
    "text": "regex, regular expression, string used identify particular patterns string data. Regular expressions used many languages (, google something regular expression, need limit just looking resources pertaining R).Regex’s can used functions stringr package. functions stringr package begin str_(), much like functions forcats began fct_(). first focus str_detect() function, detects whether particular regex present string variable. str_detect() takes name string first argument regex second argument. example,returns TRUE letter f appears anywhere string FALSE . , can examine many points forehand hit Medvedev Djokovic match. second example,returns TRUE d@ appears string FALSE . Note d@ must appear together order return TRUE. lets us examine many points ball hit deep recorded unforced error. looks likepoints ended unforced error ball hit deep,points ended unforced error ball hit wide, andpoints ended unforced error ball hit net.",
    "code": "\nstr_detect(med_djok_df$point, pattern = \"f\")\n#>   [1]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n#>  [10] FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE\n#>  [19]  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n#>  [28] FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE\n#>  [37]  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE\n#>  [46]  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE\n#>  [55]  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE  TRUE FALSE\n#>  [64]  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n#>  [73] FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE\n#>  [82]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE\n#>  [91]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE\n#> [100]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n#> [109] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE\n#> [118] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n#> [127]  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n#> [136]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE\n#> [145] FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE\n#> [154] FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE\n#> [163] FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE\n#> [172]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n#> [181] FALSE FALSE\nstr_detect(med_djok_df$point, pattern = \"d@\")\nsum(str_detect(med_djok_df$point, pattern = \"d@\"))\n#> [1] 21\nsum(str_detect(med_djok_df$point, pattern = \"w@\"))\n#> [1] 19\nsum(str_detect(med_djok_df$point, pattern = \"n@\"))\n#> [1] 22"
  },
  {
    "path": "text-data-with-tidytext-and-stringr.html",
    "id": "stringr-functions-with-dplyr",
    "chapter": " 14 Text Data with tidytext and stringr",
    "heading": "14.3.2 stringr Functions with dplyr",
    "text": "can combine stringr functions dplyr functions already know love. example, interested points end unforced error (points @ symbol ), can filter points don’t @:can use mutate() case_when() create variable corresponding error type summarise() error types made two players.output , PtWinner 1 corresponds points Djokovic won (therefore points Medvedev made unforced error) PtWinner 2 corresponds points Medvedev won (therefore points Djokovic made unforced error). see , match, Djokovic unforced errors overall. Medvedev’s unforced errors tended deep wide highest proportion Djokovic’s unforced errors balls went net.explore original “service patterns” question exercises. close section, just emphasize done simple introduction regexes. can get cumbersome, especially patterns want extract get complicated. Consider examples .Detect points aces, coded variable *. Regexes “special characters, like \\, *, ., , present variable need ”escaped” backslash. , backslash special character, needs escaped : need two \\\\ front * pull points *.Detect points start 4 using ^ denote “beginning”:Detect points end @ using $ denote “end” (safer code , just assumed @ appear anywhere else string except end).\n* Extract forehand shots hit str_extract_all(). regex says extract anything f followed number digits another non-digit symbol.purpose examples just show things can get complicated strings. purposes assessment course, responsible relatively simple cases discussed earlier section exercises.",
    "code": "\nmed_djok_df |> filter(str_detect(point, pattern = \"@\") == TRUE)\n#> # A tibble: 63 × 46\n#>    point Serving match…¹    Pt  Set1  Set2   Gm1   Gm2 Pts  \n#>    <chr> <chr>   <chr>   <dbl> <dbl> <dbl> <dbl> <dbl> <chr>\n#>  1 4f2d@ ND      202109…     1     0     0     0     0 0-0  \n#>  2 6b29… ND      202109…     3     0     0     0     0 15-15\n#>  3 5b37… ND      202109…     5     0     0     0     0 40-15\n#>  4 4f18… ND      202109…     7     0     0     0     0 40-40\n#>  5 5b28… ND      202109…     8     0     0     0     0 40-AD\n#>  6 6b27… DM      202109…    13     0     0     0     1 40-15\n#>  7 6b38… ND      202109…    14     0     0     0     2 0-0  \n#>  8 5b28… ND      202109…    16     0     0     0     2 0-30 \n#>  9 6f38… ND      202109…    17     0     0     0     2 15-30\n#> 10 5b1w@ ND      202109…    28     0     0     1     3 30-0 \n#> # … with 53 more rows, 37 more variables: `Gm#` <chr>,\n#> #   TbSet <dbl>, `TB?` <dbl>, TBpt <lgl>, Svr <dbl>,\n#> #   Ret <dbl>, `1st` <chr>, `2nd` <chr>, Notes <chr>,\n#> #   `1stSV` <dbl>, `2ndSV` <dbl>, `1stIn` <dbl>,\n#> #   `2ndIn` <dbl>, isAce <lgl>, isUnret <lgl>,\n#> #   isRallyWinner <lgl>, isForced <lgl>, isUnforced <lgl>,\n#> #   isDouble <lgl>, PtWinner <dbl>, isSvrWinner <dbl>, …\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\nmed_djok_df |> filter(str_detect(point, pattern = \"@\") == TRUE) |>\n  mutate(error_type = case_when(str_detect(point, pattern = \"d@\") ~ \"deep error\",\n                                   str_detect(point, pattern = \"w@\") ~ \"wide error\",\n            str_detect(point, pattern = \"n@\") ~ \"net error\")) |>\n  group_by(PtWinner, error_type) |>\n  summarise(n_errors = n())\n#> `summarise()` has grouped output by 'PtWinner'. You can\n#> override using the `.groups` argument.\n#> # A tibble: 7 × 3\n#> # Groups:   PtWinner [2]\n#>   PtWinner error_type n_errors\n#>      <dbl> <chr>         <int>\n#> 1        1 deep error        9\n#> 2        1 net error         6\n#> 3        1 wide error       10\n#> 4        2 deep error       12\n#> 5        2 net error        16\n#> 6        2 wide error        9\n#> 7        2 <NA>              1\nstr_detect(med_djok_df$point, pattern = \"\\\\*\")\nstr_detect(med_djok_df$point, pattern = \"^4\")\nstr_detect(med_djok_df$point, pattern = \"@$\")\nstr_extract_all(med_djok_df$point, pattern = \"f[:digit:]+\")"
  },
  {
    "path": "text-data-with-tidytext-and-stringr.html",
    "id": "exercise-12-1",
    "chapter": " 14 Text Data with tidytext and stringr",
    "heading": "14.3.3 Exercises",
    "text": "Use str_detect() dplyr functions create variable serve_location either \"wide\" point starts 4, \"body\" point starts 5, \"center\" point starts 6.Use str_detect() dplyr functions create variable serve_location either \"wide\" point starts 4, \"body\" point starts 5, \"center\" point starts 6.Use dplyr functions Serving variable count number serve locations player. (.e. many points Medvedev hit serve wide?).Use dplyr functions Serving variable count number serve locations player. (.e. many points Medvedev hit serve wide?).Use dplyr functions, Serving variable, isSrvWinner variable find proportion points player won serving locations (.e. Medvedev won 5 points serving wide lost 3 points, proportion 5 / 8 = 0.625).Use dplyr functions, Serving variable, isSrvWinner variable find proportion points player won serving locations (.e. Medvedev won 5 points serving wide lost 3 points, proportion 5 / 8 = 0.625).Note isSrvWinner variable coded 1 serving player won point 0 serving player lost point.letters v, z, , k denote volleys (different types). Use str_detect() dplyr functions figure proportion points volley hit.",
    "code": ""
  },
  {
    "path": "text-data-with-tidytext-and-stringr.html",
    "id": "chapexercise-12",
    "chapter": " 14 Text Data with tidytext and stringr",
    "heading": "14.4 Chapter Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 14.5.Chapter Exercises.",
    "code": ""
  },
  {
    "path": "text-data-with-tidytext-and-stringr.html",
    "id": "solutions-12",
    "chapter": " 14 Text Data with tidytext and stringr",
    "heading": "14.5 Exercise Solutions",
    "text": "",
    "code": ""
  },
  {
    "path": "text-data-with-tidytext-and-stringr.html",
    "id": "text-analysis-s",
    "chapter": " 14 Text Data with tidytext and stringr",
    "heading": "14.5.1 Text Analysis S",
    "text": "solutions.",
    "code": ""
  },
  {
    "path": "text-data-with-tidytext-and-stringr.html",
    "id": "basic-sentiment-analysis-s",
    "chapter": " 14 Text Data with tidytext and stringr",
    "heading": "14.5.2 Basic Sentiment Analysis S",
    "text": "solutions.",
    "code": ""
  },
  {
    "path": "text-data-with-tidytext-and-stringr.html",
    "id": "introduction-to-stringr-s",
    "chapter": " 14 Text Data with tidytext and stringr",
    "heading": "14.5.3 Introduction to stringr S",
    "text": "solutions.",
    "code": ""
  },
  {
    "path": "text-data-with-tidytext-and-stringr.html",
    "id": "chapexercise-12-S",
    "chapter": " 14 Text Data with tidytext and stringr",
    "heading": "14.5.4 Chapter Exercises S",
    "text": "solutions.",
    "code": ""
  },
  {
    "path": "text-data-with-tidytext-and-stringr.html",
    "id": "rcode-12",
    "chapter": " 14 Text Data with tidytext and stringr",
    "heading": "14.6 Non-Exercise R Code",
    "text": "",
    "code": "\nlibrary(tidyverse)\nlibrary(here)\nbeyonce <- read_csv(here(\"data/beyonce_lyrics.csv\"))\nhead(beyonce)\nbeyonce$line[1:4]\nlibrary(tidytext)\nbeyonce_unnest <- beyonce |> unnest_tokens(output = \"word\", input = \"line\")\nbeyonce_unnest\nbeyonce_unnest <- beyonce_unnest |> mutate(word = str_to_lower(word))\nbeyonce_unnest |> group_by(word) |>\n  summarise(n = n()) |>\n  arrange(desc(n))\nhead(stop_words)\nbeyonce_stop <- anti_join(beyonce_unnest, stop_words, by = c(\"word\" = \"word\"))\nbeyonce_sum <- beyonce_stop |> group_by(word) |>\n  summarise(n = n()) |>\n  arrange(desc(n)) |>\n  print(n = 25)\nbeyonce_sum\n## install.packages(\"wordcloud\")\nlibrary(wordcloud)\nbeyonce_small <- beyonce_sum |> filter(n > 50)\nwordcloud(beyonce_small$word, beyonce_small$n, \n          colors = brewer.pal(8, \"Dark2\"), scale = c(5, .2),\n          random.order = FALSE, random.color = FALSE)\nlibrary(here)\nlibrary(tidyverse)\nmed_djok_df <- read_csv(here(\"data/med_djok.csv\"))\nhead(med_djok_df)\nstr_detect(med_djok_df$point, pattern = \"f\")\nstr_detect(med_djok_df$point, pattern = \"d@\")\nsum(str_detect(med_djok_df$point, pattern = \"d@\"))\nsum(str_detect(med_djok_df$point, pattern = \"w@\"))\nsum(str_detect(med_djok_df$point, pattern = \"n@\"))\nmed_djok_df |> filter(str_detect(point, pattern = \"@\") == TRUE)\nmed_djok_df |> filter(str_detect(point, pattern = \"@\") == TRUE) |>\n  mutate(error_type = case_when(str_detect(point, pattern = \"d@\") ~ \"deep error\",\n                                   str_detect(point, pattern = \"w@\") ~ \"wide error\",\n            str_detect(point, pattern = \"n@\") ~ \"net error\")) |>\n  group_by(PtWinner, error_type) |>\n  summarise(n_errors = n())\nstr_detect(med_djok_df$point, pattern = \"\\\\*\")\nstr_detect(med_djok_df$point, pattern = \"^4\")\nstr_detect(med_djok_df$point, pattern = \"@$\")\nstr_extract_all(med_djok_df$point, pattern = \"f[:digit:]+\")"
  },
  {
    "path": "predictive-modeling-with-knn.html",
    "id": "predictive-modeling-with-knn",
    "chapter": " 15 Predictive Modeling with knn",
    "heading": " 15 Predictive Modeling with knn",
    "text": "Goalsexplain ’s necessary use training data test data building predictive model.describe k-nearest neighbors (knn) procedure.interpret confusion matrix.use knn predict level categorical response variable.",
    "code": ""
  },
  {
    "path": "predictive-modeling-with-knn.html",
    "id": "introduction-to-classification",
    "chapter": " 15 Predictive Modeling with knn",
    "heading": "15.1 Introduction to Classification",
    "text": "k-nearest neighbors (knn) introductory supervised machine learning algorithm, commonly used classification algorithm. Classification refers prediction categorical response variable two categories. example, data set SLU students, might interested predicting whether student graduates four years (response two categories: graduates 4 years doesn’t). might want classify response based various student characteristics like anticipated major, GPA, standardized test scores, etc. knn can also used predict quantitative response, ’ll focus categorical responses throughout section.’ve STAT 213, might try draw parallels knn classification using logistic regression. Note, however, logistic regression required response two levels knn can classify response variable two levels.introduce , using pokemon_full.csv data. Pokemon different Types: use Type categorical response interested predicting. simplicity, use Pokemon’s primary type use 4 different types:goal develop k-nearest-neighbors model able classify/predict Pokemon Type set predictors, like Pokemon HP, Attack, Defense, etc.",
    "code": "\nset.seed(1119)\nlibrary(tidyverse)\nlibrary(here)\npokemon <- read_csv(here(\"data/pokemon_full.csv\")) |>\n  filter(Type %in% c(\"Steel\", \"Dark\", \"Fire\", \"Ice\"))"
  },
  {
    "path": "predictive-modeling-with-knn.html",
    "id": "training-and-test-data",
    "chapter": " 15 Predictive Modeling with knn",
    "heading": "15.1.1 Training and Test Data",
    "text": "order develop knn model (note still haven’t discussed knn actually yet!), first need discuss terms applies almost predictive/classification modeling: training test data. training data set subset full data set used fit various models. example , training data set just 15 observations pedagogical purposes. commonly, training data set contain 50%-80% observations full data set.test data set consists remaining 20%-50% observations training data set. test data set used assess different performances various models fit using training data set. need division? Using full data set training model testing model “cheating:” model perform better using observation twice: fitting testing. separate test data set wasn’t used fit model gives model “fair” test, observations supposed new data model hasn’t yet seen.following code uses sample_n() function randomly select 15 observations training data set. anti_join() makes test data set without 15 pokemon training data set.ideas training data set test data set pervasive predictive classification models, including models related knn. Note going method ’s simplest: wanted take step , ’d repeat training test process 5 10 times, using ’s known k-fold cross-validation.",
    "code": "\ntrain_sample <- pokemon |>\n  sample_n(15)\ntest_sample <- anti_join(pokemon, train_sample)\n\ntrain_sample |> head()\n#> # A tibble: 6 × 14\n#>    ...1 Name    Type     HP Attack Defense Speed SpAtk SpDef\n#>   <dbl> <chr>   <chr> <dbl>  <dbl>   <dbl> <dbl> <dbl> <dbl>\n#> 1   491 Darkrai Dark     70     90      90   125   135    90\n#> 2   136 Flareon Fire     65    130      60    65    95   110\n#> 3   571 Zoroark Dark     60    105      60   105   120    60\n#> 4   221 Pilosw… Ice     100    100      80    50    60    60\n#> 5   668 Pyroar  Fire     86     68      72   106   109    66\n#> 6   262 Mighty… Dark     70     90      70    70    60    60\n#> # … with 5 more variables: Generation <dbl>,\n#> #   Legendary <lgl>, height <dbl>, weight <dbl>,\n#> #   base_experience <dbl>\n#> # ℹ Use `colnames()` to see all variable names\ntest_sample |> head()\n#> # A tibble: 6 × 14\n#>    ...1 Name    Type     HP Attack Defense Speed SpAtk SpDef\n#>   <dbl> <chr>   <chr> <dbl>  <dbl>   <dbl> <dbl> <dbl> <dbl>\n#> 1     4 Charma… Fire     39     52      43    65    60    50\n#> 2     5 Charme… Fire     58     64      58    80    80    65\n#> 3    37 Vulpix  Fire     38     41      40    65    50    65\n#> 4    38 Nineta… Fire     73     76      75   100    81   100\n#> 5    58 Growli… Fire     55     70      45    60    70    50\n#> 6    59 Arcani… Fire     90    110      80    95   100    80\n#> # … with 5 more variables: Generation <dbl>,\n#> #   Legendary <lgl>, height <dbl>, weight <dbl>,\n#> #   base_experience <dbl>\n#> # ℹ Use `colnames()` to see all variable names"
  },
  {
    "path": "predictive-modeling-with-knn.html",
    "id": "exercise-13-1",
    "chapter": " 15 Predictive Modeling with knn",
    "heading": "15.1.2 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 15.5.Explain anti_join() joins isn’t specified specifying argument works example.",
    "code": ""
  },
  {
    "path": "predictive-modeling-with-knn.html",
    "id": "knn-introduction",
    "chapter": " 15 Predictive Modeling with knn",
    "heading": "15.2 knn Introduction",
    "text": "",
    "code": ""
  },
  {
    "path": "predictive-modeling-with-knn.html",
    "id": "knn-with-k-1-and-1-predictor",
    "chapter": " 15 Predictive Modeling with knn",
    "heading": "15.2.1 knn with k = 1 and 1 Predictor",
    "text": "Suppose just 15 pokemon training data set. want predict Type just one predictor, Defense. plot shows defenses 15 pokemon training data set, points coloured Type different shapes Type.see plot Steel type Pokemon tend pretty high defense values. Now suppose want predict Type one Pokemon test data set, Dialga. know Dialga Defense stat 120: plot shows Dialga marked large black X.prediction Dialga ? ? According knn k = 1, predict Dialga Fire type. k = 1 means using 1st nearest neighbor: case point closest Dialga green triangle, corresponding Fire type Pokemon.",
    "code": "\nggplot(data = train_sample, aes(x = Defense, y = 1, colour = Type, shape = Type)) +\n  geom_point(size = 4) +  theme(axis.title.y=element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())\ndialga <- test_sample |> slice(63)\nggplot(data = train_sample, aes(x = Defense, y = 1, colour = Type, shape = Type)) +\n  geom_point(size = 4) +  theme(axis.title.y=element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks.y=element_blank()) +\n  geom_point(data = dialga, colour = \"black\", shape = 4, size = 7)"
  },
  {
    "path": "predictive-modeling-with-knn.html",
    "id": "knn-with-k-1-and-one-predictor",
    "chapter": " 15 Predictive Modeling with knn",
    "heading": "15.2.2 knn with k > 1 and One Predictor",
    "text": ", might necessarily want predict response value based single nearest neighbor. Dialga also near many purple plus signs: factor ? can extend knn different values k. example, \\(k = 3\\) looks 3 nearest neighbors, assigns prediction category appears among 3 nearest neighbors.Using k = 3, prediction Dialga ? ?",
    "code": ""
  },
  {
    "path": "predictive-modeling-with-knn.html",
    "id": "knn-with-k-1-and-more-than-one-predictor",
    "chapter": " 15 Predictive Modeling with knn",
    "heading": "15.2.3 knn with k > 1 and More Than One Predictor",
    "text": "can increase number predictors knn model well. can generally include many predictors like, visualizing becomes challenging 2 predictors nearly impossible 3 predictors. case two predictors, suppose want use Defense Speed predictors Type. Dialga, Pokemon want predict , marked large black X.\\(k = 1\\), predict Dialga Steel, closest point purple + sign top-left corner graph. \\(k = 3\\), Type predict Dialga? question, ’s little hard tell three points closest Dialga without computing distances numerically, something let R knn() function.",
    "code": "\nggplot(data = train_sample, aes(x = Defense, y = Speed, colour = Type, shape = Type)) +\n  geom_point(size = 3) +\n  geom_point(data = dialga, colour = \"black\", shape = 4, size = 5)"
  },
  {
    "path": "predictive-modeling-with-knn.html",
    "id": "scaling-predictor-variables-before-using-knn",
    "chapter": " 15 Predictive Modeling with knn",
    "heading": "15.2.4 Scaling Predictor Variables before Using knn",
    "text": "general, want scale quantitative predictors using knn relies distances points predictions. easiest see example. Suppose, Pokemon example, want use height weight predictors knn model. just 2 observations training data set: Dark Type pokemon height 15 centimeters weight 505 pounds, Fire Type Pokemon height 9 centimeters weight 250 pounds.plot also given Pokemon test data set wish predict Type , marked black X. Upon visual inspection, k = 1, looks like classify pokemon Dark. However, units weight height different scales. compute actual distances class see conclusion calculation matches visual conclusion.get around issue, customary scale quantitative predictors applying knn. One method applying\\[\nscaled_x = \\frac{x - min(x)}{max(x) - min(x)}\n\\]example, scaling weight 15 original pokemon:puts weights 0 1:height, variables contribute “equally” distance metric used knn.code scales numeric variables data set, using across() function. across() applies transformation every column data set satisfies condition given argument.",
    "code": "\ntrain_tiny <- train_sample |> slice(1:2)\nnewobs <- tibble(height = 15, weight = 350, Type = \"Unknown\")\nggplot(data = train_tiny, aes(x = height, y = weight, shape = Type)) +\n  geom_point(size = 5, aes(colour = Type)) + xlim(c(7, 17)) + ylim(c(200, 550)) +\n  geom_point(data = newobs, shape = 4, size = 10)\ntrain_sample |> select(weight) |> head()\n#> # A tibble: 6 × 1\n#>   weight\n#>    <dbl>\n#> 1    505\n#> 2    250\n#> 3    811\n#> 4    558\n#> 5    815\n#> 6    370\ntrain_sample |> mutate(weight_s = (weight - min(weight)) / \n                          (max(weight) - min(weight))) |>\n  select(weight_s) |>\n  head()\n#> # A tibble: 6 × 1\n#>   weight_s\n#>      <dbl>\n#> 1   0.187 \n#> 2   0.0835\n#> 3   0.312 \n#> 4   0.209 \n#> 5   0.314 \n#> 6   0.132\n## ?across\nlibrary(pander)\ntrain_sample |>\n  mutate(across(where(is.numeric), ~ (.x - min(.x)) /\n                                 (max(.x) - min(.x)))) |>\n  slice(1:3)\n#> # A tibble: 3 × 14\n#>    ...1 Name    Type     HP Attack Defense Speed SpAtk SpDef\n#>   <dbl> <chr>   <chr> <dbl>  <dbl>   <dbl> <dbl> <dbl> <dbl>\n#> 1 0.720 Darkrai Dark  0.417  0.444     0.4 1     1     0.658\n#> 2 0.193 Flareon Fire  0.333  0.889     0.1 0.368 0.619 0.921\n#> 3 0.838 Zoroark Dark  0.25   0.611     0.1 0.789 0.857 0.263\n#> # … with 5 more variables: Generation <dbl>,\n#> #   Legendary <lgl>, height <dbl>, weight <dbl>,\n#> #   base_experience <dbl>\n#> # ℹ Use `colnames()` to see all variable names"
  },
  {
    "path": "predictive-modeling-with-knn.html",
    "id": "exercise-13-2",
    "chapter": " 15 Predictive Modeling with knn",
    "heading": "15.2.5 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 15.5.* Consider toy example just two observations training data set unscaled weight height predictors.actual (height, weight) coordinates Fire pokemon (9, 250), actual coordinates Dark pokemon (15, 505), actual coordinates test pokemon (15, 350). mentioned , visually, pokemon looks “closer” Dark type pokemon. Verify actually case computing actual distances numerically.* scaling according formula section, coordinates (height, weight) Fire pokemon (0, 0) coordinates Dark pokemon (1, 1). (Since two observations, formula doesn’t give output 0 1 tiny example). scaled coordinates test pokemon (1, 0.39). Verify , scaling, test pokemon “closer” Dark type pokemon numerically computing distances.* scaling according formula section, coordinates (height, weight) Fire pokemon (0, 0) coordinates Dark pokemon (1, 1). (Since two observations, formula doesn’t give output 0 1 tiny example). scaled coordinates test pokemon (1, 0.39). Verify , scaling, test pokemon “closer” Dark type pokemon numerically computing distances.Consider example 15 pokemon training data set single predictor, Defense.Consider example 15 pokemon training data set single predictor, Defense.k = 2, tie Fire Steel. Come way might break ties knn algorithm.Explain knn use prediction test observations k equals number observations training data set.Explain knn use prediction test observations k equals number observations training data set.advantages making k smaller advantages making k larger?advantages making k smaller advantages making k larger?",
    "code": "\nggplot(data = train_tiny, aes(x = height, y = weight, shape = Type)) +\n  geom_point(size = 5, aes(colour = Type)) + xlim(c(7, 17)) +\n  ylim(c(200, 550)) +\n  geom_point(data = newobs, shape = 4, size = 10)\nggplot(data = train_sample, aes(x = Defense, y = 1, colour = Type, shape = Type)) +\n  geom_point(size = 4) +  theme(axis.title.y=element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks.y=element_blank()) +\n  geom_point(data = dialga, colour = \"black\", shape = 4, size = 7)"
  },
  {
    "path": "predictive-modeling-with-knn.html",
    "id": "choosing-predictors-and-k",
    "chapter": " 15 Predictive Modeling with knn",
    "heading": "15.3 Choosing Predictors and k",
    "text": "now know knn classifies observations test data set, choose predictors used knn algorithm? choose number neighbors, k? want measure “good” models different predictors different k’s , first need define “good” means.Much “choosing predictors” part trial error evaluating different models criterion talk next section. However, always helpful explore data set graphics get us good starting point. scatterplot matrix useful exploratory tool. following scatterplot matrix response variable, Type, just three candidate predictors, HP, Attack, Defense, created GGally (“g-g-ally”) package.lower argument changes number bins faceted histograms bottom row. can mostly ignore .columns argument important: allows specify columns want look . prefer putting response, Type (column 3) last slot.can examine see variables seem relationship Type. want look ?",
    "code": "\n## install.packages(\"GGally\")\nlibrary(GGally)\n#> Registered S3 method overwritten by 'GGally':\n#>   method from   \n#>   +.gg   ggplot2\n#> \n#> Attaching package: 'GGally'\n#> The following object is masked from 'package:pander':\n#> \n#>     wrap\nggpairs(data = train_sample, columns = c(4, 5, 6, 3), \n        lower = list(combo = wrap(ggally_facethist, bins = 15)))"
  },
  {
    "path": "predictive-modeling-with-knn.html",
    "id": "the-confusion-matrix",
    "chapter": " 15 Predictive Modeling with knn",
    "heading": "15.3.1 The Confusion Matrix",
    "text": ", still need metric evaluate models different predictors. One definition “good” model classification context model high proportion correct predictions test data set. make intuitive sense, hope “good” model correctly classifies Dark pokemon Dark, Fire pokemon Fire, etc.order examine performance particular model, ’ll create confusion matrix shows results model’s classification observations test data set. Note STAT 213, didn’t call confusion matrix; instead called classification table.following video explains confusion matrices detail also cement ideas training test data. https://www.youtube.com/watch?v=Kdsp6soqA7o.",
    "code": ""
  },
  {
    "path": "predictive-modeling-with-knn.html",
    "id": "using-knn-in-r",
    "chapter": " 15 Predictive Modeling with knn",
    "heading": "15.3.2 Using knn in R",
    "text": "make confusion matrix model using pokemon data set, first need obtain predictions model. ’ll use class library fit knn model pokemon data. Note , instead 15 Pokemon training data set, now 70 pokemon give reasonable number. test set remaining 50 pokemon.following code chunk sets seed get training test samples, scales numeric variables pokemon data set, randomly selects 70 pokemon training sample.first knn model investigate HP, Attack, Defense, Speed predictors. class library can fit knn models knn() function requires training test data sets predictors want use fit model. knn() function also requires response variable, Type, given vector.Now data prepared knn() function class library, fit model 9 nearest neighbors. arguments knn() aretrain, data set training data contains predictors want use (predictors response).test, data set test data contains predictors want use (predictors response).cl, vector response variable training data.k, number nearest neighbors.output knn_mod gives predicted categories test sample. can compare predictions knn model actual pokemon Types test sample table(), makes confusion matrix:columns confusion matrix give actual Pokemon types test data rows give predicted types knn model. table tells us 0 pokemon Dark type knn model correctly classified Dark. 6 pokemon Dark type knn model incorrectly classified Fire. 5 pokemon Dark type knn model incorrectly classified Ice. words, correct predictions appear diagonal, incorrect predictions appear -diagonal.One common metric used assess overall model performance model’s classification rate, computed number correct classifications divided total number observations test data set. case, classification rate isCode automatically obtain classification rate confusion matrix isWhat diag() seem code ?",
    "code": "\nlibrary(tidyverse)\nset.seed(11232020) ## run this line so that you get the same \n## results as I do!\n\n## scale the quantitative predictors\npokemon_scaled <- pokemon |>\n    mutate(across(where(is.numeric), ~ (.x - min(.x)) /\n                                 (max(.x) - min(.x)))) \n\ntrain_sample_2 <- pokemon_scaled |>\n  sample_n(70)\ntest_sample_2 <- anti_join(pokemon_scaled, train_sample_2)\n#> Joining, by = c(\"...1\", \"Name\", \"Type\", \"HP\", \"Attack\",\n#> \"Defense\", \"Speed\", \"SpAtk\", \"SpDef\", \"Generation\",\n#> \"Legendary\", \"height\", \"weight\", \"base_experience\")\n## install.packages(\"class\")\nlibrary(class)\n\n## create a data frame that only has the predictors\n## that we will use\ntrain_small <- train_sample_2 |> select(HP, Attack, Defense, Speed)\ntest_small <- test_sample_2 |> select(HP, Attack, Defense, Speed)\n\n## put our response variable into a vector\ntrain_cat <- train_sample_2$Type\ntest_cat <- test_sample_2$Type\n## fit the knn model with 9 nearest neighbors\nknn_mod <- knn(train = train_small, test = test_small,\n               cl = train_cat, k = 9)\nknn_mod\n#>  [1] Ice   Fire  Fire  Fire  Fire  Fire  Steel Fire  Ice  \n#> [10] Fire  Fire  Fire  Fire  Ice   Ice   Steel Ice   Dark \n#> [19] Ice   Fire  Steel Fire  Fire  Ice   Fire  Ice   Steel\n#> [28] Fire  Fire  Ice   Dark  Fire  Fire  Fire  Dark  Ice  \n#> [37] Ice   Fire  Ice   Fire  Fire  Fire  Fire  Fire  Fire \n#> [46] Fire  Fire  Fire  Ice   Fire \n#> Levels: Dark Fire Ice Steel\ntable(knn_mod, test_cat) \n#>        test_cat\n#> knn_mod Dark Fire Ice Steel\n#>   Dark     0    3   0     0\n#>   Fire     6   13   7     4\n#>   Ice      5    5   2     1\n#>   Steel    0    1   0     3\n(0 + 13 + 2 + 3) / 50\n#> [1] 0.36\ntab <- table(knn_mod, test_cat) \nsum(diag(tab)) / sum(tab)\n#> [1] 0.36"
  },
  {
    "path": "predictive-modeling-with-knn.html",
    "id": "exercise-13-3",
    "chapter": " 15 Predictive Modeling with knn",
    "heading": "15.3.3 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 15.5.Change predictors used change k improve classification rate model k = 9 Attack, Defense, HP, Speed predictors.",
    "code": ""
  },
  {
    "path": "predictive-modeling-with-knn.html",
    "id": "chapexercise-13",
    "chapter": " 15 Predictive Modeling with knn",
    "heading": "15.4 Chapter Exercises",
    "text": "chapter exercises section. Instead, ’ll devote -class time begin work final project.",
    "code": ""
  },
  {
    "path": "predictive-modeling-with-knn.html",
    "id": "solutions-13",
    "chapter": " 15 Predictive Modeling with knn",
    "heading": "15.5 Exercise Solutions",
    "text": "",
    "code": ""
  },
  {
    "path": "predictive-modeling-with-knn.html",
    "id": "introduction-to-classification-s",
    "chapter": " 15 Predictive Modeling with knn",
    "heading": "15.5.1 Introduction to Classification S",
    "text": "",
    "code": ""
  },
  {
    "path": "predictive-modeling-with-knn.html",
    "id": "knn-introduction-s",
    "chapter": " 15 Predictive Modeling with knn",
    "heading": "15.5.2 knn Introduction S",
    "text": "* Consider toy example just two observations training data set unscaled weight height predictors.actual (height, weight) coordinates Fire pokemon (9, 250), actual coordinates Dark pokemon (15, 505), actual coordinates test pokemon (15, 350). mentioned , visually, pokemon looks “closer” Dark type pokemon. Verify case computing actual distances numerically.* scaling according formula section, coordinates (height, weight) Fire pokemon (0, 0) coordinates Dark pokemon (1, 1). (Since two observations, formula doesn’t give output 0 1 tiny example). scaled coordinates test pokemon (1, 0.39). Verify , scaling, test pokemon “closer” Dark type pokemon bu numerically computing distances.",
    "code": "\nggplot(data = train_tiny, aes(x = height, y = weight, shape = Type)) +\n  geom_point(size = 5, aes(colour = Type)) + xlim(c(7, 17)) +\n  ylim(c(200, 550)) +\n  geom_point(data = newobs, shape = 4, size = 10)"
  },
  {
    "path": "predictive-modeling-with-knn.html",
    "id": "choosing-predictors-and-k-s",
    "chapter": " 15 Predictive Modeling with knn",
    "heading": "15.5.3 Choosing Predictors and k S",
    "text": "",
    "code": ""
  },
  {
    "path": "predictive-modeling-with-knn.html",
    "id": "rcode-13",
    "chapter": " 15 Predictive Modeling with knn",
    "heading": "15.6 Non-Exercise R Code",
    "text": "",
    "code": "\nset.seed(1119)\nlibrary(tidyverse)\nlibrary(here)\npokemon <- read_csv(here(\"data/pokemon_full.csv\")) |>\n  filter(Type %in% c(\"Steel\", \"Dark\", \"Fire\", \"Ice\"))\ntrain_sample <- pokemon |>\n  sample_n(15)\ntest_sample <- anti_join(pokemon, train_sample)\n\ntrain_sample |> head()\ntest_sample |> head()\nggplot(data = train_sample, aes(x = Defense, y = 1, colour = Type, shape = Type)) +\n  geom_point(size = 4) +  theme(axis.title.y=element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())\ndialga <- test_sample |> slice(63)\nggplot(data = train_sample, aes(x = Defense, y = 1, colour = Type, shape = Type)) +\n  geom_point(size = 4) +  theme(axis.title.y=element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks.y=element_blank()) +\n  geom_point(data = dialga, colour = \"black\", shape = 4, size = 7)\nggplot(data = train_sample, aes(x = Defense, y = Speed, colour = Type, shape = Type)) +\n  geom_point(size = 3) +\n  geom_point(data = dialga, colour = \"black\", shape = 4, size = 5)\ntrain_tiny <- train_sample |> slice(1:2)\nnewobs <- tibble(height = 15, weight = 350, Type = \"Unknown\")\nggplot(data = train_tiny, aes(x = height, y = weight, shape = Type)) +\n  geom_point(size = 5, aes(colour = Type)) + xlim(c(7, 17)) + ylim(c(200, 550)) +\n  geom_point(data = newobs, shape = 4, size = 10)\ntrain_sample |> select(weight) |> head()\ntrain_sample |> mutate(weight_s = (weight - min(weight)) / \n                          (max(weight) - min(weight))) |>\n  select(weight_s) |>\n  head()\n## ?across\nlibrary(pander)\ntrain_sample |>\n  mutate(across(where(is.numeric), ~ (.x - min(.x)) /\n                                 (max(.x) - min(.x)))) |>\n  slice(1:3)\n## install.packages(\"GGally\")\nlibrary(GGally)\nggpairs(data = train_sample, columns = c(4, 5, 6, 3), \n        lower = list(combo = wrap(ggally_facethist, bins = 15)))\nlibrary(tidyverse)\nset.seed(11232020) ## run this line so that you get the same \n## results as I do!\n\n## scale the quantitative predictors\npokemon_scaled <- pokemon |>\n    mutate(across(where(is.numeric), ~ (.x - min(.x)) /\n                                 (max(.x) - min(.x)))) \n\ntrain_sample_2 <- pokemon_scaled |>\n  sample_n(70)\ntest_sample_2 <- anti_join(pokemon_scaled, train_sample_2)\n## install.packages(\"class\")\nlibrary(class)\n\n## create a data frame that only has the predictors\n## that we will use\ntrain_small <- train_sample_2 |> select(HP, Attack, Defense, Speed)\ntest_small <- test_sample_2 |> select(HP, Attack, Defense, Speed)\n\n## put our response variable into a vector\ntrain_cat <- train_sample_2$Type\ntest_cat <- test_sample_2$Type\n## fit the knn model with 9 nearest neighbors\nknn_mod <- knn(train = train_small, test = test_small,\n               cl = train_cat, k = 9)\nknn_mod\ntable(knn_mod, test_cat) \n(0 + 13 + 2 + 3) / 50\ntab <- table(knn_mod, test_cat) \nsum(diag(tab)) / sum(tab)"
  },
  {
    "path": "connections-to-stat-113-stat-213-and-cs-140.html",
    "id": "connections-to-stat-113-stat-213-and-cs-140",
    "chapter": " 16 Connections to STAT 113, STAT 213, and CS 140",
    "heading": " 16 Connections to STAT 113, STAT 213, and CS 140",
    "text": "",
    "code": ""
  },
  {
    "path": "connections-to-stat-113-stat-213-and-cs-140.html",
    "id": "stat-113",
    "chapter": " 16 Connections to STAT 113, STAT 213, and CS 140",
    "heading": "16.1 STAT 113",
    "text": "section, discuss learned data science course connects concepts learned STAT 113. quick refresher, concepts learned STAT 113 :exploring data numerical graphical summaries. connection class fairly straightforward: ’ve learned lot actually compute numerical summaries make appropriate graphics potentially messy data.explaining sampling distributions relate confidence intervals hypothesis tests. topic probably least connected learned far class.conducting hypothesis tests creating confidence intervals answer questions interest. focus third general objective section.example use experiment designed assess effects race sex whether employee received callback job. order conduct experiment, researchers randomly assigned names resumes name associated particular race gender, sent resumes employers, recorded whether resume received callback. addition race, sex, whether employee received callback, variables collected, like resume quality, whether applicant computer skills, years experience, etc. 1 received_callback indicates applicant received callback.may recall example STAT 113: used introduce chi-square test association. example others like , appropriate graphic summary statistics provided. , create .data set called resume openintro package: ’ll need install package install.packages(\"openintro\"). , load data ",
    "code": "\nlibrary(openintro)\n#> Loading required package: airports\n#> Loading required package: cherryblossom\n#> Loading required package: usdata\nresume\n#> # A tibble: 4,870 × 30\n#>    job_ad_id job_c…¹ job_i…² job_t…³ job_f…⁴ job_e…⁵ job_o…⁶\n#>        <dbl> <chr>   <chr>   <chr>     <dbl>   <dbl> <chr>  \n#>  1       384 Chicago manufa… superv…      NA       1 unknown\n#>  2       384 Chicago manufa… superv…      NA       1 unknown\n#>  3       384 Chicago manufa… superv…      NA       1 unknown\n#>  4       384 Chicago manufa… superv…      NA       1 unknown\n#>  5       385 Chicago other_… secret…       0       1 nonpro…\n#>  6       386 Chicago wholes… sales_…       0       1 private\n#>  7       386 Chicago wholes… sales_…       0       1 private\n#>  8       385 Chicago other_… secret…       0       1 nonpro…\n#>  9       386 Chicago wholes… sales_…       0       1 private\n#> 10       386 Chicago wholes… sales_…       0       1 private\n#> # … with 4,860 more rows, 23 more variables:\n#> #   job_req_any <dbl>, job_req_communication <dbl>,\n#> #   job_req_education <dbl>, job_req_min_experience <chr>,\n#> #   job_req_computer <dbl>, job_req_organization <dbl>,\n#> #   job_req_school <chr>, received_callback <dbl>,\n#> #   firstname <chr>, race <chr>, gender <chr>,\n#> #   years_college <int>, college_degree <dbl>, …\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names"
  },
  {
    "path": "connections-to-stat-113-stat-213-and-cs-140.html",
    "id": "chi-square-test-of-association",
    "chapter": " 16 Connections to STAT 113, STAT 213, and CS 140",
    "heading": "16.1.1 Chi-square Test of Association",
    "text": "goal assess whether evidence racial discrimination study. words, variables race received_callback associated?Prepare. Let’s start writing null alternative hypotheses.\\(H_0:\\) association race received_callback.\\(H_a:\\) association race received_callback.Next, can construct summary graphic. One graphic explore two categorical variables stacked bar plot.notice recieved_callback variable scale? fix ?might also want generate two-way table:Check: two assumptions test independence observations expected counts larger 5. don’t time discuss detail assume satisfied .Calculate: next want calculate p-value hypothesis test. core tidyverse packages offer functionality hypothesis testing. Instead, functions base R perform various tests. may used lm() function STAT 213 perform hypothesis testing regression context. t.test() chisq.test() couple functions can perform one two-sample t-test (t.test()) chi-square goodness--fit test chi-square test association chisq.test().\narguments chisq.test() test association two vectors. arguments data.frames, need specify appropriate vectors directly resume$race resume$received_callback.output chisq.test() gives p-value 0.00004998 chi-square statistic 16.449 1 degree freedom.Conclude. Finally, write conclusion context problem.strong evidence race callback associated. graph shows white applicants receive callback often black applicants hypothesis test shows statistically significant.",
    "code": "\nlibrary(tidyverse)\n#> ── Attaching packages ─────────────────── tidyverse 1.3.2 ──\n#> ✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n#> ✔ tibble  3.1.8     ✔ dplyr   1.0.9\n#> ✔ tidyr   1.2.0     ✔ stringr 1.4.0\n#> ✔ readr   2.1.2     ✔ forcats 0.5.1\n#> ── Conflicts ────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\nresume_sum <- resume |> \n  mutate(received_callback = received_callback) |>\n           group_by(race, received_callback) |>\n  summarise(count = n())\n#> `summarise()` has grouped output by 'race'. You can\n#> override using the `.groups` argument.\nggplot(data = resume_sum, aes(x = race, y = count)) +\n  geom_col(aes(fill = received_callback)) +\n  scale_fill_viridis_c()\nresume <- resume |>\n  mutate(received_callback = as.factor(received_callback))\nresume_sum <- resume |> \n           group_by(race, received_callback) |>\n  summarise(count = n())\n#> `summarise()` has grouped output by 'race'. You can\n#> override using the `.groups` argument.\nggplot(data = resume_sum, aes(x = race, y = count)) +\n  geom_col(aes(fill = received_callback)) +\n  scale_fill_viridis_d()\nresume |> group_by(race, received_callback) |>\n  summarise(count = n()) |>\n  pivot_wider(names_from = c(\"race\"),\n              values_from = \"count\")\n#> `summarise()` has grouped output by 'race'. You can\n#> override using the `.groups` argument.\n#> # A tibble: 2 × 3\n#>   received_callback black white\n#>   <fct>             <int> <int>\n#> 1 0                  2278  2200\n#> 2 1                   157   235\nchisq.test(x = resume$race, y = resume$received_callback)\n#> \n#>  Pearson's Chi-squared test with Yates' continuity\n#>  correction\n#> \n#> data:  resume$race and resume$received_callback\n#> X-squared = 16.449, df = 1, p-value = 4.998e-05"
  },
  {
    "path": "connections-to-stat-113-stat-213-and-cs-140.html",
    "id": "additional-analysis",
    "chapter": " 16 Connections to STAT 113, STAT 213, and CS 140",
    "heading": "16.1.2 Additional Analysis",
    "text": "addition carrying steps statistical hypothesis test, can also use skills learned course provide information study. questions might answer include:distribution job types job_type job industries job_industry study?distribution job types job_type job industries job_industry study?first names (firstname) used bias first names?first names (firstname) used bias first names?variables associated whether applicant received callback?variables associated whether applicant received callback?answer question distribution job types job industries used study, can make simple bar plot:code, fct_infreq() orders levels job_type highest count/frequency lowest. fct_rev() reverses order , resulting bar plot, level highest count appears first.answer question whether first names biased others, might make graph proportion resumes received callback first name.can label name lowest callback rate name highest callback rate.",
    "code": "\nggplot(data = resume, aes(x = fct_rev(fct_infreq(job_type)))) +\n  geom_bar() +\n  coord_flip() +\n  labs(x = \"Job Type\")\nggplot(data = resume, aes(x = fct_rev(fct_infreq(job_industry)))) +\n  geom_bar() +\n  coord_flip() +\n  labs(x = \"Job Industry\")\nresume_firstname <- resume |>\n  group_by(firstname) |>\n  summarise(propcallback = mean(received_callback == \"1\"),\n            gender = unique(gender),\n            race = unique(race)) |>\n  arrange(desc(propcallback)) |>\n  unite(\"gender_race\", c(gender, race))\n\nggplot(data = resume_firstname, aes(x = gender_race, y = propcallback)) +\n  geom_point()\nlibrary(ggrepel)\nlabel_df <- resume_firstname |> \n  filter(propcallback == max(propcallback) |\n           propcallback == min(propcallback))\n\nggplot(data = resume_firstname, aes(x = gender_race, y = propcallback)) +\n  geom_point() +\n  geom_label_repel(data = label_df, aes(label = firstname))"
  },
  {
    "path": "connections-to-stat-113-stat-213-and-cs-140.html",
    "id": "exercise-15-1",
    "chapter": " 16 Connections to STAT 113, STAT 213, and CS 140",
    "heading": "16.1.3 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 16.5.Construct graphic make table explores whether one variables data set associated whether applicant receives callback job. variables include gender, years_college, college_degree, honors, worked_during_school, years_experience, computer_skills, special_skills, volunteer, military, employment_holes, resume_quality.Construct graphic make table explores whether one variables data set associated whether applicant receives callback job. variables include gender, years_college, college_degree, honors, worked_during_school, years_experience, computer_skills, special_skills, volunteer, military, employment_holes, resume_quality.Construct graphic make table explores one variables data set associated whether applicant receives callback job. variable Exercise 1 categorical, choose quantitative variable exercise. variable Exercise 1 quantitative, choose categorical variable exercise.Construct graphic make table explores one variables data set associated whether applicant receives callback job. variable Exercise 1 categorical, choose quantitative variable exercise. variable Exercise 1 quantitative, choose categorical variable exercise.categorical variable chose, conduct Chi-square test association see statistical evidence variable associated received_callback. test, (), write null alternative hypotheses, run test chisq.test() make note whether get warning assumptions test, write conclusion context problem.categorical variable chose, conduct Chi-square test association see statistical evidence variable associated received_callback. test, (), write null alternative hypotheses, run test chisq.test() make note whether get warning assumptions test, write conclusion context problem.",
    "code": ""
  },
  {
    "path": "connections-to-stat-113-stat-213-and-cs-140.html",
    "id": "stat-213",
    "chapter": " 16 Connections to STAT 113, STAT 213, and CS 140",
    "heading": "16.2 STAT 213",
    "text": "Much concepts connecting STAT 113 course hold connecting STAT 213 course. can still use learned explore data set, conduct hypothesis test, perform analysis exploration data set.section, however, focus tidy approach modeling. particular, use broom package return tibbles model summary information can use analysis, plotting, presentation.use coffee_ratings data set, contains observations ratings various coffees throughout world. data obtained Github account (https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-07-07/readme.md).description variable data set given .total_cup_points, score coffee panel experts (response variable section)species, species coffee bean (Arabica Robusta)aroma, aroma (smell) gradeflavor, flavor gradeaftertaste, aftertaste gradeacidity, acidity gradebody, body gradebalance, balance gradeuniformity, uniformity gradeclean_cup, clean cup gradesweetness, sweetness grademoisture, moisture gradecategory_one_defects, count category one defectsquakers, quakerscategory_two_defects, number category two defects",
    "code": ""
  },
  {
    "path": "connections-to-stat-113-stat-213-and-cs-140.html",
    "id": "broom-package-functions",
    "chapter": " 16 Connections to STAT 113, STAT 213, and CS 140",
    "heading": "16.2.1 broom Package Functions",
    "text": "broom package consists three primary functions: tidy(), glance(), augment().tidy()tidy() analagous summary() linear model object. Let’s start fitting linear model lm() total_cup_points response species, aroma, flavor, sweetness, moisture predictors.Read data, load broom package (install install.packages(\"broom\")), fit model withIn STAT 213, likely used summary() look model output:However, inconveniences involving summary(). First, ’s just nice look : output isn’t formatted way easy look . Second, can challenging pull items summary output code. example, want pull p-value moisture, need write something like:tidy() alternative puts model coefficients, standard errors, t-stats, p-values tidy tibble:advantage format output can now use tidyverse functions output. pull p-values,, grab output particular variable interest:glance()glance() puts model summary statistics tidy tibble. example, runyou notice lot statistics familiar STAT 213, including r.squared, adj.r.squared, sigma (residual standard error), statistic (overall F-statistic), AIC, BIC.augment()augment() personal favourite three. function returns tibble contains variables used fit model appended commonly used diagnostic statistics like fitted values (.fitted), cook’s distance (.cooksd), .hat values leverage, residuals (.resid).augment() data set makes really easy things like:filter() data set examine values high cook’s distance might influentialWe see right away potentially influential observation 0 total_cup_points. Examining variable , see probably data entry error can removed data.also find observations high leverageor observations outliers:Finally, can use ggplot2 skills construct plots like residuals versus fitted values plot (filtering outlying observation first):",
    "code": "\nlibrary(broom)\nlibrary(here)\n#> here() starts at /Users/highamm/Desktop/datascience234\ncoffee_df <- read_csv(here(\"data/coffee_ratings.csv\"))\n#> Rows: 1339 Columns: 43\n#> ── Column specification ────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (24): species, owner, country_of_origin, farm_name, ...\n#> dbl (19): total_cup_points, number_of_bags, aroma, flavo...\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\ncoffee_mod <- lm(total_cup_points ~ species + aroma + flavor +\n                   sweetness + moisture,\n   data = coffee_df)\nsummary(coffee_mod)\n#> \n#> Call:\n#> lm(formula = total_cup_points ~ species + aroma + flavor + sweetness + \n#>     moisture, data = coffee_df)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -9.5132 -0.3705  0.0726  0.5610  5.5844 \n#> \n#> Coefficients:\n#>                Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)     7.04039    0.77377   9.099  < 2e-16 ***\n#> speciesRobusta  2.85365    0.26861  10.624  < 2e-16 ***\n#> aroma           1.95188    0.14575  13.392  < 2e-16 ***\n#> flavor          5.09440    0.14042  36.281  < 2e-16 ***\n#> sweetness       2.23956    0.06553  34.173  < 2e-16 ***\n#> moisture       -1.88033    0.67368  -2.791  0.00533 ** \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.168 on 1333 degrees of freedom\n#> Multiple R-squared:  0.8891, Adjusted R-squared:  0.8887 \n#> F-statistic:  2137 on 5 and 1333 DF,  p-value: < 2.2e-16\nsummary(coffee_mod)$coefficients[\"moisture\", 4]\n#> [1] 0.005327594\ntidy(coffee_mod)\n#> # A tibble: 6 × 5\n#>   term           estimate std.error statistic   p.value\n#>   <chr>             <dbl>     <dbl>     <dbl>     <dbl>\n#> 1 (Intercept)        7.04    0.774       9.10 3.23e- 19\n#> 2 speciesRobusta     2.85    0.269      10.6  2.31e- 25\n#> 3 aroma              1.95    0.146      13.4  1.82e- 38\n#> 4 flavor             5.09    0.140      36.3  4.73e-201\n#> 5 sweetness          2.24    0.0655     34.2  2.41e-184\n#> 6 moisture          -1.88    0.674      -2.79 5.33e-  3\ntidy(coffee_mod) |> select(p.value)\n#> # A tibble: 6 × 1\n#>     p.value\n#>       <dbl>\n#> 1 3.23e- 19\n#> 2 2.31e- 25\n#> 3 1.82e- 38\n#> 4 4.73e-201\n#> 5 2.41e-184\n#> 6 5.33e-  3\ntidy(coffee_mod) |> filter(term == \"aroma\")\n#> # A tibble: 1 × 5\n#>   term  estimate std.error statistic  p.value\n#>   <chr>    <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 aroma     1.95     0.146      13.4 1.82e-38\nglance(coffee_mod)\n#> # A tibble: 1 × 12\n#>   r.squared adj.r…¹ sigma stati…² p.value    df logLik   AIC\n#>       <dbl>   <dbl> <dbl>   <dbl>   <dbl> <dbl>  <dbl> <dbl>\n#> 1     0.889   0.889  1.17   2137.       0     5 -2105. 4224.\n#> # … with 4 more variables: BIC <dbl>, deviance <dbl>,\n#> #   df.residual <int>, nobs <int>, and abbreviated variable\n#> #   names ¹​adj.r.squared, ²​statistic\n#> # ℹ Use `colnames()` to see all variable names\naugment(coffee_mod)\n#> # A tibble: 1,339 × 12\n#>    total_cup_…¹ species aroma flavor sweet…² moist…³ .fitted\n#>           <dbl> <chr>   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>\n#>  1         90.6 Arabica  8.67   8.83   10       0.12    91.1\n#>  2         89.9 Arabica  8.75   8.67   10       0.12    90.5\n#>  3         89.8 Arabica  8.42   8.5    10       0       89.2\n#>  4         89   Arabica  8.17   8.58   10       0.11    88.9\n#>  5         88.8 Arabica  8.25   8.5    10       0.12    88.6\n#>  6         88.8 Arabica  8.58   8.42   10       0.11    88.9\n#>  7         88.8 Arabica  8.42   8.5    10       0.11    89.0\n#>  8         88.7 Arabica  8.25   8.33    9.33    0.03    86.4\n#>  9         88.4 Arabica  8.67   8.67    9.33    0.03    89.0\n#> 10         88.2 Arabica  8.08   8.58   10       0.1     88.7\n#> # … with 1,329 more rows, 5 more variables: .resid <dbl>,\n#> #   .hat <dbl>, .sigma <dbl>, .cooksd <dbl>,\n#> #   .std.resid <dbl>, and abbreviated variable names\n#> #   ¹​total_cup_points, ²​sweetness, ³​moisture\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\naugment_df <- augment(coffee_mod)\naugment_df |> filter(.cooksd > 1)\n#> # A tibble: 1 × 12\n#>   total_cup_p…¹ species aroma flavor sweet…² moist…³ .fitted\n#>           <dbl> <chr>   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>\n#> 1             0 Arabica     0      0       0    0.12    6.81\n#> # … with 5 more variables: .resid <dbl>, .hat <dbl>,\n#> #   .sigma <dbl>, .cooksd <dbl>, .std.resid <dbl>, and\n#> #   abbreviated variable names ¹​total_cup_points,\n#> #   ²​sweetness, ³​moisture\n#> # ℹ Use `colnames()` to see all variable names\nggplot(data = coffee_df, aes(x = total_cup_points)) +\n  geom_histogram(bins = 15, fill = \"white\", colour = \"black\")\naugment_df |> filter(.hat > 0.2)\n#> # A tibble: 2 × 12\n#>   total_cup_p…¹ species aroma flavor sweet…² moist…³ .fitted\n#>           <dbl> <chr>   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>\n#> 1          59.8 Arabica   7.5   6.67    1.33    0.1    58.4 \n#> 2           0   Arabica   0     0       0       0.12    6.81\n#> # … with 5 more variables: .resid <dbl>, .hat <dbl>,\n#> #   .sigma <dbl>, .cooksd <dbl>, .std.resid <dbl>, and\n#> #   abbreviated variable names ¹​total_cup_points,\n#> #   ²​sweetness, ³​moisture\n#> # ℹ Use `colnames()` to see all variable names\naugment_df |> filter(.std.resid > 3 | .std.resid < -3)\n#> # A tibble: 25 × 12\n#>    total_cup_…¹ species aroma flavor sweet…² moist…³ .fitted\n#>           <dbl> <chr>   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>\n#>  1         82.8 Arabica  8.08   8.17   10       0.12    86.6\n#>  2         82.4 Arabica  5.08   7.75   10       0.11    78.6\n#>  3         82.3 Arabica  7.75   8.08    6.67    0.11    78.1\n#>  4         80.7 Arabica  7.67   7.5     6.67    0       75.2\n#>  5         80   Arabica  7.58   7.75   10       0       83.7\n#>  6         79.9 Arabica  7.83   7.67   10       0       83.8\n#>  7         79.2 Arabica  7.17   7.42    6.67    0.1     73.6\n#>  8         78.6 Arabica  7.92   7.58   10       0.1     83.3\n#>  9         78.3 Arabica  7.17   6.08   10       0.11    74.2\n#> 10         77.6 Arabica  7.58   7.67    6       0.12    74.1\n#> # … with 15 more rows, 5 more variables: .resid <dbl>,\n#> #   .hat <dbl>, .sigma <dbl>, .cooksd <dbl>,\n#> #   .std.resid <dbl>, and abbreviated variable names\n#> #   ¹​total_cup_points, ²​sweetness, ³​moisture\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\nggplot(data = augment_df |> filter(.fitted > 25), aes(x = .fitted, y = .resid)) +\n  geom_point() "
  },
  {
    "path": "connections-to-stat-113-stat-213-and-cs-140.html",
    "id": "exercise-15-2",
    "chapter": " 16 Connections to STAT 113, STAT 213, and CS 140",
    "heading": "16.2.2 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 16.5.Add couple predictors linear model fit earlier. , use glance() obtain model fit statistics. model “better” according metrics learned STAT 213?Add couple predictors linear model fit earlier. , use glance() obtain model fit statistics. model “better” according metrics learned STAT 213?one fitted models, construct histogram residuals assess normality assumption (using ggplot2 augment()).one fitted models, construct histogram residuals assess normality assumption (using ggplot2 augment()).Make table 5 coffees highest predicted coffee rating, according one models.Make table 5 coffees highest predicted coffee rating, according one models.",
    "code": ""
  },
  {
    "path": "connections-to-stat-113-stat-213-and-cs-140.html",
    "id": "cs-140",
    "chapter": " 16 Connections to STAT 113, STAT 213, and CS 140",
    "heading": "16.3 CS 140",
    "text": "section, repeat couple topics CS 140, Python, R. particular ,write function. syntax R similar Python.write function. syntax R similar Python.perform iteration repeat similar task multiple times.perform iteration repeat similar task multiple times.start, suppose interested scraping hitting data SLU’s baseball team web address https://saintsathletics.com/sports/baseball/stats/2022. hitting data, want create statistic player’s weighted -base-average (wOBA). Information wOBA can found : https://www.mlb.com/glossary/advanced-stats/weighted--base-average. following code modified project completed Jack Sylvia data visualization course.Code task given following chunk.can make sure statistic calculated :",
    "code": "\nlibrary(tidyverse)\nlibrary(rvest)\n#> \n#> Attaching package: 'rvest'\n#> The following object is masked from 'package:readr':\n#> \n#>     guess_encoding\n\nurl_SLU <- \"https://saintsathletics.com/sports/baseball/stats/2022\"\ntab_SLU <- read_html(url_SLU) |> html_nodes(\"table\")\nSLU_Hitting <- tab_SLU[[1]] |> html_table(fill = TRUE) |>\n  head(-2) |>\n  select(-23) |>\n  mutate(wOBA = (0.69 * BB + 0.72 * HBP + 0.89 * (H-`2B`-`3B`-`HR`) + 1.27 * `2B` + 1.62 * `3B` + 2.10 * HR) / (AB + BB + SF + HBP))\nSLU_Hitting |> select(wOBA, everything()) |> arrange(desc(wOBA))\n#> # A tibble: 20 × 23\n#>     wOBA   `#` Player    AVG   OPS `GP-GS`    AB     R     H\n#>    <dbl> <int> <chr>   <dbl> <dbl> <chr>   <int> <int> <int>\n#>  1 0.514     7 \"Brink… 0.556 1.16  5-1         9     3     5\n#>  2 0.497    25 \"Liber… 0.379 1.16  25-19      66    19    25\n#>  3 0.46      1 \"Verra… 0.5   1.1   4-0         2     1     1\n#>  4 0.452    13 \"Butle… 0.325 1.08  35-35     126    31    41\n#>  5 0.433     6 \"Clark… 0.367 1.02  29-19      79    24    29\n#>  6 0.425    11 \"Circe… 0.252 1.00  35-33     111    27    28\n#>  7 0.412     9 \"Burke… 0.308 0.885 6-4        13     4     4\n#>  8 0.391    30 \"Watso… 0.318 0.853 35-33     110    29    35\n#>  9 0.365    19 \"Delan… 0.33  0.817 34-34     115    18    38\n#> 10 0.358     8 \"Forgi… 0.244 0.799 27-27      86    22    21\n#> 11 0.356     5 \"Desja… 0.284 0.741 28-20      67    23    19\n#> 12 0.347    37 \"Goret… 0.268 0.771 32-32      97    18    26\n#> 13 0.345     3 \"Haun,… 0     0.5   11-0        2     1     0\n#> 14 0.335    18 \"Feder… 0.275 0.733 25-7       40    13    11\n#> 15 0.309    23 \"Conno… 0.286 0.661 6-0         7     2     2\n#> 16 0.294    22 \"Court… 0.256 0.63  20-10      39     7    10\n#> 17 0.292    20 \"Court… 0.281 0.646 27-17      64     6    18\n#> 18 0.285    41 \"Comer… 0.222 0.582 31-21      72    16    16\n#> 19 0.154    24 \"Colan… 0.095 0.295 15-3       21     6     2\n#> 20 0        36 \"Boldu… 0     0     2-0         2     0     0\n#> # … with 14 more variables: `2B` <int>, `3B` <int>,\n#> #   HR <int>, RBI <int>, TB <int>, `SLG%` <dbl>, BB <int>,\n#> #   HBP <int>, SO <int>, GDP <int>, `OB%` <dbl>, SF <int>,\n#> #   SH <int>, `SB-ATT` <chr>\n#> # ℹ Use `colnames()` to see all variable names"
  },
  {
    "path": "connections-to-stat-113-stat-213-and-cs-140.html",
    "id": "functions",
    "chapter": " 16 Connections to STAT 113, STAT 213, and CS 140",
    "heading": "16.3.1 Functions",
    "text": "Now, suppose might want repeat scraping calculation wOBA years SLU teams. , course, obtain new URL address copy paste code used , replacing old URL address new one. reasonable thing wanted one url. , wanted 10, 20, 50, 1000, urls, might consider writing function scrape data calculate wOBA.format function R :used functions throughout entire semester, always functions others defined imported R packages. expand toolbox, might encounter situations want write specialized functions performing tasks covered functions others written.get back example, let’s write simple function, called get_sum_squares, computes sum squares numeric vector argument named x_vec. sum squares function take number x_vec, square , add numbers .Now, let’s test function numeric vector c(2, 4, 1)Now, move back example. want write function called get_hitting_data takes url_name, scrapes data url, calculates wOBA variables scraped. Note function work urls contain data table formatted various baseball statistics column names.create function, can simply copy paste code replace SLU url web address argument url_name body function.can test function SLU url:",
    "code": "\nname_of_function <- function(argument1, argument2, ....) {\n  body_of_function ## performs various tasks with the arguments\n  \n  return(output) ## tells the function what to return\n}\nget_sum_squares <- function(x_vec) {\n  \n  sum_of_squares <- sum(x_vec ^ 2)\n  \n  return(sum_of_squares)\n}\nget_sum_squares(x_vec = c(2, 4, 1))\n#> [1] 21\nget_hitting_data <- function(url_name) {\n  \n  tab <- read_html(url_name) |> html_nodes(\"table\")\n  \n  hitting <- tab[[1]] |> html_table(fill = TRUE) |>\n    head(-2) |>\n    select(-23) |>\n    mutate(wOBA = (0.69 * BB + 0.72 * HBP + 0.89 *\n                     (H- `2B` - `3B` - `HR`) +\n                     1.27 * `2B` + 1.62 * `3B` + 2.10 * HR) / \n             (AB + BB + SF + HBP),\n           url_name = url_name)\n  \n  return(hitting)\n}\nget_hitting_data(url_name = \"https://saintsathletics.com/sports/baseball/stats/2022\")\n#> # A tibble: 20 × 24\n#>      `#` Player    AVG   OPS `GP-GS`    AB     R     H  `2B`\n#>    <int> <chr>   <dbl> <dbl> <chr>   <int> <int> <int> <int>\n#>  1     6 \"Clark… 0.367 1.02  29-19      79    24    29     5\n#>  2    19 \"Delan… 0.33  0.817 34-34     115    18    38     5\n#>  3    13 \"Butle… 0.325 1.08  35-35     126    31    41     9\n#>  4    30 \"Watso… 0.318 0.853 35-33     110    29    35     6\n#>  5     5 \"Desja… 0.284 0.741 28-20      67    23    19     1\n#>  6    20 \"Court… 0.281 0.646 27-17      64     6    18     2\n#>  7    37 \"Goret… 0.268 0.771 32-32      97    18    26     6\n#>  8    11 \"Circe… 0.252 1.00  35-33     111    27    28     9\n#>  9     8 \"Forgi… 0.244 0.799 27-27      86    22    21     4\n#> 10    41 \"Comer… 0.222 0.582 31-21      72    16    16     0\n#> 11     7 \"Brink… 0.556 1.16  5-1         9     3     5     0\n#> 12     1 \"Verra… 0.5   1.1   4-0         2     1     1     0\n#> 13    25 \"Liber… 0.379 1.16  25-19      66    19    25     8\n#> 14     9 \"Burke… 0.308 0.885 6-4        13     4     4     1\n#> 15    23 \"Conno… 0.286 0.661 6-0         7     2     2     0\n#> 16    18 \"Feder… 0.275 0.733 25-7       40    13    11     3\n#> 17    22 \"Court… 0.256 0.63  20-10      39     7    10     1\n#> 18    24 \"Colan… 0.095 0.295 15-3       21     6     2     0\n#> 19     3 \"Haun,… 0     0.5   11-0        2     1     0     0\n#> 20    36 \"Boldu… 0     0     2-0         2     0     0     0\n#> # … with 15 more variables: `3B` <int>, HR <int>,\n#> #   RBI <int>, TB <int>, `SLG%` <dbl>, BB <int>, HBP <int>,\n#> #   SO <int>, GDP <int>, `OB%` <dbl>, SF <int>, SH <int>,\n#> #   `SB-ATT` <chr>, wOBA <dbl>, url_name <chr>\n#> # ℹ Use `colnames()` to see all variable names"
  },
  {
    "path": "connections-to-stat-113-stat-213-and-cs-140.html",
    "id": "iteration",
    "chapter": " 16 Connections to STAT 113, STAT 213, and CS 140",
    "heading": "16.3.2 Iteration",
    "text": "Now suppose want use function scrape 2022 baseball statistics teams Liberty League. 10 teams total. websites team’s statistics well school name given tibble :One option obtain hitting statistics 10 teams calculating wOBA (assuming tables structured way web page) apply function 10 times bind together results. first three applications function shown .just 10 teams, approach certainly doable bit annoying. , wanted type calculation league teams, MLB (Major League Baseball)? , multiple years team?better approach use iteration write code repeatedly scrape data website calculate wOBA statistic function. CS 140, primary form iteration used probably loop. loops R similar syntax loops Python. However, general, loops clunky, can take lot lines code, can difficult read.section, instead focus functional programming approach iteration map() function family purrr. purrr part core tidyverse package gets loaded library(tidyverse). map() function two arguments: first vector list second function. map() applies function second argument element vector list first argument. example, consider applying get_sum_squares function wrote earlier list vectors:output list sums squares calculated get_sum_squares() function.apply map() approach iteration baseball web urls, create object called url_vec urls school., apply map() first argument url_vec second argument function get_hitting_data() wrote earlier.Scraping performing wOBA calculation take seconds. output list 10 tibbles. bind_rows() function used stack rows different data frames tibbles can also used stack rows data frames tibbles given list. apply function scraped data add name school data frame left_join():data set, can now things like figure top 3 hitters team, according wOBA metric:find players team bats AB:",
    "code": "\nschool_df <- tibble(school_name = c(\"SLU\", \"Clarkson\", \"Rochester\", \"RIT\", \"Ithaca\", \"Skidmore\", \"RPI\", \"Union\", \"Bard\", \"Vassar\"),\n                    hitting_web_url = c(\"https://saintsathletics.com/sports/baseball/stats/2022\",\n                 \"https://clarksonathletics.com/sports/baseball/stats/2022\", \n                 \"https://uofrathletics.com/sports/baseball/stats/2022\",\n                 \"https://ritathletics.com/sports/baseball/stats/2022\",\n                 \"https://athletics.ithaca.edu/sports/baseball/stats/2022\",\n                 \"https://skidmoreathletics.com/sports/baseball/stats/2022\",\n                 \"https://rpiathletics.com/sports/baseball/stats/2022\",\n                 \"https://unionathletics.com/sports/baseball/stats/2022\",\n                 \"https://bardathletics.com/sports/baseball/stats/2022\",\n                 \"https://www.vassarathletics.com/sports/baseball/stats/2022\"))\nschool_df\nget_hitting_data(url_name = \"https://saintsathletics.com/sports/baseball/stats/2022\")\nget_hitting_data(url_name = \"https://clarksonathletics.com/sports/baseball/stats/2022\")\nget_hitting_data(url_name = \"https://uofrathletics.com/sports/baseball/stats/2022\")\nnum_list <- list(vec1 = c(1, 4, 5), vec2 = c(9, 8, 3, 5), vec3 = 1)\nmap(num_list, get_sum_squares)\n#> $vec1\n#> [1] 42\n#> \n#> $vec2\n#> [1] 179\n#> \n#> $vec3\n#> [1] 1\nurl_vec <- school_df$hitting_web_url\nhitting_list <- map(url_vec, get_hitting_data)\nhitting_list\nhitting_ll <- hitting_list |> bind_rows() |>\n  left_join(school_df, by = c(\"url_name\" = \"hitting_web_url\"))\nhitting_ll\n#> # A tibble: 213 × 25\n#>      `#` Player    AVG   OPS `GP-GS`    AB     R     H  `2B`\n#>    <int> <chr>   <dbl> <dbl> <chr>   <int> <int> <int> <int>\n#>  1     6 \"Clark… 0.367 1.02  29-19      79    24    29     5\n#>  2    19 \"Delan… 0.33  0.817 34-34     115    18    38     5\n#>  3    13 \"Butle… 0.325 1.08  35-35     126    31    41     9\n#>  4    30 \"Watso… 0.318 0.853 35-33     110    29    35     6\n#>  5     5 \"Desja… 0.284 0.741 28-20      67    23    19     1\n#>  6    20 \"Court… 0.281 0.646 27-17      64     6    18     2\n#>  7    37 \"Goret… 0.268 0.771 32-32      97    18    26     6\n#>  8    11 \"Circe… 0.252 1.00  35-33     111    27    28     9\n#>  9     8 \"Forgi… 0.244 0.799 27-27      86    22    21     4\n#> 10    41 \"Comer… 0.222 0.582 31-21      72    16    16     0\n#> # … with 203 more rows, and 16 more variables: `3B` <int>,\n#> #   HR <int>, RBI <int>, TB <int>, `SLG%` <dbl>, BB <int>,\n#> #   HBP <int>, SO <int>, GDP <int>, `OB%` <dbl>, SF <int>,\n#> #   SH <int>, `SB-ATT` <chr>, wOBA <dbl>, url_name <chr>,\n#> #   school_name <chr>\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\nhitting_ll |> group_by(school_name) |>\n  arrange(desc(wOBA)) |>\n  slice(1:3) |>\n  select(Player, school_name, wOBA)\n#> # A tibble: 30 × 3\n#> # Groups:   school_name [10]\n#>    Player                                      schoo…¹  wOBA\n#>    <chr>                                       <chr>   <dbl>\n#>  1 \"Toby, Jared\\r\\n                          … Bard    0.488\n#>  2 \"Dumper, Sam\\r\\n                          … Bard    0.475\n#>  3 \"Myers, Jordan\\r\\n                        … Bard    0.431\n#>  4 \"Cantor, Danny\\r\\n                        … Clarks… 0.805\n#>  5 \"Price, Grant\\r\\n                         … Clarks… 0.475\n#>  6 \"Doyle, Caleb\\r\\n                         … Clarks… 0.473\n#>  7 \"Shirley, Buzz\\r\\n                        … Ithaca  0.524\n#>  8 \"Fabbo, Louis\\r\\n                         … Ithaca  0.465\n#>  9 \"Fabian, Matt\\r\\n                         … Ithaca  0.428\n#> 10 \"Blackall, Patrick \\r\\n                   … RIT     0.399\n#> # … with 20 more rows, and abbreviated variable name\n#> #   ¹​school_name\n#> # ℹ Use `print(n = ...)` to see more rows\nhitting_ll |> group_by(school_name) |>\n  arrange(desc(AB)) |>\n  slice(1:3) |>\n  select(Player, school_name, AB)\n#> # A tibble: 30 × 3\n#> # Groups:   school_name [10]\n#>    Player                                      schoo…¹    AB\n#>    <chr>                                       <chr>   <int>\n#>  1 \"Toby, Jared\\r\\n                          … Bard      107\n#>  2 \"Myers, Jordan\\r\\n                        … Bard      107\n#>  3 \"Luscher, Alex\\r\\n                        … Bard      101\n#>  4 \"Brouillette, Colby\\r\\n                   … Clarks…   127\n#>  5 \"Wilson, Kent\\r\\n                         … Clarks…   126\n#>  6 \"Doyle, Caleb\\r\\n                         … Clarks…   103\n#>  7 \"Pedersen, Connor\\r\\n                     … Ithaca    207\n#>  8 \"Cutaia, Nicholas\\r\\n                     … Ithaca    180\n#>  9 \"Merod, Gil\\r\\n                           … Ithaca    169\n#> 10 \"Reilly, Chris\\r\\n                        … RIT       152\n#> # … with 20 more rows, and abbreviated variable name\n#> #   ¹​school_name\n#> # ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "path": "connections-to-stat-113-stat-213-and-cs-140.html",
    "id": "exercise-15-3",
    "chapter": " 16 Connections to STAT 113, STAT 213, and CS 140",
    "heading": "16.3.3 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 16.5.code scrapes data wikipedia page listing billboard end year “hot 100” songs year 2021Wrap code function scrapes data Wikipedia user-provided year_scrape argument.Wrap code function scrapes data Wikipedia user-provided year_scrape argument.Create either vector years 2014 2021 list years 2014 2021. Use vector list, along function wrote Exercise 1, scrape data tables year.Create either vector years 2014 2021 list years 2014 2021. Use vector list, along function wrote Exercise 1, scrape data tables year.Combine data frames scraped Exercise 2 use combined data set figure artist appears highest number times billboard hot 100 list within years 2014 2021.Combine data frames scraped Exercise 2 use combined data set figure artist appears highest number times billboard hot 100 list within years 2014 2021.* Note solution Exercise 3 likely imperfect songs feature another musical artist. songs present problem counting number songs artist?* Note solution Exercise 3 likely imperfect songs feature another musical artist. songs present problem counting number songs artist?",
    "code": "\nlibrary(rvest)\nlibrary(tidyverse)\n\nyear_scrape <- 2021\nurl <- paste0(\"https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_\", year_scrape)\n\n## convert the html code into something R can read\nbillboard_tab <- read_html(url) |> html_nodes(\"table\")\n\n## grabs the tables\nbillboard_df <- billboard_tab[[1]] |> html_table() |>\n  mutate(year = year_scrape)\nbillboard_df\n#> # A tibble: 100 × 4\n#>      No. Title                                Artist…¹  year\n#>    <int> <chr>                                <chr>    <dbl>\n#>  1     1 \"\\\"Levitating\\\"\"                     Dua Lipa  2021\n#>  2     2 \"\\\"Save Your Tears\\\"\"                The Wee…  2021\n#>  3     3 \"\\\"Blinding Lights\\\"\"                The Wee…  2021\n#>  4     4 \"\\\"Mood\\\"\"                           24kGold…  2021\n#>  5     5 \"\\\"Good 4 U\\\"\"                       Olivia …  2021\n#>  6     6 \"\\\"Kiss Me More\\\"\"                   Doja Ca…  2021\n#>  7     7 \"\\\"Leave the Door Open\\\"\"            Silk So…  2021\n#>  8     8 \"\\\"Drivers License\\\"\"                Olivia …  2021\n#>  9     9 \"\\\"Montero (Call Me by Your Name)\\\"\" Lil Nas…  2021\n#> 10    10 \"\\\"Peaches\\\"\"                        Justin …  2021\n#> # … with 90 more rows, and abbreviated variable name\n#> #   ¹​`Artist(s)`\n#> # ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "path": "connections-to-stat-113-stat-213-and-cs-140.html",
    "id": "chapexercise-15",
    "chapter": " 16 Connections to STAT 113, STAT 213, and CS 140",
    "heading": "16.4 Chapter Exercises",
    "text": "chapter exercises section connections STAT 113, STAT 213, CS 140.",
    "code": ""
  },
  {
    "path": "connections-to-stat-113-stat-213-and-cs-140.html",
    "id": "solutions-15",
    "chapter": " 16 Connections to STAT 113, STAT 213, and CS 140",
    "heading": "16.5 Exercise Solutions",
    "text": "",
    "code": ""
  },
  {
    "path": "connections-to-stat-113-stat-213-and-cs-140.html",
    "id": "stat-113-s",
    "chapter": " 16 Connections to STAT 113, STAT 213, and CS 140",
    "heading": "16.5.1 STAT 113 S",
    "text": "",
    "code": ""
  },
  {
    "path": "connections-to-stat-113-stat-213-and-cs-140.html",
    "id": "stat-213-s",
    "chapter": " 16 Connections to STAT 113, STAT 213, and CS 140",
    "heading": "16.5.2 STAT 213 S",
    "text": "",
    "code": ""
  },
  {
    "path": "connections-to-stat-113-stat-213-and-cs-140.html",
    "id": "cs-140-s",
    "chapter": " 16 Connections to STAT 113, STAT 213, and CS 140",
    "heading": "16.5.3 CS 140 S",
    "text": "* Note solution Exercise 3 likely imperfect songs feature another musical artist. songs present problem counting number songs artist?group_by() musical artist use counting function n(), artist artist featuring musician counted separately table.",
    "code": ""
  },
  {
    "path": "connections-to-stat-113-stat-213-and-cs-140.html",
    "id": "rcode-15",
    "chapter": " 16 Connections to STAT 113, STAT 213, and CS 140",
    "heading": "16.6 Non-Exercise R Code",
    "text": "",
    "code": "\nlibrary(openintro)\nresume\nlibrary(tidyverse)\nresume_sum <- resume |> \n  mutate(received_callback = received_callback) |>\n           group_by(race, received_callback) |>\n  summarise(count = n())\nggplot(data = resume_sum, aes(x = race, y = count)) +\n  geom_col(aes(fill = received_callback)) +\n  scale_fill_viridis_c()\nresume <- resume |>\n  mutate(received_callback = as.factor(received_callback))\nresume_sum <- resume |> \n           group_by(race, received_callback) |>\n  summarise(count = n())\nggplot(data = resume_sum, aes(x = race, y = count)) +\n  geom_col(aes(fill = received_callback)) +\n  scale_fill_viridis_d()\nresume |> group_by(race, received_callback) |>\n  summarise(count = n()) |>\n  pivot_wider(names_from = c(\"race\"),\n              values_from = \"count\")\nchisq.test(x = resume$race, y = resume$received_callback)\nggplot(data = resume, aes(x = fct_rev(fct_infreq(job_type)))) +\n  geom_bar() +\n  coord_flip() +\n  labs(x = \"Job Type\")\nggplot(data = resume, aes(x = fct_rev(fct_infreq(job_industry)))) +\n  geom_bar() +\n  coord_flip() +\n  labs(x = \"Job Industry\")\nresume_firstname <- resume |>\n  group_by(firstname) |>\n  summarise(propcallback = mean(received_callback == \"1\"),\n            gender = unique(gender),\n            race = unique(race)) |>\n  arrange(desc(propcallback)) |>\n  unite(\"gender_race\", c(gender, race))\n\nggplot(data = resume_firstname, aes(x = gender_race, y = propcallback)) +\n  geom_point()\nlibrary(ggrepel)\nlabel_df <- resume_firstname |> \n  filter(propcallback == max(propcallback) |\n           propcallback == min(propcallback))\n\nggplot(data = resume_firstname, aes(x = gender_race, y = propcallback)) +\n  geom_point() +\n  geom_label_repel(data = label_df, aes(label = firstname))\nlibrary(broom)\nlibrary(here)\ncoffee_df <- read_csv(here(\"data/coffee_ratings.csv\"))\ncoffee_mod <- lm(total_cup_points ~ species + aroma + flavor +\n                   sweetness + moisture,\n   data = coffee_df)\nsummary(coffee_mod)\nsummary(coffee_mod)$coefficients[\"moisture\", 4]\ntidy(coffee_mod)\ntidy(coffee_mod) |> select(p.value)\ntidy(coffee_mod) |> filter(term == \"aroma\")\nglance(coffee_mod)\naugment(coffee_mod)\naugment_df <- augment(coffee_mod)\naugment_df |> filter(.cooksd > 1)\nggplot(data = coffee_df, aes(x = total_cup_points)) +\n  geom_histogram(bins = 15, fill = \"white\", colour = \"black\")\naugment_df |> filter(.hat > 0.2)\naugment_df |> filter(.std.resid > 3 | .std.resid < -3)\nggplot(data = augment_df |> filter(.fitted > 25), aes(x = .fitted, y = .resid)) +\n  geom_point() \nlibrary(tidyverse)\nlibrary(rvest)\n\nurl_SLU <- \"https://saintsathletics.com/sports/baseball/stats/2022\"\ntab_SLU <- read_html(url_SLU) |> html_nodes(\"table\")\nSLU_Hitting <- tab_SLU[[1]] |> html_table(fill = TRUE) |>\n  head(-2) |>\n  select(-23) |>\n  mutate(wOBA = (0.69 * BB + 0.72 * HBP + 0.89 * (H-`2B`-`3B`-`HR`) + 1.27 * `2B` + 1.62 * `3B` + 2.10 * HR) / (AB + BB + SF + HBP))\nSLU_Hitting |> select(wOBA, everything()) |> arrange(desc(wOBA))\nget_sum_squares <- function(x_vec) {\n  \n  sum_of_squares <- sum(x_vec ^ 2)\n  \n  return(sum_of_squares)\n}\nget_sum_squares(x_vec = c(2, 4, 1))\nget_hitting_data <- function(url_name) {\n  \n  tab <- read_html(url_name) |> html_nodes(\"table\")\n  \n  hitting <- tab[[1]] |> html_table(fill = TRUE) |>\n    head(-2) |>\n    select(-23) |>\n    mutate(wOBA = (0.69 * BB + 0.72 * HBP + 0.89 *\n                     (H- `2B` - `3B` - `HR`) +\n                     1.27 * `2B` + 1.62 * `3B` + 2.10 * HR) / \n             (AB + BB + SF + HBP),\n           url_name = url_name)\n  \n  return(hitting)\n}\nget_hitting_data(url_name = \"https://saintsathletics.com/sports/baseball/stats/2022\")\nschool_df <- tibble(school_name = c(\"SLU\", \"Clarkson\", \"Rochester\", \"RIT\", \"Ithaca\", \"Skidmore\", \"RPI\", \"Union\", \"Bard\", \"Vassar\"),\n                    hitting_web_url = c(\"https://saintsathletics.com/sports/baseball/stats/2022\",\n                 \"https://clarksonathletics.com/sports/baseball/stats/2022\", \n                 \"https://uofrathletics.com/sports/baseball/stats/2022\",\n                 \"https://ritathletics.com/sports/baseball/stats/2022\",\n                 \"https://athletics.ithaca.edu/sports/baseball/stats/2022\",\n                 \"https://skidmoreathletics.com/sports/baseball/stats/2022\",\n                 \"https://rpiathletics.com/sports/baseball/stats/2022\",\n                 \"https://unionathletics.com/sports/baseball/stats/2022\",\n                 \"https://bardathletics.com/sports/baseball/stats/2022\",\n                 \"https://www.vassarathletics.com/sports/baseball/stats/2022\"))\nschool_df\nget_hitting_data(url_name = \"https://saintsathletics.com/sports/baseball/stats/2022\")\nget_hitting_data(url_name = \"https://clarksonathletics.com/sports/baseball/stats/2022\")\nget_hitting_data(url_name = \"https://uofrathletics.com/sports/baseball/stats/2022\")\nnum_list <- list(vec1 = c(1, 4, 5), vec2 = c(9, 8, 3, 5), vec3 = 1)\nmap(num_list, get_sum_squares)\nurl_vec <- school_df$hitting_web_url\nhitting_list <- map(url_vec, get_hitting_data)\nhitting_list\nhitting_ll <- hitting_list |> bind_rows() |>\n  left_join(school_df, by = c(\"url_name\" = \"hitting_web_url\"))\nhitting_ll\nhitting_ll |> group_by(school_name) |>\n  arrange(desc(wOBA)) |>\n  slice(1:3) |>\n  select(Player, school_name, wOBA)\nhitting_ll |> group_by(school_name) |>\n  arrange(desc(AB)) |>\n  slice(1:3) |>\n  select(Player, school_name, AB)"
  },
  {
    "path": "introduction-to-sql-with-dbplyr.html",
    "id": "introduction-to-sql-with-dbplyr",
    "chapter": " 17 Introduction to SQL with dbplyr",
    "heading": " 17 Introduction to SQL with dbplyr",
    "text": "Goals:explain database , different data set, might use database.use dbplyr translate R code dplyr SQL queries database tables.draw parallels dplyr functions syntax used SQL.dplyr functions ’ve used (ones early semester xxxx_join() family recently) corresponding components SQL. SQL stands Structured Query Language common language used databases. Compared dplyr, general, SQL code much harder read, SQL isn’t designed specifically data analysis like dplyr . section, introduce databases give brief introduction SQL analyzing data database.",
    "code": ""
  },
  {
    "path": "introduction-to-sql-with-dbplyr.html",
    "id": "what-is-a-database",
    "chapter": " 17 Introduction to SQL with dbplyr",
    "heading": "17.1 What is a Database",
    "text": "R Data Science textbook defines database “collection data frames,” called database table. key differences data frame (’ve using entire semester) database table. summarised R Data Science :database table can larger stored disk data frame stored memory size limited.database table usually indices data frames .many , , data base tables “row-oriented” tidy data frames “column-oriented.”Databases run Database Management Systems. R Data Science textbook divides Database Management Systems 3 types:client-server like PostgreSQL SQL ServerCloud-based like Amazon’s RedshiftIn-process like SQLiteWe won’t really discuss , advanced course database systems CS department give information Database Management Systems (databases general).connect database R depends type database management system. R package major Database Management Systems. purposes, connect Database management system depends heavily type, focus database management system contained R package duckdb.also need database interface connect database tables duckdb DBI package.Note: section connecting database management systems may confusing, particularly computer science background. don’t let derail learning rest chapter, consist primarily R code ! take-home message need way connect system within R. ’s challenging give specific directions connection depends type system, avoiding connecting database management system duckdb R package using functions DBI package.https://r4ds.hadley.nz/databases.html\nSQL short Structured Query Language.\nfirst load duckdb DBI libraries make connection database management system, name con:can type con see stores:’ve created brand-new database, can next add data tables duckdb_read_csv() function. Compared read_csv() readr package, duckdb_read_csv() couple extra arguments: conn argument giving database management connection name argument giving name want give data table:doListTables() function lists names data tables database just created:, dbExistsTable() can used examine whether data table exists current database:Note , many practical situations, data tables already exist database working , step duckdb_read_csv() necessary.use raw SQL code query database just created, can create string SQL code, name sql, pass dbGetQuery() function. also load tidyverse package use as_tibble() function convert data.frame tibble.",
    "code": "\nlibrary(DBI)\nlibrary(duckdb)\ncon <- DBI::dbConnect(duckdb::duckdb())\ncon\n#> <duckdb_connection 9cd00 driver=<duckdb_driver 09a40 dbdir=':memory:' read_only=FALSE>>\nlibrary(here)\n#> here() starts at /Users/highamm/Desktop/datascience234\nduckdb_read_csv(conn = con, name = \"tennis2018\", \n                files = here(\"data/atp_matches_2018.csv\"))\nduckdb_read_csv(conn = con, name = \"tennis2019\", \n                files = here(\"data/atp_matches_2019.csv\"))\ndbListTables(con)\n#> [1] \"tennis2018\" \"tennis2019\"\ndbExistsTable(con, \"tennis2019\")\n#> [1] TRUE\ndbExistsTable(con, \"tennis2020\")\n#> [1] FALSE\nlibrary(tidyverse)\n#> ── Attaching packages ─────────────────── tidyverse 1.3.2 ──\n#> ✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n#> ✔ tibble  3.1.8     ✔ dplyr   1.0.9\n#> ✔ tidyr   1.2.0     ✔ stringr 1.4.0\n#> ✔ readr   2.1.2     ✔ forcats 0.5.1\n#> ── Conflicts ────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n\nsql <- \"\n  SELECT surface, winner_name, loser_name, w_ace, l_ace, minutes\n  FROM tennis2019 \n  WHERE minutes > 240\n\"\ndbGetQuery(con, sql)|>\n  as_tibble()\n#> # A tibble: 30 × 6\n#>    surface winner_name           loser…¹ w_ace l_ace minutes\n#>    <chr>   <chr>                 <chr>   <int> <int>   <int>\n#>  1 Hard    Joao Sousa            Guido …    19    18     241\n#>  2 Hard    Jeremy Chardy         Ugo Hu…    29    20     244\n#>  3 Hard    Roberto Bautista Agut Andy M…     7    19     249\n#>  4 Hard    Joao Sousa            Philip…    28    20     258\n#>  5 Hard    Alex Bolt             Gilles…    11    14     244\n#>  6 Hard    Milos Raonic          Stan W…    39    28     241\n#>  7 Hard    Marin Cilic           Fernan…     8    27     258\n#>  8 Hard    Kei Nishikori         Pablo …    15     5     305\n#>  9 Hard    Frances Tiafoe        David …     3    11     244\n#> 10 Clay    Alexander Zverev      John M…    17     0     248\n#> # … with 20 more rows, and abbreviated variable name\n#> #   ¹​loser_name\n#> # ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "path": "introduction-to-sql-with-dbplyr.html",
    "id": "exercise-16-1",
    "chapter": " 17 Introduction to SQL with dbplyr",
    "heading": "17.1.1 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 17.5.* Though know SQL code, can probably figure code . matches returned query?* Though know SQL code, can probably figure code . matches returned query?dplyr equivalent function SQL code ? dplyr equivalent function SELECT SQL code ?dplyr equivalent function SQL code ? dplyr equivalent function SELECT SQL code ?",
    "code": ""
  },
  {
    "path": "introduction-to-sql-with-dbplyr.html",
    "id": "dbplyr-a-database-version-of-dplyr",
    "chapter": " 17 Introduction to SQL with dbplyr",
    "heading": "17.2 dbplyr: A Database Version of dplyr",
    "text": "dbplyr package allow us continue write dplyr-style code query databases instead writing native SQL, code-chunk .begin loading package creating database table object tbl() function. case, create database table tennis2019 data name tennis_db:Examine print tennis_db, look similar print tibble data.frame. Let’s use dplyr code obtain matches lasted longer 240 minutes keep columns. name result tennis_query1:note result still database object: ’s “usual” tibble. One major difference database object usual tibble tennis_query1 tell us many rows data (see ?? specification rows). code wrote actually looking entire data set matches longer 240 minutes: saving time performing query part database table. useful behaviour database tables , large, code might take long time run.want obtain result query tibble, can use collect() function:result tibble can now use R functions (just functions dplyr packages).show_query() function can used tennis_query1 give SQL code executed:get better idea SQL code looks like, let’s make one query dplyr code use show_query() function give native SQL:show_query() shows native SQL code pivot: yikes! Remember SQL designed data analysis, doesn’t always look pretty. ’ll one simpler query:Can match SQL code corresponding dplyr functions used?",
    "code": "\nlibrary(dbplyr)\n#> \n#> Attaching package: 'dbplyr'\n#> The following objects are masked from 'package:dplyr':\n#> \n#>     ident, sql\ntennis_db <- tbl(con, \"tennis2019\")\ntennis_db\n#> # Source:   table<tennis2019> [?? x 49]\n#> # Database: DuckDB 0.3.5-dev1410 [root@Darwin 21.6.0:R 4.2.1/:memory:]\n#>    tourney…¹ tourn…² surface draw_…³ tourn…⁴ tourn…⁵ match…⁶\n#>    <chr>     <chr>   <chr>     <int> <chr>     <int>   <int>\n#>  1 2019-M020 Brisba… Hard         32 A        2.02e7     300\n#>  2 2019-M020 Brisba… Hard         32 A        2.02e7     299\n#>  3 2019-M020 Brisba… Hard         32 A        2.02e7     298\n#>  4 2019-M020 Brisba… Hard         32 A        2.02e7     297\n#>  5 2019-M020 Brisba… Hard         32 A        2.02e7     296\n#>  6 2019-M020 Brisba… Hard         32 A        2.02e7     295\n#>  7 2019-M020 Brisba… Hard         32 A        2.02e7     294\n#>  8 2019-M020 Brisba… Hard         32 A        2.02e7     293\n#>  9 2019-M020 Brisba… Hard         32 A        2.02e7     292\n#> 10 2019-M020 Brisba… Hard         32 A        2.02e7     291\n#> # … with more rows, 42 more variables: winner_id <int>,\n#> #   winner_seed <chr>, winner_entry <chr>,\n#> #   winner_name <chr>, winner_hand <chr>, winner_ht <int>,\n#> #   winner_ioc <chr>, winner_age <dbl>, loser_id <int>,\n#> #   loser_seed <chr>, loser_entry <chr>, loser_name <chr>,\n#> #   loser_hand <chr>, loser_ht <int>, loser_ioc <chr>,\n#> #   loser_age <dbl>, score <chr>, best_of <int>, …\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\ntennis_query1 <- tennis_db |> \n  filter(minutes > 240) |> \n  select(minutes, winner_name, loser_name, minutes, tourney_name)\ntennis_query1\n#> # Source:   SQL [?? x 4]\n#> # Database: DuckDB 0.3.5-dev1410 [root@Darwin 21.6.0:R 4.2.1/:memory:]\n#>    minutes winner_name           loser_name          tourn…¹\n#>      <int> <chr>                 <chr>               <chr>  \n#>  1     241 Joao Sousa            Guido Pella         Austra…\n#>  2     244 Jeremy Chardy         Ugo Humbert         Austra…\n#>  3     249 Roberto Bautista Agut Andy Murray         Austra…\n#>  4     258 Joao Sousa            Philipp Kohlschrei… Austra…\n#>  5     244 Alex Bolt             Gilles Simon        Austra…\n#>  6     241 Milos Raonic          Stan Wawrinka       Austra…\n#>  7     258 Marin Cilic           Fernando Verdasco   Austra…\n#>  8     305 Kei Nishikori         Pablo Carreno Busta Austra…\n#>  9     244 Frances Tiafoe        David Goffin        Miami …\n#> 10     248 Alexander Zverev      John Millman        Roland…\n#> # … with more rows, and abbreviated variable name\n#> #   ¹​tourney_name\n#> # ℹ Use `print(n = ...)` to see more rows\ntennis_query1 |>\n  collect()\n#> # A tibble: 30 × 4\n#>    minutes winner_name           loser_name          tourn…¹\n#>      <int> <chr>                 <chr>               <chr>  \n#>  1     241 Joao Sousa            Guido Pella         Austra…\n#>  2     244 Jeremy Chardy         Ugo Humbert         Austra…\n#>  3     249 Roberto Bautista Agut Andy Murray         Austra…\n#>  4     258 Joao Sousa            Philipp Kohlschrei… Austra…\n#>  5     244 Alex Bolt             Gilles Simon        Austra…\n#>  6     241 Milos Raonic          Stan Wawrinka       Austra…\n#>  7     258 Marin Cilic           Fernando Verdasco   Austra…\n#>  8     305 Kei Nishikori         Pablo Carreno Busta Austra…\n#>  9     244 Frances Tiafoe        David Goffin        Miami …\n#> 10     248 Alexander Zverev      John Millman        Roland…\n#> # … with 20 more rows, and abbreviated variable name\n#> #   ¹​tourney_name\n#> # ℹ Use `print(n = ...)` to see more rows\ntennis_query1 |>\n  show_query()\n#> <SQL>\n#> SELECT \"minutes\", \"winner_name\", \"loser_name\", \"tourney_name\"\n#> FROM \"tennis2019\"\n#> WHERE (\"minutes\" > 240.0)\nmedvedev_query <- tennis_db |>\n  pivot_longer(c(winner_name, loser_name), names_to = \"win_loss\",\n               values_to = \"player\") |>\n  filter(player == \"Daniil Medvedev\") |>\n  group_by(win_loss) |>\n  summarise(win_loss_count = n())\nmedvedev_query\n#> # Source:   SQL [2 x 2]\n#> # Database: DuckDB 0.3.5-dev1410 [root@Darwin 21.6.0:R 4.2.1/:memory:]\n#>   win_loss    win_loss_count\n#>   <chr>                <dbl>\n#> 1 winner_name             59\n#> 2 loser_name              21\nshow_query(medvedev_query)\n#> <SQL>\n#> SELECT \"win_loss\", COUNT(*) AS \"win_loss_count\"\n#> FROM (\n#>   (\n#>     SELECT\n#>       \"tourney_id\",\n#>       \"tourney_name\",\n#>       \"surface\",\n#>       \"draw_size\",\n#>       \"tourney_level\",\n#>       \"tourney_date\",\n#>       \"match_num\",\n#>       \"winner_id\",\n#>       \"winner_seed\",\n#>       \"winner_entry\",\n#>       \"winner_hand\",\n#>       \"winner_ht\",\n#>       \"winner_ioc\",\n#>       \"winner_age\",\n#>       \"loser_id\",\n#>       \"loser_seed\",\n#>       \"loser_entry\",\n#>       \"loser_hand\",\n#>       \"loser_ht\",\n#>       \"loser_ioc\",\n#>       \"loser_age\",\n#>       \"score\",\n#>       \"best_of\",\n#>       \"round\",\n#>       \"minutes\",\n#>       \"w_ace\",\n#>       \"w_df\",\n#>       \"w_svpt\",\n#>       \"w_1stIn\",\n#>       \"w_1stWon\",\n#>       \"w_2ndWon\",\n#>       \"w_SvGms\",\n#>       \"w_bpSaved\",\n#>       \"w_bpFaced\",\n#>       \"l_ace\",\n#>       \"l_df\",\n#>       \"l_svpt\",\n#>       \"l_1stIn\",\n#>       \"l_1stWon\",\n#>       \"l_2ndWon\",\n#>       \"l_SvGms\",\n#>       \"l_bpSaved\",\n#>       \"l_bpFaced\",\n#>       \"winner_rank\",\n#>       \"winner_rank_points\",\n#>       \"loser_rank\",\n#>       \"loser_rank_points\",\n#>       'winner_name' AS \"win_loss\",\n#>       \"winner_name\" AS \"player\"\n#>     FROM \"tennis2019\"\n#>   )\n#>   UNION ALL\n#>   (\n#>     SELECT\n#>       \"tourney_id\",\n#>       \"tourney_name\",\n#>       \"surface\",\n#>       \"draw_size\",\n#>       \"tourney_level\",\n#>       \"tourney_date\",\n#>       \"match_num\",\n#>       \"winner_id\",\n#>       \"winner_seed\",\n#>       \"winner_entry\",\n#>       \"winner_hand\",\n#>       \"winner_ht\",\n#>       \"winner_ioc\",\n#>       \"winner_age\",\n#>       \"loser_id\",\n#>       \"loser_seed\",\n#>       \"loser_entry\",\n#>       \"loser_hand\",\n#>       \"loser_ht\",\n#>       \"loser_ioc\",\n#>       \"loser_age\",\n#>       \"score\",\n#>       \"best_of\",\n#>       \"round\",\n#>       \"minutes\",\n#>       \"w_ace\",\n#>       \"w_df\",\n#>       \"w_svpt\",\n#>       \"w_1stIn\",\n#>       \"w_1stWon\",\n#>       \"w_2ndWon\",\n#>       \"w_SvGms\",\n#>       \"w_bpSaved\",\n#>       \"w_bpFaced\",\n#>       \"l_ace\",\n#>       \"l_df\",\n#>       \"l_svpt\",\n#>       \"l_1stIn\",\n#>       \"l_1stWon\",\n#>       \"l_2ndWon\",\n#>       \"l_SvGms\",\n#>       \"l_bpSaved\",\n#>       \"l_bpFaced\",\n#>       \"winner_rank\",\n#>       \"winner_rank_points\",\n#>       \"loser_rank\",\n#>       \"loser_rank_points\",\n#>       'loser_name' AS \"win_loss\",\n#>       \"loser_name\" AS \"player\"\n#>     FROM \"tennis2019\"\n#>   )\n#> ) \"q01\"\n#> WHERE (\"player\" = 'Daniil Medvedev')\n#> GROUP BY \"win_loss\"\nover20aces <- tennis_db |> filter(w_ace > 20) |>\n  select(w_ace, winner_name) |>\n  group_by(winner_name) |>\n  summarise(nmatch = n()) |>\n  arrange(desc(nmatch))\nover20aces\n#> # Source:     SQL [?? x 2]\n#> # Database:   DuckDB 0.3.5-dev1410 [root@Darwin 21.6.0:R 4.2.1/:memory:]\n#> # Ordered by: desc(nmatch)\n#>    winner_name        nmatch\n#>    <chr>               <dbl>\n#>  1 John Isner             15\n#>  2 Reilly Opelka          14\n#>  3 Milos Raonic           10\n#>  4 Sam Querrey             9\n#>  5 Nick Kyrgios            8\n#>  6 Alexander Bublik        7\n#>  7 Ivo Karlovic            6\n#>  8 Jan Lennard Struff      4\n#>  9 Jo-Wilfried Tsonga      4\n#> 10 Alexander Zverev        4\n#> # … with more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nover20aces |> show_query()\n#> <SQL>\n#> SELECT \"winner_name\", COUNT(*) AS \"nmatch\"\n#> FROM (\n#>   SELECT \"w_ace\", \"winner_name\"\n#>   FROM \"tennis2019\"\n#>   WHERE (\"w_ace\" > 20.0)\n#> ) \"q01\"\n#> GROUP BY \"winner_name\"\n#> ORDER BY \"nmatch\" DESC"
  },
  {
    "path": "introduction-to-sql-with-dbplyr.html",
    "id": "exercise-16-2",
    "chapter": " 17 Introduction to SQL with dbplyr",
    "heading": "17.2.1 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 17.5.* Obtain distribution surface variable making table total number matches played surface 2019 season using dplyr functions tennis_db. , use show_query() show corresponding SQL code.* Obtain distribution surface variable making table total number matches played surface 2019 season using dplyr functions tennis_db. , use show_query() show corresponding SQL code.Create new variable difference winner_rank_points loser_rank_points using dplyr function. , query return column just created, winner_name column, loser_name column. Use show_query() function show corresponding SQL code.Create new variable difference winner_rank_points loser_rank_points using dplyr function. , query return column just created, winner_name column, loser_name column. Use show_query() function show corresponding SQL code.Perform query choosing tennis_db use show_query() function show corresponding SQL code.Perform query choosing tennis_db use show_query() function show corresponding SQL code.",
    "code": ""
  },
  {
    "path": "introduction-to-sql-with-dbplyr.html",
    "id": "sql",
    "chapter": " 17 Introduction to SQL with dbplyr",
    "heading": "17.3 SQL",
    "text": "purpose section explore SQL syntax little , focusing connections dplyr. Knowing dplyr quite helpful learning SQL syntax , syntax differs, concepts quite similar. Much text section paraphrased R Data Science textbook.five core components SQL query. two basic SELECT statement (similar select(), , discussed , mutate() summarise()) statement (similar data argument). Using show_query() function directly tennis_db shows SQL query SELECTs columns (denoted *), tennis2019 database.ORDER statements control rows returned (similar filter()) order rows get returned (similar arrange()):Finally, GROUP used aggregation (similar dplyr group_by() summarise() combination).code chunk, remove na.rm = TRUE argument run query. learn?SQL syntax must always follow order SELECT, , , GROUP , ORDER , even though operations can performed different order specified. one aspect makes SQL harder pick something like dplyr, specify want done order want.give little detail 5 operations.SELECT: SELECT covers lot dplyr functions. code , explore used SQL choose columns get returned, rename columns, create new variables:SELECT choose columns return:SELECT rename columns:SELECT create new variableSELECT create new variable summary:GROUP : GROUP covers aggregation similar way dplyr’s group_by() function:: used filter(), though SQL uses different Boolean operators R (example, & becomes , | becomes ).ORDER : ORDER used arrange(). one quite straightforward:SQL also corresponding syntax xxxx_join() family functions, time discuss detail. Note really just scratched surface SQL. entire courses devoted learning SQL syntax databases general. ever find situation need learn SQL, either course job, major head-start dplyr knowledge!",
    "code": "\ntennis_db |> show_query()\n#> <SQL>\n#> SELECT *\n#> FROM \"tennis2019\"\ntennis_db |> filter(winner_hand == \"L\") |>\n  arrange(desc(tourney_date)) |>\n  show_query()\n#> <SQL>\n#> SELECT *\n#> FROM \"tennis2019\"\n#> WHERE (\"winner_hand\" = 'L')\n#> ORDER BY \"tourney_date\" DESC\ntennis_db |>\n  group_by(winner_name) |>\n  summarise(meanace = mean(w_ace, na.rm = TRUE)) |>\n  show_query()\n#> <SQL>\n#> SELECT \"winner_name\", AVG(\"w_ace\") AS \"meanace\"\n#> FROM \"tennis2019\"\n#> GROUP BY \"winner_name\"\ntennis_db |> select(1:4) |> show_query()\n#> <SQL>\n#> SELECT \"tourney_id\", \"tourney_name\", \"surface\", \"draw_size\"\n#> FROM \"tennis2019\"\ntennis_db |> rename(tournament = tourney_name) |>\n  show_query()\n#> <SQL>\n#> SELECT\n#>   \"tourney_id\",\n#>   \"tourney_name\" AS \"tournament\",\n#>   \"surface\",\n#>   \"draw_size\",\n#>   \"tourney_level\",\n#>   \"tourney_date\",\n#>   \"match_num\",\n#>   \"winner_id\",\n#>   \"winner_seed\",\n#>   \"winner_entry\",\n#>   \"winner_name\",\n#>   \"winner_hand\",\n#>   \"winner_ht\",\n#>   \"winner_ioc\",\n#>   \"winner_age\",\n#>   \"loser_id\",\n#>   \"loser_seed\",\n#>   \"loser_entry\",\n#>   \"loser_name\",\n#>   \"loser_hand\",\n#>   \"loser_ht\",\n#>   \"loser_ioc\",\n#>   \"loser_age\",\n#>   \"score\",\n#>   \"best_of\",\n#>   \"round\",\n#>   \"minutes\",\n#>   \"w_ace\",\n#>   \"w_df\",\n#>   \"w_svpt\",\n#>   \"w_1stIn\",\n#>   \"w_1stWon\",\n#>   \"w_2ndWon\",\n#>   \"w_SvGms\",\n#>   \"w_bpSaved\",\n#>   \"w_bpFaced\",\n#>   \"l_ace\",\n#>   \"l_df\",\n#>   \"l_svpt\",\n#>   \"l_1stIn\",\n#>   \"l_1stWon\",\n#>   \"l_2ndWon\",\n#>   \"l_SvGms\",\n#>   \"l_bpSaved\",\n#>   \"l_bpFaced\",\n#>   \"winner_rank\",\n#>   \"winner_rank_points\",\n#>   \"loser_rank\",\n#>   \"loser_rank_points\"\n#> FROM \"tennis2019\"\ntennis_db |> mutate(prop_first_won = w_1stIn / w_1stWon) |>\n  select(prop_first_won, winner_name) |>\n  show_query()\n#> <SQL>\n#> SELECT \"w_1stIn\" / \"w_1stWon\" AS \"prop_first_won\", \"winner_name\"\n#> FROM \"tennis2019\"\ntennis_db |> summarise(mean_length = mean(minutes)) |>\n  show_query()\n#> <SQL>\n#> SELECT AVG(\"minutes\") AS \"mean_length\"\n#> FROM \"tennis2019\"\ntennis_db |> group_by(winner_name) |>\n  summarise(meanlength = mean(minutes)) |>\n  show_query()\n#> <SQL>\n#> SELECT \"winner_name\", AVG(\"minutes\") AS \"meanlength\"\n#> FROM \"tennis2019\"\n#> GROUP BY \"winner_name\"\ntennis_db |> filter(winner_age > 35 | loser_age > 35) |>\n  show_query()\n#> <SQL>\n#> SELECT *\n#> FROM \"tennis2019\"\n#> WHERE (\"winner_age\" > 35.0 OR \"loser_age\" > 35.0)\ntennis_db |> arrange(desc(winner_rank_points)) |>\n  show_query()\n#> <SQL>\n#> SELECT *\n#> FROM \"tennis2019\"\n#> ORDER BY \"winner_rank_points\" DESC"
  },
  {
    "path": "introduction-to-sql-with-dbplyr.html",
    "id": "exercise-16-3",
    "chapter": " 17 Introduction to SQL with dbplyr",
    "heading": "17.3.1 Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 17.5.much section, created code dplyr seen code translates SQL. exercises, intead given SQL code asked write dplyr code achieves thing.* Examine SQL code write equivalent dplyr code.Examine SQL code write equivalent dplyr code.Examine SQL code write equivalent dplyr code.",
    "code": "SELECT * \nFROM \"tennis2019\"\nWHERE (\"tourney_name\" = 'Wimbledon')SELECT \"winner_name\", \"loser_name\", \"w_ace\", \"l_ace\", \"w_ace\" - \"l_ace\" AS \"ace_diff\"\nFROM \"tennis2019\"\nORDER BY \"ace_diff\" DESCSELECT \"tourney_name\", AVG(\"minutes\") AS \"mean_min\"\nFROM \"tennis2019\"\nGROUP BY \"tourney_name\""
  },
  {
    "path": "introduction-to-sql-with-dbplyr.html",
    "id": "chapexercise-16",
    "chapter": " 17 Introduction to SQL with dbplyr",
    "heading": "17.4 Chapter Exercises",
    "text": "Exercises marked * indicate exercise solution end chapter 17.5.Run following code:Make hypothesis function like slice() compatible dbplyr.* Try run function lubridate forcats tennis_db mutate(). function work? expect work?* Try run function lubridate forcats tennis_db mutate(). function work? expect work?Run following code write ! translated SQL.Run following code write ! translated SQL.Run following code write %% symbol translated SQL.",
    "code": "\ntennis_db |> slice(1000:1005) |>\n  show_query()\ntennis_db |> filter(winner_name != \"Daniil Medvedev\") |>\n  show_query()\n#> <SQL>\n#> SELECT *\n#> FROM \"tennis2019\"\n#> WHERE (\"winner_name\" != 'Daniil Medvedev')\ntennis_db |>\n  filter(winner_name %in% c(\"Daniil Medvedev\", \"Dominic Thiem\")) |>\n  show_query()\n#> <SQL>\n#> SELECT *\n#> FROM \"tennis2019\"\n#> WHERE (\"winner_name\" IN ('Daniil Medvedev', 'Dominic Thiem'))"
  },
  {
    "path": "introduction-to-sql-with-dbplyr.html",
    "id": "solutions-16",
    "chapter": " 17 Introduction to SQL with dbplyr",
    "heading": "17.5 Exercise Solutions",
    "text": "",
    "code": ""
  },
  {
    "path": "introduction-to-sql-with-dbplyr.html",
    "id": "what-is-a-database-s",
    "chapter": " 17 Introduction to SQL with dbplyr",
    "heading": "17.5.1 What is a Database S",
    "text": "* Though know SQL code, can probably figure code . matches returned query?code keeping matches longer 240 minutes. also getting rid columns except specified SELECT.",
    "code": ""
  },
  {
    "path": "introduction-to-sql-with-dbplyr.html",
    "id": "dbplyr-a-database-version-of-dplyr-s",
    "chapter": " 17 Introduction to SQL with dbplyr",
    "heading": "17.5.2 dbplyr: A Database Version of dplyr S",
    "text": "* Obtain distribution surface variable making table total number matches played surface 2019 season using dplyr functions tennis_db. , use show_query() show corresponding SQL code.",
    "code": "\ntennis_db |> group_by(surface) |> summarise(nmatch = n())\n#> # Source:   SQL [3 x 2]\n#> # Database: DuckDB 0.3.5-dev1410 [root@Darwin 21.6.0:R 4.2.1/:memory:]\n#>   surface nmatch\n#>   <chr>    <dbl>\n#> 1 Hard      1626\n#> 2 Clay       828\n#> 3 Grass      327\n\ntennis_db |> group_by(surface) |> summarise(nmatch = n()) |>\n  show_query()\n#> <SQL>\n#> SELECT \"surface\", COUNT(*) AS \"nmatch\"\n#> FROM \"tennis2019\"\n#> GROUP BY \"surface\""
  },
  {
    "path": "introduction-to-sql-with-dbplyr.html",
    "id": "sql-s",
    "chapter": " 17 Introduction to SQL with dbplyr",
    "heading": "17.5.3 SQL S",
    "text": "* Examine SQL code write equivalent dplyr code.",
    "code": "SELECT * \nFROM \"tennis2019\"\nWHERE (\"tourney_name\" = 'Wimbledon')\ntennis_db |>\n  filter(tourney_name == \"Wimbledon\")\n#> # Source:   SQL [?? x 49]\n#> # Database: DuckDB 0.3.5-dev1410 [root@Darwin 21.6.0:R 4.2.1/:memory:]\n#>    tourney…¹ tourn…² surface draw_…³ tourn…⁴ tourn…⁵ match…⁶\n#>    <chr>     <chr>   <chr>     <int> <chr>     <int>   <int>\n#>  1 2019-540  Wimble… Grass       128 G        2.02e7     100\n#>  2 2019-540  Wimble… Grass       128 G        2.02e7     101\n#>  3 2019-540  Wimble… Grass       128 G        2.02e7     102\n#>  4 2019-540  Wimble… Grass       128 G        2.02e7     103\n#>  5 2019-540  Wimble… Grass       128 G        2.02e7     104\n#>  6 2019-540  Wimble… Grass       128 G        2.02e7     105\n#>  7 2019-540  Wimble… Grass       128 G        2.02e7     106\n#>  8 2019-540  Wimble… Grass       128 G        2.02e7     107\n#>  9 2019-540  Wimble… Grass       128 G        2.02e7     108\n#> 10 2019-540  Wimble… Grass       128 G        2.02e7     109\n#> # … with more rows, 42 more variables: winner_id <int>,\n#> #   winner_seed <chr>, winner_entry <chr>,\n#> #   winner_name <chr>, winner_hand <chr>, winner_ht <int>,\n#> #   winner_ioc <chr>, winner_age <dbl>, loser_id <int>,\n#> #   loser_seed <chr>, loser_entry <chr>, loser_name <chr>,\n#> #   loser_hand <chr>, loser_ht <int>, loser_ioc <chr>,\n#> #   loser_age <dbl>, score <chr>, best_of <int>, …\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n## check query:\ntennis_db |>\n  filter(tourney_name == \"Wimbledon\") |>\n  show_query()\n#> <SQL>\n#> SELECT *\n#> FROM \"tennis2019\"\n#> WHERE (\"tourney_name\" = 'Wimbledon')"
  },
  {
    "path": "introduction-to-sql-with-dbplyr.html",
    "id": "chapexercise-16-S",
    "chapter": " 17 Introduction to SQL with dbplyr",
    "heading": "17.5.4 Chapter Exercises S",
    "text": "* Try run function lubridate forcats tennis_db mutate(). function work? expect work?result error. functions compatible dbplyr package can used database table. Functions specific R, like lubridate forcats work collect() database table data frame tibble:",
    "code": "\ntennis_db |> mutate(tourney_name_reorder = fct_reorder(tourney_name, \n                                                       draw_size))\ntennis_db |> collect() |>\n  mutate(tourney_name_reorder = fct_reorder(tourney_name, \n                                                       draw_size))\n#> # A tibble: 2,781 × 50\n#>    tourney…¹ tourn…² surface draw_…³ tourn…⁴ tourn…⁵ match…⁶\n#>    <chr>     <chr>   <chr>     <int> <chr>     <int>   <int>\n#>  1 2019-M020 Brisba… Hard         32 A        2.02e7     300\n#>  2 2019-M020 Brisba… Hard         32 A        2.02e7     299\n#>  3 2019-M020 Brisba… Hard         32 A        2.02e7     298\n#>  4 2019-M020 Brisba… Hard         32 A        2.02e7     297\n#>  5 2019-M020 Brisba… Hard         32 A        2.02e7     296\n#>  6 2019-M020 Brisba… Hard         32 A        2.02e7     295\n#>  7 2019-M020 Brisba… Hard         32 A        2.02e7     294\n#>  8 2019-M020 Brisba… Hard         32 A        2.02e7     293\n#>  9 2019-M020 Brisba… Hard         32 A        2.02e7     292\n#> 10 2019-M020 Brisba… Hard         32 A        2.02e7     291\n#> # … with 2,771 more rows, 43 more variables:\n#> #   winner_id <int>, winner_seed <chr>, winner_entry <chr>,\n#> #   winner_name <chr>, winner_hand <chr>, winner_ht <int>,\n#> #   winner_ioc <chr>, winner_age <dbl>, loser_id <int>,\n#> #   loser_seed <chr>, loser_entry <chr>, loser_name <chr>,\n#> #   loser_hand <chr>, loser_ht <int>, loser_ioc <chr>,\n#> #   loser_age <dbl>, score <chr>, best_of <int>, …\n#> # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names"
  },
  {
    "path": "introduction-to-sql-with-dbplyr.html",
    "id": "rcode-16",
    "chapter": " 17 Introduction to SQL with dbplyr",
    "heading": "17.6 Non-Exercise R Code",
    "text": "",
    "code": "\nlibrary(DBI)\nlibrary(duckdb)\ncon <- DBI::dbConnect(duckdb::duckdb())\ncon\nlibrary(here)\nduckdb_read_csv(conn = con, name = \"tennis2018\", \n                files = here(\"data/atp_matches_2018.csv\"))\nduckdb_read_csv(conn = con, name = \"tennis2019\", \n                files = here(\"data/atp_matches_2019.csv\"))\ndbListTables(con)\ndbExistsTable(con, \"tennis2019\")\ndbExistsTable(con, \"tennis2020\")\nlibrary(tidyverse)\n\nsql <- \"\n  SELECT surface, winner_name, loser_name, w_ace, l_ace, minutes\n  FROM tennis2019 \n  WHERE minutes > 240\n\"\ndbGetQuery(con, sql)|>\n  as_tibble()\nlibrary(dbplyr)\ntennis_db <- tbl(con, \"tennis2019\")\ntennis_db\ntennis_query1 <- tennis_db |> \n  filter(minutes > 240) |> \n  select(minutes, winner_name, loser_name, minutes, tourney_name)\ntennis_query1\ntennis_query1 |>\n  collect()\ntennis_query1 |>\n  show_query()\nmedvedev_query <- tennis_db |>\n  pivot_longer(c(winner_name, loser_name), names_to = \"win_loss\",\n               values_to = \"player\") |>\n  filter(player == \"Daniil Medvedev\") |>\n  group_by(win_loss) |>\n  summarise(win_loss_count = n())\nmedvedev_query\nshow_query(medvedev_query)\nover20aces <- tennis_db |> filter(w_ace > 20) |>\n  select(w_ace, winner_name) |>\n  group_by(winner_name) |>\n  summarise(nmatch = n()) |>\n  arrange(desc(nmatch))\nover20aces\n\nover20aces |> show_query()\ntennis_db |> show_query()\ntennis_db |> filter(winner_hand == \"L\") |>\n  arrange(desc(tourney_date)) |>\n  show_query()\ntennis_db |>\n  group_by(winner_name) |>\n  summarise(meanace = mean(w_ace, na.rm = TRUE)) |>\n  show_query()\ntennis_db |> select(1:4) |> show_query()\ntennis_db |> rename(tournament = tourney_name) |>\n  show_query()\ntennis_db |> mutate(prop_first_won = w_1stIn / w_1stWon) |>\n  select(prop_first_won, winner_name) |>\n  show_query()\ntennis_db |> summarise(mean_length = mean(minutes)) |>\n  show_query()\ntennis_db |> group_by(winner_name) |>\n  summarise(meanlength = mean(minutes)) |>\n  show_query()\ntennis_db |> filter(winner_age > 35 | loser_age > 35) |>\n  show_query()\ntennis_db |> arrange(desc(winner_rank_points)) |>\n  show_query()"
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "2  Getting Started with R and R Studio",
    "section": "",
    "text": "Goals:"
  },
  {
    "objectID": "02-ggplot2.html",
    "href": "02-ggplot2.html",
    "title": "3  Plotting with ggplot2",
    "section": "",
    "text": "Goals:"
  },
  {
    "objectID": "02-ggplot2.html#introduction-and-basic-terminology",
    "href": "02-ggplot2.html#introduction-and-basic-terminology",
    "title": "3  Plotting with ggplot2",
    "section": "3.1 Introduction and Basic Terminology",
    "text": "3.1 Introduction and Basic Terminology\nWe will begin our data science journey with plotting in the ggplot2 package. We are starting with plotting for a couple of reasons:\n\nPlotting is cool! We get to see an immediate result of our coding efforts in the form of a nice-to-look-at plot.\nIn an exploratory data analysis, you would typically start by making plots of your data.\nPlotting can lead us to ask and subsequently investigate interesting questions, as we will see in our first example.\n\nWe will first use a data set on the 2000 United States Presidential election between former President George Bush and Al Gore obtained from http://www.econometrics.com/intro/votes.htm. For those unfamiliar with U.S. political elections, it is enough to know that each state is allocated a certain number of “electoral votes” for the president: states award all of their electoral votes to the candidate that receives the most ballots in that state. You can read more about this strange system on Wikipedia.\nFlorida is typically a highly-contentious “battleground” state. The data set that we have has the following variables, recorded for each of the 67 counties in Florida:\n\nGore, the number of people who voted for Al Gore in 2000\nBush, the number of people who voted for George Bush in 2000\nBuchanan, the number of people who voted for the third-party candidate Buchanan\nNader, the number of people who voted for the third-party candidate Nader\nOther, the number of people who voted for a candidate other than the previous 4 listed\nCounty, the name of the county in Florida\n\nTo get started exploring the data, complete the following steps that you learned in Week 0:\n\nOpen your R Project by double clicking the .RProj icon in the folder on your desktop, or, by opening R Studio and clicking File -> Open Project.\nCreate a new .qmd file in the same folder as your Notes R Project using File -> New File -> Quarto.\nFinally, read in and name the data set pres_df, and take a look at the data set by running the head(pres_df) line, which shows the first few observations of the data set:\n\n\nlibrary(tidyverse)\npres_df <- read_table(\"data/PRES2000.txt\") \n## don't worry about the `read_table` function....yet\nhead(pres_df)\n\n# A tibble: 6 × 6\n    Gore   Bush Buchanan Nader Other County  \n   <dbl>  <dbl>    <dbl> <dbl> <dbl> <chr>   \n1  47365  34124      263  3226   751 ALACHUA \n2   2392   5610       73    53    26 BAKER   \n3  18850  38637      248   828   242 BAY     \n4   3075   5414       65    84    35 BRADFORD\n5  97318 115185      570  4470   852 BREVARD \n6 386561 177323      788  7101  1623 BROWAR  \n\n\nPay special attention to the variable names: we’ll need to use these names when we make all of our plots. And, R is case-sensitive, meaning that we will, for example, need to use Gore, not gore.\nWe are trying to go very light on the technical code terminology to start out with (but we will come back to some things later in the semester). The terminology will make a lot more sense once you’ve actually worked with data. But, there are three terms that will be thrown around quite a bit in the next few weeks: function, argument, and object.\n\na function in R is always* (*always for this class) followed by an open ( and ended with a closed ). In non-technical terms, a function does something to its inputs and is often analogous to an English verb. For example, the mean() function calculates the mean, the rank() functions ranks a variable from lowest to highest, and the labs() is used to add labels to a plot. Every function has a help file that can be accessed by typing in ?name_of_function. Try typing ?mean in your lower left window.\nan argument is something that goes inside the parentheses in a function. Arguments could include objects, or they might not. In the bottom-left window, type ?mean to view the Help file on this R function. We see that mean() has 3 arguments: x, which is an R object, trim, and na.rm. trim = 0 is the default, which means that, by default, R will not trim any of the numbers when computing the mean.\nan object is something created in R, usually with <-. So, looking at the code above where we read in the data, pres_df is an R object.\n\nAll of this will make more sense as we go through these first couple of weeks."
  },
  {
    "objectID": "02-ggplot2.html#basic-plot-structure",
    "href": "02-ggplot2.html#basic-plot-structure",
    "title": "3  Plotting with ggplot2",
    "section": "3.2 Basic Plot Structure",
    "text": "3.2 Basic Plot Structure\nWe will use the ggplot() function in the ggplot2 package to construct visualizations of data. the ggplot() function has 3 basic components:\n\na data argument, specifying the name of your data set (pres_df above)\na mapping argument, specifying that specifies the aesthetics of your plot (aes()). Common aesthetics are x position, y position, colour, size, shape, group, and fill.\na geom_    () component, specifying the geometric shape used to display the data.\n\nThe components are combined in the following form:\n\nggplot(data = name_of_data, aes(x = name_of_x_var, \n                                          y = name_of_y_var,\n                                          colour = name_of_colour_var,\n                                          etc.)) +\n  geom_nameofgeom() +\n  .....<other stuff>\n\nThe structure of ggplot() plots is based on the Grammar of Graphics https://www.springer.com/gp/book/9780387245447. As with most new things, the components above will be easier to think about with some examples."
  },
  {
    "objectID": "02-ggplot2.html#graphing-a-single-variable",
    "href": "02-ggplot2.html#graphing-a-single-variable",
    "title": "3  Plotting with ggplot2",
    "section": "3.3 Graphing a Single Variable",
    "text": "3.3 Graphing a Single Variable\n\n3.3.1 Histograms and Frequency Plots for a Quantitative Variable\nLet’s go ahead and begin our exploration of the data by making a histogram of the number of people who voted for Gore in each county. Recall that a histogram is useful if we would like a graph of a single quantitative variable. Copy the following code to an R chunk and run the code:\n\nggplot(data = pres_df, aes(x = Gore)) +\n  geom_histogram(colour = \"black\", fill = \"white\") +\n  xlab(\"Votes for Gore in Florida\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nWhat do the 1e+05, 2e+05, etc. labels on the x-axis mean?\nR gives us a message to “Pick a better value with binwidth” instead of the default bins = 30. Add , bins = 15 inside the parentheses of geom_histogram() to change the number of bins in the histogram.\nChange the colour of the inside of the bins to “darkred”. Do you think that the colour of the inside of the bins maps to colour or fill? Try both!\nThere are a couple of observations with very high vote values. What could explain these large outliers?\n\nAnother graph useful in visualizing a single quantitative variable is a frequency plot. The code to make a frequency plot is given below. We are simply replacing geom_histogram() with geom_freqpoly().\n\nggplot(data = pres_df, aes(x = Gore)) +\n  geom_freqpoly(colour = \"black\") +\n  xlab(\"Votes for Gore in Florida\") \n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe frequency plot is just like a histogram but the counts are connected by a line instead of represented with bins. You can see how they relate by including both a geom_freqpoly() and a geom_histogram() in your plot, though it doesn’t make for the prettiest graph:\n\nggplot(data = pres_df, aes(x = Gore)) +\n  geom_freqpoly(colour = \"black\") +\n  xlab(\"Votes for Gore in Florida\") +\n  geom_histogram() \n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n3.3.2 R Code Style\nWe want our code to be as readable as possible. This not only benefits other people who may read your code (like me), but it also benefits you, particularly if you read your own code in the future. I try to follow the Style Guide in the Advanced R book: http://adv-r.had.co.nz/Style.html. Feel free to skim through that, but you don’t need to worry about it too much: you should be able to pick up on some important elements just from going through this course. You might actually end up having better code style if you haven’t had any previous coding experience.\nAs a quick example of why code style can be important, consider the following two code chunks, both of which produce the same graph.\n\nggplot(data=pres_df,mapping=aes(x=Gore))+geom_histogram(colour=\"black\",fill=\"white\")+\n  xlab(\"Votes for Gore in Florida\")\n\n\nggplot(data = pres_df, aes(x = Gore)) +\n  geom_histogram(colour = \"black\", fill = \"white\") +\n  xlab(\"Votes for Gore in Florida\")\n\nWhich code chunk would you want to read two years from now? Which code chunk would you want your classmate/friend/coworker to read? (assuming you like your classmate/friend/coworker….)\n\n\n3.3.3 Bar Plots for a Categorical Variable\nRecall from STAT 113 that bar plots are useful if you want to examine the distribution of one categorical variable. Side-by-side bar plots or stacked bar plots are plots that are useful for looking at the relationship between two categorical variables. There actually aren’t any categorical variables that would be interesting to plot in this data set, so we’ll make one, called winner using code that we don’t need to understand until next week. winner will be \"Gore\" if Gore won the county and \"Bush\" if Bush won the county. We’ll name this new data set pres_cat.\n\npres_cat <- pres_df |> mutate(winner = if_else(Gore > Bush,\n                                                true = \"Gore\",\n                                                false = \"Bush\"))\npres_cat\n\n# A tibble: 67 × 7\n     Gore   Bush Buchanan Nader Other County    winner\n    <dbl>  <dbl>    <dbl> <dbl> <dbl> <chr>     <chr> \n 1  47365  34124      263  3226   751 ALACHUA   Gore  \n 2   2392   5610       73    53    26 BAKER     Bush  \n 3  18850  38637      248   828   242 BAY       Bush  \n 4   3075   5414       65    84    35 BRADFORD  Bush  \n 5  97318 115185      570  4470   852 BREVARD   Bush  \n 6 386561 177323      788  7101  1623 BROWAR    Gore  \n 7   2155   2873       90    39    17 CALHOUN   Bush  \n 8  29645  35426      182  1462   181 CHARLOTTE Bush  \n 9  25525  29765      270  1379   261 CITRUS    Bush  \n10  14632  41736      186   562   237 CLAY      Bush  \n# … with 57 more rows\n\n\nUsing this data set, we can make a bar plot with geom_bar(). The beauty of ggplot() is that the code is super-similar to what we used for histograms and frequency plots!\n\nggplot(data = pres_cat, aes(x = winner)) +\n  geom_bar()\n\n\n\n\nNote that, sometimes, data are in format such that one column contains the levels of the categorical variable while another column contains the counts directly. For example, we can create such a data set using code that we will learn next week:\n\npres_cat2 <- pres_cat |> group_by(winner) |>\n  summarise(nwins = n())\npres_cat2\n\n# A tibble: 2 × 2\n  winner nwins\n  <chr>  <int>\n1 Bush      51\n2 Gore      16\n\n\nThis data set has just two observations and contains a column for the two major presidential candidates and a column for the number of counties that each candidate won. If we wanted to make a barplot showing the number of wins for each candidate, we can’t use geom_bar(). Predict what the result will be from running the following code.\n\nggplot(pres_cat2, aes(x = winner)) +\n  geom_bar()\n\nInstead, we can use geom_col(), which takes an x aesthetic giving the column with names of the levels of our categorical variable, and a y aesthetic giving the column with the counts:\n\nggplot(pres_cat2, aes(x = winner, y = nwins)) +\n  geom_col()\n\n\n\n\n\n\n3.3.4 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-2).\n\nChange the frequency plot to plot the number of votes for Bush instead of the number for Gore. Are there any obvious outliers in the Bush frequency plot?\nDo you have a preference for histograms or a preference for frequency plots? Can you think of a situation where one would be more desirable than the other?\nIt looks like Bush won a lot more….does that necessarily mean that Bush won more votes in total in Florida? Why or why not?\n\nWe will be using survey data from STAT 113 in the 2018-2019 academic year for many exercises in this section. For those who may not have taken STAT 113 from having AP credit or another reason, the STAT 113 survey is given to all students in STAT 113 across all sections. Some analyses in Intro Stat are then carried out using the survey.\n\nlibrary(tidyverse)\nstat113_df <- read_csv(\"data/stat113.csv\")\nhead(stat113_df)\n\n# A tibble: 6 × 12\n  Year   Sex     Hgt   Wgt Haircut   GPA Exerc…¹ Sport    TV Award Pulse Socia…²\n  <chr>  <chr> <dbl> <dbl>   <dbl> <dbl>   <dbl> <chr> <dbl> <chr> <dbl> <chr>  \n1 Sopho… M        66   155       0  2.9       15 Yes       8 Olym…    72 Snapch…\n2 First… F        69   170      17  3.87      14 Yes      12 Olym…    51 Instag…\n3 First… F        64   130      40  3.3        5 No        5 Olym…    68 Instag…\n4 First… M        68   157      35  3.21      10 Yes      15 Olym…    54 Snapch…\n5 First… M        72   175      20  3.1        2 No        5 Nobel    NA Instag…\n6 Junior F        62   150      50  3.3        8 Yes       5 Olym…    86 Instag…\n# … with abbreviated variable names ¹​Exercise, ²​SocialMedia\n\n\nThe data set contains the following variables:\n\nYear, FirstYear, Sophomore, Junior, or Senior\nSex, M or F (for this data set, Sex is considered binary).\nHgt, height, in inches.\nWgt, weight, in pounds.\nHaircut, how much is paid for a haircut, typically.\nGPA\nExercise, amount of hours of exercise in a typical week.\nSport, whether or not the student plays a varsity sport.\nTV, amount of hours spent watching TV in a typical week.\nAward, Award preferred: choices are Olympic Medal, Nobel Prize, or Academy Award.\nPulse, pulse rate, in beats per minute.\nSocialMedia, most used social media platform (Instagram, SnapChat, FaceBook, Twitter, Other, or None).\n\n\n* Create a histogram of the Exercise variable, change the x-axis label to be “Exercise (hours per typical week)”, change the number of bins to 14, and change the fill of the bins to be “lightpink2” and the outline colour of the bins to be black.\n* We can change the y-axis of a histogram to be “density” instead of a raw count. This means that each bar shows a proportion of cases instead of a raw count. Google something like “geom_histogram with density” to figure out how to create a y aes() to show density instead of count.\nConstruct a histogram using a quantitative variable of your choice. Change the fill and colour using http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf to help you choose colours.\n\n\n\n\n\nConstruct a bar plot for a variable of your choosing. What do you find?\n\n\n\n\n\nWhat format would the STAT 113 data set need to be in to construct your bar plot with geom_col() instead of geom_bar()?"
  },
  {
    "objectID": "02-ggplot2.html#graphing-two-quantitative-variables-faceting-and-aes-options",
    "href": "02-ggplot2.html#graphing-two-quantitative-variables-faceting-and-aes-options",
    "title": "3  Plotting with ggplot2",
    "section": "3.4 Graphing Two Quantitative Variables, Faceting, and aes() Options",
    "text": "3.4 Graphing Two Quantitative Variables, Faceting, and aes() Options\n\n3.4.1 Scatterplots\nMoving back to the 2000 presidential election data set, thus far, we’ve figured out that there a couple of counties with very large numbers of votes for Gore and very large number of votes for Bush. We don’t know the reason for this (if some counties are very democratic, very republican, or if some counties are just more populous). Do the counties that have a large number of votes for Bush also tend to have a large number of votes for Gore? And what about the other candidates: do they have any interesting patterns?\nLet’s start by making a scatterplot of the number of votes for Gore and the number of votes for Bush. Note that the geom_ for making a scatterplot is called geom_point() because we are adding a layer of points to the plot.\n\nggplot(data = pres_df, aes(x = Gore, y = Bush)) +\n  geom_point()\n\n\n\n\nWhat patterns do you see in the scatterplot?\nNow, change the x variable from Gore to Buchanan. You should notice something strange in this scatterplot. Try to come up with one explanation for why the outlying point has so many votes for Buchanan.\nIn trying to come up with an explanation, it would be nice to figure out which Florida county has that outlying point and it would be nice if we knew something about Florida counties. To remedy the first issue, recall that we can type View(pres_df) to pull up the data set. Once you have the new window open, click on the column heading Buchanan to sort the votes for Buchanan from high to low to figure out which county is the outlier.\nUse some Google sleuthing skills to find an explanation: try to search for “2000 united states presidential election [name of outlier county]”. Write a sentence about what you find. Hint: if nothing useful pops up, try adding the term “butterfly ballot” to your search.\n\nWe have used the 2000 Presidential data set to find out something really interesting! In particular, we have used exploratory data analysis to examine a data set, without having a specific question of interest that we want to answer. This type of exploring is often really useful, but does have some drawbacks, which we will discuss later in the semester.\n\n\n3.4.2 Aesthetics in aes()\nFor the remainder of this chapter, we will work with some fitness data collected from my Apple Watch since November 2018. The higham_fitness_clean.csv contains information on the following variables:\n\nStart, the month, day, and year that the fitness data was recorded on\nmonth, the month\nweekday, the day of the week\ndayofyear, the day of the year (so that 304 corresponds to the 304th day of the year)\ndistance, distance walked in miles\nsteps, the number of steps taken\nflights, the number of flights of stairs climbed\nactive_cals, the number of calories burned from activity\nstepgoal, whether or not I reached 10,000 steps for the day\nweekend_ind, a variable for whether or not the day of the week was a weekend day (Saturday or Sunday) or a weekday (Monday - Friday).\n\n\nlibrary(tidyverse)\nfitness_full <- read_csv(\"data/higham_fitness_clean.csv\") |> mutate(weekend_ind = case_when(weekday == \"Sat\" | weekday == \"Sun\" ~ \"weekend\",\n  TRUE ~ \"weekday\"))\n\nRows: 993 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): month, weekday\ndbl  (6): active_cals, distance, flights, steps, dayofyear, stepgoal\ndate (1): Start\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nFirst, let’s make a basic scatterplot to illustrate why it’s so important to plot your data. I’ll use the variable distance as the x-variable and active_cals as the y-variable.\n\nggplot(data = fitness_full, aes(x = distance, y = active_cals)) +\n  geom_point()\n\n\n\n\nOne aspect of the plot that you may notice is that there are observations where I burned 0 or very few active calories, yet walked/jogged/ran/moved some distance. Is it possible to not burn any calories and move ~ 4 miles? Probably not, so let’s drop these observations from the data set and make a note of why we dropped those observations. Unfortunately, we don’t have the tools to do this yet, so just run the following chunk of code without worrying too much about the syntax.\n\n## drop observations that have active calories < 50. \n## assuming that these are data errors or \n## days where the Apple Watch wasn't worn.\nfitness <- fitness_full |>\n  filter(active_cals > 50)\n\nLet’s make the plot again with the fitness data set instead of fitness_full to see if the outliers are actually gone. This time, we will put the aes() in the geom_point() function:\n\nggplot(data = fitness) +\n  geom_point(aes(x = distance, y = active_cals))\n\n\n\n\nPutting the aes() in ggplot() and putting the aes() in geom_point() results in the same graph in this case. When you put the aes() in ggplot(), R perpetuates these aes() aesthetics in all geom_s in your plotting command. However, if you put your aes() in geom_point(), then any future geoms that you use will need you to re-specify different aes(). We’ll see an example of this in the exercises.\nOther aes() Options\nIn addition to x and y, we can also use aes() to map variables to things like colour, size, and shape. For example, we might make a scatterplot with Start on the x-axis (for the date) and active_cals on the y-axis, colouring by whether or not the day of the week was a weekend.\n\nggplot(data = fitness) +\n  geom_point(aes(x = Start, y = active_cals, colour = weekend_ind))\n\n\n\n\nIs there anything useful that you notice about the plot? Is there anything about the plot that could be improved?\nInstead of using colour, you can also specify the point shape. This could be useful, for example, if you are printing something in black and white.\n\nggplot(data = fitness) +\n  geom_point(aes(x = Start, y = active_cals, shape = weekend_ind))\n\n\n\n\nDo you prefer the colour or the shape? Why?\nFinally, another common aes() is size. For example, we could make the size of the points in the scatterplot change depending on how many flights of stairs I climbed.\n\nggplot(data = fitness) +\n  geom_point(aes(x = Start, y = active_cals, size = flights))\n\n\n\n\nI don’t think any of the previous three plots are necessarily the “best” and need some work, but, part of the fun of exploratory data analysis is making trying out different plots to see what “works.”\nInside vs Outside aes()\nWe’ve changed the colour of the points to correspond to weekend_ind, but what if we just wanted to change the colour of points to all be the same colour, \"purple\". Try running the following code chunk:\n\nggplot(data = fitness) +\n  geom_point(aes(x = Start, y = active_cals, colour = \"purple\"))\n\n\n\n\nWhat does the graph look like? Did it do what you expected?\nPutting colour = ____ inside aes() or outside aes() achieves different things. In general,\n\nwhen we want to map something in our data set (fitness) to something in our plot (x, y, colour, size, etc.), we put that inside the aes() as in geom_point(aes(colour = weekend_ind)).\nWhen we assign fixed characteristics that don’t come from the data, we put them outside the aes(), as in geom_point(colour = \"purple\").\n\nYou can also change the overall point size and shape. The standard size is 1 so the following code chunk makes the points bigger. The standard shape is 19: you can try changing that to other integers to see what other shapes you can get.\n\nggplot(data = fitness) +\n  geom_point(aes(x = Start, y = active_cals), size = 1.5, shape = 19)\n\n\n\n\n\n\n3.4.3 Using More Than One geom()\nWe might also be interested in fitting a smooth curve to our scatterplot. When we want to put more than one “geom” on our plot, we can use multiple geoms. Since I want the aes() to apply to both geom_point() and geom_smooth(), I am going to move the aes() command to the overall ggplot() line of code:\n\nggplot(data = fitness, aes(x = Start, y = active_cals)) +\n  geom_point() +\n  geom_smooth(span = 0.3)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nWithin geom_smooth(), you can set se = FALSE to get rid of the grey standard errors around each of the lines, and you can setmethod = \"lm\" to fit straight linear regression lines instead of smooth curves:\n\nggplot(data = fitness, aes(x = Start, y = active_cals)) +\n  geom_point() +\n  geom_smooth(se = FALSE, method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nDoes it look like there is an increasing overall trend? decreasing? Does it make sense to use a line to model the relationship or did you prefer the smooth curve?\n\n\n3.4.4 Line Plots with geom_line()\nLine plots are often useful when you have a quantitative variable that you’d like to explore over time. The y-axis is the quantitative variable while the x-axis is typically time. More generally, line plots are often used when the x-axis variable has one discrete value for each y-axis variable. For example, suppose we want to explore how my step count has changed through time over the past couple of years. Compare the standard scatterplot with the following line plot: which do you prefer?\n\nggplot(data = fitness, aes(x = Start, y = steps)) +\n  geom_point() + geom_smooth() + xlab(\"Date\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\nggplot(data = fitness, aes(x = Start, y = steps)) +\n  geom_line() + geom_smooth() + xlab(\"Date\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nCan you spot the start of the pandemic in the graph? What seemed to happen with the step count?\n\n\n3.4.5 Faceting\nUsing colour to colour points of different levels of a categorical variable is generally fine when there are just a couple of levels and/or there is little overlap among the levels. But, what if there are a lot more than two categories to colour by. For example, let’s move back to the STAT 113 survey data set and investigate the relationship between Pulse and Exercise for different class Year’s. We might hypothesize that students who get more exercise tend to have lower pulse rates.\n\nggplot(data = stat113_df, aes(x = Exercise, y = Pulse,\n                           colour = Year)) +\n  geom_point() +\n  geom_smooth(se = TRUE)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 40 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 40 rows containing missing values (`geom_point()`).\n\n\n\n\n\nWhen there are many different categories for a categorical variable (there are only 4 categories for Year, but this particular plot is still a bit difficult to read), it can sometimes be useful to facet the plot by that variable instead of trying to use different colours or shapes.\n\nggplot(data = stat113_df, aes(x = Exercise, y = Pulse)) +\n  geom_point() +\n  geom_smooth(se = TRUE) +\n  facet_wrap(~ Year)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 40 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 40 rows containing missing values (`geom_point()`).\n\n\n\n\n\nWe have eliminated the colour = argument and added facet_wrap( ~ name_of_facet_variable). Doing so creates a different scatterplot and smooth line for each level of name_of_facet_variable.\nWhat can you see from this plot that was harder to see from the plot with colour?\nDoes the data seem to support the hypothesis that more exercise is associated with lower pulse rates in this sample of students?\n\n\n3.4.6 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-2).\n\nFix the code chunk where we tried to specify the colour of all points to be purple to actually make all of the points “purple” by moving colour = \"purple\" outside the parentheses in aes() (but still inside geom_point()).\nIn the console (bottom-left) window, type ?geom_smooth and scroll down to “Arguments.” Find span, read about it, and then, within the geom_smooth() argument of the line plot with steps vs. date, add a span argument to make the smooth line wigglier.\nExplain why it doesn’t make sense to construct a line plot of Exercise vs. GPA.\n* Make a scatterplot of Hgt on the y-axis and Wgt on the x-axis, colouring by Sport. Add a smooth fitted curve to your scatterplot. Then, move colour = Sport from an aes() in the ggplot() function to an aes() in the geom_point() function. What changes in the plot? Can you give an explanation as to why that change occurs?\n* Faceting can be used for other types of plots too! Make a pair of faceted histograms for a quantitative variable of your choosing that are faceted by a categorical variable of your choosing."
  },
  {
    "objectID": "02-ggplot2.html#boxplots-stacked-barplots-and-others",
    "href": "02-ggplot2.html#boxplots-stacked-barplots-and-others",
    "title": "3  Plotting with ggplot2",
    "section": "3.5 Boxplots, Stacked Barplots and Others",
    "text": "3.5 Boxplots, Stacked Barplots and Others\nThere are a few other common geoms that will be useful throughout the semester. These only skim the surface: we’ll come back to plotting in a few weeks, after we’re able to do more with data wrangling and reshaping.\n\n3.5.1 Graphing a Quant. Variable vs. a Cat. Variable\nAnother common plot used in Intro Stat courses is a boxplot. Side-by-side boxplots are particularly useful if you want to compare a quantitative response variable across two or more levels of a categorical variable. Let’s stick with the STAT 113 survey data to examine the relationship between Exercise and Award preference.\n\nggplot(data = stat113_df, aes(x = Award, y = Exercise)) +\n  geom_boxplot()\n\nWarning: Removed 7 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nWhat can you conclude from the plot?\n\nAn alternative to side-by-side boxplots are violin plots:\n\nggplot(data = stat113_df, aes(x = Award, y = Exercise)) +\n  geom_violin()\n\nWarning: Removed 7 rows containing non-finite values (`stat_ydensity()`).\n\n\n\n\n\nRead about Violin plots by typing ?geom_violin into your console (bottom-left window). How are they different than boxplots?\n\n\n3.5.2 Graphing Two Categorical Variables\nThe only combination of two variables that we have yet to explore are two variables that are both categorical. Let’s look at the relationship between Year and SocialMedia first using a stacked bar plot.\nTo make the graph, we specify position = \"fill\" so that the bars are “filled” by stepgoal.\n\nggplot(data = stat113_df, aes(x = Year, fill = SocialMedia)) +\n  geom_bar(position = \"fill\") +\n  ylab(\"Proportion\")\n\n\n\n\nWhat patterns do you notice from the plot? Is there anything about the plot that could be improved?\n\n\n3.5.3 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-2).\n\n* Change the colour of the inside of the boxplots in the Exercise vs. Award graph to be \"blue\". Do you think you’ll use colour = \"blue\" or fill = \"blue\"?\n* Create a side-by-side boxplot that compares the GPAs of students who prefer different Awards. Then change the fill of the boxplot to be a colour of your choice. What do you notice in the plot?\n* When making the previous plot, R gives us a warning message that it “Removed 70 rows containing non-finite values”. This is R’s robotic way of telling us that 70 GPA values are missing in the data set. Use what you know about how the data was collected (Fall and Spring semester of the 2018-2019 school-year) to guess why these are missing.\n* Make a stacked bar plot for two variables of your choosing in the STAT 113 data set. Comment on something that you notice in the plot."
  },
  {
    "objectID": "02-ggplot2.html#chapexercise-2",
    "href": "02-ggplot2.html#chapexercise-2",
    "title": "3  Plotting with ggplot2",
    "section": "3.6 Chapter Exercises",
    "text": "3.6 Chapter Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-2).\n\n* The default of geom_smooth() is to use LOESS (locally estimated scatterplot smoothing). Read about LOESS here: here. Write one or two sentences explaining what LOESS does.\n* Thus far, we have only faceted by a single variable. Use Google to figure out how to facet by two variables to make a plot that shows the relationship between GPA (y-axis) and Exercise (x-axis) with four facets: one for male students who play a sport, one for female students who play a sport, one for male students who do not play a sport, and one for female students who do not play a sport.\n* In Intro-Stat, boxplots are typically introduced using the * symbol to identify outliers. Using a combination of the help ?geom_boxplot and Googling “R point shapes”, figure out how to modify your side-by-side boxplots so that the outliers are shown using *, not the default dots. Then, using Google, figure out how to add the mean to each boxplot as a “darkgreen” diamond-shaped symbol with stat_summary().\nA common theme that we’ll see throughout the course is that it’s advantageous to know as much background information as possible about the data set we are analyzing. Data sets will be easier to analyze and pose questions about if you’re familiar with the subject matter.\n\nGive an example of something that you know about STAT 113 and the survey data set that helped you answer or pose a question that someone from another university (and therefore unfamiliar with our intro stat course) wouldn’t know.\nGive an example of something that you don’t know about the fitness data set that the person who owns the fitness data would know. Why does that give an advantage to the person who is more familiar with the fitness data?"
  },
  {
    "objectID": "02-ggplot2.html#solutions-2",
    "href": "02-ggplot2.html#solutions-2",
    "title": "3  Plotting with ggplot2",
    "section": "3.7 Exercise Solutions",
    "text": "3.7 Exercise Solutions\n\n3.7.1 Introduction etc. S\n\n\n3.7.2 Basic Plot Structure S\n\n\n3.7.3 Graphing a Single Variable S\n\n* Create a histogram of the Exercise variable, change the x-axis label to be “Exercise (hours per typical week)”, change the number of bins to 14, and change the fill of the bins to be “lightpink2” and the outline colour of the bins to be black.\n\n\nggplot(data = stat113_df, aes(x = Exercise)) +\n  geom_histogram(bins = 14, fill = \"lightpink2\", colour = \"black\") +\n  xlab(\"Exercise (hours per typical week)\")\n\nWarning: Removed 7 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n\n* We can change the y-axis of a histogram to be “density” instead of a raw count. This means that each bar shows a proportion of cases instead of a raw count. Google something like “geom_histogram with density” to figure out how to create a y aes() to show density instead of count.\n\n\nggplot(data = stat113_df, aes(x = Exercise, y = ..density..)) +\n  geom_histogram(bins = 14, fill = \"lightpink2\", colour = \"black\") +\n  xlab(\"Exercise (hours per typical week)\")\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\nWarning: Removed 7 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n\n\n3.7.4 Graphing Two Quant. etc. S\n\n* Make a scatterplot of Hgt on the y-axis and Wgt on the x-axis, colouring by Sport. Add a smooth fitted curve to your scatterplot. Then, move colour = Sport from an aes() in the ggplot() function to an aes() in the geom_point() function. What changes in the plot? Can you give an explanation as to why that change occurs?\n\n\nggplot(data = stat113_df, aes(x = Wgt, y = Hgt, colour = Sport)) +\n  geom_point() +\n  geom_smooth()\n\n\n\nggplot(data = stat113_df, aes(x = Wgt, y = Hgt)) +\n  geom_point(aes(colour = Sport)) +\n  geom_smooth()\n\n\n\n\nThe points are now coloured by Sport but there is only one smooth fitted line. This makes sense because geom_point() now has the two global aesthetics x and y, as well as the colour aesthetic. geom_smooth() no longer has the colour aesthetic but still inherits the two global aesthetics, x and y.\n\n* Faceting can be used for other types of plots too! Make a pair of faceted histograms for a quantitative variable of your choosing that are faceted by a categorical variable of your choosing.\n\nAnswers will vary:\n\nggplot(data = stat113_df, aes(x = GPA)) + \n  geom_histogram(bins = 15) +\n  facet_wrap( ~ Sport)\n\n\n\n\n\n\n3.7.5 Boxplots, Stacked, etc. S\n\n* Change the colour of the inside of the boxplots in the Exercise vs. Award graph to be \"blue\". Do you think you’ll use colour = \"blue\" or fill = \"blue\"?\n\n\nggplot(data = stat113_df, aes(x = Award, y = Exercise)) +\n  geom_boxplot(fill = \"blue\")\n\nWarning: Removed 7 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nfill because it’s the inside of the boxplots that we want to modify. colour will modify the outline colour.\n\n* Create a side-by-side boxplot that compares the GPAs of students who prefer different Awards. Then change the fill of the boxplot to be a colour of your choice. What do you notice in the plot?\n\n\nggplot(data = stat113_df, aes(x = Award, y = GPA)) +\n  geom_boxplot(fill = \"lightpink1\")\n\nWarning: Removed 70 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nThere are a few outlier students, but the three groups overall seem to have similar GPAs.\n\n* When making the previous plot, R gives us a warning message that it “Removed 70 rows containing non-finite values”. This is R’s robotic way of telling us that 70 GPA values are missing in the data set. Use what you know about how the data was collected (Fall and Spring semeseter of the 2018-2019 school-year) to guess why these are missing.\n\nSTAT 113 has first-year students: first-years taking the course in the fall would not have a GPA to report. Additionally, another reason might be that a student chose not to report his or her GPA.\n\n* Make a stacked bar plot for two variables of your choosing in the STAT 113 data set. Comment on something that you notice in the plot.\n\nAnswers will vary.\n\nggplot(data = stat113_df, aes(x = Sport, fill = Award)) +\n  geom_bar(position = \"fill\")\n\n\n\n\nAs we might expect, it does seem like a higher proportion of students who play a sport would prefer to win an Olympic medal, compared with students who do not play a sport.\n\n\n3.7.6 Chapter Exercises S\n\n* The default of geom_smooth() is to use LOESS (locally estimated scatterplot smoothing). Read about LOESS here: here. Write one or two sentences explaining what LOESS does.\n\nLoess uses a bunch of local regressions to predict the y-variable at each point, giving more weight to observations near the point of interest on the x-axis. Once this is done for every point, the predictions are connected with a smooth curve.\n\n* Thus far, we have only faceted by a single variable. Use Google to figure out how to facet by two variables to make a plot that shows the relationship between GPA (y-axis) and Exercise (x-axis) with four facets: one for male students who play a sport, one for female students who play a sport, one for male students who do not play a sport, and one for female students who do not play a sport.\n\n\nggplot(data = stat113_df |> filter(!is.na(Sport) & !is.na(Sex)),\n  aes(x = Exercise, y = GPA)) + \n  geom_point() + geom_smooth() +\n  facet_grid(Sex ~ Sport)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 71 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 71 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n* In Intro-Stat, boxplots are typically introduced using the * symbol to identify outliers. Using a combination of the help ?geom_boxplot and Googling “R point shapes”, figure out how to modify your side-by-side boxplots so that the outliers are shown using *, not the default dots.\n\nThen, using Google, figure out how to add the mean to each boxplot as a “darkgreen” diamond-shaped symbol with stat_summary().\n\nggplot(data = stat113_df, aes(x = Sex, y = GPA)) +\n  geom_boxplot(fill = \"lightpink1\", outlier.shape = 8) +\n  stat_summary(fun = mean, shape = 18, colour = \"darkgreen\")\n\nWarning: Removed 70 rows containing non-finite values (`stat_boxplot()`).\n\n\nWarning: Removed 70 rows containing non-finite values (`stat_summary()`).\n\n\nWarning: Removed 3 rows containing missing values (`geom_segment()`)."
  },
  {
    "objectID": "02-ggplot2.html#rcode-2",
    "href": "02-ggplot2.html#rcode-2",
    "title": "3  Plotting with ggplot2",
    "section": "3.8 Non-Exercise R Code",
    "text": "3.8 Non-Exercise R Code\n\nlibrary(tidyverse)\npres_df <- read_table(\"data/PRES2000.txt\") \n## don't worry about the `read_table` function....yet\nhead(pres_df)\nggplot(data = pres_df, aes(x = Gore)) +\n  geom_histogram(colour = \"black\", fill = \"white\") +\n  xlab(\"Votes for Gore in Florida\")\nggplot(data = pres_df, aes(x = Gore)) +\n  geom_freqpoly(colour = \"black\") +\n  xlab(\"Votes for Gore in Florida\") \nggplot(data = pres_df, aes(x = Gore)) +\n  geom_freqpoly(colour = \"black\") +\n  xlab(\"Votes for Gore in Florida\") +\n  geom_histogram() \npres_cat <- pres_df |> mutate(winner = if_else(Gore > Bush,\n                                                true = \"Gore\",\n                                                false = \"Bush\"))\npres_cat\nggplot(data = pres_cat, aes(x = winner)) +\n  geom_bar()\npres_cat2 <- pres_cat |> group_by(winner) |>\n  summarise(nwins = n())\npres_cat2\nggplot(pres_cat2, aes(x = winner)) +\n  geom_bar()\nggplot(pres_cat2, aes(x = winner, y = nwins)) +\n  geom_col()\nggplot(data = pres_df, aes(x = Gore, y = Bush)) +\n  geom_point()\nlibrary(tidyverse)\nfitness_full <- read_csv(\"data/higham_fitness_clean.csv\") |> mutate(weekend_ind = case_when(weekday == \"Sat\" | weekday == \"Sun\" ~ \"weekend\",\n  TRUE ~ \"weekday\"))\nggplot(data = fitness_full, aes(x = distance, y = active_cals)) +\n  geom_point()\n## drop observations that have active calories < 50. \n## assuming that these are data errors or \n## days where the Apple Watch wasn't worn.\nfitness <- fitness_full |>\n  filter(active_cals > 50)\nggplot(data = fitness) +\n  geom_point(aes(x = distance, y = active_cals))\nggplot(data = fitness) +\n  geom_point(aes(x = Start, y = active_cals, colour = weekend_ind))\nggplot(data = fitness) +\n  geom_point(aes(x = Start, y = active_cals, shape = weekend_ind))\nggplot(data = fitness) +\n  geom_point(aes(x = Start, y = active_cals, size = flights))\nggplot(data = fitness) +\n  geom_point(aes(x = Start, y = active_cals, colour = \"purple\"))\nggplot(data = fitness) +\n  geom_point(aes(x = Start, y = active_cals), size = 1.5, shape = 19)\nggplot(data = fitness, aes(x = Start, y = active_cals)) +\n  geom_point() +\n  geom_smooth(span = 0.3)\nggplot(data = fitness, aes(x = Start, y = active_cals)) +\n  geom_point() +\n  geom_smooth(se = FALSE, method = \"lm\")\nggplot(data = fitness, aes(x = Start, y = steps)) +\n  geom_point() + geom_smooth() + xlab(\"Date\")\nggplot(data = fitness, aes(x = Start, y = steps)) +\n  geom_line() + geom_smooth() + xlab(\"Date\")\nggplot(data = stat113_df, aes(x = Exercise, y = Pulse,\n                           colour = Year)) +\n  geom_point() +\n  geom_smooth(se = TRUE)\nggplot(data = stat113_df, aes(x = Exercise, y = Pulse)) +\n  geom_point() +\n  geom_smooth(se = TRUE) +\n  facet_wrap(~ Year)\nggplot(data = stat113_df, aes(x = Award, y = Exercise)) +\n  geom_boxplot()\nggplot(data = stat113_df, aes(x = Award, y = Exercise)) +\n  geom_violin()\nggplot(data = stat113_df, aes(x = Year, fill = SocialMedia)) +\n  geom_bar(position = \"fill\") +\n  ylab(\"Proportion\")"
  },
  {
    "objectID": "03-dplyr.html",
    "href": "03-dplyr.html",
    "title": "4  Wrangling with dplyr",
    "section": "",
    "text": "Goals:\nThroughout this chapter, we will use the babynames data set in the babynames R package. To begin, install the babynames package by typing install.packages(\"babynames\") in your bottom-left console winow, and read about the data set by running\nand then typing ?babynames in your bottom-left window of R Studio. We see that this data set contains baby name data provided by the SSA in the United States dating back to 1880:\nThe second data set that we will use has 27 observations, one for each of SLU’s majors and contains 3 variables:\nThe data has kindly been provided by Dr. Ramler. With your Notes R Project open, you can read in the data set with\nThere are many interesting and informative plots that we could make with either data set, but most require some data wrangling first. This chapter will provide the foundation for such wrangling skills."
  },
  {
    "objectID": "03-dplyr.html#mutate-create-variables",
    "href": "03-dplyr.html#mutate-create-variables",
    "title": "4  Wrangling with dplyr",
    "section": "4.1 mutate(): Create Variables",
    "text": "4.1 mutate(): Create Variables\nSometimes, we will want to create a new variable that’s not in the data set, oftentimes using if_else(), case_when(), or basic algebraic operations on one or more of the columns already present in the data set.\nR understands the following symbols:\n\n+ for addition, - for subtraction\n* for multiplication, / for division\n^ for raising something to a power (3 ^ 2 is equal to 9)\n\nR also does the same order of operations as usual: parentheses, then exponents, then multiplication and division, then addition and subtraction.\nFor example, suppose that we want to create a variable in slumajors_df that has the total number of students graduating in each major. We can do this with mutate():\n\nslumajors_df |> mutate(ntotal = nfemales + nmales)\n\n# A tibble: 27 × 4\n   Major                        nfemales nmales ntotal\n   <chr>                           <dbl>  <dbl>  <dbl>\n 1 Anthropology                       34     15     49\n 2 Art & Art History                  65     11     76\n 3 Biochemistry                       14     11     25\n 4 Biology                           162     67    229\n 5 Business in the Liberal Arts      135    251    386\n 6 Chemistry                          26     14     40\n 7 Computer Science                   21     47     68\n 8 Conservation Biology               38     20     58\n 9 Economics                         128    349    477\n10 English                           131     54    185\n# … with 17 more rows\n\n\nThere’s a lot to break down in that code chunk: most importantly, we’re seeing our first of many, many, many, many, many, many, many instances of using |> to pipe! The |> operator approximately reads take slumajors_df “and then” mutate() it.\nPiping is a really convenient, easy-to-read way to build a sequence of commands. How you can read the above code is:\n\nTake slumajors_df and with slumajors_df,\nperform a mutate() step to create the new variable called ntotal, which is nfemales plus nmales.\n\nSince this is our first time using mutate(), let’s also delve into what the function is doing. In general, mutate() reads:\nmutate(name_of_new_variable = operations_on_old_variables).\nR just automatically assumes that you want to do the operation for every single row in the data set, which is often quite convenient!\nWe might also want to create a variable that is the percentage of students identifying as female for each major:\n\nslumajors_df |>\n  mutate(percfemale = 100 * nfemales / (nfemales + nmales))\n\n# A tibble: 27 × 4\n   Major                        nfemales nmales percfemale\n   <chr>                           <dbl>  <dbl>      <dbl>\n 1 Anthropology                       34     15       69.4\n 2 Art & Art History                  65     11       85.5\n 3 Biochemistry                       14     11       56  \n 4 Biology                           162     67       70.7\n 5 Business in the Liberal Arts      135    251       35.0\n 6 Chemistry                          26     14       65  \n 7 Computer Science                   21     47       30.9\n 8 Conservation Biology               38     20       65.5\n 9 Economics                         128    349       26.8\n10 English                           131     54       70.8\n# … with 17 more rows\n\n\nBut what happened to ntotal? Is it still in the printout? It’s not: when we created the variable ntotal, we didn’t actually save the new data set as anything. So R makes and prints the new variable, but it doesn’t get saved to any data set. If we want to save the new data set, then we can use the <- operator. Here, we’re saving the new data set with the same name as the old data set: slumajors_df. Then, we’re doing the same thing for the percfemale variable. We won’t always want to give the new data set the same name as the old one: we’ll talk about this more in the chapter exercises.\n\nslumajors_df <- slumajors_df |>\n  mutate(percfemale = 100 * nfemales / (nfemales + nmales))\n\n\nslumajors_df <- slumajors_df |> mutate(ntotal = nfemales + nmales)\n\nBut, you can pipe as many things together as you want to, so it’s probably easier to just create both variables in one go. The following chunk says to “Take slumajors_df and create a new variable ntotal. With that new data set, create a new variable called percfemale.” Finally, the slumajors_df <- at the beginning says to “save this new data set as a data set with the same name, slumajors_df.”\n\nslumajors_df <- slumajors_df |>\n  mutate(ntotal = nfemales + nmales) |>\n  mutate(percfemale = 100 * nfemales / (nfemales + nmales))\n\n\n4.1.1 if_else() and case_when()\nSuppose that you want to make a new variable that is conditional on another variable (or more than one variable) in the data set. Then we would typically use mutate() coupled with\n\nif_else() if your new variable is created on only one condition\ncase_when() if your new variable is created on more than one condition\n\nSuppose we want to create a new variable that tells us whether or not the Major has a majority of Women. That is, we want this new variable, morewomen to be \"Yes\" if the Major has more than 50% women and \"No\" if it has 50% or less.\n\nslumajors_df |> mutate(morewomen = if_else(percfemale > 50,\n                                            true = \"Yes\",\n                                            false = \"No\"))\n\n# A tibble: 27 × 6\n   Major                        nfemales nmales percfemale ntotal morewomen\n   <chr>                           <dbl>  <dbl>      <dbl>  <dbl> <chr>    \n 1 Anthropology                       34     15       69.4     49 Yes      \n 2 Art & Art History                  65     11       85.5     76 Yes      \n 3 Biochemistry                       14     11       56       25 Yes      \n 4 Biology                           162     67       70.7    229 Yes      \n 5 Business in the Liberal Arts      135    251       35.0    386 No       \n 6 Chemistry                          26     14       65       40 Yes      \n 7 Computer Science                   21     47       30.9     68 No       \n 8 Conservation Biology               38     20       65.5     58 Yes      \n 9 Economics                         128    349       26.8    477 No       \n10 English                           131     54       70.8    185 Yes      \n# … with 17 more rows\n\n\nThe mutate() statement reads: create a new variable called morewomen that is equal to \"Yes\" if percfemale > 50 is true and is equal to \"No\" if perfemale is not > 0.5. The first argument is the condition, the second is what to name the new variable when the condition holds, and the third is what to name the variable if the condition does not hold.\nWe use conditions all of the time in every day life. For example, New York had a quarantine order stating that people coming from 22 states in July 2020 would need to quarantine. In terms of a condition, this would read “if you are traveling to New York from one of the 22 states, then you need to quarantine for 2 weeks. Else, if not, then you don’t need to quarantine.” The trick in using these conditions in R is getting used to the syntax of the code.\nWe can see from the above set up that if we had more than one condition, then we’d need to use a different function (or use nested if_else() statements, which can be a nightmare to read). If we have more than one condition for creating the new variable, we will use case_when().\nFor example, when looking at the output, we see that Biochemistry has 56% female graduates. That’s “about” a 50/50 split, so suppose we want a variable called large_majority that is “female” when the percent women is 70 or more, “male” when the percent women is 30 or less, and “none” when the percent female is between 30 and 70.\n\nslumajors_df |> mutate(large_majority =\n                          case_when(percfemale >= 70 ~ \"female\",\n                                    percfemale <= 30 ~ \"male\",\n                                    percfemale > 30 & percfemale < 70 ~ \"none\")) \n\n# A tibble: 27 × 6\n   Major                        nfemales nmales percfemale ntotal large_majority\n   <chr>                           <dbl>  <dbl>      <dbl>  <dbl> <chr>         \n 1 Anthropology                       34     15       69.4     49 none          \n 2 Art & Art History                  65     11       85.5     76 female        \n 3 Biochemistry                       14     11       56       25 none          \n 4 Biology                           162     67       70.7    229 female        \n 5 Business in the Liberal Arts      135    251       35.0    386 none          \n 6 Chemistry                          26     14       65       40 none          \n 7 Computer Science                   21     47       30.9     68 none          \n 8 Conservation Biology               38     20       65.5     58 none          \n 9 Economics                         128    349       26.8    477 male          \n10 English                           131     54       70.8    185 female        \n# … with 17 more rows\n\n\nThe case_when() function reads “When the percent female is more than or equal to 70, assign the new variable large_majority the value of”female”, when it’s less or equal to 30, assign the more than 30 and less than 70, assign the variable the value of “none” .” The & is a boolean operator: we’ll talk more about that later so don’t worry too much about that for now.\nLet’s save these two new variables to the slumajors_df:\n\nslumajors_df <- slumajors_df |>\n  mutate(morewomen = if_else(percfemale > 50,\n                             true = \"Yes\",\n                             false = \"No\")) |>\n  mutate(large_majority =\n           case_when(percfemale >= 70 ~ \"female\",\n                     percfemale <= 30 ~ \"male\",\n                     percfemale > 30 & percfemale < 70 ~ \"none\")) \n\n\n\n4.1.2 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-3).\n\nDo you think it is ethical to exclude non-binary genders from analyses and graphs in the slumajors data set? Why or why not?\n* Create a new variable that is called major_size and is “large” when the total number of majors is 100 or more and “small” when the total number of majors is less than 100.\nCreate a new variable that is called major_size2 and is “large when the total number of majors is 150 or more,”medium” when the total number of majors is between 41 and 149, and “small” when the total number of majors is 40 or fewer.\nAbout 55% of SLU students identify as female. So, in the definition of the morewomen variable, does it make more sense to use 55% as the cutoff or 50%?\n* Investigate what happens with case_when() when you give overlapping conditions and when you give conditions that don’t cover all observations. For overlapping conditions, create a variable testcase that is \"Yes\" when percfemale is greater than or equal to 40 and \"No\" when percfemale is greater than 60 For conditions that don’t cover all observations, create a variable testcase2 that is \"Yes\" when percfemale is greater than or equal to 55 and \"No\" when percfemale is less than 35.\nWith one or two of the newly created variables from mutate(), create a plot that investigates a question of interest you might have about the data."
  },
  {
    "objectID": "03-dplyr.html#arrange-ordering-rows-select-choosing-columns-and-slice-and-filter-choosing-rows",
    "href": "03-dplyr.html#arrange-ordering-rows-select-choosing-columns-and-slice-and-filter-choosing-rows",
    "title": "4  Wrangling with dplyr",
    "section": "4.2 arrange() (Ordering Rows), select() (Choosing Columns), and slice() and filter() (Choosing Rows)",
    "text": "4.2 arrange() (Ordering Rows), select() (Choosing Columns), and slice() and filter() (Choosing Rows)\narrange() is used to order rows in the data set according to some variable, select() is used to choose columns to keep (or get rid of) and filter() is used to keep (or get rid of) only some of the observations (rows).\n\n4.2.1 arrange(): Ordering Rows\nThe arrange() function allows us to order rows in the data set using one or more variables. The function is very straightforward. Suppose that we want to order the rows so that the majors with the lowest percfemale are first:\n\nslumajors_df |> arrange(percfemale)\n\n# A tibble: 27 × 7\n   Major                        nfemales nmales percfem…¹ ntotal morew…² large…³\n   <chr>                           <dbl>  <dbl>     <dbl>  <dbl> <chr>   <chr>  \n 1 Economics                         128    349      26.8    477 No      male   \n 2 Physics                             6     14      30       20 No      male   \n 3 Computer Science                   21     47      30.9     68 No      none   \n 4 Business in the Liberal Arts      135    251      35.0    386 No      none   \n 5 Music                              13     21      38.2     34 No      none   \n 6 Geology                            28     41      40.6     69 No      none   \n 7 History                            62     82      43.1    144 No      none   \n 8 Philosophy                         24     29      45.3     53 No      none   \n 9 Mathematics                        74     83      47.1    157 No      none   \n10 Government                        127    116      52.3    243 Yes     none   \n# … with 17 more rows, and abbreviated variable names ¹​percfemale, ²​morewomen,\n#   ³​large_majority\n\n\nWhich major has the lowest percentage of female graduates?\nWe see that, by default, arrange() orders the rows from low to high. To order from high to low so that the majors with the highest percfemale are first, use desc() around the variable that you are ordering by:\n\nslumajors_df |> arrange(desc(percfemale))\n\n# A tibble: 27 × 7\n   Major                   nfemales nmales percfemale ntotal morewomen large_m…¹\n   <chr>                      <dbl>  <dbl>      <dbl>  <dbl> <chr>     <chr>    \n 1 Art & Art History             65     11       85.5     76 Yes       female   \n 2 Psychology                   278     61       82.0    339 Yes       female   \n 3 French                        27      7       79.4     34 Yes       female   \n 4 Spanish                       35     10       77.8     45 Yes       female   \n 5 Statistics                    28      9       75.7     37 Yes       female   \n 6 Global Studies                69     27       71.9     96 Yes       female   \n 7 Neuroscience                  61     24       71.8     85 Yes       female   \n 8 Performance & Comm Arts      144     57       71.6    201 Yes       female   \n 9 Religious Studies             10      4       71.4     14 Yes       female   \n10 English                      131     54       70.8    185 Yes       female   \n# … with 17 more rows, and abbreviated variable name ¹​large_majority\n\n\nWhat is the major with the highest percentage of women graduates?\n\n\n4.2.2 select() Choose Columns\nWe might also be interested in getting rid of some of the columns in a data set. One reason to do this is if there are an overwhelming (30+) columns in a data set, but we know that we just need a few of them. The easiest way to use select() is to just input the names of the columns that you want to keep. For example, if we were only interested in majors and their totals, we could do\n\nslumajors_df |> select(Major, ntotal)\n\n# A tibble: 27 × 2\n   Major                        ntotal\n   <chr>                         <dbl>\n 1 Anthropology                     49\n 2 Art & Art History                76\n 3 Biochemistry                     25\n 4 Biology                         229\n 5 Business in the Liberal Arts    386\n 6 Chemistry                        40\n 7 Computer Science                 68\n 8 Conservation Biology             58\n 9 Economics                       477\n10 English                         185\n# … with 17 more rows\n\n\nIf I wanted to use this data set for anything else, I’d also need to name, or rename, it with <-. We would probably want to name it something other than slumajors_df so as to not overwrite the original data set, in case we want to use those other variables again later!\nWe might also want to use select() to get rid of one or two columns. If this is the case, we denote any column you want to get rid of with -. For example, we might want to get rid of the ntotal column that we made and get rid of the nmales and nfemales columns:\n\nslumajors_df |> select(-ntotal, -nfemales, -nmales)\n\n# A tibble: 27 × 4\n   Major                        percfemale morewomen large_majority\n   <chr>                             <dbl> <chr>     <chr>         \n 1 Anthropology                       69.4 Yes       none          \n 2 Art & Art History                  85.5 Yes       female        \n 3 Biochemistry                       56   Yes       none          \n 4 Biology                            70.7 Yes       female        \n 5 Business in the Liberal Arts       35.0 No        none          \n 6 Chemistry                          65   Yes       none          \n 7 Computer Science                   30.9 No        none          \n 8 Conservation Biology               65.5 Yes       none          \n 9 Economics                          26.8 No        male          \n10 English                            70.8 Yes       female        \n# … with 17 more rows\n\n\nselect() comes with many useful helper functions, but these are oftentimes not needed. One of the helper functions that is actually often useful is everything(). We can, for example, use this after using mutate() to put the variable that was just created at the front of the data set to make sure there weren’t any unexpected issues:\n\nslumajors_df |> mutate(propfemale = percfemale / 100) |>\n  select(propfemale, everything())\n\n# A tibble: 27 × 8\n   propfemale Major                nfema…¹ nmales percf…² ntotal morew…³ large…⁴\n        <dbl> <chr>                  <dbl>  <dbl>   <dbl>  <dbl> <chr>   <chr>  \n 1      0.694 Anthropology              34     15    69.4     49 Yes     none   \n 2      0.855 Art & Art History         65     11    85.5     76 Yes     female \n 3      0.56  Biochemistry              14     11    56       25 Yes     none   \n 4      0.707 Biology                  162     67    70.7    229 Yes     female \n 5      0.350 Business in the Lib…     135    251    35.0    386 No      none   \n 6      0.65  Chemistry                 26     14    65       40 Yes     none   \n 7      0.309 Computer Science          21     47    30.9     68 No      none   \n 8      0.655 Conservation Biology      38     20    65.5     58 Yes     none   \n 9      0.268 Economics                128    349    26.8    477 No      male   \n10      0.708 English                  131     54    70.8    185 Yes     female \n# … with 17 more rows, and abbreviated variable names ¹​nfemales, ²​percfemale,\n#   ³​morewomen, ⁴​large_majority\n\n\nVerify that propfemale now appears first in the data set. everything() tacks on all of the remaining variables after propfemale. So, in this case, it’s a useful way to re-order the columns so that what you might be most interested in appears first.\n\n\n4.2.3 slice() and filter(): Choose Rows\nInstead of choosing which columns to keep, we can also choose certain rows to keep using either slice() or filter().\nslice() allows you to specify the row numbers corresponding to rows that you want to keep. For example, suppose that we only want to keep the rows with the five most popular majors:\n\nslumajors_df |> arrange(desc(ntotal)) |>\n  slice(1, 2, 3, 4, 5)\n\n# A tibble: 5 × 7\n  Major                        nfemales nmales percfemale ntotal morew…¹ large…²\n  <chr>                           <dbl>  <dbl>      <dbl>  <dbl> <chr>   <chr>  \n1 Economics                         128    349       26.8    477 No      male   \n2 Business in the Liberal Arts      135    251       35.0    386 No      none   \n3 Psychology                        278     61       82.0    339 Yes     female \n4 Government                        127    116       52.3    243 Yes     none   \n5 Biology                           162     67       70.7    229 Yes     female \n# … with abbreviated variable names ¹​morewomen, ²​large_majority\n\n\nWe can alternatively use slice(1:5), which is shorthand for slice(1, 2, 3, 4, 5). While slice() is useful, it is relatively simple. We’ll come back to it again in a few weeks as well when we discuss subsetting in base R.\nfilter() is a way to keep rows by specifying a condition related to one or more of the variables in the data set. We’ve already seen conditions in if_else() and case_when() statements, but they’ll now be used to “filter” the rows in our data set.\nWe can keep rows based on a categorical variable or a quantitative variable or a combination of any number of categorical and quantitative variables. R uses the following symbols to make comparisons. We’ve already been using the more intuitive symbols (like < and >):\n\n< and <= for less than and less than or equal to, respectively\n> and >= for greater than and greater than or equal to, respectively\n== for equal to (careful: equal to is a double equal sign ==)\n!= for not equal to (in general, ! denotes “not”)\n\nIt’s probably time for a change of data set too! We’ll be working with the babynames data set for the rest of this chapter:\n\nlibrary(babynames)\nbabynames\n\n# A tibble: 1,924,665 × 5\n    year sex   name          n   prop\n   <dbl> <chr> <chr>     <int>  <dbl>\n 1  1880 F     Mary       7065 0.0724\n 2  1880 F     Anna       2604 0.0267\n 3  1880 F     Emma       2003 0.0205\n 4  1880 F     Elizabeth  1939 0.0199\n 5  1880 F     Minnie     1746 0.0179\n 6  1880 F     Margaret   1578 0.0162\n 7  1880 F     Ida        1472 0.0151\n 8  1880 F     Alice      1414 0.0145\n 9  1880 F     Bertha     1320 0.0135\n10  1880 F     Sarah      1288 0.0132\n# … with 1,924,655 more rows\n\n\nIf needed, we can remind ourselves what is in the babynames data set by typing ?babynames in the console window.\nWhat do the following statements do? See if you can guess before running the code.\n\nbabynames |> filter(name == \"Matthew\")\nbabynames |> filter(year >= 2000)\nbabynames |> filter(sex != \"M\")\nbabynames |> filter(prop > 0.05)\nbabynames |> filter(year == max(year))\n\nWhy are some things put in quotes, like \"Matthew\" while some things aren’t, like 2000? Can you make out a pattern?\nWe can also combine conditions on multiple variables in filter() using Boolean operators. We’ve already seen one of these in the case_when() statement above: & means “and”.\nLook at the Venn diagrams in R for Data Science to learn about the various Boolean operators you can use in R: https://r4ds.had.co.nz/transform.html#logical-operators. The Boolean operators can be used in other functions in R as well, as we’ve already seen with if_else() and case_when().\nThe following gives some examples. See if you can figure out what each line of code is doing before running it.\n\nbabynames |> filter(n > 20000 | prop > 0.05)\nbabynames |> filter(sex == \"F\" & name == \"Mary\")\nbabynames |> filter(sex == \"F\" & name == \"Mary\" & prop > 0.05)\n\n\n\n4.2.4 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-3).\n\nWhat happens when you arrange() by one of the categorical variables in the slumajors_df data set?\n* Use select() and everything() to put the large_majority variable as the first column in the slumajors_df data set.\n* In the babynames data set, use filter(), mutate() with rank(), and arrange() to print the 10 most popular Male babynames in 2017.\nIn the babynames data set, use filter() to keep only the rows with your name (or, another name that interests you) and one sex (either \"M\" or \"F\"). Name the new data set something and then construct a line plot that looks at the either the n or prop of your chosen name through year."
  },
  {
    "objectID": "03-dplyr.html#summarise-and-group_by-create-summaries",
    "href": "03-dplyr.html#summarise-and-group_by-create-summaries",
    "title": "4  Wrangling with dplyr",
    "section": "4.3 summarise() and group_by(): Create Summaries",
    "text": "4.3 summarise() and group_by(): Create Summaries\nThe summarise() function is useful to get summaries from the data. For example, suppose that we want to know the average major size at SLU across the five year span or the total number of majors across those five years. Then we can use summarise() and a summary function, like mean(), sum(), median(), max(), min(), n(), etc. You’ll notice that the format of summarise() is extremely similar to the format of mutate(). Using the slumajors_df data again just for one quick example,\n\nslumajors_df |>\n  summarise(meantotalmajor = mean(ntotal),\n            totalgrad = sum(ntotal))\n\n# A tibble: 1 × 2\n  meantotalmajor totalgrad\n           <dbl>     <dbl>\n1           124.      3347\n\n\n\n4.3.1 group_by(): Groups\nsummarise() is often most useful when paired with a group_by() statement. Doing so allows us to get summaries across different groups.\nFor example, suppose that you wanted the total number of registered births per year in the babynames data set:\n\nbabynames |> group_by(year) |>\n  summarise(totalbirths = sum(n))\n\n# A tibble: 138 × 2\n    year totalbirths\n   <dbl>       <int>\n 1  1880      201484\n 2  1881      192696\n 3  1882      221533\n 4  1883      216946\n 5  1884      243462\n 6  1885      240854\n 7  1886      255317\n 8  1887      247394\n 9  1888      299473\n10  1889      288946\n# … with 128 more rows\n\n\ngroup_by() takes a grouping variable, and then, using summarise() computes the given summary function on each group.\nMost summary functions are intuitive if you’ve had intro stat. But, if you’re not sure whether the summary for getting the maximum is maximum() or max(), just try both!\nThe n() function can be used within summarise() to obtain the number of observations. It will give you the total number of rows, if used without group_by()\n\nbabynames |> summarise(totalobs = n())\n\n# A tibble: 1 × 1\n  totalobs\n     <int>\n1  1924665\n\n\nNote that n() typically doesn’t have any inputs. It’s typically more useful when paired with group_by(): this allows us to see the number of observations within each year, for instance:\n\nbabynames |> group_by(year) |>\n  summarise(ngroup = n())\n\n# A tibble: 138 × 2\n    year ngroup\n   <dbl>  <int>\n 1  1880   2000\n 2  1881   1935\n 3  1882   2127\n 4  1883   2084\n 5  1884   2297\n 6  1885   2294\n 7  1886   2392\n 8  1887   2373\n 9  1888   2651\n10  1889   2590\n# … with 128 more rows\n\n\n\n\n4.3.2 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-3).\n\nCompare summarise() with mutate() using the following code. What’s the difference between the two functions?\n\n\nslumajors_df |>\n  summarise(meantotalmajor = mean(ntotal),\n            totalgrad = sum(ntotal)) \nslumajors_df |>\n  mutate(meantotalmajor = mean(ntotal),\n            totalgrad = sum(ntotal)) |>\n  select(meantotalmajor, totalgrad, everything())\n\n\nUsing the data set from the group_by() and n() combination,\n\n\nbabynames |> group_by(year) |>\n  summarise(ngroup = n())\n\n# A tibble: 138 × 2\n    year ngroup\n   <dbl>  <int>\n 1  1880   2000\n 2  1881   1935\n 3  1882   2127\n 4  1883   2084\n 5  1884   2297\n 6  1885   2294\n 7  1886   2392\n 8  1887   2373\n 9  1888   2651\n10  1889   2590\n# … with 128 more rows\n\n\nmake a line plot with ngroup on the x-axis and year on the y-axis. How would you interpret the plot?\n\n* Create a data set that has a column for name and a column that shows the total number of births for that name across all years and both sexes.\n* group_by() can also be used with other functions, including mutate(). Use group_by() and mutate() to rank the names from most to least popular in each year-sex combination.\n* From the data set in 4, filter() the data to keep only the most popular name in each year-sex combination and then construct a summary table showing how many times each name appears as the most popular name.\n* Run the following code. Intuitively, a slice(1, 2, 3, 4, 5) should grab the first five rows of the data set, but, when we try to run that, we get 1380 rows. Try to figure out what the issue is by using Google to search something like “dplyr not slicing correctly after using group by.” What do you find?\n\n\nbabynames_test <- babynames |>\n  group_by(year, sex) |> mutate(ntest = n / prop)\nbabynames_test |> slice(1, 2, 3, 4, 5)\n\n# A tibble: 1,380 × 6\n# Groups:   year, sex [276]\n    year sex   name          n   prop   ntest\n   <dbl> <chr> <chr>     <int>  <dbl>   <dbl>\n 1  1880 F     Mary       7065 0.0724  97605.\n 2  1880 F     Anna       2604 0.0267  97605.\n 3  1880 F     Emma       2003 0.0205  97605.\n 4  1880 F     Elizabeth  1939 0.0199  97605.\n 5  1880 F     Minnie     1746 0.0179  97605.\n 6  1880 M     John       9655 0.0815 118400.\n 7  1880 M     William    9532 0.0805 118400.\n 8  1880 M     James      5927 0.0501 118400.\n 9  1880 M     Charles    5348 0.0452 118400.\n10  1880 M     George     5126 0.0433 118400.\n# … with 1,370 more rows"
  },
  {
    "objectID": "03-dplyr.html#missing-values",
    "href": "03-dplyr.html#missing-values",
    "title": "4  Wrangling with dplyr",
    "section": "4.4 Missing Values",
    "text": "4.4 Missing Values\nBoth of the data sets that we’ve worked with are nice in that they do not have any missing values. We’ll see plenty of examples of data sets with missing values later, so we should examine how the various functions that we’ve talked about so far tackle missing values.\nMissing values in R are denoted with NA for “Not Available.” Run the following code to create a toy data set with some missing values so that we can see how the various functions we’ve used so far deal with NA values.\n\ntoy_df <- tibble(x = c(NA, 3, 4, 7),\n                 y = c(1, 4, 3, 2),\n                 z = c(\"A\", \"A\", \"B\", NA))\ntoy_df\n\n# A tibble: 4 × 3\n      x     y z    \n  <dbl> <dbl> <chr>\n1    NA     1 A    \n2     3     4 A    \n3     4     3 B    \n4     7     2 <NA> \n\n\n\n4.4.1 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-3).\n\n* mutate(). Try to create a new variable with mutate() involving x. What does R do with the missing value?\narrange(). Try arranging the data set by x. What does R do with the missing value?\nfilter(). Try filtering so that only observations where x is less than 5 are kept. What does R do with the missing value?\nsummarise(). Try using summarise() with a function involving x. What does R return?\ngroup_by() and summarise(). To your statement in 4, add a group_by(z) statement before your summarise(). What does R return now?\n\n\n\n4.4.2 Removing Missing Values\nMissing values should not be removed without carefully examination and a note of what the consequences might be (e.g. why are these values missing?). We have a toy data set that is meaningless, so we aren’t asking those questions now, but we will for any data set that does have missing values!\nIf we have investigated the missing values and are comfortable with removing them, many functions that we would use in summarise() have an na.rm argument that we can set to TRUE to tell summarise() to remove any NAs before taking the mean(), median(), max(), etc.\n\ntoy_df |> summarise(meanx = mean(x, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  meanx\n  <dbl>\n1  4.67\n\n\nIf we want to remove the missing values more directly, we can use the is.na() function in combination with filter(). If the variable is NA (Not Available) for an observation, is.na() evaluates to TRUE; if not, is.na() evaluates to FALSE. Test this out using mutate() to create a new variable for whether Median is missing:\n\ntoy_df |> mutate(missingx = is.na(x))\n\n# A tibble: 4 × 4\n      x     y z     missingx\n  <dbl> <dbl> <chr> <lgl>   \n1    NA     1 A     TRUE    \n2     3     4 A     FALSE   \n3     4     3 B     FALSE   \n4     7     2 <NA>  FALSE   \n\n\nmissingx is TRUE only for the the first observation. We can use this to our advantage with filter() to filter it out of the data set, without going through the extra step of actually making a new variable missingx:\n\ntoy_df |> filter(is.na(x) != TRUE)\n\n# A tibble: 3 × 3\n      x     y z    \n  <dbl> <dbl> <chr>\n1     3     4 A    \n2     4     3 B    \n3     7     2 <NA> \n\n\nYou’ll commonly see this written as short-hand in people’s code you may come across as:\n\ntoy_df |> filter(!is.na(x))\n\n# A tibble: 3 × 3\n      x     y z    \n  <dbl> <dbl> <chr>\n1     3     4 A    \n2     4     3 B    \n3     7     2 <NA> \n\n\nwhich says to “keep anything that does not have a missing x value” (recall that the ! means “not”)."
  },
  {
    "objectID": "03-dplyr.html#more-about-the-pipe",
    "href": "03-dplyr.html#more-about-the-pipe",
    "title": "4  Wrangling with dplyr",
    "section": "4.5 More about the Pipe",
    "text": "4.5 More about the Pipe\nWe are jumping straight into using piping, but we do want to have an appreciation on how terrible life would be without it. What piping does is make whatever is given before the |> pipe the first argument of whatever function follows the |>. So\n\ndf |> mutate(x = y + 4)\n\nis equivalent to\n\nmutate(df, x = y + 4)\n\nIt might also help to use an analogy when thinking about piping. Consider the Ke$ha’s morning routine in the opening of the song Tik Tok. If we were to write her morning routine in terms of piping,\n\nkesha |> wake_up(time = \"morning\", feels_like = \"P-Diddy\") |>\n  grab(glasses) |>\n  brush(teeth, item = \"jack\", unit = \"bottle\") |> ....\n\nKesha first wakes up in the morning, and then the Kesha that has woken up grabs her glasses, and then the Kesha who has woken up and has her glasses brushes her teeth, etc.\nThe pipe operator |> is loaded automatically with R. We’ve been using the pipe quite a bit, but let’s delve a little deeper into what it’s actually doing. We will use the videogame_clean.csv data file, which contains variables on video games from 2004 - 2019, including\n\ngame, the name of the game\nrelease_date, the release date of the game\nrelease_date2, a second coding of release date\nprice, the price in dollars,\nowners, the number of owners (given in a range)\nmedian_playtime, the median playtime of the game\nmetascore, the score from the website Metacritic\nprice_cat, 1 for Low (less than 10.00 dollars), 2 for Moderate (between 10 and 29.99 dollars), and 3 for High (30.00 or more dollars)\nmeta_cat, Metacritic’s review system, with the following categories: “Overwhelming Dislike”, “Generally Unfavorable”, “Mixed Reviews”, “Generally Favorable”, “Universal Acclaim”.\nplaytime_miss, whether median play time is missing (TRUE) or not (FALSE)\n\nLoad in the data set with\n\nvideogame_df <- read_csv(\"data/videogame_clean.csv\")\n\nRows: 26688 Columns: 15\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (7): game, release_date, owners, meta_cat, developer, publisher, meta_c...\ndbl  (6): price, median_playtime, metascore, price_cat, number, average_play...\nlgl  (1): playtime_miss\ndate (1): release_date2\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(videogame_df)\n\n# A tibble: 6 × 15\n  game   relea…¹ release_…² price owners media…³ metas…⁴ price…⁵ meta_…⁶ playt…⁷\n  <chr>  <chr>   <date>     <dbl> <chr>    <dbl>   <dbl>   <dbl> <chr>   <lgl>  \n1 Half-… Nov 16… 2004-11-16  9.99 10,00…      66      96       1 Univer… FALSE  \n2 Count… Nov 1,… 2004-11-01  9.99 10,00…     128      88       1 Genera… FALSE  \n3 Count… Mar 1,… 2004-03-01  9.99 10,00…       3      65       1 Mixed … FALSE  \n4 Half-… Nov 1,… 2004-11-01  4.99 5,000…       0      NA       1 <NA>    TRUE   \n5 Half-… Jun 1,… 2004-06-01  9.99 2,000…       0      NA       1 <NA>    TRUE   \n6 CS2D   Dec 24… 2004-12-24 NA    1,000…      10      NA      NA <NA>    FALSE  \n# … with 5 more variables: number <dbl>, developer <chr>, publisher <chr>,\n#   average_playtime <dbl>, meta_cat_factor <chr>, and abbreviated variable\n#   names ¹​release_date, ²​release_date2, ³​median_playtime, ⁴​metascore,\n#   ⁵​price_cat, ⁶​meta_cat, ⁷​playtime_miss\n\n\nLet’s say we want to filter() the data set to include only videogames with a metascore that isn’t missing. We’ve been using code like\n\nvideogame_df |> filter(!is.na(metascore))\n\nWhat the pipe is doing is putting videogame_df as the first argument in the filter() function so that the piping statement in the chunk above is equivalent to:\n\nfilter(videogame_df, !is.na(metascore))\n\nIf we want to first filter out games with a non-missing metascore, get rid of all observations with a median play time of 0, and then obtain the “median” median_playtime for each of the 3 price categories, we would typically use\n\nvideogame_df |> filter(!is.na(metascore)) |>\n  filter(median_playtime > 0) |>\n  group_by(price_cat) |>\n  summarise(avg_med_time = median(median_playtime, na.rm = TRUE))\n\nWe see from the summary that, in general, games do tend to give you more “bang for the buck”: more expensive games tend to have a larger median play time. Consecutive pipes build on each other: we can slowly build out what the pipe is doing step-by-step. Starting from the top, videogame_df is the first argument in the filter(!is.na(metascore)) function:\n\nfilter(videogame_df, !is.na(metascore))\n\nfilter(videogame_df, !is.na(metascore)) is the first argument in filter(median_playtime > 0):\n\nfilter(filter(videogame_df, !is.na(metascore)), median_playtime > 0)\n\nfilter(filter(videogame_df, !is.na(metascore)), median_playtime > 0) is the first argument in group_by(price_cat):\n\ngroup_by(filter(filter(videogame_df, !is.na(metascore)),\n                median_playtime > 0), price_cat)\n\nand group_by(filter(filter(videogame_df, !is.na(metascore)), median_playtime > 0), price_cat) is the first argument of summarise(avg_med_time = median(median_playtime, na.rm = TRUE)):\n\nsummarise(group_by(filter(filter(videogame_df, !is.na(metascore)),\n  median_playtime > 0), price_cat), \n  avg_med_time = median(median_playtime, na.rm = TRUE))\n\nand we obtain the same result without the |> pipe. So, why use the pipe? Compare the code the uses the pipe operator to find the average median playtime to the code that doesn’t. Which is easier to read? Which do you think is easier to write? The example shows that, for our purposes, the pipe is most useful in aiding the readability of our code. It’s a lot easier to see what’s happening in the code chunk with the pipes than it is in the previous code chunk without the pipe because, with the pipe, we can read the code from left to right and top to bottom. Without the pipe, we need to read the code from the “inside to the outside”, which is much more challenging.\n\n4.5.1 When You Can’t Use the Pipe\nSo, the pipe is a convenient way to put what precedes the pipe as the first argument to the function that follows the pipe. It’s important to understand this as you learn more about R because, while the functions in tidyverse are purposefully made to make good use of the pipe, not all functions in R utilize the pipe. Most of the functions in the tidyverse have a first argument that is a data set (which is why we can use pipes consecutively), but this isn’t the case with all R functions.\nFor example, if you have taken STAT 213, you’ve used lm() to fit many different types of linear models. If you haven’t taken STAT 213, lm(response ~ explanatory, data = name_of_data_set) stands for “linear model” and can be used to fit the simple linear regression model that you learned about in STAT 113. You might expect something like this to work:\n\nvideogame_df |> lm(metascore ~ price)\n\nError in as.data.frame.default(data): cannot coerce class '\"formula\"' to a data.frame\n\n\nBut it throws us an error. Typing in ?lm reveals that its first argument is a formula to fit the model, not a data set. So the function is trying to run\n\nlm(videogame_df, metascore ~ price)\n\nError in as.data.frame.default(data): cannot coerce class '\"formula\"' to a data.frame\n\n\nwhich doesn’t work because the arguments to the function are mixed up (the formula should appear first and the data set should appear second).\nFor one final note about the pipe, note that the pipe operator |> is relatively new. Previously, the primary pipe operator used was %>% and came from the magrittr package. For almost all cases, the two operators are equivalent. However, when scanning the Internet for help with code, you will probably see |> used in many of people’s responses on sites like StackOverflow.\n\n\n4.5.2 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-3).\n\n* Recode the following to look cleaner by using the pipe |>.\n\n\nfitness_df <- read_csv(\"data/higham_fitness_clean.csv\")\n\nRows: 993 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): month, weekday\ndbl  (6): active_cals, distance, flights, steps, dayofyear, stepgoal\ndate (1): Start\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsummarise(group_by(filter(fitness_df, weekday == \"Sat\" | weekday == \"Sun\"),\n                   month),\n          meanweekend = mean(distance, na.rm = TRUE)) \n\n# A tibble: 12 × 2\n   month meanweekend\n   <chr>       <dbl>\n 1 Apr          5.30\n 2 Aug          5.52\n 3 Dec          3.30\n 4 Feb          4.87\n 5 Jan          3.89\n 6 Jul          4.85\n 7 Jun          4.18\n 8 Mar          4.86\n 9 May          5.00\n10 Nov          3.06\n11 Oct          3.82\n12 Sep          4.02\n\n\n\nExplain why the following code gives a warning message and returns NA. Use the list of Arguments in ?mean in your explanation.\n\n\nfitness_df |> mean(distance, na.rm = TRUE)\n\nWarning in mean.default(fitness_df, distance, na.rm = TRUE): argument is not\nnumeric or logical: returning NA\n\n\n[1] NA"
  },
  {
    "objectID": "03-dplyr.html#chapexercise-3",
    "href": "03-dplyr.html#chapexercise-3",
    "title": "4  Wrangling with dplyr",
    "section": "4.6 Chapter Exercises",
    "text": "4.6 Chapter Exercises\n\nWe found both in the SLU majors data set and in the FiveThirtyEight majors data set that Statistics has a higher proportion of women than almost all other STEM fields. Read the first two sections of this article. Write 2-3 sentences about the article’s reasoning of why there are more women in statistics than in other STEM fields.\n* a. Choose 5 names that interest you and create a new data set that only has data on those 5 names.\n\n\nUse group_by() and summarise() to add together the number of Females and Males for each name in each year. Hint: you can group_by() more than one variable!\nMake a line plot showing the popularity of these 5 names over time.\n\n\n\nChoose a year and a sex that interests you and filter the data set to only contain observations from that year and sex.\n\n\n\nCreate a new variable that ranks the names from most popular to least popular.\nCreate a bar plot that shows the 10 most popular names as well as the count for each name.\n\n\n* In some cases throughout this chapter, we’ve renamed data sets using <- with the same name like\n\n\ntoy_df <- toy_df |> mutate(newvar = x / y)\n\nIn other cases, we’ve given the data set a new name, like\n\ntoy_small <- toy_df |> filter(!is.na(x))\n\nFor which of the functions below is a generally “safe” to name the data set using the same name after using the function. Why?\n\nmutate()\narrange()\nfilter()\nsummarise()\nselect()\n\n\nPose a question about the babynames data set and then answer your question with either a graphic or a data summary."
  },
  {
    "objectID": "03-dplyr.html#solutions-3",
    "href": "03-dplyr.html#solutions-3",
    "title": "4  Wrangling with dplyr",
    "section": "4.7 Exercise Solutions",
    "text": "4.7 Exercise Solutions\n\n4.7.1 mutate() S\n\n* Create a new variable that is called major_size and is “large” when the total number of majors is 100 or more and “small” when the total number of majors is less than 100.\n\n\nslumajors_df |> mutate(major_size = if_else(ntotal >= 100,\n                                             true = \"large\",\n                                             false = \"small\"))\n\n# A tibble: 27 × 8\n   Major                   nfema…¹ nmales percf…² ntotal morew…³ large…⁴ major…⁵\n   <chr>                     <dbl>  <dbl>   <dbl>  <dbl> <chr>   <chr>   <chr>  \n 1 Anthropology                 34     15    69.4     49 Yes     none    small  \n 2 Art & Art History            65     11    85.5     76 Yes     female  small  \n 3 Biochemistry                 14     11    56       25 Yes     none    small  \n 4 Biology                     162     67    70.7    229 Yes     female  large  \n 5 Business in the Libera…     135    251    35.0    386 No      none    large  \n 6 Chemistry                    26     14    65       40 Yes     none    small  \n 7 Computer Science             21     47    30.9     68 No      none    small  \n 8 Conservation Biology         38     20    65.5     58 Yes     none    small  \n 9 Economics                   128    349    26.8    477 No      male    large  \n10 English                     131     54    70.8    185 Yes     female  large  \n# … with 17 more rows, and abbreviated variable names ¹​nfemales, ²​percfemale,\n#   ³​morewomen, ⁴​large_majority, ⁵​major_size\n\n## OR\nslumajors_df |>\n  mutate(major_size = case_when(ntotal >= 100 ~ \"large\",\n                                ntotal < 100 ~ \"small\"))\n\n# A tibble: 27 × 8\n   Major                   nfema…¹ nmales percf…² ntotal morew…³ large…⁴ major…⁵\n   <chr>                     <dbl>  <dbl>   <dbl>  <dbl> <chr>   <chr>   <chr>  \n 1 Anthropology                 34     15    69.4     49 Yes     none    small  \n 2 Art & Art History            65     11    85.5     76 Yes     female  small  \n 3 Biochemistry                 14     11    56       25 Yes     none    small  \n 4 Biology                     162     67    70.7    229 Yes     female  large  \n 5 Business in the Libera…     135    251    35.0    386 No      none    large  \n 6 Chemistry                    26     14    65       40 Yes     none    small  \n 7 Computer Science             21     47    30.9     68 No      none    small  \n 8 Conservation Biology         38     20    65.5     58 Yes     none    small  \n 9 Economics                   128    349    26.8    477 No      male    large  \n10 English                     131     54    70.8    185 Yes     female  large  \n# … with 17 more rows, and abbreviated variable names ¹​nfemales, ²​percfemale,\n#   ³​morewomen, ⁴​large_majority, ⁵​major_size\n\n\n\n* Investigate what happens with case_when() when you give overlapping conditions and when you give conditions that don’t cover all observations. For overlapping conditions, create a variable testcase that is \"Yes\" when percfemale is greater than or equal to 40 and \"No\" when percfemale is greater than 60 For conditions that don’t cover all observations, create a variable testcase2 that is \"Yes\" when percefemale is greater than or equal to 55 and \"No\" when percfemale is less than 35.\n\n\n\n# A tibble: 27 × 9\n   Major           nfema…¹ nmales percf…² ntotal morew…³ large…⁴ testc…⁵ testc…⁶\n   <chr>             <dbl>  <dbl>   <dbl>  <dbl> <chr>   <chr>   <chr>   <chr>  \n 1 Anthropology         34     15    69.4     49 Yes     none    Yes     Yes    \n 2 Art & Art Hist…      65     11    85.5     76 Yes     female  Yes     Yes    \n 3 Biochemistry         14     11    56       25 Yes     none    Yes     Yes    \n 4 Biology             162     67    70.7    229 Yes     female  Yes     Yes    \n 5 Business in th…     135    251    35.0    386 No      none    <NA>    No     \n 6 Chemistry            26     14    65       40 Yes     none    Yes     Yes    \n 7 Computer Scien…      21     47    30.9     68 No      none    <NA>    No     \n 8 Conservation B…      38     20    65.5     58 Yes     none    Yes     Yes    \n 9 Economics           128    349    26.8    477 No      male    <NA>    No     \n10 English             131     54    70.8    185 Yes     female  Yes     Yes    \n# … with 17 more rows, and abbreviated variable names ¹​nfemales, ²​percfemale,\n#   ³​morewomen, ⁴​large_majority, ⁵​testcase, ⁶​testcase2\n\n\nFor overlapping cases, case_when prioritizes the first case given.\nFor non-coverage, any observation that is not covered is given an NA.\n\n\n4.7.2 arrange(), select(), …. S\n\n* Use select() and everything() to put the large_majority variable as the first column in the slumajors_df data set.\n\n\nslumajors_df |> select(large_majority, everything())\n\n# A tibble: 27 × 7\n   large_majority Major                    nfema…¹ nmales percf…² ntotal morew…³\n   <chr>          <chr>                      <dbl>  <dbl>   <dbl>  <dbl> <chr>  \n 1 none           Anthropology                  34     15    69.4     49 Yes    \n 2 female         Art & Art History             65     11    85.5     76 Yes    \n 3 none           Biochemistry                  14     11    56       25 Yes    \n 4 female         Biology                      162     67    70.7    229 Yes    \n 5 none           Business in the Liberal…     135    251    35.0    386 No     \n 6 none           Chemistry                     26     14    65       40 Yes    \n 7 none           Computer Science              21     47    30.9     68 No     \n 8 none           Conservation Biology          38     20    65.5     58 Yes    \n 9 male           Economics                    128    349    26.8    477 No     \n10 female         English                      131     54    70.8    185 Yes    \n# … with 17 more rows, and abbreviated variable names ¹​nfemales, ²​percfemale,\n#   ³​morewomen\n\n\n\n* In the babynames data set, use filter(), mutate() with rank(), and arrange() to print the 10 most popular Male babynames in 2017.\n\n\nbabynames |> filter(sex == \"M\" & year == 2017) |>\n  mutate(rankname = rank(desc(n))) |>\n  filter(rankname <= 10)\n\n# A tibble: 10 × 6\n    year sex   name         n    prop rankname\n   <dbl> <chr> <chr>    <int>   <dbl>    <dbl>\n 1  2017 M     Liam     18728 0.00954        1\n 2  2017 M     Noah     18326 0.00933        2\n 3  2017 M     William  14904 0.00759        3\n 4  2017 M     James    14232 0.00725        4\n 5  2017 M     Logan    13974 0.00712        5\n 6  2017 M     Benjamin 13733 0.00699        6\n 7  2017 M     Mason    13502 0.00688        7\n 8  2017 M     Elijah   13268 0.00676        8\n 9  2017 M     Oliver   13141 0.00669        9\n10  2017 M     Jacob    13106 0.00668       10\n\n\n\n\n4.7.3 summarise() and group_by() S\n\n* Create a data set that has a column for name and a column that shows the total number of births for that name across all years and both sexes.\n\n\nbabynames |> group_by(name) |>\n  summarise(totalbirths = sum(n))\n\n# A tibble: 97,310 × 2\n   name      totalbirths\n   <chr>           <int>\n 1 Aaban             107\n 2 Aabha              35\n 3 Aabid              10\n 4 Aabir               5\n 5 Aabriella          32\n 6 Aada                5\n 7 Aadam             254\n 8 Aadan             130\n 9 Aadarsh           199\n10 Aaden            4658\n# … with 97,300 more rows\n\n\n\n* group_by() can also be used with other functions, including mutate(). Use group_by() and mutate() to rank the names from most to least popular in each year-sex combination.\n\n\nranked_babynames <- babynames |> group_by(year, sex) |>\n  mutate(rankname = rank((desc(n))))\n\n\n* From the data set in 4, filter() the data to keep only the most popular name in each year-sex combination and then construct a summary table showing how many times each name appears as the most popular name.\n\n\nranked_babynames |> filter(rankname == 1) |>\n  group_by(name) |>\n  summarise(nappear = n()) |>\n  arrange(desc(nappear))\n\n# A tibble: 18 × 2\n   name     nappear\n   <chr>      <int>\n 1 Mary          76\n 2 John          44\n 3 Michael       44\n 4 Robert        17\n 5 Jennifer      15\n 6 Jacob         14\n 7 James         13\n 8 Emily         12\n 9 Jessica        9\n10 Lisa           8\n11 Linda          6\n12 Emma           5\n13 Noah           4\n14 Sophia         3\n15 Ashley         2\n16 Isabella       2\n17 David          1\n18 Liam           1\n\n\n\n* Run the following code. Intuitively, a slice(1, 2, 3, 4, 5) should grab the first five rows of the data set, but, when we try to run that, we get 1380 rows. Try to figure out what the issue is by using Google to search something like “dplyr not slicing correctly after using group by.” What do you find?\n\n\nbabynames_test <- babynames |>\n  group_by(year, sex) |> mutate(ntest = n / prop)\nbabynames_test |> slice(1, 2, 3, 4, 5)\n\n# A tibble: 1,380 × 6\n# Groups:   year, sex [276]\n    year sex   name          n   prop   ntest\n   <dbl> <chr> <chr>     <int>  <dbl>   <dbl>\n 1  1880 F     Mary       7065 0.0724  97605.\n 2  1880 F     Anna       2604 0.0267  97605.\n 3  1880 F     Emma       2003 0.0205  97605.\n 4  1880 F     Elizabeth  1939 0.0199  97605.\n 5  1880 F     Minnie     1746 0.0179  97605.\n 6  1880 M     John       9655 0.0815 118400.\n 7  1880 M     William    9532 0.0805 118400.\n 8  1880 M     James      5927 0.0501 118400.\n 9  1880 M     Charles    5348 0.0452 118400.\n10  1880 M     George     5126 0.0433 118400.\n# … with 1,370 more rows\n\n\nFunctions like slice() and rank() operate on defined groups in the data set if using a function like group_by() first. Sometimes this feature is quite convenient. But, if we no longer want slice() or rank() or other functions to account for these groups, we need to add an ungroup() pipe, which simply drops the groups that we had formed:\n\nbabynames_test |> ungroup() |> slice(1:5)\n\n# A tibble: 5 × 6\n   year sex   name          n   prop  ntest\n  <dbl> <chr> <chr>     <int>  <dbl>  <dbl>\n1  1880 F     Mary       7065 0.0724 97605.\n2  1880 F     Anna       2604 0.0267 97605.\n3  1880 F     Emma       2003 0.0205 97605.\n4  1880 F     Elizabeth  1939 0.0199 97605.\n5  1880 F     Minnie     1746 0.0179 97605.\n\n\n\n\n4.7.4 Missing Values S\n\n* mutate(). Try to create a new variable with mutate() involving x. What does R do with the missing value?\n\n\ntoy_df |> mutate(xy = x * y)\n\n# A tibble: 4 × 5\n      x     y z     newvar    xy\n  <dbl> <dbl> <chr>  <dbl> <dbl>\n1    NA     1 A      NA       NA\n2     3     4 A       0.75    12\n3     4     3 B       1.33    12\n4     7     2 <NA>    3.5     14\n\n\nR puts another NA in place of x times y for the observation with the missing x.\n\n\n4.7.5 Piping S\n\n* Recode the following to look cleaner by using the pipe |>:\n\n\nfitness_df <- read_csv(\"data/higham_fitness_clean.csv\")\n\nRows: 993 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): month, weekday\ndbl  (6): active_cals, distance, flights, steps, dayofyear, stepgoal\ndate (1): Start\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsummarise(group_by(filter(fitness_df, weekday == 1 | weekday == 7),\n                   month),\n          meanweekend = mean(distance, na.rm = TRUE)) \n\n# A tibble: 0 × 2\n# … with 2 variables: month <chr>, meanweekend <dbl>\n\n\n\nfitness_df |> filter(weekday == 1 | weekday == 7) |>\n  group_by(month) |>\n  summarise(meanweekend = mean(distance, na.rm = TRUE)) \n\n# A tibble: 0 × 2\n# … with 2 variables: month <chr>, meanweekend <dbl>\n\n\n\n\n4.7.6 Chapter Exercises S\n\n* a. Choose 5 names that interest you and create a new data set that only has data on those 5 names.\n\n\nUse group_by() and summarise() to add together the number of Females and Males for each name in each year. Hint: you can group_by() more than one variable!\nMake a line plot showing the popularity of these 5 names over time.\n\n\nbaby5 <- babynames |> filter(name == \"Matthew\" | name == \"Ivan\" |\n                                name == \"Jessica\" | name == \"Robin\" |\n                                name == \"Michael\")\nbaby5_tot <- baby5 |> group_by(year, name) |>\n  summarise(ntot = sum(n))\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\nggplot(data = baby5_tot, aes(x = year, y = ntot, colour = name)) +\n  geom_line()\n\n\n\n\n\n* In some cases throughout this chapter, we’ve renamed data sets using <- with the same name like\n\n\ntoy_df <- toy_df |> mutate(newvar = x / y)\n\nIn other cases, we’ve given the data set a new name, like\n\ntoy_small <- toy_df |> filter(!is.na(x))\n\nFor which of the functions below is a generally “safe” to name the data set using the same name after using the function. Why?\n\nmutate()\n\nUsually fine: mutating creates a new variable, which doesn’t change any of the other variables in the data set, if things get messed up with the new variable.\n\narrange()\n\nUsually fine: ordering the rows a certain way won’t change any plots and doesn’t change any of the underlying data.\n\nfilter()\n\nUsually not the best practice. Naming the data set the same name after the filter means that you permanently lose data that you filtered out, unless you re-read in the data set at the beginning.\n\nsummarise()\n\nUsually not the best practice. Again, naming the summarized data set the same as the original data means that you lose the original data, unless you re-read it in at the beginning. For example,\n\ntoy_df <- toy_df |> summarise(meanx = mean(x))\ntoy_df\n\n# A tibble: 1 × 1\n  meanx\n  <dbl>\n1    NA\n\n\nmeans that we now have no way to access the original data in toy_df.\n\nselect()\n\nThis can sometimes be okay if you’re sure that the variables you are removing won’t ever be used."
  },
  {
    "objectID": "03-dplyr.html#rcode-3",
    "href": "03-dplyr.html#rcode-3",
    "title": "4  Wrangling with dplyr",
    "section": "4.8 Non-Exercise R Code",
    "text": "4.8 Non-Exercise R Code\n\nlibrary(babynames)\nhead(babynames)\nlibrary(tidyverse)\nslumajors_df <- read_csv(\"data/SLU_Majors_15_19.csv\")\nslumajors_df\nslumajors_df |> mutate(ntotal = nfemales + nmales)\nslumajors_df |>\n  mutate(percfemale = 100 * nfemales / (nfemales + nmales))\nslumajors_df <- slumajors_df |>\n  mutate(percfemale = 100 * nfemales / (nfemales + nmales))\nslumajors_df <- slumajors_df |> mutate(ntotal = nfemales + nmales)\nslumajors_df <- slumajors_df |>\n  mutate(ntotal = nfemales + nmales) |>\n  mutate(percfemale = 100 * nfemales / (nfemales + nmales))\nslumajors_df |> mutate(morewomen = if_else(percfemale > 50,\n                                            true = \"Yes\",\n                                            false = \"No\"))\nslumajors_df |> mutate(large_majority =\n                          case_when(percfemale >= 70 ~ \"female\",\n                                    percfemale <= 30 ~ \"male\",\n                                    percfemale > 30 & percfemale < 70 ~ \"none\")) \nslumajors_df <- slumajors_df |>\n  mutate(morewomen = if_else(percfemale > 50,\n                             true = \"Yes\",\n                             false = \"No\")) |>\n  mutate(large_majority =\n           case_when(percfemale >= 70 ~ \"female\",\n                     percfemale <= 30 ~ \"male\",\n                     percfemale > 30 & percfemale < 70 ~ \"none\")) \nslumajors_df |> arrange(percfemale)\nslumajors_df |> arrange(desc(percfemale))\nslumajors_df |> select(Major, ntotal)\nslumajors_df |> select(-ntotal, -nfemales, -nmales)\nslumajors_df |> mutate(propfemale = percfemale / 100) |>\n  select(propfemale, everything())\nslumajors_df |> arrange(desc(ntotal)) |>\n  slice(1, 2, 3, 4, 5)\nlibrary(babynames)\nbabynames\nbabynames |> filter(name == \"Matthew\")\nbabynames |> filter(year >= 2000)\nbabynames |> filter(sex != \"M\")\nbabynames |> filter(prop > 0.05)\nbabynames |> filter(year == max(year))\nbabynames |> filter(n > 20000 | prop > 0.05)\nbabynames |> filter(sex == \"F\" & name == \"Mary\")\nbabynames |> filter(sex == \"F\" & name == \"Mary\" & prop > 0.05)\nslumajors_df |>\n  summarise(meantotalmajor = mean(ntotal),\n            totalgrad = sum(ntotal))\nbabynames |> group_by(year) |>\n  summarise(totalbirths = sum(n))\nbabynames |> summarise(totalobs = n())\nbabynames |> group_by(year) |>\n  summarise(ngroup = n())\ntoy_df <- tibble(x = c(NA, 3, 4, 7),\n                 y = c(1, 4, 3, 2),\n                 z = c(\"A\", \"A\", \"B\", NA))\ntoy_df\ntoy_df |> summarise(meanx = mean(x, na.rm = TRUE))\ntoy_df |> mutate(missingx = is.na(x))\ntoy_df |> filter(is.na(x) != TRUE)\ntoy_df |> filter(!is.na(x))\nvideogame_df |> filter(!is.na(metascore))\nfilter(videogame_df, !is.na(metascore))\nvideogame_df |> filter(!is.na(metascore)) |>\n  filter(median_playtime > 0) |>\n  group_by(price_cat) |>\n  summarise(avg_med_time = median(median_playtime, na.rm = TRUE))\nfilter(videogame_df, !is.na(metascore))\nfilter(filter(videogame_df, !is.na(metascore)), median_playtime > 0)\ngroup_by(filter(filter(videogame_df, !is.na(metascore)),\n                median_playtime > 0), price_cat)\nsummarise(group_by(filter(filter(videogame_df, !is.na(metascore)),\n  median_playtime > 0), price_cat), \n  avg_med_time = median(median_playtime, na.rm = TRUE))"
  },
  {
    "objectID": "05-comm.html",
    "href": "05-comm.html",
    "title": "5  Communication with Quarto",
    "section": "",
    "text": "Special Note: Quarto and .qmd files and R Markdown and .qmd files are extremely similar. In the previous version of this section, I used R Markdown. Quarto has an advantage in that .qmd files also work with Python and Julia, so they are generally better for an all-purpose data scientist. If you spot any references to R Markdown or a .qmd file, just mentally convert R Markdown to Quarto and .rmd to qmd.\nGoals:\nOverall: If you’re making some quick plots just for you, some of the things on communication won’t apply. But, if you’re planning on sharing results (usually you are, eventually), then communication tools become much more important."
  },
  {
    "objectID": "05-comm.html#reproducbility",
    "href": "05-comm.html#reproducbility",
    "title": "4  Communication with Quarto",
    "section": "4.1 Reproducbility",
    "text": "4.1 Reproducbility\nWe’ve been using Quarto for a while now, but have not yet talked about any of its features or how to do anything except insert a new code chunk. By the end of this section, we want to be able to use some of the Quarto options to make a nice-looking document (so that you can implement some of these options in your first mini-project).\nReproducibility is a concept that has recently gained popularity in the sciences for describing analyses that another researcher is able to repeat. That is, an analysis is reproducible if you provide enough information that the person sitting next to you can obtain identical results as long as they follow your procedures. An analysis is not reproducible if this isn’t the case. Quarto makes it easy for you to make your analysis reproducible for a couple of reasons:\n\nan Quarto file will not render unless all of your code runs, meaning that you won’t accidentally give someone code that doesn’t work.\nQuarto combines the “coding” steps with the “write-up” steps into one coherent document that contains the code, all figures and tables, and any explanations.\n\n\n4.1.1 R Scripts vs. Quarto\nWe’ve been using Quarto for the entirety of this course. But, you may have noticed that when you go to File -> New File to open a new Quarto Document file, there are a ton of other options. The first option is R Script. Go ahead and open a new R Script file now.\nThe file you open should be completely blank. An R Script is a file that reads only R code. It cannot have any text in it at all, unless that text is commented out with a #. For example, you could copy and paste all of the code that is inside a code chunk in a .qmd file to the .R file and run it line by line.\nSo, what are the advantages and disadvantages of using an R Script file compared to using an Quarto file? Let’s start with the advantages of Quarto. Quarto allows you to fully integrate text explanations of the code and results, the actual tables and figures themselves, and the code to make those tables and figures in one cohesive document. As we will see, if using R Scripts to write-up an analysis in Word, there is a lot of copy-pasting involved of results. For this reason, using Quarto often results in more reproducible analyses.\nThe advantage of an R Script would be in a situation where you really aren’t presenting results to anyone and you also don’t need any text explanations. This often occurs in two situations. (1) There are a lot of data preparation steps. In this case, you would typically complete all of these data prep steps in an R script and then write the resulting clean data to a .csv that you’d import in an Quarto file. (2) What you’re doing is complicated statistically. If this is the case, then the code is much more of a focus than the text or creating figures so you’d use an R Script.\nWe will “demo” a reproducible analysis in class.\n\n\n4.1.2 Spell-Checking\nIf using Quarto for communication, you probably want to utilize its spell-check feature. Go to Edit -> Check Spelling, and you’ll be presented with a spell-checker that lets you change the spelling of any words you may have misspelled.\n\n\n4.1.3 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-5).\n\nWhat’s the difference between R and Quarto?\n\n\n\n\n\nWhy is an Quarto analysis more reproducible than the base R script analysis?\n\n\n\n\n\nWhy is an Quarto analysis easier to make more reproducible than an analysis with Excel?\n\n\n\n\n\nYour friend Chaz is doing a data analysis project in Excel to compare the average GPA of student athletes with the average GPA of non-student athletes. He has two variables: whether or not a student is a student athlete and GPA. He decides that a two-sample t-test is an appropriate procedure for this data (recall from Intro Stat that this procedure is appropriate for comparing a quantitative response (GPA) across two groups). Here are the steps of his analysis.\n\n\nHe writes the null and alternative hypotheses in words and in statistical notation.\nHe uses Excel to make a set of side-by-side boxplots. He changes the labels and the limits on the y-axis using Point-and-Click Excel operations.\nFrom his boxplots, he see that there are 3 outliers in the non-athlete group. These three students have GPAs of 0 because they were suspended for repeatedly refusing to wear masks indoors. Chaz decides that these 3 students should be removed from the analysis because, if they had stayed enrolled, their GPAs would have been different than 0. He deletes these 3 rows in Excel.\nChaz uses the t.test function in Excel to run the test. He writes down the degrees of freedom, the T-stat, and the p-value.\nChaz copies his graph to Word and writes a conclusion in context of the problem.\n\nState 2 aspects of Chaz’s analysis that are not reproducible."
  },
  {
    "objectID": "05-comm.html#quarto-files",
    "href": "05-comm.html#quarto-files",
    "title": "5  Communication with Quarto",
    "section": "5.2 Quarto Files",
    "text": "5.2 Quarto Files\nLet’s talk a bit more about the components of the Quarto file used to make the reproducible analysis shown in class.\nFirst, open a new Quarto file by clicking File -> New File -> Quarto Document and keep the new file so that it renders to HTML for now.\nThe first four to five lines at the top of the file make up the YAML (Yet Another Markup Language) header. We’ll come back to this at the end, as it’s the more frustrating part to learn.\nDelete the code below the YAML header and then paste the following code chunks to your clean .qmd file:\n\nlibrary(tidyverse)\nhead(cars)\nggplot(data = cars, aes(x = speed, y = dist)) +\n  geom_point()\n\n\nsummary(cars)\n\nThe cars data set is built into R so there’s no need to do anything to read it in (it already exists in R itself).\n\n5.2.1 Code Chunk Options\nFirst, render your new file (and give it a name, when prompted). You should see some code, a couple of results tables, and a scatterplot.\nChunk options allow you to have some control over what gets printed to the file that you render. For example, you may or may not want: the code to be printed, the figure to be printed, the tables to be printed, the tidyverse message to be printed, etc. There are a ton of chunk options to give us control over the code and output that is shown! We are going to just focus on a few that are more commonly used.\nThe options below are common execute options in Quarto.\n\necho. This is set to either true to print the code or false to not print the code. In a blank line after ```{r}, insert the following, which tells Quarto not to print the code in that chunk: #| echo: false. Then, re-render your document to make sure that the code making the plot is actually hidden in the .html output.\n\nYou can keep adding other options on new lines. Some other options include:\n\nwarning. This is set to either true to print warnings and messages or false to not print warnings and messages. For example, when we load in the tidyverse, a message automatically prints out. In that same code chunk, add a new line with #| warning: false to get rid of the message. Re-render to make sure the message is actually gone.\noutput. By default, this is set to true and shows output of tables and figures. Change this to false to not print any output from running code. Practice adding a #| output: false to the code chunk in your Quarto file with summary(cars) and re-render to make sure the output from summary(cars) is gone.\neval. eval is set to true if the code is to be evaluated and false if not.\n\nBesides execute options, there are also options pertaining to the size of figures and figure captions. Some common examples include\n\nfig-height and fig-width control the height and width of figures. By default, these are both 7, but we can change the fig-height and fig-width to make figures take up less space in the rendered .html document (fig-height: 5, for example).\nfig-cap adds a figure caption to your figure. Try inserting #| fig-cap: \"Figure 1: caption text blah blah blah\" to your chunk options for the chunk with the plot.\n\n\n\n5.2.2 Global Options\nWhat we have discussed so far is how to change code and output options for individual chunks of code. But, it can be a pain to add a certain option to every single chunk of code that we want the option to apply to. What we can do instead is change the global option for the code and/or output options so that they apply to all code chunks, unless specifically overwritten in that chunk.\nWe can change the execute options (echo, warning, eval, output, and more) globally by adding a line to the YAML header at the top of your Quarto file. Try adding\nexecute: \n  echo: false\nas two new lines in between the title and format lines of the YAML header. This tells Quarto to not echo any code in any code chunk. However, note that you can change a local code chunk to have #| echo: true to override the global setting for that chunk.\nAdditional global execute options go on new lines:\nexecute: \n  echo: false\n  warning: false\nThe non-execute options pertaining to figure size can be changed globally by specifying the option after the html part of the YAML header. The following changes all figure heights to be 2, unless a chunk overrides the global setting:\n---\ntitle: \"Quarto Test\"\nexecute: \n  echo: false\nformat: \n  html:\n    fig-height: 2\n---\nImportant Note: We need to pay particular attention to the spacing of things in the YAML header. Notice, for example, that echo: false is indented by exactly two spaces. Try adding a space or deleting a space, and you’ll get an error!\n\n\n5.2.3 Figures and Tables\nWe’ve already seen that Figures will pop up automatically (unless we set output: false), which is quite convenient. Making tables that look nice requires one extra step.\nDelete the output: false option that you added earlier to the chunk with summary(cars). When you render your .qmd file now, results tables from head(cars) and summary(cars) look kind of ugly. We will focus on using the kable() function from the knitr package to make these tables much more aesthetically pleasing. Another option is to use the pander() function in the pander package. Both pander() and kable() are very simple functions to generate tables but will be more than sufficient for our purposes. To generate more complicated tables, see the xtable package.\nTo use these functions, simply add a |> pipe with the name of the table function you want to use. head(cars) |> kable() will make a nice-looking table with kable and head(cars) |> pander() will use pander(). Before using kable() from the knitr package, you’ll need to install the knitr package with install.packages(\"knitr\") and load its library by adding the line library(knitr) above head(cars) |> kable(). Before using pander(), you’ll need to install the pander package with install.packages(\"pander\") and then load its library by adding the line library(pander) above head(cars) |> pander(). Try these out in your Quarto file.\nWhich table do you like better in this case?\nThere are plenty of options for making tables look presentable, which we will discuss in the Exercises. Keep in mind that you probably wouldn’t use these when making tables for yourself. They’re much more useful when you’re writing a report that you want to share with others.\n\n\n5.2.4 Non-Code Options\nQuarto combines R (in the code chunks, which we’ve already discussed) with the Markdown syntax, which comprises the stuff outside the code chunks, like what you’re reading right now!\nThere are so many Markdown options, but most of the time, if you want to do something specific, you can just Google it. The purpose of what follows is just to get us familiar with the very basics and things you will probably use most often.\nBullet Points and Sub-bullet Points: Denoted with a * and -, respectively. The sub bullets should be indented by 4 spaces. Note that bullet points are not code and should not appear in a code chunk.\n* Bullet 1\n* Bullet 2\n    - Sub bullet 1\n    - Sub bullet 2\n    - Sub bullet 3\n\nNote: Everything in Markdown is very particular with spacing. Things often have to be very precise. I personally just love it, but it can be frustrating sometimes. For example, indenting a sub-bullet by 3 spaces instead of 4 spaces will not make a sub-bullet.\n* Bullet 1\n   - Sub bullet 1\nNumbered Lists are the same as bulleted ones, except * is replaced with numbers 1., 2., etc.\nBold, Italics, Code. Surround text with __bold text__ to make text bold, _italic text_ to make text Italics, and backticks to make text look like Code.\nLinks: The simplest way to create a link to something on the web is to surround it with < > as in <https://www.youtube.com/watch?v=gJf_DDAfDXs>\nIf you want to name you link something other than the web address, use [name of link](https://www.youtube.com/watch?v=gJf_DDAfDXs), which should show up in your rendered document as “name of link” and, when clicked on, take you to the youtube video.\nHeaders: Headers are created with ## with fewer hashtags resulting in a bigger Header. Typing in #Big Header at the beginning of a line would make a big header, ### Medium Header would make a medium header, and ##### Small Header would make a small header. Headers are important because they get mapped to a table of contents.\nThere’s a lot of other stuff to explore: <a href=“https://rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf” target=“blank> https://rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf .\nBut, if you want to do something other than the basics, Google will definitely help.\n\n\n5.2.5 YAML\nWe have briefly discussed what’s given at the top of every .qmd file: the YAML header. The YAML header is the most frustrating part to change because it’s the most particular with spacing.\nIn addition to controlling global chunk options, we can also use the YAML header to specify a theme from  Bootswatch.\nThere are 25 themes in the Bootswatch project. The YAML header below uses the darkly theme. Try pasting in this YAML header and rendering the document to see the outputted theme.\n---\ntitle: \"Quarto Test\"\nexecute: \n  echo: false\n  warning: false\nformat: \n  html:\n    fig-height: 2\n    theme: darkly\n    self-contained: true\n---\nWe can also add a table of contents, which will create a table of contents based on headers you have created with ##.\n---\ntitle: \"Quarto Test\"\nexecute: \n  echo: false\n  warning: false\nformat: \n  html:\n    fig-height: 2\n    theme: darkly\n    toc: true\n    self-contained: true\n---\nThere are so many other options available for theming, and, if you know any css, you can provide your own .css file to customize the theme.\n\n\n5.2.6 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-4).\nFor the rest of this section, we will use the built-in R data set mtcars, which has observations on makes and models of cars. The variables we will be using are:\n\ncyl, the number of cylinders a car has\nmpg, the mileage of the car, in miles per gallon\n\nBecause the data set is loaded every time R is started up, there is no need to have a line that reads in the data set. We can examine the first few observations with\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\n* Create a table showing the mean mpg for each cyl group (cyl stands for cylinder and can be 4-cylinder, 6-cylinder, or 8-cylinder) with both kable() and pander(). Hint: remember to call the knitr library and the pander library.\n* Type ?kable into your console window and scroll through the Help file. Change the rounding of the mean so that it only displays one number after the decimal. Then, add a caption to the table that says “My First Table Caption!!”\n* Google “How to Change Column Names in kable” and replace the column names with “Cylinder Numb.” and “Mean Mileage”.\nFind a table that you plan to use in your first mini-project. Use the column names, caption, and digits options to make this table look nicer with the kable() function.\n\n\n\n\n\nCreate a new R chunk and copy and paste the following into your new R chunk. Don’t worry about what factor() is doing: we will cover that next week!\n\n\nlibrary(tidyverse)\nhead(mtcars)\nggplot(data = mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_boxplot()\n\nModify the R chunk so that: (a) the figure height is 3, (b) the code from the R chunk shows in the .html file, (c) the table from running head(cars) are hidden in the .html file. Make (b) and (c) a local chunk option, but set (a) as a global option that applies to all of your R chunks.\n\n\n\n\nChange your global options in your first project to (a) hide messages from loading in the tidyverse and (b) show all code.\nUse bullet points in the Introduction in your first mini-project that explains what a few of the important variables are. Then, add a header to your project that marks the Introduction.\nChange the YAML header in your project so that you are the Author (with something like author: \"Your Name\" and so that the file uses a theme from Bootswatch other than the default theme."
  },
  {
    "objectID": "05-comm.html#ggplot2-communication",
    "href": "05-comm.html#ggplot2-communication",
    "title": "5  Communication with Quarto",
    "section": "5.3 ggplot2 Communication",
    "text": "5.3 ggplot2 Communication\nWhen we first introduced plotting, we used histograms, boxplots, frequency plots, bar plots, scatterplots, line plots, and more to help us explore our data set. You will probably make many different plots in a single analysis, and, when exploring, it’s fine to keep these plots unlabeled and untitled with the default colour scheme and theme. They’re just for you, and you typically understand the data and what each variable means.\nHowever, when you’ve finished exploring and you’d like to communicate your results, both graphically and numerically, you’ll likely want to tweak your plots to look more aesthetically pleasing. You certainly wouldn’t be presenting every exploratory plot you made so this tweaking needs to be done on only a few plots. You might consider:\n\nchanging the x-axis and y-axis labels, changing the legend title, adding a title, adding a subtitle, and adding a caption with + labs()\nchanging the limits of the x-axis and y-axis with + xlim() and + ylim()\nchanging the colour scheme to be more visually appealing and easy to see for people with colour-vision-deficiency (CVD)\nlabeling certain points or lines with + geom_label() or + geom_text()\nchanging from the default theme with + theme_<name_of_theme>()\n\nThe bullet about labeling only certain points is the data set is one reason why we are doing this second ggplot2 section now, as opposed to immediately after the first ggplot2 section. As we will see, we’ll make use of combining what we’ve learned in dplyr to help us label interesting observations in our plots.\nThe Data\nThe Happy Planet Index (HPI) is a measure of how efficiently a country uses its ecological resources to give its citizens long “happy” lives. You can read more about this data here:  here.\nBut, the basic idea is that the HPI is a metric that computes how happy and healthy a country’s citizens are, but adjusts that by that country’s ecological footprint (how much “damage” the country does to planet Earth). The data set was obtained from  https://github.com/aepoetry/happy_planet_index_2016. Variables in the data set are:\n\nHPIRank, the rank of the country’s Happy Planet Index (lower is better)\nCountry, the name of the country\nLifeExpectancy, the average life expectancy of a citizen (in years)\nWellbeing, the average well being score (on a scale from 1 - 10). See the ladder question in the documentation for how this was calculated.\nHappyLifeYears, a combination of LifeExpectancy and Wellbeing\nFootprint, the ecological footprint per person (higher footprint means the average person in the country is less ecologically friendly)\n\nRead in the data set with\n\nlibrary(tidyverse)\nhpi_df <- read_csv(\"data/hpi-tidy.csv\")\nhead(hpi_df)\n\n# A tibble: 6 × 11\n  HPIRank Country     LifeExpe…¹ Wellb…² Happy…³ Footp…⁴ Happy…⁵ Popul…⁶ GDPca…⁷\n    <dbl> <chr>            <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1     109 Afghanistan       48.7    4.76    29.0   0.540    36.8  3.44e7   1207.\n2      18 Albania           76.9    5.27    48.8   1.81     54.1  3.20e6   8592.\n3      26 Algeria           73.1    5.24    46.2   1.65     52.2  3.55e7   8433.\n4     127 Angola            51.1    4.21    28.2   0.891    33.2  1.91e7   6186.\n5      17 Argentina         75.9    6.44    55.0   2.71     54.1  4.04e7  16012.\n6      53 Armenia           74.2    4.37    41.9   1.73     46.0  3.09e6   5463.\n# … with 2 more variables: GovernanceRank <chr>, Region <chr>, and abbreviated\n#   variable names ¹​LifeExpectancy, ²​Wellbeing, ³​HappyLifeYears, ⁴​Footprint,\n#   ⁵​HappyPlanetIndex, ⁶​Population, ⁷​GDPcapita\n\n\nLet’s look at the relationship between HappyLifeYears and Footprint for countries of different Regions of the world.\n\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point()\n\n\n\n\nWhich region seems to have the most variability in their Ecological Footprint?\n\n5.3.1 Change Labels and Titles\nWe can add + labs() to change various labels and titles throughout the plot:\n\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  labs(title = \"Countries with a Higher Ecological Footprint Tend to Have Citizens with Longer, Happier Lives\", \n       ## add title\n       subtitle = \"HappyLifeYears is a Combination of Life Expectancy and Citizen Well-Being\", \n       ## add subtitle (smaller text size than the title)\n       caption = \"Data Source: http://happyplanetindex.org/countries\", \n       ## add caption to the bottom of the figure\n       x = \"Ecological Footprint\", ## change x axis label\n       y = \"Happy Life Years\", ## change y axis label\n       colour = \"World Region\") ## change label of colour legend\n\n\n\n\nAny aes() that you use in your plot gets its own label and can be changed by name_of_aethetic = \"Your Label\". In the example above, we changed all three aes() labels: x, y, and colour.\nWhat is the only text on the plot that we aren’t able to change with labs()?\n\n\n5.3.2 Changing x and y axis Limits\nWe can also change the x-axis limits and the y-axis limits to, for example, start at 0 for the y-axis:\n\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  ylim(c(0, 70))\n\n\n\n\nIn this case, it makes the points on the plot a bit harder to see. You can also change where and how often tick marks appear on the x and y-axes. For special things like this, I think it’s best to just resort to Google (“ggplot how to change x-axis breaks tick marks” should help).\n\n\n5.3.3 Changing A Colour Scale\nWe want to use our graphics to communicate with others as clearly as possible. We also want to be as inclusive as possible in our communications. This means that, if we choose to use colour, our graphics should be made so that a colour-vision-deficient (CVD) person can read our graphs. About 4.5% of people are colour vision deficient, so it’s actually quite likely that a CVD person will view the graphics that you make (depending on how many people you share it with) More Information on CVD.\nThe colour scales from R Colour Brewer are readable for common types of CVD. A list of scales can be found here.\nYou would typically use the top scales if the variable you are colouring by is ordered sequentially (called seq for sequential, like grades in a course: A, B, C, D, F), the bottom scales if the variable is diverging (called div for diverging, like Republican / Democrat lean so that the middle is colourless), and the middle set of scales if the variable is not unordered and is categorical (called qual for qualitative like the names of different treatment drugs for a medical experiment).\nIn which of those 3 situations are we in for the World Region graph?\nIf we want to use one of these colour scales, we just need to add scale_colour_brewer() with the name of the scale we want to use.\n\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Accent\")\n\n\n\n\nTry changing the palette to something else besides \"Accent\". Do you like the new palette better or worse?\nOne more option to easily change the colour scale is to use the viridis package. The base viridis functions automatically load with ggplot2 so there’s no need to call the package with library(viridis). The viridis colour scales were made to be both aesthetically pleasing and CVD-friendly.\n\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_viridis_d(option = \"plasma\")\n\n\n\n\nA drawback of the viridis package is that the yellow can be really hard to see (at least for me).\nRead the examples section of the Help file for ?scale_colour_viridis_d. What’s the difference between scale_colour_viridis_d(), ?scale_colour_viridis_c(), and scale_colour_viridis_b()?\nWhich do you like better: the Colour Brewer scale or the Viridis scale?\n\n\n5.3.4 Labeling Points or Lines of Interest\nOne goal we might have with communication is highlighting particular points in a data set that show something interesting. For example, we might want to label the points on the graph corresponding to the countries with the highest HPI in each region: these countries are doing the best in terms of using resources efficiently to maximize citizen happiness. Or, we might want to highlight some “bad” example of countries that are the least efficient in each region. Or, we might want to label the country that we are from on the graph.\nAll of this can be done with geom_label(). Let’s start by labeling all of the points. geom_label() needs one aesthetic called label which is the name of the column in the data set with the labels you want to use.\n\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_label(aes(label = Country))\n\n\n\n\nYikes! It’s quite uncommon to want to label all of the points. Let’s see if we can instead label each country with the best HPI in that country’s region. To do so, we first need to use our dplyr skills to create a new data set that has these 7 “best” countries. When we used group_by(), we typically used summarise() afterward. But, group_by() works with filter() as well!\n\nplot_df <- hpi_df |> group_by(Region) |>\n  filter(HPIRank == min(HPIRank))\n\nWhat is the code in the previous chunk doing?\nNow that we have this new data set, we can use it within geom_label(). Recall that the data = argument in ggplot() carries on through all geoms unless we specify otherwise. Now is our chance to “specify otherwise” by including another data = argument within geom_label():\n\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_label(data = plot_df, aes(label = Country))\n\n\n\n\nWhy do you think the colour legend changed to showing the letter “a” for each region?\n\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point(aes(colour = Region)) +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_label(data = plot_df, aes(label = Country), show.legend = FALSE)\n\n\n\n\nWhy does the code chunk above change all of the “a”’s back to points?\nA common issue, even with few labels, is that some of the labels could overlap. The ggrepel package solves this problem by including a geom_label_repel() geom that automatically repels any overlapping labels:\n\nlibrary(ggrepel)\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_label_repel(data = plot_df, aes(label = Country),\n                   show.legend = FALSE) \n\n\n\n\nAnd a final issue with the plot is that it’s not always very clear which point on the plot is being labeled. A trick used in the R for Data Science book is to surround the points that are being labeled with an open circle using an extra geom_point() function:\n\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears, colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_label_repel(data = plot_df, aes(label = Country), show.legend = FALSE) +\n  geom_point(data = plot_df, size = 3, shape = 1, show.legend = FALSE) \n\n\n\n\nIn the code above, shape = 1 says that the new point should be an open circle and size = 3 makes the point bigger, ensuring that it goes around the original point. show.legend = FALSE ensures that the larger open circles don’t become part of the legend.\nYou can use this same strategy to label specific countries. I’m interested in where the United States of America falls on this graph because I’m from the U.S. I’m also interested in where Denmark falls because that’s the country I’m most interested in visiting. Feel free to replace those countries with any that you’re interested in!\n\nplot_df_us <- hpi_df |>\n  filter(Country == \"United States of America\" | Country == \"Denmark\")\n\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_point(data = plot_df_us, size = 3, shape = 1,\n             show.legend = FALSE) +\n  geom_label_repel(data = plot_df_us, aes(label = Country),\n                   show.legend = FALSE)\n\n\n\n\n\n\n5.3.5 Plot Themes\nPlot themes are an easy way to change many aspects of your plot with an overall theme that someone developed. The default theme for ggplot2 graphs is theme_grey(), which is the graph with the grey background that we’ve been using in the entirety of this class. The 7 other themes are given in R for Data Science in Figure 28.3.\nHowever, there are many more choices in the ggthemes package. Load the package with library(ggthemes) and check out https://yutannihilation.github.io/allYourFigureAreBelongToUs/ggthemes/ for a few of the themes in the package. My personal favorites, all given below, are theme_solarized(), theme_fivethirtyeight(), and theme_economist(), but choosing a theme is mostly a matter of personal taste.\n\nlibrary(ggthemes)\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_point(data = plot_df_us, size = 3, shape = 1, show.legend = FALSE) +\n  geom_label_repel(data = plot_df_us, aes(label = Country), show.legend = FALSE) +\n  theme_solarized()\n\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_point(data = plot_df_us, size = 3, shape = 1, show.legend = FALSE) +\n  geom_label_repel(data = plot_df_us, aes(label = Country), show.legend = FALSE) +\n  theme_fivethirtyeight()\n\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_point(data = plot_df_us, size = 3, shape = 1, show.legend = FALSE) +\n  geom_label_repel(data = plot_df_us, aes(label = Country), show.legend = FALSE) +\n  theme_economist()\n\nThere’s still much more you can do with ggplot2. In fact, there are entire books on it. But, for most other specializations, you can usually use Google to help you!\n\n\n5.3.6 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-5).\nThe theme() function is a way to really specialise your plot. We will explore some of these in the exercise below.\n\nUsing only options in theme() or options to change colours, shapes, sizes, etc., create the ugliest possible ggplot2 graph that you can make. You may not change the underlying data for this graph, but your goal is to investigate some of the options given in theme(). A list of theme options is given in this link.\n\n\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point()\n\n\n\n\nWe will practice more with communicating with plots in the chapter exercises."
  },
  {
    "objectID": "05-comm.html#chapexercise-5",
    "href": "05-comm.html#chapexercise-5",
    "title": "5  Communication with Quarto",
    "section": "5.4 Chapter Exercises",
    "text": "5.4 Chapter Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-5).\nSome data sets exist within specific R packages. For example, Jenny Bryan, who is quite famous in the stats/data science community, has put together the gapminder package so that users in R have access to a specific data set on countries throughout the world. https://github.com/jennybc/gapminder.\nTo load a data set within a specific R package, you first need to install the package with install.packages(\"gapminder\") and load the package itself:\n\nlibrary(gapminder)\n\nThen, name the data set something. In this case, the name of the data set is gapminder, but it’s not always the same name as the package itself. We will name the data set country_df.\n\ncountry_df <- gapminder\n\nExplore the data set with head(country_df) and ?gapminder before proceeding to the following exercises.\n\n* Make a line graph that shows the relationship between lifeExp and year for each of the countries in the data set, faceting the graph by continent and also colouring by continent (though this is redundant). Add an x-axis label, a y-axis label, a legend label, and a title to the graph.\n* Change the colour palette to be CVD-friendly using either scale_colour_brewer() or scale_colour_viridis_d().\n* We can see a couple of interesting trends in life expectancy. There is one country in Africa and one country in Asia that sees a sharp decline in life expectancy at one point. In Europe, there is one country that has a substantially lower life expectancy than the rest in the 1950s but catches up to other European countries by the 2000s. Use filter() to create a data set that only has these 3 countries. Then, use geom_label() to label all three countries on your plot.\n\n\n\n\n\nGoogle the history of the countries in Africa and Asia that you just labeled. Add a very short description of why each country experienced a dip in life expectancy as a caption in your graph.\n\n\n\n\n\nRead the help file for ?annotate. How is this different than geom_label(). Which one allows finer tuning? Which one takes more code to use? One of the functions (annotate() or geom_label()) becomes more of a pain to use when there are many labels. Which one becomes harder to use and why?\n\n\n\n\n\nSuppose that we want the legend to appear on the bottom of the graph. Without using an entirely different theme, use Google to figure out how to move the legend from the right-hand side to the bottom.\n\n\n\n\n\nIf there are a lot of overlapping points or overlapping lines, we can use alpha to control the transparency of the lines. Google “change transparency of lines in ggplot” and change the alpha so that the lines are more transparent.\n\n\n\n\n\nChange the theme of your plot to a theme in the ggthemes package. Then, change the order of your two commands to change the legend position and to change the overall theme. What happens?\n\n\n\n\n\nModify your .qmd file so that:\n\n\nonly the figure that you made in Exercise 8 prints on your .html file. (Hint: use global options to help with this).\nnone of the code gets printed.\nthe warnings/messages that R prints by default are hidden in all code chunks.\nthe figure height is 5 instead of the default 7.\n\n\nRead the following up to “How can software tools make our research more reproducible?” https://ropensci.github.io/reproducibility-guide/sections/introduction/. How does what is discussed in the article related to Quarto?"
  },
  {
    "objectID": "05-comm.html#solutions-5",
    "href": "05-comm.html#solutions-5",
    "title": "5  Communication with Quarto",
    "section": "5.5 Exercise Solutions",
    "text": "5.5 Exercise Solutions\n\n5.5.1 Reproducibility S\n\n\n5.5.2 Quarto Files S\n\n* Create a table showing the mean mpg for each cyl group (cyl stands for cylinder and can be 4-cylinder, 6-cylinder, or 8-cylinder) with both kable() and pander(). Hint: remember to call the knitr library and the pander library.\n\n\nlibrary(knitr)\nlibrary(pander)\nlibrary(tidyverse)\nmpg_df <- mtcars |> group_by(cyl) |>\n  summarise(meanmpg = mean(mpg))\nmpg_df |> kable()\n\n\n\n\ncyl\nmeanmpg\n\n\n\n\n4\n26.66364\n\n\n6\n19.74286\n\n\n8\n15.10000\n\n\n\n\nmpg_df |> pander()\n\n\n\n\n\n\n\n\ncyl\nmeanmpg\n\n\n\n\n4\n26.66\n\n\n6\n19.74\n\n\n8\n15.1\n\n\n\n\n\n\n* Type ?kable into your console window and scroll through the Help file. Change the rounding of the mean so that it only displays one number after the decimal. Then, add a caption to the table that says “My First Table Caption!!”\n\n\nmpg_df |> kable(digits = 1, caption = \"My First Table Caption!!\")\n\n\nMy First Table Caption!!\n\n\ncyl\nmeanmpg\n\n\n\n\n4\n26.7\n\n\n6\n19.7\n\n\n8\n15.1\n\n\n\n\n\n\n* Google “How to Change Column Names in kable” and replace the column names with “Cylinder Numb.” and “Mean Mileage”.\n\n\nmpg_df |> kable(digits = 1, caption = \"My First Table Caption!!\",\n  col.names = c(\"Cylinder Numb.\", \"Mean Mileage\"))\n\n\nMy First Table Caption!!\n\n\nCylinder Numb.\nMean Mileage\n\n\n\n\n4\n26.7\n\n\n6\n19.7\n\n\n8\n15.1\n\n\n\n\n\n\n\n5.5.3 ggplot2 Communication S\n\n\n5.5.4 Chapter Exercises S\n\n* Make a line graph that shows the relationship between lifeExp and year for each of the countries in the data set, faceting the graph by continent and also colouring by continent (though this is redundant). Add an x-axis label, a y-axis label, a legend label, and a title to the graph.\n\n\n\n\n\n* Change the colour palette to be CVD-friendly using either scale_colour_brewer() or scale_colour_viridis_d().\n\n\n\n\n\n* We can see a couple of interesting trends in life expectancy. There is one country in Africa and one country in Asia that sees a sharp decline in life expxectancy at one point. In Europe, there is one country that has a substantially lower life expectancy than the rest in the 1950s but catches up to other European countries by the 2000s. Use filter() to create a data set that only has these 3 countries. Then, use geom_label() to label all three countries on your plot.\n\n\ninterest_countries <- country_df |> filter((year == 1952 & continent == \"Europe\" &\n    lifeExp < 50) | (year == 1992 & continent == \"Africa\" &\n    lifeExp < 30) | (year == 1977 & continent == \"Asia\" & \n    lifeExp < 35))\nggplot(data = country_df, aes(x = year, y = lifeExp, group = country,\n  colour = continent)) +\n  geom_line() +\n  facet_wrap( ~ continent) +\n  scale_colour_brewer(palette = \"Set2\") +\n  labs(x = \"Year\", y = \"Life Expectancy (Years)\", colour = \"Continent\",\n    title = \"Life Expectancy Increases Across time for nearly every country\") +\n  geom_label(data = interest_countries, aes(label = country),\n    nudge_x = 7)"
  },
  {
    "objectID": "05-comm.html#rcode-5",
    "href": "05-comm.html#rcode-5",
    "title": "5  Communication with Quarto",
    "section": "5.6 Non-Exercise R Code",
    "text": "5.6 Non-Exercise R Code\n\nlibrary(tidyverse)\nhpi_df <- read_csv(\"data/hpi-tidy.csv\")\nhead(hpi_df)\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point()\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  labs(title = \"Countries with a Higher Ecological Footprint Tend to Have Citizens with Longer, Happier Lives\", \n       ## add title\n       subtitle = \"HappyLifeYears is a Combination of Life Expectancy and Citizen Well-Being\", \n       ## add subtitle (smaller text size than the title)\n       caption = \"Data Source: http://happyplanetindex.org/countries\", \n       ## add caption to the bottom of the figure\n       x = \"Ecological Footprint\", ## change x axis label\n       y = \"Happy Life Years\", ## change y axis label\n       colour = \"World Region\") ## change label of colour legend\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  ylim(c(0, 70))\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Accent\")\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_viridis_d(option = \"plasma\")\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_label(aes(label = Country))\nplot_df <- hpi_df |> group_by(Region) |>\n  filter(HPIRank == min(HPIRank))\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_label(data = plot_df, aes(label = Country))\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point(aes(colour = Region)) +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_label(data = plot_df, aes(label = Country), show.legend = FALSE)\nlibrary(ggrepel)\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_label_repel(data = plot_df, aes(label = Country),\n                   show.legend = FALSE) \nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears, colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_label_repel(data = plot_df, aes(label = Country), show.legend = FALSE) +\n  geom_point(data = plot_df, size = 3, shape = 1, show.legend = FALSE) \nplot_df_us <- hpi_df |>\n  filter(Country == \"United States of America\" | Country == \"Denmark\")\n\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_point(data = plot_df_us, size = 3, shape = 1,\n             show.legend = FALSE) +\n  geom_label_repel(data = plot_df_us, aes(label = Country),\n                   show.legend = FALSE)\nlibrary(ggthemes)\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_point(data = plot_df_us, size = 3, shape = 1, show.legend = FALSE) +\n  geom_label_repel(data = plot_df_us, aes(label = Country), show.legend = FALSE) +\n  theme_solarized()\n\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_point(data = plot_df_us, size = 3, shape = 1, show.legend = FALSE) +\n  geom_label_repel(data = plot_df_us, aes(label = Country), show.legend = FALSE) +\n  theme_fivethirtyeight()\n\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  geom_point(data = plot_df_us, size = 3, shape = 1, show.legend = FALSE) +\n  geom_label_repel(data = plot_df_us, aes(label = Country), show.legend = FALSE) +\n  theme_economist()\nggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears,\n                          colour = Region)) +\n  geom_point()"
  },
  {
    "objectID": "14-workflow.html",
    "href": "14-workflow.html",
    "title": "6  Workflow and Other Skills",
    "section": "",
    "text": "Goals:"
  },
  {
    "objectID": "14-workflow.html#r-projects-and-file-organization",
    "href": "14-workflow.html#r-projects-and-file-organization",
    "title": "6  Workflow and Other Skills",
    "section": "6.1 R Projects and File Organization",
    "text": "6.1 R Projects and File Organization\nR Projects are a convenient way to keep related code, data sets, and analyses together. Read this very short introduction in R for Data Science here: https://r4ds.had.co.nz/workflow-projects.html#paths-and-directories and https://r4ds.had.co.nz/workflow-projects.html#rstudio-projects.\nWhy should you rarely use an absolute directory?\nLook at the top of your bottom-left terminal window. If you’ve made an R project (which you should have!), you should see that a file path that is the current folder you’re working in. This is where R Studio will look for files by default.\nThe here package can help keep track of where files are and with reading in files. Install the here package with install.packages(\"here\"). Then, load the package and run the here() function with\n\nlibrary(here)\n\nhere() starts at /Users/highamm/Desktop/ds234_quarto\n\nhere()\n\nhere() prints the directory of where your current R project is. When reading in a data set with read_csv(), we would use\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.2      ✔ forcats 0.5.1 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nathletes_df <- read_csv(\"data/athletesdata.csv\")\n\nNew names:\nRows: 100 Columns: 9\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(3): Name, Sport, Gender dbl (6): ...1, Rank, endorsements, totalpay, salary,\nage\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -> `...1`\n\n\nR can read in this data set successfully: it starts at the path given in the console that was printed from here(), looks for a folder called data in that path and looks for a file called athletesdata.csv in the data folder.\nSo, if you zipped up your project and sent it to someone else, they’d be able to open it and read that data file without needing to change any directory code!\nThe here() function from the here package can be used for more than just printing out the current working directory. To see its usefulness, suppose that, in the folder that has your current R project, you want to make a folder called Quizzes that has your .qmd files for your quizzes in this class. Make this folder, create a new .qmd file, paste in the R chunk that reads in the athletesdata.csv data set, save the file, and then try to render the file.\nYou should get an error that the athletesdata.csv data file is not found. When rendering a .qmd file, R looks for the data/ folder within the folder that contains the .qmd file. Since the data/ folder is in the folder with your R Project, not in the folder with your .qmd file, R can’t find it.\nTo fix the issue, we could specify the entire file path to the data/ file. But, a better fix would be to use the here() function, which tells R to start looking for folders and files in the folder with our R Project:\n\nathletes_test_read <- read_csv(here(\"data/athletesdata.csv\"))\n\nNew names:\nRows: 100 Columns: 9\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(3): Name, Sport, Gender dbl (6): ...1, Rank, endorsements, totalpay, salary,\nage\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -> `...1`\n\n\nThis allows us to have Quarto files within folders in our R project.\n\n6.1.1 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-14).\n\nTake some time to modify your files in this course by creating a few folders to help keep things a bit more organized. You might consider making a Quizzes folder and a Projects folder, for example. Move the relevant files to these folders and modify each file to load in the here package and use the here() function to read in any relevant data sets. We’ve been using the here() function quite a bit already so it may already be there!\nClick the “Packages” button in the lower-right hand window to bring up the packages menu. Instead of using library(name_of_package), you can click the check-box by the package name to load it into R. Try it out by un-checking and then re-checking tidyverse. Explain, from a reproducibility perspective, why loading packages this way is not good practice."
  },
  {
    "objectID": "14-workflow.html#code-style",
    "href": "14-workflow.html#code-style",
    "title": "6  Workflow and Other Skills",
    "section": "6.2 Code Style",
    "text": "6.2 Code Style\nWriting code that is “readable” is helpful not only for others but also for yourself, especially if the project you are working on is long-term. What constitutes “readable” code varies a bit, but there are some general principles that are more widely accepted for “good” code. Much of the coding “style” you have seen so far has been imposed by me: I have my own style for writing code so naturally, I use that style in the code I write for our course materials.\n\n6.2.1 Names\nNames of objects that we create should be descriptive yet short. Sometimes, thinking of a name that makes sense can be very challenging! Some examples of “bad” names for objects include names that are too generic that we won’t be able to distinguish them later:\n\ndf1 <- mtcars |> filter(cyl == 4)\ndf2 <- mtcars |> filter(cyl == 6)\ndf3 <- mtcars |> filter(cyl == 8)\n\nBetter names for the above data frames would be cyl4_df, cyl6_df, and cyl8_df, respectively, as these names tell us more about what is in each data frame.\nOther “bad” names for objects are names that are too long:\n\ncars_with_4_cylinders_data_set <- mtcars |> filter(cyl == 4)\n\nLong names are descriptive but can be a pain to type out and to read.\nYou may have noticed that my coding “style” is to separate words in names with an _: cyl4_df. Others may choose to separate words in names with a .: cyl4.df while others may use capitalization for the second word: cyl4Df. The most important thing here is to be consistent in your choice. In other words, using _ instead of . isn’t necessarily better, but it would be poor practice to mix naming notation.\n\ncyl4_df <- mtcars |> filter(cyl == 4)\ncyl6.df <- mtcars |> filter(cyl == 6)\n\nIf we mixed, then we always have to keep track of whether an object is named with _ or . or with capitalization.\nFinally, you may have noticed that most of our data frames our named with the suffix _df. I have worked that into my own coding style because I like keeping track of what is a dataframe (or tibble) and what isn’t. This is generally more helpful as you encounter different types of objects (model output, lists, matrices, vectors, etc.).\n\n\n6.2.2 Code Readability\nWe can also follow some general practices to make our code more “readable.” We have already been employing most of these practices throughout the semester: R Studio generally makes code readable by indenting appropriately.\nAppropriately using spacing can make code much more readable. Consider the following ggplot() code. For example, the following code chunk executes a scatterplot with a fitted regression line but it’s generally tough to read.\n\nggplot(data=mtcars,aes(x=wt,y=drat))+geom_point()+geom_smooth(method=\"lm\",se=FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nA couple of conventions can help: (1) spaces around any equal sign, plus sign, and after any comma and (2) putting code after each plus sign on a different line.\n\nggplot(data = mtcars, aes(x = wt, y = drat)) +\ngeom_point() +\ngeom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nIndenting subsequent lines in ggplot2 code or in a dplyr pipeline shows that the subsequent lines “go with” the first line:\n\nggplot(data = mtcars, aes(x = wt, y = drat)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe same concepts of using multiple lines holds for a piping statement as well. In general,\n\nmtcars |> filter(cyl == 4) |>\n  group_by(vs) |>\n  summarise(mean_mpg = mean(mpg, na.rm = TRUE))\n\n# A tibble: 2 × 2\n     vs mean_mpg\n  <dbl>    <dbl>\n1     0     26  \n2     1     26.7\n\n\nis easier to read than\n\nmtcars |> filter(cyl == 4) |> group_by(vs) |> summarise(mean_mpg = mean(mpg, na.rm = TRUE))\n\n# A tibble: 2 × 2\n     vs mean_mpg\n  <dbl>    <dbl>\n1     0     26  \n2     1     26.7\n\n\n\n\n6.2.3 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-14).\n\nChange the following object names to be “better:”\n\n\ncars_where_wt_is_larger_than_3_tons <- mtcars |> filter(wt > 3)\n\n\ndataset <- mtcars |> group_by(am) |>\n  summarise(mean_disp = mean(disp),\n            med_disp = median(disp),\n            sd_disp = sd(disp))\n\n\nChange the style of the following code to make the code more readable.\n\n\nggplot(data=mtcars,aes(x = mpg))+geom_histogram(colour=\"black\",fill=\"white\",bins=15) + facet_wrap(~cyl, ncol=1)"
  },
  {
    "objectID": "14-workflow.html#debugging-code",
    "href": "14-workflow.html#debugging-code",
    "title": "6  Workflow and Other Skills",
    "section": "6.3 Debugging Code",
    "text": "6.3 Debugging Code\nThe previous section on code readability can be seen as one step to helping with code debugging: code that is easier to read is code that is easier to spot errors in. Additionally, there are some other strategies we can take when our code is not working to figure out what the issue is.\n\n6.3.1 Identify the Problem\nWe run R code for our data analyses from “top to bottom,” which makes it a bit easier to identify where the problem code is occurring. We can run our code from the top of our .qmd file, line by line, until we see the red Error message.\nOften this Error message will occur in a ggplot statement or a piping statement. If this is the case, then a further strategy is to run the ggplot statement + sign by + sign or to run the piping statement pipe by pipe to further isolate the error. For example, take the following ggplot code, which generates a somewhat cryptic error.\n\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(colour = disp) +\n  facet_wrap(~ cyl) \n\nIn this case, the error message does help us locate the issue, but that is not always the case. If we are not sure what the error is, what we can do is run\n\nggplot(data = mtcars, aes(x = wt, y = mpg))\n\nto see if we get an error. We don’t, so we move on to the code after the next + sign:\n\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point()\n\nWe still don’t get an error so we move on to the code after the next + sign:\n\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(colour = disp)\n\nWe have our error. So now, instead of isolating the error to a particular chunk of code, we have isolated the error to a particular line of code: we know the issue is something with how we are using geom_smooth(). (We are missing aes() to refer to the variable disp).\nThe same strategy can be used with piping. The following code, used to figure out the average bill length to bill depth ratio in Adelie penguins, does not give an error but instead outputs something that we might not expect: a tibble with an NaN (Not a Number) value (note that you must install the palmerpenguins package install.packages(\"palmerpenguins\") before loading the palmerpenguins library.\n\nlibrary(palmerpenguins)\npenguins |> filter(species == \"Adeie\") |>\n  mutate(bill_ratio = bill_length_mm / bill_depth_mm) |>\n  summarise(mean_ratio = mean(bill_ratio))\n\nWe can troubleshoot by running the code “pipe by pipe,” starting with the code through the first filter() pipe:\n\npenguins |> filter(species == \"Adeie\")\n\nRight away, we see a problem: we get a tibble with no data because we misspelled Adelie:\n\npenguins |> filter(species == \"Adelie\")\n\nAfter correcting this issue, we can continue through the pipes:\n\npenguins |> filter(species == \"Adelie\") |>\n  mutate(bill_ratio = bill_length_mm / bill_depth_mm)\n\nThere doesn’t seem to be any issues in our mutate() statement so we can go to the next pipe.\n\npenguins |> filter(species == \"Adelie\") |>\n  mutate(bill_ratio = bill_length_mm / bill_depth_mm) |>\n  summarise(mean_ratio = mean(bill_ratio))\n\nWe get an NA value, and we have isolated the issue to something with summarise(), or, possibly something with mutate() that does not set something up quite right for summarise(). Can you figure out the issue?\nIn addition to isolating the coding issue, a couple of other very basic strategies for trying to fix problematic code are to use a search engine like google to see if anyone else has a similar error message to the one you may have and to restart R to make sure that you are working from a clean slate.\nThe “restart R” strategy can be particularly helpful if you have code that will run but your .qmd file will not render. This can happen if you have, for example, created a data set that you use in a later chunk of code but have since deleted the code that created that data set. For example, suppose we create cyl4_df and make a plot:\n\ncyl4_df <- mtcars |> filter(cyl == 4)\n\nggplot(data = cyl4_df, aes(x = mpg)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nBut, later we delete the line creating cyl4_df. The plot will still work because cyl4_df is already in our environment but the file will not render because we are missing that crucial line of code. Restarting R can help us identify this issue because the plot will no longer work and we will get a sensible error message like cyl4_df not found.\n\n\n6.3.2 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-14).\n\nFind the error in the following code chunk by running the code “+ sign by + sign).\n\n\nggplot(data = mtcars, aes(x = hp, y = drat)) +\n  geom_point(aes(colour = factor(gear))) +\n  facet_wrap(cyl) +\n  geom_smooth()\n\n\nFind the error in the following code chunk by running the code “pipe by pipe.”\n\n\npenguins |> mutate(flipper_ratio = flipper_length_mm / body_mass_g) |>\n  group_by(species, island) |>\n  summarise(mean_flipper = mean(flipper_ratio, na.rm = TRUE)) |>\n  arrange(flipper_ratio) |>\n  pivot_wider(names_from = c(\"species\"), values_from = \"mean_flipper\")\n\n\nFind the error in the following code chunk by running the code “pipe by pipe.”\n\n\npenguins |> mutate(flipper_ratio = flipper_length_mm / body_mass_g) |>\n  filter(flipper_ratio > median(flipper_ratio)) |>\n  group_by(species) |>\n  summarise(count_var = n())"
  },
  {
    "objectID": "14-workflow.html#context-outliers-and-missing-values",
    "href": "14-workflow.html#context-outliers-and-missing-values",
    "title": "6  Workflow and Other Skills",
    "section": "6.4 Context, Outliers, and Missing Values",
    "text": "6.4 Context, Outliers, and Missing Values\nThe primary purpose of this section is to explore why we should always think critically about the data set we are analyzing as opposed to simply making summary tables without thinking about how they could be interpreted. In other words, we need to both examine the data set to see if there are things like missing values or outliers that could affect interpretation as well as consider the context that the data set comes from.\n\n6.4.1 Context\nConsidering context includes thinking about questions like:\n\nwhere did the data set come from? Who collected it?\nare there missing values coded as NAs in the data set. Would these affect our analysis or are they missing “at random.” Missing values coded as NA are referred to as explicitly missing values.\nare there missing values in the data set that are not actually observations at all? These are implicitly missing. An example might be collecting data on students attending this class. Students not present at the time the data was collected are implicitly missing.\ndoes the data come from an observational study or an experiment?\n\nThere are many other questions we could ask pertaining to context, but many such questions depend on the particular data collected. As an example, consider the data set on majors at SLU from 2015 to 2020. For now, you can ignore the extra code given to read in the data: pivoting functions and variable types are topics we will learn about in the upcoming weeks.\n\nmajors_df <- read_csv(here(\"data/majors.csv\")) |>\n  pivot_longer(-1, names_to = \"year\", values_to = \"n_majors\") |>\n  mutate(year = as.numeric(year)) |>\n  rename(major = `...1`)\n\nNew names:\nRows: 63 Columns: 17\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): ...1 dbl (16): 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014,\n2015, ...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -> `...1`\n\nhead(majors_df)\n\n# A tibble: 6 × 3\n  major         year n_majors\n  <chr>        <dbl>    <dbl>\n1 Biochemistry  2005        2\n2 Biochemistry  2006        6\n3 Biochemistry  2007        5\n4 Biochemistry  2008        8\n5 Biochemistry  2009        3\n6 Biochemistry  2010        7\n\n\nIn the data, the n_majors variable represents the number of students graduating with that particular major in that particular year. For example, in the year 2005, there were just 2 students graduating with a Biochemistry major.\nSuppose we are interested in trends among majors in Estudios Hispanicos (Spanish). In the United States, there are many people who speak Spanish so we might expect this to be a somewhat popular major. In particular, we want to see if there is an increase or decrease in the number of these majors since 2005. We can make a line chart with:\n\nspanish_df <- majors_df |> filter(major == \"Estudios Hispanicos (Spanish)\")\nggplot(data = spanish_df, aes(x = year, y = n_majors)) +\n  geom_line() +\n  geom_smooth(se = FALSE)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nWhat would we conclude based on this plot?\nThe topic of this subsection is the context in which a data set arises in. So, you might guess that the conclusion one would make based on the line graph (that the spanish major at SLU seems to be in decline) does not tell the full story. In fact, about a decade ago, an International Economics Combined major was introduced, in which students complete courses in both Economics as well as foreign language studies. The most popular choice for the foreign language is Spanish.\nWe can make a graph of the number of International Economics Combined majors:\n\nint_econ_df <- majors_df |> filter(major == \"Int'l Economics (Combined)\")\nggplot(data = int_econ_df, aes(x = year, y = n_majors)) +\n  geom_line() +\n  geom_smooth(se = FALSE)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nHow does the new contextual information about the International Economics major influence your conclusions about the popularity of Spanish studies at SLU?\nYou should find throughout the semester that the data sets on topics that you are more familiar with are easier to analyze than the data sets on topics that you are not as familiar with. A large part of the reasoning for this is that you have much more contextual information with data topics that you have prior knowledge on. That extra contextual information generally allows us to pose deeper questions, identify potentially erroneous data, and write more subtle conclusions. We will discuss context more throughout the semester and will also have another focus on context when we discuss data ethics.\n\n\n6.4.2 Outliers and Missing Values\nOutliers in a data analysis can affect certain summary statistics, like the mean and the standard deviation (as you learned in STAT 113). They could also be observations that warrant further investigation if we are interested in why a particular point is an outlier.\nMissing values can also cause us to reach a potentially misleading conclusion if we do not carefully consider why such values are missing.\nWe will talk about the consequences of outliers and missing values next, but first, we will discuss how to determine if there are outliers or missing values in the data set. An easy function to use for this purpose is the skim() function from the skimr package. Install the skimr package and then use the skim() function on thevideogame_clean.csv file, which contains variables on video games from 2004 - 2019, including\n\ngame, the name of the game\nrelease_date, the release date of the game\nrelease_date2, a second coding of release date\nprice, the price in dollars,\nowners, the number of owners (given in a range)\nmedian_playtime, the median playtime of the game\nmetascore, the score from the website Metacritic\nprice_cat, 1 for Low (less than 10.00 dollars), 2 for Moderate (between 10 and 29.99 dollars), and 3 for High (30.00 or more dollars)\nmeta_cat, Metacritic’s review system, with the following categories: “Overwhelming Dislike”, “Generally Unfavorable”, “Mixed Reviews”, “Generally Favorable”, “Universal Acclaim”.\nplaytime_miss, whether median play time is missing (TRUE) or not (FALSE)\n\nThe data set was modified from https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-07-30.\n\nlibrary(skimr)\nlibrary(here)\nvideogame_df <- read_csv(here(\"data/videogame_clean.csv\"))\n\nRows: 26688 Columns: 15\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (7): game, release_date, owners, meta_cat, developer, publisher, meta_c...\ndbl  (6): price, median_playtime, metascore, price_cat, number, average_play...\nlgl  (1): playtime_miss\ndate (1): release_date2\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n## skim(videogame_df)\n\nSee if you can find in the output the following:\n\nthe number of rows in the data set and the number of columns\nthe number of missing values for each variable\nthe number of unique values for each character variable\nthe completion rate (the proportion of values that are non-missing).\n\nIn particular, the number of missing values is given by nmissing and complete_rate gives the proportion of values that are non-missing. These give us some idea about if missing values exist for certain variables, and, if so, how many exist for each variable.\nAlso, at the bottom of the output, you should see tiny histograms of each numeric variable and some summary statistics. Looking at the min, max, and the histograms of each variable can inform us about whether each variable has any outliers. For example, we see that the histograms for price, median_playtime, and average_playtime all look extremely skewed right because of the outlier(s) on the upper end.\n\n\nSo, we now know that there are outliers and missing values for certain variables in the videogame data set. How might these affect the tables and graphs that we make?\nFirst, let’s focus on the metascore variable, which gives Metacritic’s overall aggregated review score for the videogames. Note that the complete_rate for the metascore variable is only 0.107: almost 90% of the videogames do not have a metascore.\nSo, suppose we are interested in exploring what the “typical” metascore is. We can figure out what the average metascore and what the median metascore of the non-missing videogames is with:\n\nvideogame_df |> summarise(mean_meta = mean(metascore, na.rm = TRUE),\n                          med_meta = median(metascore, na.rm = TRUE))\n\n# A tibble: 1 × 2\n  mean_meta med_meta\n      <dbl>    <dbl>\n1      71.9       73\n\n\nIgnoring the missing values, we would say that, on average, videogames receive metascores around 72 points. The question we need to ask ourselves is: “Is it reasonable to assume that the missing games receive similar reviews to the non-missing games so that we can thin about the 71.9 as the average review score of all games?”\nHow you answer might depend on what you understand about videogames and the review process. But I would argue that the missing games would be reviewed worse than the non-missing games. Major games usually get the most reviews and also usually have the most funding while there are many minor games that have little funding, would not get reviewed, and, if they did get reviewed, may get a lower rating.\nYou can certainly make a different argument: we don’t know if my argument is correct or not without further data. The most important thing to do is to at least think about and make clear possible limitations in your conclusions from a data analysis.\nAs a second example, consider an exploration of the relationship between median_playtime of a game and its metascore. We can make a scatterplot of the relationship, ignoring the missing values, with\n\nggplot(data = videogame_df, aes(x = metascore, y = median_playtime)) +\n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 23843 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 23843 rows containing missing values (`geom_point()`).\n\n\n\n\n\nWe see some clear outliers, which we will talk about next, but would the missing values for metascore affect conclusions we draw from the graph? The answer would be “yes” if we think videogames with missing metascores would follow a different overall trend than those with non-missing metascores and “no” if we think that, if the videogames with missing metascores were rated, they would follow a similar trend as those already in the graph.\nFor this question, I would make the argument that the games would follow a similar trend. But again, that is an assumption I would need to make and need to be explicit about.\nWe also mentioned the idea of implicit missing values. These would be videogames that do not appear in the data set at all. In other words, was this set of videogames a sample or is it all videogames ever published in the United States? If it is a sample, how were they selected, and, if they are a convenience sample, what were the types of games that were left out?\n\nOutliers can also pose interesting challenges in data analysis. For example, consider again the graph of median_playtime vs. metascore. To focus on outliers now, we will ignore the missing values in metascore.\n\nggplot(data = videogame_df, aes(x = metascore, y = median_playtime)) +\n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 23843 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 23843 rows containing missing values (`geom_point()`).\n\n\n\n\n\nWe see some clear outliers in median_playtime: games with a median playtime of thousands of hours. Once again, having some knowledge about videogames can help us determine what to do with these outliers.\nThe most important thing to do when dealing with outliers is to be explicit about what you, as the analyst, choose to keep in the graph or summary table and what you choose to remove. If you choose to remove values, give your reasoning, and, if there is space, you can also give a second graph that has the data without removing any outliers.\nIn this example, a median playtime of 3000+ hours seems a bit excessive, but it’s more challenging to determine what a reasonable cutoff for “excessive” is. Is it reasonable for a game to have a median playtime of 1000 hours? What aobut 2000 hours? 500 hours? Choosing which points to keep will affect the fit of the smoother. As you may have learned in STAT 113 or STAT 213, observations that have a high control over the fit of a smoother or regression line are influential.\n\n\n6.4.3 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-14).\n\nThe STAT 113 survey data set contains responses from 397 STAT 113 students from a survey that students take at the beginning of the semester. There are 5 categorical variables and 7 numeric variables. Of the categorical variables, how many variables have 0 missing values? Of the numeric variables, how many variables have 0 missing values?\n\n\nlibrary(tidyverse)\nstat113_df <- read_csv(here(\"data/stat113.csv\"))\n\nRows: 397 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Year, Sex, Sport, Award, SocialMedia\ndbl (7): Hgt, Wgt, Haircut, GPA, Exercise, TV, Pulse\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nChoose a variable that has some missing values that you would feel comfortable ignoring the missing values in a table or graph. Give a one to two sentence reason.\nChoose a variable that has some missing values that you would not feel comfortable ignoring the missing values in a table or graph. Give a one to two sentence reason.\nFind the mean and median median_playtime for all videogames in the metacritic videogame data set. Then, remove the games with a median_playtime over 1000 hours. Compute the mean and median median_playtime of the data set without these games. Which measure, the mean or the median was more affected by having the outliers present?"
  },
  {
    "objectID": "14-workflow.html#reprexes-and-tibble",
    "href": "14-workflow.html#reprexes-and-tibble",
    "title": "6  Workflow and Other Skills",
    "section": "6.5 Reprexes and tibble",
    "text": "6.5 Reprexes and tibble\nA reproducible example, or reprex, is a chunk of code that we can give to someone else that runs without any outside data. These are used often on StackExchange. We can create a data set directly within R with the tibble() function in the tibble package. This is most useful when we want to make a small reproducible example so that someone else may help with our code.\nThe following code chunk is not a reprex because people would not necessarily have the data set parsedf.csv.\n\n## Hello! How do I get rid of the units from the values in\n## my variable `x`? Thanks!\nlibrary(tidyverse)\ntest_df <- read_csv(here(\"data/parsedf.csv\"))\nhead(test_df)\n\n# A tibble: 3 × 2\n  x                   y\n  <chr>           <dbl>\n1 20,000 dollars      1\n2 40 dollars          2\n3 only 13 dollars     3\n\n\nSuppose that we want to post on StackExchange for someone or ask a friend to help us convert a variable from a character vector with units to a numeric vector without units. We want to be able to give any possible helpers a small example data set to work with and isolate the problem or question that we have. For this, we can create our own tiny data set with tibble():\n\n## Hello! How do I get rid of the units from the values in\n## my variable `xvar`? Thanks!\nlibrary(tidyverse)\ntest_df2 <- tibble(xvar = c(\"20,000 dollars\", \"40 dollars\"),\n                   yvar = c(1, 2))\ntest_df2\n\n# A tibble: 2 × 2\n  xvar            yvar\n  <chr>          <dbl>\n1 20,000 dollars     1\n2 40 dollars         2\n\n\nWhy is library(tidyverse) necessary in the code chunk above for my reprex?\nWe can copy and paste the code chunk above to our question: it’s code that anyone can run as long as they have the tidyverse package installed, and really encourages more people to help.\nAs a second example, we might have a question about how to find the mean for many numeric variables. For example, in the stat113.csv file, there are many numeric variables. We can compute the mean of each numeric variable by writing a separate summarise() statement for each variable. But we also may be interested in a quicker way. So, since our helper might not have the stat113.csv file, we can create a reprex for our problem:\n\n## is there a way to get a summary measure, like the mean, for \n## all numeric variables in a data set without writing a separate\n## summarise() statement for each variable?\n\nlibrary(tidyverse)\nsum_df <- tibble(xvar = c(\"A\", \"B\"), yvar = c(1, 4), zvar = c(-1, 4),\n                 svar = c(\"G\", \"g\"), tvar = c(99, 100000))\n\nNote how we included some categorical variables in our reprex data set. We want code that will work even if there are categorical variables in our data set, so we must include them in our reprex in this example to be as general as possible.\nFor reference, the across() function can be used to answer our question (though it’s not the point of this section). The code below reads that we should summarise() across() all variables that are numeric (is.numeric) and that our summary measure should be the mean.\n\nsum_df |> summarise(across(where(is.numeric), mean))\n\n# A tibble: 1 × 3\n   yvar  zvar   tvar\n  <dbl> <dbl>  <dbl>\n1   2.5   1.5 50050.\n\n\n\n6.5.1 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-14).\nFor Project 2, we will work with some course evaluation data for a professor at SLU. Overall, you’ll answer some questions about how the professor can improve their courses at SLU by looking at course evaluation data. The variables and data set will be described in more detail in the project description.\n\n* Suppose that you can’t figure out how to create a semester variable and a year variable from Term in evals_prof_S21.csv. (You want to split the Term variable into two variables: Semester with levels F and S and Year with levels 19, 20, and 21).\n\n\nlibrary(tidyverse)\nevals_df <- read_csv(here(\"data/evals_prof_S21.csv\"))\nhead(evals_df)\n\n# A tibble: 6 × 10\n  Term  Course Question    Agree…¹ Agree Agree…² Neutral Disag…³ Disag…⁴ Disag…⁵\n  <chr> <chr>  <chr>         <dbl> <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 F19   113-02 1. Course …       9     9       1       5       0       0       0\n2 F19   113-02 2. Effecti…      12     8       1       2       1       0       0\n3 F19   113-02 3. Environ…      11     8       2       3       0       0       0\n4 F19   113-02 5a. Fair A…       5    13       3       1       1       0       1\n5 F19   113-02 5b. Timely…       8    12       1       2       1       0       0\n6 F19   113-02 5c. Constr…       5     8       4       6       1       0       0\n# … with abbreviated variable names ¹​`Agree strongly`, ²​`Agree Somewhat`,\n#   ³​`Disagree Somewhat`, ⁴​Disagree, ⁵​`Disagree Strongly`\n\n\nPut together a reprex using tibble() that someone would be able to run to help you figure out your question."
  },
  {
    "objectID": "14-workflow.html#chapexercise-14",
    "href": "14-workflow.html#chapexercise-14",
    "title": "6  Workflow and Other Skills",
    "section": "6.6 Chapter Exercises",
    "text": "6.6 Chapter Exercises\nThere are no chapter exercises for this section on workflow."
  },
  {
    "objectID": "14-workflow.html#solutions-14",
    "href": "14-workflow.html#solutions-14",
    "title": "6  Workflow and Other Skills",
    "section": "6.7 Exercise Solutions",
    "text": "6.7 Exercise Solutions\n\n6.7.1 R Projects and File Organization S\n\n\n6.7.2 Code Style S\n\n\n6.7.3 Debugging Code S\n\n\n6.7.4 Context, Outliers, and Missing Values S\n\n\n6.7.5 Reprexes and tibble S\n\n* Suppose that you can’t figure out how to create a semester variable and a year variable from Term in evals_prof_S21.csv. (You want to split the Term variable into two variables: Semester with levels F and S and Year with levels 19, 20, and 21).\n\n\nlibrary(tidyverse)\nevals_df <- read_csv(here(\"data/evals_prof_S21.csv\"))\nhead(evals_df)\n\n# A tibble: 6 × 10\n  Term  Course Question    Agree…¹ Agree Agree…² Neutral Disag…³ Disag…⁴ Disag…⁵\n  <chr> <chr>  <chr>         <dbl> <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 F19   113-02 1. Course …       9     9       1       5       0       0       0\n2 F19   113-02 2. Effecti…      12     8       1       2       1       0       0\n3 F19   113-02 3. Environ…      11     8       2       3       0       0       0\n4 F19   113-02 5a. Fair A…       5    13       3       1       1       0       1\n5 F19   113-02 5b. Timely…       8    12       1       2       1       0       0\n6 F19   113-02 5c. Constr…       5     8       4       6       1       0       0\n# … with abbreviated variable names ¹​`Agree strongly`, ²​`Agree Somewhat`,\n#   ³​`Disagree Somewhat`, ⁴​Disagree, ⁵​`Disagree Strongly`\n\n\nPut together a reprex using tibble() that someone would be able to run to help you figure out your question.\n\nlibrary(tidyverse)\ndf <- tibble(Term = c(\"F19\", \"S20\"), x = c(1, 2))\n## Hello! I need help creating a variable that has F/S and \n## a separate year variable that has 19 and 20 from the data set above.\n## Thanks!"
  },
  {
    "objectID": "14-workflow.html#rcode-14",
    "href": "14-workflow.html#rcode-14",
    "title": "6  Workflow and Other Skills",
    "section": "6.8 Non-Exercise R Code",
    "text": "6.8 Non-Exercise R Code\n\nlibrary(here)\nhere()\nlibrary(tidyverse)\nathletes_df <- read_csv(\"data/athletesdata.csv\")\nathletes_test_read <- read_csv(here(\"data/athletesdata.csv\"))\ndf1 <- mtcars |> filter(cyl == 4)\ndf2 <- mtcars |> filter(cyl == 6)\ndf3 <- mtcars |> filter(cyl == 8)\ncars_with_4_cylinders_data_set <- mtcars |> filter(cyl == 4)\ncyl4_df <- mtcars |> filter(cyl == 4)\ncyl6.df <- mtcars |> filter(cyl == 6)\nggplot(data=mtcars,aes(x=wt,y=drat))+geom_point()+geom_smooth(method=\"lm\",se=FALSE)\nggplot(data = mtcars, aes(x = wt, y = drat)) +\ngeom_point() +\ngeom_smooth(method = \"lm\", se = FALSE)\nggplot(data = mtcars, aes(x = wt, y = drat)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\nmtcars |> filter(cyl == 4) |>\n  group_by(vs) |>\n  summarise(mean_mpg = mean(mpg, na.rm = TRUE))\nmtcars |> filter(cyl == 4) |> group_by(vs) |> summarise(mean_mpg = mean(mpg, na.rm = TRUE))\ncyl4_df <- mtcars |> filter(cyl == 4)\n\nggplot(data = cyl4_df, aes(x = mpg)) +\n  geom_histogram()\nmajors_df <- read_csv(here(\"data/majors.csv\")) |>\n  pivot_longer(-1, names_to = \"year\", values_to = \"n_majors\") |>\n  mutate(year = as.numeric(year)) |>\n  rename(major = `...1`)\nhead(majors_df)\nspanish_df <- majors_df |> filter(major == \"Estudios Hispanicos (Spanish)\")\nggplot(data = spanish_df, aes(x = year, y = n_majors)) +\n  geom_line() +\n  geom_smooth(se = FALSE)\nint_econ_df <- majors_df |> filter(major == \"Int'l Economics (Combined)\")\nggplot(data = int_econ_df, aes(x = year, y = n_majors)) +\n  geom_line() +\n  geom_smooth(se = FALSE)\nlibrary(skimr)\nlibrary(here)\nvideogame_df <- read_csv(here(\"data/videogame_clean.csv\"))\n## skim(videogame_df)\nvideogame_df |> summarise(mean_meta = mean(metascore, na.rm = TRUE),\n                          med_meta = median(metascore, na.rm = TRUE))\nggplot(data = videogame_df, aes(x = metascore, y = median_playtime)) +\n  geom_point() +\n  geom_smooth()\nggplot(data = videogame_df, aes(x = metascore, y = median_playtime)) +\n  geom_point() +\n  geom_smooth()\n## Hello! How do I get rid of the units from the values in\n## my variable `x`? Thanks!\nlibrary(tidyverse)\ntest_df <- read_csv(here(\"data/parsedf.csv\"))\nhead(test_df)\n## Hello! How do I get rid of the units from the values in\n## my variable `xvar`? Thanks!\nlibrary(tidyverse)\ntest_df2 <- tibble(xvar = c(\"20,000 dollars\", \"40 dollars\"),\n                   yvar = c(1, 2))\ntest_df2"
  },
  {
    "objectID": "04-tidyr.html",
    "href": "04-tidyr.html",
    "title": "7  Tidying with tidyr",
    "section": "",
    "text": "Goals:\nThe Data: We will first use a polling data set that contains variables collected from a few different polls in July 2016 for the U.S. presidential election. The data set was scraped from RealClear politics https://www.realclearpolitics.com/epolls/latest_polls/president/ by Dr. Ramler. The variables are:"
  },
  {
    "objectID": "06-basics.html",
    "href": "06-basics.html",
    "title": "8  Coding in Base R",
    "section": "",
    "text": "Goals:\nMotivation\n“Base R” generally refers to R code that we can use without loading in any outside packages (so this code is not in the tidyverse family of packages). Why is the chapter on R basics not the first chapter that we discuss? There certainly are advantages of doing things that way, but there are also advantages of not starting out with something like “classes of variables in R.”\nFirst, it’s not the most inherently interesting thing to look at. It’s a lot more fun to make plots and wrangle data. As long as someone makes sure that the variables are already of the “correct” class, then there’s no need to talk about this.\nSecond, much of what we discuss here will make more sense, having the previous four chapters under our belt. We’ll be able to see how misspecified variable classes cause issues in certain summaries and plots and we already know how to make those plots and get those summaries."
  },
  {
    "objectID": "06-basics.html#variable-classes-in-r",
    "href": "06-basics.html#variable-classes-in-r",
    "title": "8  Coding in Base R",
    "section": "8.1 Variable Classes in R",
    "text": "8.1 Variable Classes in R\nR has a few different classes that variables take, including numeric, factor, character Date, and logical. Before we delve into the specifics of what these classes mean, let’s try to make some plots to illustrate why we should care about what these classes mean.\nThe videogame_clean.csv file contains variables on video games from 2004 - 2019, including\n\ngame, the name of the game\nrelease_date, the release date of the game\nrelease_date2, a second coding of release date\nprice, the price in dollars,\nowners, the number of owners (given in a range)\nmedian_playtime, the median playtime of the game\nmetascore, the score from the website Metacritic\nprice_cat, 1 for Low (less than 10.00 dollars), 2 for Moderate (between 10 and 29.99 dollars), and 3 for High (30.00 or more dollars)\nmeta_cat, Metacritic’s review system, with the following categories: “Overwhelming Dislike”, “Generally Unfavorable”, “Mixed Reviews”, “Generally Favorable”, “Universal Acclaim”.\nplaytime_miss, whether median play time is missing (TRUE) or not (FALSE)\n\nThe data set was modified from https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-07-30.\nRun the code in the following R chunk to read in the data.\n\nlibrary(tidyverse)\nlibrary(here)\nvideogame_df <- read_csv(here(\"data/videogame_clean.csv\"))\nhead(videogame_df)\n\n# A tibble: 6 × 15\n  game   relea…¹ release_…² price owners media…³ metas…⁴ price…⁵ meta_…⁶ playt…⁷\n  <chr>  <chr>   <date>     <dbl> <chr>    <dbl>   <dbl>   <dbl> <chr>   <lgl>  \n1 Half-… Nov 16… 2004-11-16  9.99 10,00…      66      96       1 Univer… FALSE  \n2 Count… Nov 1,… 2004-11-01  9.99 10,00…     128      88       1 Genera… FALSE  \n3 Count… Mar 1,… 2004-03-01  9.99 10,00…       3      65       1 Mixed … FALSE  \n4 Half-… Nov 1,… 2004-11-01  4.99 5,000…       0      NA       1 <NA>    TRUE   \n5 Half-… Jun 1,… 2004-06-01  9.99 2,000…       0      NA       1 <NA>    TRUE   \n6 CS2D   Dec 24… 2004-12-24 NA    1,000…      10      NA      NA <NA>    FALSE  \n# … with 5 more variables: number <dbl>, developer <chr>, publisher <chr>,\n#   average_playtime <dbl>, meta_cat_factor <chr>, and abbreviated variable\n#   names ¹​release_date, ²​release_date2, ³​median_playtime, ⁴​metascore,\n#   ⁵​price_cat, ⁶​meta_cat, ⁷​playtime_miss\n# ℹ Use `colnames()` to see all variable names\n\n\nA data frame or tibble holds variables that are allowed to be different classes. If a variable is a different class than you would expect, you’ll get some strange errors or results when trying to wrangle the data or make graphics.\nRun the following lines of code. In some cases, we are only using the first 100 observations in videogame_small. Otherwise, the code would take a very long time to run.\n\nvideogame_small <- videogame_df |> slice(1:100)\nggplot(data = videogame_small, aes(x = release_date, y = price)) +\n  geom_point() \n\nWarning: Removed 5 rows containing missing values (geom_point).\n\n\n\n\nggplot(data = videogame_small, aes(x = release_date2, y = metascore)) +\n  geom_point(aes(colour = price_cat))\n\nWarning: Removed 43 rows containing missing values (geom_point).\n\n\n\n\n\nIn the first plot, release_date isn’t ordered according to how you would expect (by date). Instead, R orders it alphabetically.\nIn the second plot, we would expect to get a plot with 3 different colours, one for each level of price_cat. Instead, we get a continuous colour scale, which doesn’t make sense, given that price_cat can only be 1, 2, or 3.\nBoth plots are not rendered correctly because the variable classes are not correct in the underlying data set. Up until this point, the data that has been provided has almost always had the correct variable classes, by default, but that won’t always be the case!\nWe’ve actually seen both of these issues before as well (the Date issue in the exercise data and the continuous colour scale in the cars data), but, in both of these instances, code was provided to “fix” the problem. After this section, you’ll have the tools to fix many class issues on your own!\nIf you examine the output of the following line of code\n\nhead(videogame_df)\n\n# A tibble: 6 × 15\n  game   relea…¹ release_…² price owners media…³ metas…⁴ price…⁵ meta_…⁶ playt…⁷\n  <chr>  <chr>   <date>     <dbl> <chr>    <dbl>   <dbl>   <dbl> <chr>   <lgl>  \n1 Half-… Nov 16… 2004-11-16  9.99 10,00…      66      96       1 Univer… FALSE  \n2 Count… Nov 1,… 2004-11-01  9.99 10,00…     128      88       1 Genera… FALSE  \n3 Count… Mar 1,… 2004-03-01  9.99 10,00…       3      65       1 Mixed … FALSE  \n4 Half-… Nov 1,… 2004-11-01  4.99 5,000…       0      NA       1 <NA>    TRUE   \n5 Half-… Jun 1,… 2004-06-01  9.99 2,000…       0      NA       1 <NA>    TRUE   \n6 CS2D   Dec 24… 2004-12-24 NA    1,000…      10      NA      NA <NA>    FALSE  \n# … with 5 more variables: number <dbl>, developer <chr>, publisher <chr>,\n#   average_playtime <dbl>, meta_cat_factor <chr>, and abbreviated variable\n#   names ¹​release_date, ²​release_date2, ³​median_playtime, ⁴​metascore,\n#   ⁵​price_cat, ⁶​meta_cat, ⁷​playtime_miss\n# ℹ Use `colnames()` to see all variable names\n\n\nyou’ll see that, at the very top of the output, right below the variable names, R provides you with the classes of variables in the tibble.\n\n<chr> is character, used for strings or text.\n<fct> is used for variables that are factors, typically used for character variables with a finite number of possible values the variable can take on.\n<date> is used for dates.\n<dbl> stands for double and is used for the numeric class.\n<int> is for numbers that are all integers. In practice, there is not much difference between this class and class dbl.\n<lgl> is for logical, variables that are either TRUE or FALSE.\n\n\n8.1.1 Referencing Variables and Using str()\nWe can use name_of_dataset$name_of_variable to look at a specific variable in a data set:\n\nvideogame_df$game\n\nprints the first thousand entries of the variable game. There are a few ways to get the class of this variable: the way that we will use most often is with str(), which stands for “structure”, and gives the class of the variable, the number of observations (26688), as well as the first couple of observations:\n\nstr(videogame_df$game)\n\n chr [1:26688] \"Half-Life 2\" \"Counter-Strike: Source\" ...\n\n\nYou can also get a variable’s class more directly with class()\n\nclass(videogame_df$game)\n\n[1] \"character\""
  },
  {
    "objectID": "06-basics.html#classes-in-detail",
    "href": "06-basics.html#classes-in-detail",
    "title": "8  Coding in Base R",
    "section": "8.2 Classes in Detail",
    "text": "8.2 Classes in Detail\nThe following gives summary information about each class of variables in R:\n\n8.2.1 <chr> and <fct> Class\nWith the character class, R will give you a warning and/or a missing value if you try to perform any numerical operations:\n\nmean(videogame_df$game)\n\nWarning in mean.default(videogame_df$game): argument is not numeric or logical:\nreturning NA\n\n\n[1] NA\n\nvideogame_df |> summarise(maxgame = max(game))\n\n# A tibble: 1 × 1\n  maxgame\n  <chr>  \n1 <NA>   \n\n\nYou also can’t convert a character class to numeric. You can, however, convert a character class to a <fct> class, using as.factor(). The <fct> class will be useful when we discuss the forcats package, but isn’t particularly useful now.\n\nclass(videogame_df$meta_cat)\n\n[1] \"character\"\n\nclass(as.factor(videogame_df$meta_cat))\n\n[1] \"factor\"\n\n\nIn general, as._____ will lets you convert between classes. Note, however, that we aren’t saving our converted variable anywhere. If we wanted the conversion to the factor to be saved in the data set, we can use mutate():\n\nvideogame_df <- videogame_df |>\n  mutate(meta_cat_factor = as.factor(meta_cat))\nstr(videogame_df$meta_cat_factor)\n\n Factor w/ 4 levels \"Generally Favorable\",..: 4 1 3 NA NA NA 4 1 3 NA ...\n\n\nFor most R functions, it won’t matter whether your variable is in class character or class factor. In general, though, character classes are for variables that have a ton of different levels, like the name of the videogame, whereas factors are reserved for categorical variables with a finite number of levels.\n\n\n8.2.2 <date> Class\nThe <date> class is used for dates, and the <datetime> class is used for Dates with times. R requires a very specific format for dates and times. Note that, while to the human eye, both of the following variables contain dates, only one is of class <date>:\n\nstr(videogame_df$release_date)\n\n chr [1:26688] \"Nov 16, 2004\" \"Nov 1, 2004\" \"Mar 1, 2004\" \"Nov 1, 2004\" ...\n\nstr(videogame_df$release_date2)\n\n Date[1:26688], format: \"2004-11-16\" \"2004-11-01\" \"2004-03-01\" \"2004-11-01\" \"2004-06-01\" ...\n\n\nrelease_date is class character, which is why we had the issue with the odd ordering of the dates earlier. You can try converting it using as.Date, but this function doesn’t always work:\n\nas.Date(videogame_df$release_date)\n\nError in charToDate(x): character string is not in a standard unambiguous format\n\n\nDates and times can be pretty complicated. In fact, we will spend an entire week covering them using the lubridate package.\nOn variables that are in Date format, like release_date2, we can use numerical operations:\n\nmedian(videogame_df$release_date2, na.rm = TRUE)\n\n[1] \"2017-06-09\"\n\nmean(videogame_df$release_date2, na.rm = TRUE)\n\n[1] \"2016-09-15\"\n\n\nWhat do you think taking the median or taking the mean of a date class means?\n\n\n8.2.3 <dbl> and <int> Class\nClass <dbl> and <int> are probably the most self-explanatory classes. <dbl>, the numeric class, are just variables that have only numbers in them while <int> only have integers (…, -2, -1, 0, 1, 2, ….). You can do numerical operations on either of these classes (and we’ve been doing them throughout the semester). For our purposes, these two classes are interchangeable.\n\nstr(videogame_df$price)\n\n num [1:26688] 9.99 9.99 9.99 4.99 9.99 ...\n\n\nProblems arise when numeric variables are coded as something non-numeric, or when non-numeric variables are coded as numeric. For example, examine:\n\nstr(videogame_df$price_cat)\n\n num [1:26688] 1 1 1 1 1 NA 2 1 1 1 ...\n\n\nprice_cat is categorical but is coded as 1 for cheap games, 2 for moderately priced games, and 3 for expensive games. Therefore, R thinks that the variable is numeric, when, it’s actually a factor.\n\nstr(as.factor(videogame_df$price_cat))\n\n Factor w/ 3 levels \"1\",\"2\",\"3\": 1 1 1 1 1 NA 2 1 1 1 ...\n\n\nThis is the cause of the odd colour scale that we encountered earlier and can be fixed by converting price_cat to a factor:\n\nvideogame_df <- videogame_df |>\n  mutate(price_factor = as.factor(price_cat)) \nggplot(data = videogame_df, aes(x = release_date2, y = metascore)) +\n  geom_point(aes(colour = price_factor))\n\nWarning: Removed 23838 rows containing missing values (geom_point).\n\n\n\n\n\n\n\n8.2.4 <lgl> Class\nFinally, there is a class of variables called logical. These variables can only take 2 values: TRUE or FALSE. For example, playtime_miss, a variable for whether or not the median_playtime variable is missing or not, is logical:\n\nstr(videogame_df$playtime_miss)\n\n logi [1:26688] FALSE FALSE FALSE TRUE TRUE FALSE ...\n\n\nIt’s a little strange at first, but R can perform numeric operations on logical classes. All R does is treat every TRUE as a 1 and every FALSE as a 0. Therefore, sum() gives the total number of TRUEs and mean() gives the proportion of TRUEs. So, we can find the number and proportion of games that are missing their median_playtime as:\n\nsum(videogame_df$playtime_miss)\n\n[1] 25837\n\nmean(videogame_df$playtime_miss)\n\n[1] 0.968113\n\n\nThere’s a lot of games that are missing this information!\nWe’ve actually used the ideas of logical variables for quite some time now, particularly in statements involving if_else(), case_when(), filter(), and mutate().\nThe primary purpose of this section is to be able to identify variable classes and be able to convert between the different variable types with mutate() to “fix” variables with the incorrect class.\n\n\n8.2.5 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-6).\nWe will use the fitness data set again for this set of exercises, as the data set has some of the issues with variable class that we have discussed. However, in week 1, some of the work of the work to fix those issues was already done before you saw the data. Now, you’ll get to fix a couple of those issues! Read in the data set with:\n\nlibrary(tidyverse)\nfitness_df <- read_csv(here(\"data/higham_fitness_notclean.csv\"))\n\n\n* What is the issue with the following plot? After you figure out the issue, use mutate() to create a new variable that fixes the issue and then reconstruct the graph.\n\n\nggplot(data = fitness_df, aes(x = active_cals)) +\n  geom_freqpoly(aes(group = weekday, colour = weekday))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n* What is another variable in the data set that has an incorrect class?\nCreate a new variable, called step_goal that is 1 or TRUE if at least 10000 steps were walked and 0 or FALSE if fewer than 10000 steps were walked. Using this variable, find the total number of days where the goal was met and the proportion of the days where the goal was met."
  },
  {
    "objectID": "06-basics.html#object-types-and-subsetting",
    "href": "06-basics.html#object-types-and-subsetting",
    "title": "8  Coding in Base R",
    "section": "8.3 Object Types and Subsetting",
    "text": "8.3 Object Types and Subsetting\nVariables of these different classes can be stored in a variety of different objects in R. We have almost exclusively used the tibble object type. The tidy tibble\n\nis “rectangular” and has a specific number of rows and columns.\nhas columns that are variables\neach column must have elements that are of the same class, but different columns can be of different classes. This allows us to have character and numeric variables in the same tibble.\n\n\n8.3.1 tibble and data.frame\nThe tibble object is very similar to the data.frame object. You can also check what type of object you’re working with using the str() command:\n\nstr(videogame_df) ## look at the beginning to see \"tibble\"\n\nWe will have a small section on tibbles in the coming weeks so we won’t focus on them here. But, we should take note that, to reference a specific element in a tibble, called indexing, you can use [# , #]. So, for example, videogame_df[5, 3] grabs the value in the fifth row and the third column:\n\nvideogame_df[5, 3]\n\n# A tibble: 1 × 1\n  release_date2\n  <date>       \n1 2004-06-01   \n\n\nMore often, we’d want to grab an entire row (or range of rows) or an entire column. We can do this by leaving the row number blank (to grab the entire column) or by leaving the column number blank (to grab the entire row):\n\nvideogame_df[ ,3] ## grab the third column\n\nvideogame_df[5, ] ## grab the fifth row\n\nWe can also grab a range of columns or rows using the : operator:\n\n3:7\n\nvideogame_df[ ,3:7] ## grab columns 3 through 7\n\nvideogame_df[3:7, ] ## grab rows 3 through 7\n\nor we can grab different columns or rows using c():\n\nvideogame_df[ ,c(1, 3, 4)] ## grab columns 1, 3, and 4\n\nvideogame_df[c(1, 3, 4), ] ## grab rows 1, 3, and 4\n\nTo get rid of an entire row or column, use -: videogame_df[ ,-c(1, 2)] drops the first and second columns while videogame_df[-c(1, 2), ] drops the first and second rows.\n\n\n8.3.2 Vectors\nA vector is an object that holds “things”, or elements, of the same class. You can create a vector in R using the c() function, which stands for “concatenate”. We’ve used the c() function before to bind things together; we just hadn’t yet discussed it in the context of creating a vector.\n\nvec1 <- c(1, 3, 2)\nvec2 <- c(\"b\", 1, 2)\nvec3 <- c(FALSE, FALSE, TRUE)\nstr(vec1); str(vec2); str(vec3)\n\n num [1:3] 1 3 2\n\n\n chr [1:3] \"b\" \"1\" \"2\"\n\n\n logi [1:3] FALSE FALSE TRUE\n\n\nNotice that vec2 is a character class. R requires all elements in a vector to be of one class; since R knows b can’t be numeric, it makes all of the numbers characters as well.\nUsing the dataset$variable draws out a vector from a tibble or data.frame:\n\nvideogame_df$metascore\n\nIf you wanted to make the above vector “by hand”, you’d need to have a lot of patience: c(96, 88, 65, NA, NA, NA, 93, .........)\nJust like tibbles, you can save vectors as something for later use:\n\nmetavec <- videogame_df$metascore\nmean(metavec, na.rm = TRUE)\n\n[1] 71.89544\n\n\nHow would you get the mean metascore using dplyr functions?\nVectors are one-dimensional: if we want to grab the 100th element of a vector we just use name_of_vector[100]:\n\nmetavec[100] ## 100th element is missing\n\n[1] NA\n\n\nBe aware that, if you’re coming from a math perspective, a “vector” in R doesn’t correspond to a “vector” in mathematics or physics.\n\n\n8.3.3 Lists\nLists are one of the more flexible objects in R: you can put objects of different classes in the same list and lists aren’t required to be rectangular (like tibbles are). Lists are extremely useful because of this flexibility, but, we won’t use them much in this class. Therefore, we will just see an example of a list before moving on:\n\ntestlist <- list(\"a\", 4, c(1, 4, 2, 6),\n                 tibble(x = c(1, 2), y = c(3, 2)))\ntestlist\n\n[[1]]\n[1] \"a\"\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 1 4 2 6\n\n[[4]]\n# A tibble: 2 × 2\n      x     y\n  <dbl> <dbl>\n1     1     3\n2     2     2\n\n\ntestlist has four elements: a single character \"a\", a single number 4, a vector of 1, 4, 2, 6, and a tibble with a couple of variables. Lists can therefore be used to store complex information that wouldn’t be as easily stored in a vector or tibble.\n\n\n8.3.4 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-6).\n\n* Look at the subsetting commands with [ , ]. What dplyr functions can you use to do the same thing?\nCreate a tibble called last100 that only has the last 100 days in the data set using both (1) indexing with [ , ] and (2) a dplyr function.\nCreate a tibble that doesn’t have the flights variable using both (1) indexing with [ , ] and (2) a dplyr function.\n* Use the following steps to create a new variable weekend_ind, which will be “weekend” if the day of the week is Saturday or Sunday and “weekday” if the day of the week is any other day. The current weekday variable is coded so that 1 represents Sunday, 2 represents Monday, …., and 7 represents Saturday.\n\n\nCreate a vector that has the numbers corresponding to the two weekend days. Name the vector and then create a second vector that has the numbers corresponding to the five weekday days.\nUse dplyr functions and the %in% operator to create the new weekend_ind variable. You can use the following code chunk to help with what %in% does:\n\n\n1 %in% c(1, 2, 3, 4)\n2 %in% c(1, 2, 3, 4)\n\n2 %in% c(3, 4, 5, 6)"
  },
  {
    "objectID": "06-basics.html#other-useful-base-r-functions",
    "href": "06-basics.html#other-useful-base-r-functions",
    "title": "8  Coding in Base R",
    "section": "8.4 Other Useful Base R Functions",
    "text": "8.4 Other Useful Base R Functions\nIn addition to functions like %in% in the previous exercise, there are many other useful base R functions. The following give some of the functions that I think are most useful for data science.\nGenerating Data: rnorm(), sample(), and set.seed()\nrnorm() can be used to generate a certain number of normal random variables with a given mean and standard deviation. It has three arguments: the sample size, the mean, and the standard deviation.\nsample() can be used to obtain a sample from a vector, either with or without replacement: it has two required arguments: the vector that we want to sample from and size, the size of the sample.\nset.seed() can be used to fix R’s random seed. This can be set so that, for example, each person in our class can get the same random sample as long we all set the same seed.\nThese can be combined to quickly generate toy data. For example, below we are generating two quantitative variables (that are normally distributed) and two categorical variables:\n\nset.seed(15125141)\ntoy_df <- tibble(xvar = rnorm(100, 3, 4),\n                 yvar = rnorm(100, -5, 10),\n                 group1 = sample(c(\"A\", \"B\", \"C\"), size = 100, replace = TRUE),\n                 group2 = sample(c(\"Place1\", \"Place2\", \"Place3\"), size = 100,\n                                 replace = TRUE))\ntoy_df\n\n# A tibble: 100 × 4\n     xvar   yvar group1 group2\n    <dbl>  <dbl> <chr>  <chr> \n 1  0.516 -13.5  B      Place2\n 2 -0.891 -13.3  A      Place2\n 3  5.58  -14.3  B      Place2\n 4  2.42   -4.91 C      Place1\n 5  1.43   -5.86 B      Place2\n 6  6.61   12.7  B      Place2\n 7 -2.04   -9.28 A      Place1\n 8  7.56    1.89 A      Place3\n 9 -0.425 -30.1  C      Place1\n10  4.14    2.65 C      Place2\n# … with 90 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nTables: We can use the table() function with the $ operator to quickly generate tables of categorical variables:\n\ntable(toy_df$group1)\n\n\n A  B  C \n27 39 34 \n\ntable(toy_df$group1, toy_df$group2)\n\n   \n    Place1 Place2 Place3\n  A     10      8      9\n  B      9     20     10\n  C     10     10     14\n\n\nOthers: There are quite a few other useful base R functions. nrow() can be used on a data frame to quickly look at the number of rows of the data frame and summary() can be used to get a quick summary of a vector:\n\nnrow(toy_df)\n\n[1] 100\n\nsummary(toy_df$yvar)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-30.123 -12.938  -5.380  -6.630  -1.298  13.858 \n\n\nThere are also some useful functions for viewing a data frame. View() function can be used in your console window on a data frame: View(toy_df) to pull up a spreadsheet-like view of the data set in a different window within R Studio.\nOptions to print() allow us to view more rows or more columns in the console printout:\n\ntoy_df |>\n  print(n = 60) ## print out 60 rows\n\n# A tibble: 100 × 4\n     xvar    yvar group1 group2\n    <dbl>   <dbl> <chr>  <chr> \n 1  0.516 -13.5   B      Place2\n 2 -0.891 -13.3   A      Place2\n 3  5.58  -14.3   B      Place2\n 4  2.42   -4.91  C      Place1\n 5  1.43   -5.86  B      Place2\n 6  6.61   12.7   B      Place2\n 7 -2.04   -9.28  A      Place1\n 8  7.56    1.89  A      Place3\n 9 -0.425 -30.1   C      Place1\n10  4.14    2.65  C      Place2\n11  5.03   -8.82  C      Place1\n12  2.98  -22.7   C      Place1\n13  5.97   -2.67  B      Place3\n14  0.882   1.59  A      Place1\n15  2.14   -5.63  B      Place3\n16  7.74    5.79  C      Place3\n17  5.20   -3.17  B      Place2\n18  2.89   -0.697 A      Place1\n19  2.71    1.09  B      Place2\n20  5.87   -4.87  C      Place1\n21  5.65  -10.3   B      Place2\n22 -0.520  -4.77  B      Place3\n23 -0.130 -18.3   B      Place3\n24 -0.174 -18.9   A      Place3\n25  4.33    4.63  A      Place3\n26  0.462 -12.8   A      Place3\n27  5.53   -3.36  C      Place1\n28  1.66   -5.34  A      Place1\n29 -0.469 -13.2   C      Place2\n30  7.51  -13.4   B      Place1\n31 -1.82   -6.47  C      Place3\n32 -2.44   -2.17  C      Place3\n33  1.52  -12.6   C      Place2\n34  4.60   -6.69  A      Place2\n35  3.10  -25.5   A      Place2\n36 -0.682 -20.4   A      Place1\n37 -5.72    2.65  B      Place3\n38  0.976 -12.1   B      Place3\n39  1.39    2.78  B      Place2\n40  6.67  -14.6   A      Place1\n41  3.09  -10.4   B      Place2\n42 -1.98  -12.8   A      Place3\n43  0.225  13.9   C      Place1\n44  5.71   -3.50  A      Place3\n45  5.57   -2.02  B      Place3\n46  8.96   -1.86  B      Place2\n47  3.80  -11.3   C      Place1\n48  7.40   -1.32  C      Place2\n49  0.988   4.04  B      Place2\n50  1.93   -5.24  A      Place2\n51  5.23   13.2   C      Place2\n52 -2.13  -19.6   A      Place2\n53  6.05   -4.42  A      Place3\n54  0.865  -9.47  A      Place1\n55  4.16  -16.4   B      Place1\n56 -2.73   -4.09  B      Place2\n57 -0.532   7.70  C      Place3\n58 -2.96  -11.6   C      Place3\n59  4.34   -5.99  B      Place2\n60  6.72  -13.6   B      Place2\n# … with 40 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\ntoy_df |>\n  print(width = Inf) ## print out all of the columns\n\n# A tibble: 100 × 4\n     xvar   yvar group1 group2\n    <dbl>  <dbl> <chr>  <chr> \n 1  0.516 -13.5  B      Place2\n 2 -0.891 -13.3  A      Place2\n 3  5.58  -14.3  B      Place2\n 4  2.42   -4.91 C      Place1\n 5  1.43   -5.86 B      Place2\n 6  6.61   12.7  B      Place2\n 7 -2.04   -9.28 A      Place1\n 8  7.56    1.89 A      Place3\n 9 -0.425 -30.1  C      Place1\n10  4.14    2.65 C      Place2\n# … with 90 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nWe will stop here, but will surely encounter more base R functions as we run into different types of problems.\n\n8.4.1 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-6).\n\nUse dplyr and tidyr functions to re-create the tables generated from\n\n\ntable(toy_df$group1)\n\n\n A  B  C \n27 39 34 \n\ntable(toy_df$group1, toy_df$group2)\n\n   \n    Place1 Place2 Place3\n  A     10      8      9\n  B      9     20     10\n  C     10     10     14"
  },
  {
    "objectID": "06-basics.html#chapexercise-6",
    "href": "06-basics.html#chapexercise-6",
    "title": "8  Coding in Base R",
    "section": "8.5 Chapter Exercises",
    "text": "8.5 Chapter Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-6).\nWork through the following exercises pertaining to the video game data set.\n\n* Read in the data set and use filter() to remove any rows with missing metascores, missing median playtime, or have a median playtime of 0 hours.\n\nNote: We usually don’t want to remove missing values without a valid reason. In this case, a missing metascore means that the game wasn’t “major” enough to get enough critic reviews, and a missing or 0 hour median playtime means that there weren’t enough users who uploaded their playtime to the database. Therefore, any further analyses are constructed to games that are popular enough to both get enough reviews on metacritic and have enough users upload their median playtimes.\n\nvideogame_df <- read_csv(here(\"data/videogame_clean.csv\"))\n\n\n* Make a scatterplot with median_playtime on the y-axis and metascore on the x-axis with the filtered data set.\n* Something you may notice is that many of the points directly overlap one another. This is common when at least one of the variables on a scatterplot is discrete: metascore can only take on integer values in this case. Change geom_point() in your previous plot to geom_jitter(). Then, use the help to write a sentence about what geom_jitter() does.\n* Another option is to control point transparency with alpha. In your geom_jitter() statement, change alpha so that you can still see all of the points, but so that you can tell in the plot where a lot of points are overlapping.\n* Label the points that have median playtimes above 1500 hours. You may want to use the ggrepel package so that the labels don’t overlap.\nChoose one of the games that got labeled and Google that game’s median, or possibly average, play time. Is it in the vicinity as the median_playtime recorded in our data set?\n\n\n\n\n\nWhat should be done about the outliers? We will discuss and investigate this issue as a class."
  },
  {
    "objectID": "06-basics.html#solutions-6",
    "href": "06-basics.html#solutions-6",
    "title": "8  Coding in Base R",
    "section": "8.6 Exercise Solutions",
    "text": "8.6 Exercise Solutions\n\n8.6.1 Variable Classes S\n\n\n8.6.2 Classes in Detail S\n\n* What is the issue with the following plot? After you figure out the issue, use mutate() to create a new variable that fixes the issue and then reconstruct the graph.\n\n\nggplot(data = fitness_df, aes(x = active_cals)) +\n  geom_freqpoly(aes(group = weekday, colour = weekday))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe issue is that weekday should be a factor, not numeric.\n\nfitness_df <- fitness_df |> mutate(weekday_cat = as.factor(weekday))\nggplot(data = fitness_df, aes(x = active_cals)) +\n  geom_freqpoly(aes(group = weekday_cat, colour = weekday_cat)) +\n  scale_colour_viridis_d()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n* What is another variable in the data set that has an incorrect class?\n\nMonth should be an ordered factor, not numeric.\n\n\n8.6.3 Object Types S\n\n* Look at the subsetting commands with [ , ]. What dplyr functions can you use to do the same thing?\n\nslice() can be used for the row indexing while select() can be used for the column indexing.\n\n* Use the following steps to create a new variable weekend_ind, which will be “weekend” if the day of the week is Saturday or Sunday and “weekday” if the day of the week is any other day. The current weekday variable is coded so that 1 represents Sunday, 2 represents Monday, …., and 7 represents Saturday.\n\n\nCreate a vector that has the numbers corresponding to the two weekend days. Name the vector and then create a second vector that has the numbers corresponding to the five weekday days.\n\n\nvecweekend <- c(1, 7)\nvecweekday <- 2:6 ## or c(2, 3, 4, 5, 6)\n\n\nUse dplyr functions and the %in% operator to create the new weekend_ind variable. You can use the following code chunk to help with what %in% does:\n\n\n1 %in% c(1, 2, 3, 4)\n\n[1] TRUE\n\n2 %in% c(1, 2, 3, 4)\n\n[1] TRUE\n\n2 %in% c(3, 4, 5, 6)\n\n[1] FALSE\n\n\n\nfitness_df |>\n  mutate(weekend_ind = case_when(weekday %in% vecweekend ~ \"weekend\",\n                                 weekday %in% vecweekday ~ \"weekday\")) |>\n  select(weekend_ind, everything())\n\n# A tibble: 993 × 11\n   weekend_ind Start      active_…¹ dista…² flights  steps month weekday dayof…³\n   <chr>       <date>         <dbl>   <dbl>   <dbl>  <dbl> <dbl>   <dbl>   <dbl>\n 1 weekday     2018-11-28      57.8   0.930       0  1885.    11       4     332\n 2 weekday     2018-11-29     509.    4.64       18  8953.    11       5     333\n 3 weekday     2018-11-30     599.    6.05       12 11665     11       6     334\n 4 weekend     2018-12-01     661.    6.80        6 12117     12       7     335\n 5 weekend     2018-12-02     527.    4.61        1  8925.    12       1     336\n 6 weekday     2018-12-03     550.    3.96        2  7205     12       2     337\n 7 weekday     2018-12-04     670.    6.60        5 12483.    12       3     338\n 8 weekday     2018-12-05     557.    4.91        6  9258.    12       4     339\n 9 weekday     2018-12-06     997.    7.50       13 14208     12       5     340\n10 weekday     2018-12-07     533.    4.27        8  8269.    12       6     341\n# … with 983 more rows, 2 more variables: stepgoal <dbl>, weekday_cat <fct>,\n#   and abbreviated variable names ¹​active_cals, ²​distance, ³​dayofyear\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n## can also use if_else, which is actually a little simpler in this case:\nfitness_df |> mutate(weekend_ind = if_else(weekday %in% vecweekend,\n  true = \"weekend\", false = \"weekday\")) |>\n  select(weekend_ind, everything())\n\n# A tibble: 993 × 11\n   weekend_ind Start      active_…¹ dista…² flights  steps month weekday dayof…³\n   <chr>       <date>         <dbl>   <dbl>   <dbl>  <dbl> <dbl>   <dbl>   <dbl>\n 1 weekday     2018-11-28      57.8   0.930       0  1885.    11       4     332\n 2 weekday     2018-11-29     509.    4.64       18  8953.    11       5     333\n 3 weekday     2018-11-30     599.    6.05       12 11665     11       6     334\n 4 weekend     2018-12-01     661.    6.80        6 12117     12       7     335\n 5 weekend     2018-12-02     527.    4.61        1  8925.    12       1     336\n 6 weekday     2018-12-03     550.    3.96        2  7205     12       2     337\n 7 weekday     2018-12-04     670.    6.60        5 12483.    12       3     338\n 8 weekday     2018-12-05     557.    4.91        6  9258.    12       4     339\n 9 weekday     2018-12-06     997.    7.50       13 14208     12       5     340\n10 weekday     2018-12-07     533.    4.27        8  8269.    12       6     341\n# … with 983 more rows, 2 more variables: stepgoal <dbl>, weekday_cat <fct>,\n#   and abbreviated variable names ¹​active_cals, ²​distance, ³​dayofyear\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\n\n\n8.6.4 Chapter Exercises S\n\n* Read in the data set and use filter() to remove any rows with missing metascores, missing median playtime, or have a median playtime of 0 hours.\n\nNote: We usually don’t want to remove missing values without a valid reason. In this case, a missing metascore means that the game wasn’t “major” enough to get enough critic reviews, and a missing or 0 hour median playtime means that there weren’t enough users who uploaded their playtime to the database. Therefore, any further analyses are constructed to games that are popular enough to both get enough reviews on metacritic and have enough users upload their median playtimes.\n\nvideogame_df <- read_csv(here(\"data/videogame_clean.csv\"))\n\n\nvideogame_nomiss <- videogame_df |>\n  filter(!is.na(median_playtime) &\n           !is.na(metascore) &\n           median_playtime != 0)\n\n\n* Make a scatterplot with median_playtime on the y-axis and metascore on the x-axis with the filtered data set.\n\n\nggplot(data = videogame_nomiss, aes(x = metascore,\n                                    y = median_playtime)) + \n  geom_point()\n\n\n\n\n\n* Something you may notice is that many of the points directly overlap one another. This is common when at least one of the variables on a scatterplot is discrete: metascore can only take on integer values in this case. Change geom_point() in your previous plot to geom_jitter(). Then, use the help to write a sentence about what geom_jitter() does.\n\n\nggplot(data = videogame_nomiss, aes(x = metascore,\n                                    y = median_playtime)) + \n  geom_jitter()\n\n\n\n\ngeom_jitter() adds a small amount of “noise” to each data point so that points don’t overlap quite as much.\n\n* Another option is to control point transparency with alpha. In your geom_jitter() statement, change alpha so that you can still see all of the points, but so that you can tell in the plot where a lot of points are overlapping.\n\n\nggplot(data = videogame_nomiss, aes(x = metascore,\n                                    y = median_playtime)) + \n  geom_jitter(alpha = 0.4)\n\n\n\n## can see a lot of ponits have median playtimes close to 0\n\n\n* Label the points that have median playtimes above 1500 hours. You may want to use the ggrepel package so that the labels don’t overlap.\n\n\nlibrary(ggrepel)\nvideogame_long <- videogame_nomiss |> filter(median_playtime > 1500)\nggplot(data = videogame_nomiss,\n       aes(x = metascore, y = median_playtime)) + \n  geom_jitter(alpha = 0.4) +\n  geom_label_repel(data = videogame_long, aes(label = game))"
  },
  {
    "objectID": "06-basics.html#rcode-6",
    "href": "06-basics.html#rcode-6",
    "title": "8  Coding in Base R",
    "section": "8.7 Non-Exercise R Code",
    "text": "8.7 Non-Exercise R Code\n\nlibrary(tidyverse)\nlibrary(here)\nvideogame_df <- read_csv(here(\"data/videogame_clean.csv\"))\nhead(videogame_df)\nvideogame_small <- videogame_df |> slice(1:100)\nggplot(data = videogame_small, aes(x = release_date, y = price)) +\n  geom_point() \n\nggplot(data = videogame_small, aes(x = release_date2, y = metascore)) +\n  geom_point(aes(colour = price_cat))\nhead(videogame_df)\nvideogame_df$game\nstr(videogame_df$game)\nclass(videogame_df$game)\nmean(videogame_df$game)\nvideogame_df |> summarise(maxgame = max(game))\nclass(videogame_df$meta_cat)\nclass(as.factor(videogame_df$meta_cat))\nvideogame_df <- videogame_df |>\n  mutate(meta_cat_factor = as.factor(meta_cat))\nstr(videogame_df$meta_cat_factor)\nstr(videogame_df$release_date)\nstr(videogame_df$release_date2)\nmedian(videogame_df$release_date2, na.rm = TRUE)\nmean(videogame_df$release_date2, na.rm = TRUE)\nstr(videogame_df$price)\nstr(videogame_df$price_cat)\nstr(as.factor(videogame_df$price_cat))\nvideogame_df <- videogame_df |>\n  mutate(price_factor = as.factor(price_cat)) \nggplot(data = videogame_df, aes(x = release_date2, y = metascore)) +\n  geom_point(aes(colour = price_factor))\nstr(videogame_df$playtime_miss)\nsum(videogame_df$playtime_miss)\nmean(videogame_df$playtime_miss)\nstr(videogame_df) ## look at the beginning to see \"tibble\"\nvideogame_df[5, 3]\nvideogame_df[ ,3] ## grab the third column\n\nvideogame_df[5, ] ## grab the fifth row\n3:7\n\nvideogame_df[ ,3:7] ## grab columns 3 through 7\n\nvideogame_df[3:7, ] ## grab rows 3 through 7\nvideogame_df[ ,c(1, 3, 4)] ## grab columns 1, 3, and 4\n\nvideogame_df[c(1, 3, 4), ] ## grab rows 1, 3, and 4\nvec1 <- c(1, 3, 2)\nvec2 <- c(\"b\", 1, 2)\nvec3 <- c(FALSE, FALSE, TRUE)\nstr(vec1); str(vec2); str(vec3)\nvideogame_df$metascore\nmetavec <- videogame_df$metascore\nmean(metavec, na.rm = TRUE)\nmetavec[100] ## 100th element is missing\ntestlist <- list(\"a\", 4, c(1, 4, 2, 6),\n                 tibble(x = c(1, 2), y = c(3, 2)))\ntestlist\nset.seed(15125141)\ntoy_df <- tibble(xvar = rnorm(100, 3, 4),\n                 yvar = rnorm(100, -5, 10),\n                 group1 = sample(c(\"A\", \"B\", \"C\"), size = 100, replace = TRUE),\n                 group2 = sample(c(\"Place1\", \"Place2\", \"Place3\"), size = 100,\n                                 replace = TRUE))\ntoy_df\ntable(toy_df$group1)\n\ntable(toy_df$group1, toy_df$group2)\nnrow(toy_df)\nsummary(toy_df$yvar)\ntoy_df |>\n  print(n = 60) ## print out 60 rows\ntoy_df |>\n  print(width = Inf) ## print out all of the columns"
  },
  {
    "objectID": "07-forcats.html",
    "href": "07-forcats.html",
    "title": "9  Factors with forcats",
    "section": "",
    "text": "Goals:"
  },
  {
    "objectID": "07-forcats.html#change-factor-levels",
    "href": "07-forcats.html#change-factor-levels",
    "title": "9  Factors with forcats",
    "section": "9.1 Change Factor Levels",
    "text": "9.1 Change Factor Levels\nThe Data: The pokemon_allgen.csv data set contains observations on Pokemon from the first 6 Generations (the first 6 games). There are 20 variable in this data set, but, of particular interest for this chapter are\n\nType 1, the first Type characteristic of the Pokemon (a factor with 13 levels)\nType 2, the second Type characteristic of the Pokemon (a factor with 13 levels, NA if the Pokemon only has one type)\nGeneration, the generation the Pokemon first appeared in (a factor with 6 levels)\n\nRead in the data set with read_csv(). Then, use a mutate() statement to make a Generation_cat variable that is a factor.\n\nlibrary(tidyverse)\nlibrary(here)\npokemon_df <- read_csv(here(\"data/pokemon_allgen.csv\")) |>\n  mutate(Generation_cat = factor(Generation))\n\nOne easy way to get a quick summary of a factor variable is to use group_by() and n() within a summarise() statement:\n\npokemon_df |> group_by(`Type 1`) |>\n  summarise(counttype = n())\n\n# A tibble: 18 × 2\n   `Type 1` counttype\n   <chr>        <int>\n 1 Bug             75\n 2 Dark            31\n 3 Dragon          41\n 4 Electric        90\n 5 Fairy           18\n 6 Fighting        27\n 7 Fire            56\n 8 Flying           6\n 9 Ghost           58\n10 Grass           73\n11 Ground          42\n12 Ice             24\n13 Normal         108\n14 Poison          30\n15 Psychic         73\n16 Rock            47\n17 Steel           29\n18 Water          119\n\n\n\n\n9.1.1 fct_recode() to Rename Levels\nNow, let’s make a bar plot that examines how many Legendary Pokemon first appear in each generation, using dplyr commands that we’ve used and a simple geom_col():\n\npokemon_legend <- pokemon_df |> filter(Legendary == TRUE) |>\n  group_by(Generation_cat) |>\n  summarise(nlegend = n())\nggplot(data = pokemon_legend, aes(x = Generation_cat, y = nlegend)) +\n  geom_col()\n\n\n\n\nWe’ve discussed how to change many aspects of ggplot2 graphs, but we haven’t discussed how to rename the labels of levels of a categorical variable, whether those appear in the x-axis or in a separate legend. The easiest way to do this is to rename the levels in the factor itself using fct_recode(). Suppose, for example, that we want to relabel the Generation number with the actual region corresponding to each game (Kanto, Johto, Hoenn, Sinnoh, Unova, and Kalos). The function fct_recode() takes the name of a factor already present in the data set as its first argument and then a series of renaming schemes (new_name = “old_name”) as its remaining arguments.\n\npokemon_legend <- pokemon_legend |>\n  mutate(Generation_cat2 = fct_recode(Generation_cat, Kanto = \"1\",\n                                      Johto = \"2\", Hoenn = \"3\",\n                                      Sinnoh = \"4\", Unova = \"5\",\n                                      Kalos = \"6\")) |>\n  select(Generation_cat2, everything())\nhead(pokemon_legend)\n\n# A tibble: 6 × 3\n  Generation_cat2 Generation_cat nlegend\n  <fct>           <fct>            <int>\n1 Kanto           1                    6\n2 Johto           2                    5\n3 Hoenn           3                   34\n4 Sinnoh          4                   17\n5 Unova           5                   27\n6 Kalos           6                   13\n\nggplot(data = pokemon_legend,\n       aes(x = Generation_cat2, y = nlegend)) +\n  geom_col()\n\n\n\n\n\n\n9.1.2 Collapsing Many Levels Into Fewer Levels with fct_collapse()\nSometimes, you might want to collapse the levels of two or more factors into a single level. With the Pokemon data set, there isn’t an example where this really makes sense, but, in the exercises, you’ll see a good use for this function with the social survey data set. For practice, we can collapse the Ice and Dark type Pokemon into a new level called Coolest and we can collapse the Poison, Fighting, and Fire type Pokemon into a new level called Least_Cool.\n\npokemon_long <- pokemon_df |> pivot_longer(c(`Type 1`, `Type 2`),\n                            names_to = \"Number\",\n                            values_to = \"Type\")\npokemon_long |>\n  mutate(new_type = fct_collapse(Type, Coolest = c(\"Ice\", \"Dark\"),\n                                 Least_Cool = c(\"Fire\", \"Fighting\", \"Poison\"))) |>\n  select(new_type, Type, everything())\n\n# A tibble: 1,894 × 22\n   new_type   Type    `#` Name  Total    HP Attack Defense Sp. A…¹ Sp. D…² Speed\n   <fct>      <chr> <dbl> <chr> <dbl> <dbl>  <dbl>   <dbl>   <dbl>   <dbl> <dbl>\n 1 Grass      Grass     1 Bulb…   318    45     49      49      65      65    45\n 2 Least_Cool Pois…     1 Bulb…   318    45     49      49      65      65    45\n 3 Grass      Grass     2 Ivys…   405    60     62      63      80      80    60\n 4 Least_Cool Pois…     2 Ivys…   405    60     62      63      80      80    60\n 5 Grass      Grass     3 Venu…   525    80     82      83     100     100    80\n 6 Least_Cool Pois…     3 Venu…   525    80     82      83     100     100    80\n 7 Grass      Grass     3 Venu…   525    80     82      83     100     100    80\n 8 Least_Cool Pois…     3 Venu…   525    80     82      83     100     100    80\n 9 Least_Cool Fire      4 Char…   309    39     52      43      60      50    65\n10 <NA>       <NA>      4 Char…   309    39     52      43      60      50    65\n# … with 1,884 more rows, 11 more variables: Generation <dbl>, Legendary <lgl>,\n#   id <chr>, identifier <chr>, height <dbl>, weight <dbl>,\n#   base_experience <dbl>, order <dbl>, is_default <dbl>, Generation_cat <fct>,\n#   Number <chr>, and abbreviated variable names ¹​`Sp. Atk`, ²​`Sp. Def`\n\n\nWhat happens to the levels that aren’t being re-specified?\n\n\n\n9.1.3 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-7).\n\nWhat dplyr function(s) could you also use to create the new levels that were created with fct_collapse()? Why might it be a little easier to use fct_collapse()?\n* We did not properly explore the data set before making the graphs above, and, in fact, there is some double counting of Pokemon in this data set (this is another example where being familiar with the data set you’re working with is advantageous: people familiar with Pokemon know that there are fewer than 947 Pokemon in Generations 1 through 6).\n\nFigure out why some Pokemon are double counted. Then, create a new data set that only keeps one observation per Pokemon #.\n\nCreate the bar plot with your non-duplicated data set. Are your results significantly changed?"
  },
  {
    "objectID": "07-forcats.html#reorder-factor-levels",
    "href": "07-forcats.html#reorder-factor-levels",
    "title": "9  Factors with forcats",
    "section": "9.2 Reorder Factor Levels",
    "text": "9.2 Reorder Factor Levels\n\n9.2.1 Change the Order of Levels by a Quantitative Variable with fct_reorder()\nYou might also be interested in re-ordering the x or y-axis of a particular graph so that the order of the factors correspond to, for example, the median of a quantitative variable for each level. The reason you would want to do this is easiest to see with an example. For example, suppose you want to look at the most common Pokemon types across the first 6 generations. We use the non-duplicated data set from the previous section’s exercises, we pivot the data so that type is in one column, and we remove observations with missing Type, which correspond to the second Type of Pokemon that only have a single Type:\n\npokemon_nodup <- pokemon_df |> group_by(`#`) |> slice(1) |>\n  ungroup()\npokemon_long <- pokemon_nodup |>\n  pivot_longer(c(`Type 1`, `Type 2`),\n               names_to = \"Number\",\n               values_to = \"Type\")\npokemon_sum <- pokemon_long |>\n  group_by(Type) |>\n  summarise(count_type = n()) |>\n  filter(!is.na(Type))\nggplot(data = pokemon_sum, aes(x = Type,\n                               y = count_type)) +\n  geom_col() +\n  coord_flip()  ## flips the x and y axes\n\n\n\n\nHow does R order the levels of the Type factor, by default? How might you like them to be ordered to make the graph more readable?\n\nThe following code creates a new factor variable called Type_ordered that orders type by the count_type variable. fct_reorder() takes a factor as its first argument and a numeric variable to re-order that factor by as its second argument. The bar plot is then reconstructed with this new variable.\n\npokemon_sum <- pokemon_sum |> \n  mutate(Type_ordered = fct_reorder(.f = Type, .x = count_type))\nggplot(data = pokemon_sum, aes(x = Type_ordered,\n                               y = count_type)) +\n  geom_col() +\n  coord_flip()\n\n\n\n\n\n\n9.2.2 Lollipop Plots\nLollipop plots are a popular alternative to bar plots because they often look cleaner with less ink. To make a lollipop plot in R, we specify two different geoms: geom_segment() to form the stick of the lollipop and geom_point() to form the pop part of the lollipop. geom_segment() requires 4 aesthetics: x, xend, y, and yend.\n\nggplot(data = pokemon_sum, aes(x = Type_ordered,\n                               y = count_type)) +\n  geom_segment(aes(x = Type_ordered, xend = Type_ordered,\n                   y = 0, yend = count_type)) +\n  geom_point() +\n  coord_flip()\n\n\n\n\nfct_reorder() also works with boxplots or simple point plots that show, for example, the median response for each level of a factor. The following set of plots investigate how the Defense stat changes for different Pokemon types\n\npokemon_long <- pokemon_long |>\n  filter(!is.na(Type)) |>\n  mutate(Type_Deford = fct_reorder(.f = Type, .x = Defense,\n                                   .fun = median))\nggplot(data = pokemon_long, aes(x = Type_Deford,\n                               y = Defense)) +\n  geom_boxplot() + \n  coord_flip()\n\n\n\n\nThe following code makes a point plot that shows the median defense for each type instead of boxplots.\n\npokemon_med <- pokemon_long |> group_by(Type_Deford) |>\n  summarise(med_def = median(Defense)) |>\n  mutate(Type_Deford = fct_reorder(.f = Type_Deford, .x = med_def,\n                                   .fun = median))\n\nggplot(data = pokemon_med, aes(x = med_def, y = Type_Deford)) +\n  geom_point()\n\n\n\n\nFinally, we can make a lollipop plot of median defense.\n\nggplot(data = pokemon_med, aes(x = Type_Deford, y = med_def)) +\n  geom_segment(aes(xend = Type_Deford, y = 0, yend = med_def)) +\n  geom_point() +\n  coord_flip()\n\n\n\n\nDo you have a preference between the boxplot graph, the point plot, and the lollipop plot?\n\nNew Data. The gun_violence_us.csv data set was obtained from https://www.openintro.org/book/statdata/index.php?data=gun_violence_us and contains the following variables on gun violence in 2014:\n\nstate, the name of the U.S. state\nmortality_rate, number of deaths from gun violence per 100,000 people\nownership_rate, the proportion of adults who own a gun\nregion, region of the U.S. (South, West, NE, and MW)\n\n\nmortality_df <- read_csv(here(\"data/gun_violence_us.csv\")) |>\n  mutate(region = factor(region))\n\n\n\n9.2.3 Re-Leveling By Two Quantitative Variables with fct_reorder2()\nSuppose that we want to investigate the relationship between mortality_rate and ownership_rate using this data set. Run the following code to create a scatterplot of mortality_rate vs. ownership_rate with fitted linear regression lines for each region of the United States:\n\nggplot(data = mortality_df,\n       aes(x = ownership_rate, y = mortality_rate, colour = region)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\nNotice the order of the levels in the legend. Most people would prefer the order to actually match up with where the lines in the plot end, not for the order to be alphabetical. To achieve this, we can use fct_reorder2() to change the order of the factor levels:\n\nmortality_df <- mortality_df |>\n  mutate(region_2 = fct_reorder2(region,\n                                 .x = ownership_rate,\n                                 .y = mortality_rate))\nggplot(data = mortality_df,\n       aes(x = ownership_rate, y = mortality_rate, colour = region_2)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\nDid it change the order of the levels how you would expect? fct_reorder2() actually looks at points, not lines, when determining the ordering. If you want the levels to match up exactly, then we’ll have to reorder the levels manually with fct_relevel():\n\n\n9.2.4 Reordering Levels Manually with fct_relevel()\nFactors are ordered alphabetically by default. If we want precise control over the order of the levels of a factor, we can use fct_relevel(), which takes a factor and a vector of the new levels as inputs:\n\nmortality_df <- mortality_df |>\n  mutate(region_3 = fct_relevel(region, c(\"South\", \"West\", \"MW\", \"NE\")))\nggplot(data = mortality_df,\n       aes(x = ownership_rate, y = mortality_rate, colour = region_3)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\nReordering the levels of a factor manually might also be useful in fitting linear models. Recall that, by default, R makes the reference group in a linear model the first level alphabetically. If you’d like a different reference group, you can reorder the levels of the factor:\n\nmod <- lm(mortality_rate ~ ownership_rate + region, data = mortality_df)\nmod2 <- lm(mortality_rate ~ ownership_rate + region_2, data = mortality_df)\nmod3 <- lm(mortality_rate ~ ownership_rate + region_3, data = mortality_df)\nsummary(mod)\nsummary(mod2)\nsummary(mod3)\n\n\n\n9.2.5 Exercises\n\nMake the side-by-side boxplots again with the pokemon data but do not use ungroup() by running the following code.\n\n\npokemon_nodup <- pokemon_df |> group_by(`#`) |> slice(1) ## |>\n  ## ungroup()\npokemon_long <- pokemon_nodup |>\n  pivot_longer(c(`Type 1`, `Type 2`),\n               names_to = \"Number\",\n               values_to = \"Type\")\n\npokemon_long <- pokemon_long |>\n  filter(!is.na(Type)) |>\n  mutate(Type_Deford = fct_reorder(.f = Type, .x = Defense,\n                                   .fun = median))\nggplot(data = pokemon_long, aes(x = Type_Deford,\n                               y = Defense)) +\n  geom_boxplot() + \n  coord_flip()\n\n\n\n\nWhy aren’t the types ordered by median defense anymore?\n\nThe .fun argument in fct_reorder() controls how the Type factor is ordered. Change this to specify ordering by the mean, max, and min. What ordering makes the most sense? Why?"
  },
  {
    "objectID": "07-forcats.html#chapexercise-7",
    "href": "07-forcats.html#chapexercise-7",
    "title": "9  Factors with forcats",
    "section": "9.3 Chapter Exercises",
    "text": "9.3 Chapter Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-7).\nFor these chapter exercises, we will use a data set on National Football League standings from 2000 to 2020. Read in the data set with:\n\nlibrary(tidyverse)\nlibrary(here)\nstandings_df <- read_csv(here(\"data/standings.csv\"))\n\nRows: 638 Columns: 15\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): team, team_name, playoffs, sb_winner\ndbl (11): year, wins, loss, points_for, points_against, points_differential,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nstandings_df\n\n# A tibble: 638 × 15\n   team        team_…¹  year  wins  loss point…² point…³ point…⁴ margi…⁵ stren…⁶\n   <chr>       <chr>   <dbl> <dbl> <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 Miami       Dolphi…  2000    11     5     323     226      97     6.1     1  \n 2 Indianapol… Colts    2000    10     6     429     326     103     6.4     1.5\n 3 New York    Jets     2000     9     7     321     321       0     0       3.5\n 4 Buffalo     Bills    2000     8     8     315     350     -35    -2.2     2.2\n 5 New England Patrio…  2000     5    11     276     338     -62    -3.9     1.4\n 6 Tennessee   Titans   2000    13     3     346     191     155     9.7    -1.3\n 7 Baltimore   Ravens   2000    12     4     333     165     168    10.5    -2.5\n 8 Pittsburgh  Steele…  2000     9     7     321     255      66     4.1    -0.2\n 9 Jacksonvil… Jaguars  2000     7     9     367     327      40     2.5    -1.4\n10 Cincinnati  Bengals  2000     4    12     185     359    -174   -10.9     0.4\n# … with 628 more rows, 5 more variables: simple_rating <dbl>,\n#   offensive_ranking <dbl>, defensive_ranking <dbl>, playoffs <chr>,\n#   sb_winner <chr>, and abbreviated variable names ¹​team_name, ²​points_for,\n#   ³​points_against, ⁴​points_differential, ⁵​margin_of_victory,\n#   ⁶​strength_of_schedule\n\n\nThe important variables that we will use include:\n\nteam, the city where the team is based in\nteam_name, the name of the team\nplayoffs, whether or not the team made the playoffs that year\nsb_winner, whether or not the team won the superbowl that year\n\n\nUse the table() function with table(name_of_data_frame$name_of_variable) to make a table of team_name. This is useful to use for categorical variables to give a quick summary of what the levels are and how many times each level appears in the data set.\nUntil a couple of years ago, the Washington Commanders team used to be known as the Washington Redskins. Because of the obvious racism the name conveys, in 2022, the name was changed from Redskins to Commanders. Use a forcats function to rename the Redskins team_name to Commanders. Note that, usually, we have been renaming the new variable after we use a forcats function, but, oftentimes, it makes sense to just overwrite the old variable by using the same name in our mutate() statement.\nUse a function from tidyr to combine team and team_name into a single variable called franchise. You may want to specify sep = \" \" for consistency with the city names.\nThere are a couple of franchises in the national football league that moved cities in the late 2010s. In particular, the San Diego Chargers became the Los Angeles Chargers and the St. Louis Rams became the Los Angeles Rams (this is another instance where being familiar with context is helpful here: it may have taken you much longer to figure this out, had you not known much about the NFL). Use a forcats function to put the San Diego Chargers and Los Angeles Chargers into a single level, San Diego LA Chargers, and to put the St. Louis Rams and Los Angeles Rams into a single level, St. Louis LA Rams.\nUsing the updated data set, create a lollipop plot of the ten franchises who have made the playoffs most often. You will need to do some work with dplyr before making the plot.\n\n\n\n\n\nCustomize your lollipop plot by changing the way the points look at the end and / or the way the “stems” of the lollipops look. You may use https://r-graph-gallery.com/301-custom-lollipop-chart.html for inspiration.\n\n\n\nThe following are the old chapter exercises for forcats: I’ve left them in here in case you want some extra practice! We will use the general social survey data set, which is in the forcats library in R. You should some of this Wikipedia page to better understand where this data comes from Wikipedia.\nMost variables are self-explanatory, but a couple that aren’t are:\n\npartyid, political leaning and\ndenom, religious denomination (if unfamiliar with this, you can think of it as a “more specific” subset of a particular religion).\n\nNote that some of these exercises are from the R for Data Science textbook.\nLoad in the data set with\n\nlibrary(tidyverse)\ngss_cat\n\n\n* Using a forcats function, change the name of the level Not str republican to be Weak republican and change the name of the level Not str democrat to be Weak democrat. These names more closely match the levels Strong republican and Strong democrat. Then, create a table of counts that shows the number of respondents in each political party partyid.\n\nNote: Levels that aren’t specified in your forcats function do not change.\nNote 2: In naming something Weak republican, you’ll need to use backticks since there is a space in the level name.\n\n* Use a forcats function so that partyid just has 4 categories: Other (corresponding to No answer, Don’t know, Other party), Ind (corresponding to Ind,near rep, Independent, Ind, near dem), Rep (corresponding to Strong republican and Not str republican), and Dem (corresponding to Not str democrat and Strong democrat).\n* Run the code to create the following plot that shows the average number of hours of television people watch from various religions.\n\n\nrelig_summary <- gss_cat |>\n  group_by(relig) |>\n  summarise(\n    age = mean(age, na.rm = TRUE),\n    tvhours = mean(tvhours, na.rm = TRUE),\n    n = n()\n  )\n\nggplot(data = relig_summary, aes(tvhours, relig)) +\n  geom_point()\n\nThen, use a forcats function create a new variable in the data set that reorders the religion factor levels and make a lollipop plot so that the religion watches the most television, on average, is on the top, and the religion that watches the least television, on average, is on the bottom.\n\n* Run the code to make the following line plot that shows age on the x-axis, the proportion on the y-axis, and is coloured by various marital statuses (married, divorced, widowed, etc.):\n\n\nby_age <- gss_cat |>\n  filter(!is.na(age)) |>\n  count(age, marital) |>\n  group_by(age) |>\n  mutate(prop = n / sum(n))\n\nggplot(by_age, aes(age, prop,\n                  colour = marital)) +\n  geom_line(na.rm = TRUE) +\n  labs(colour = \"marital\")\n\nThen, use a forcats function to make the plot so that the legend labels line up better with the different coloured marital status lines (e.g. so that the label for widowed is the first that appears in the legend, the label for married is second, etc.).\n\nWe haven’t talked much about creating two-way tables (or contingency tables). These are generally quite difficult to make with the tidyverse functions, but you can use the base R table() and prop.table() functions to make these.\n\nUsing data only from the year 2014, run the following code to make 4 two-way tables with the party_small variable that was constructed earlier and race:\n\ngss_cat <- gss_cat |> mutate(party_small = fct_collapse(partyid,\n                                              Other = c(\"No answer\", \"Don't know\", \"Other party\"),\n                                              Ind = c(\"Ind,near rep\", \"Independent\", \"Ind,near dem\"),\n                                              Rep = c(\"Strong republican\", \"Not str republican\"),\n                                              Dem = c(\"Not str democrat\", \"Strong democrat\")))\n\ngss_recent <- gss_cat |> filter(year == 2014)\n\ntab1 <- table(gss_recent$party_small, gss_recent$race)\ntab1\n\n       \n        Other Black White Not applicable\n  Other     8    12    68              0\n  Rep      22    17   498              0\n  Ind     152   108   828              0\n  Dem      80   249   496              0\n\nprop.table(tab1)\n\n       \n              Other       Black       White Not applicable\n  Other 0.003152088 0.004728132 0.026792750    0.000000000\n  Rep   0.008668243 0.006698188 0.196217494    0.000000000\n  Ind   0.059889677 0.042553191 0.326241135    0.000000000\n  Dem   0.031520883 0.098108747 0.195429472    0.000000000\n\nprop.table(tab1, margin = 1)\n\n       \n             Other      Black      White Not applicable\n  Other 0.09090909 0.13636364 0.77272727     0.00000000\n  Rep   0.04096834 0.03165736 0.92737430     0.00000000\n  Ind   0.13970588 0.09926471 0.76102941     0.00000000\n  Dem   0.09696970 0.30181818 0.60121212     0.00000000\n\nprop.table(tab1, margin = 2)\n\n       \n             Other      Black      White Not applicable\n  Other 0.03053435 0.03108808 0.03597884               \n  Rep   0.08396947 0.04404145 0.26349206               \n  Ind   0.58015267 0.27979275 0.43809524               \n  Dem   0.30534351 0.64507772 0.26243386               \n\n\nUse the help on ?prop.table to figure out how each of these three tables are constructed.\nWhich table do you think is most informative? What conclusions does it help you to draw?"
  },
  {
    "objectID": "07-forcats.html#solutions-7",
    "href": "07-forcats.html#solutions-7",
    "title": "9  Factors with forcats",
    "section": "9.4 Exercise Solutions",
    "text": "9.4 Exercise Solutions\n\n9.4.1 Change Factor Levels S\n\n* We did not properly explore the data set before making the graphs above, and, in fact, there is some double counting of Pokemon in this data set (this is another example where being familiar with the data set you’re working with is advantageous: people familiar with Pokemon know that there are fewer than 947 Pokemon in Generations 1 through 6).\n\nFigure out why some Pokemon are double counted. Then, create a new data set that only keeps one observation per Pokemon #.\n\npokemon_nodup <- pokemon_df |> group_by(`#`) |> slice(1) |>\n  ungroup()\n\n\n\n9.4.2 Reorder Factor Levels S\n\n\n9.4.3 Chapter Exercises S\n\n* Using a forcats function, change the name of the level Not str republican to be Weak republican and change the name of the level Not str democrat to be Weak democrat. These names more closely match the levels Strong republican and Strong democrat. Then, create a table of counts that shows the number of respondents in each political party partyid.\n\nNote: Levels that aren’t specified in your forcats function do not change.\nNote 2: In naming something Weak republican, you’ll need to use backticks since there is a space in the level name.\n\ngss_cat |>\n  mutate(partyid_new = fct_recode(partyid,\n                                  `Weak republican` = \"Not str republican\",\n                                  `Weak democrat` = \"Not str democrat\")) |> group_by(partyid_new) |>\n  summarise(ncount = n())\n\n\n* Use a forcats function so that partyid just has 4 categories: Other (corresponding to No answer, Don’t know, Other party), Ind (corresponding to Ind,near rep, Independent, Ind, near dem), Rep (corresponding to Strong republican and Not str republican), and Dem (corresponding to Not str democrat and Strong democrat).\n\n\ngss_cat <- gss_cat |> mutate(party_small = fct_collapse(partyid,\n                                              Other = c(\"No answer\", \"Don't know\", \"Other party\"),\n                                              Ind = c(\"Ind,near rep\", \"Independent\", \"Ind,near dem\"),\n                                              Rep = c(\"Strong republican\", \"Not str republican\"),\n                                              Dem = c(\"Not str democrat\", \"Strong democrat\")))\n\n\n* Run the code to create the following plot that shows the average number of hours of television people watch from various religions.\n\n\nrelig_summary <- gss_cat |>\n  group_by(relig) |>\n  summarise(\n    age = mean(age, na.rm = TRUE),\n    tvhours = mean(tvhours, na.rm = TRUE),\n    n = n()\n  )\n\nggplot(data = relig_summary, aes(tvhours, relig)) +\n  geom_point()\n\nThen, use a forcats function create a new variable in the data set that reorders the religion factor levels and remake the barplot so that the religion watches the most television, on average, is on the top, and the religion that watches the least television, on average, is on the bottom.\n\nrelig_summary <- relig_summary |>\n  mutate(relig = fct_reorder(relig, tvhours))\nggplot(data = relig_summary, aes(x = relig, y = tvhours)) +\n  geom_segment(aes(x = relig, xend = relig, y = 0, yend = tvhours)) +\n  geom_point() +\n  coord_flip()\n\n\n\n\n\n* Run the code to make the following line plot that shows age on the x-axis, the proportion on the y-axis, and is coloured by various marital statuses (married, divorced, widowed, etc.):\n\n\nby_age <- gss_cat |>\n  filter(!is.na(age)) |>\n  count(age, marital) |>\n  group_by(age) |>\n  mutate(prop = n / sum(n))\n\nggplot(by_age, aes(age, prop,\n                  colour = marital)) +\n  geom_line(na.rm = TRUE) +\n  labs(colour = \"marital\")\n\nThen, use a forcats function to make the plot so that the legend labels line up better with the different coloured marital status lines (e.g. so that the label for widowed is the first that appears in the legend, the label for married is second, etc.).\n\nby_age2 <- by_age |> ungroup() |>\n  mutate(marital2 = fct_reorder2(marital, .x = age, .y = prop))\nggplot(by_age2, aes(age, prop,\n                  colour = marital2)) +\n  geom_line(na.rm = TRUE) +\n  labs(colour = \"marital\") +\n  scale_colour_viridis_d()"
  },
  {
    "objectID": "07-forcats.html#rcode-7",
    "href": "07-forcats.html#rcode-7",
    "title": "9  Factors with forcats",
    "section": "9.5 Non-Exercise R Code",
    "text": "9.5 Non-Exercise R Code\n\nlibrary(tidyverse)\nlibrary(here)\npokemon_df <- read_csv(here(\"data/pokemon_allgen.csv\")) |>\n  mutate(Generation_cat = factor(Generation))\npokemon_df |> group_by(`Type 1`) |>\n  summarise(counttype = n())\npokemon_legend <- pokemon_df |> filter(Legendary == TRUE) |>\n  group_by(Generation_cat) |>\n  summarise(nlegend = n())\nggplot(data = pokemon_legend, aes(x = Generation_cat, y = nlegend)) +\n  geom_col()\npokemon_legend <- pokemon_legend |>\n  mutate(Generation_cat2 = fct_recode(Generation_cat, Kanto = \"1\",\n                                      Johto = \"2\", Hoenn = \"3\",\n                                      Sinnoh = \"4\", Unova = \"5\",\n                                      Kalos = \"6\")) |>\n  select(Generation_cat2, everything())\nhead(pokemon_legend)\nggplot(data = pokemon_legend,\n       aes(x = Generation_cat2, y = nlegend)) +\n  geom_col()\npokemon_long <- pokemon_df |> pivot_longer(c(`Type 1`, `Type 2`),\n                            names_to = \"Number\",\n                            values_to = \"Type\")\npokemon_long |>\n  mutate(new_type = fct_collapse(Type, Coolest = c(\"Ice\", \"Dark\"),\n                                 Least_Cool = c(\"Fire\", \"Fighting\", \"Poison\"))) |>\n  select(new_type, Type, everything())\npokemon_nodup <- pokemon_df |> group_by(`#`) |> slice(1) |>\n  ungroup()\npokemon_long <- pokemon_nodup |>\n  pivot_longer(c(`Type 1`, `Type 2`),\n               names_to = \"Number\",\n               values_to = \"Type\")\npokemon_sum <- pokemon_long |>\n  group_by(Type) |>\n  summarise(count_type = n()) |>\n  filter(!is.na(Type))\nggplot(data = pokemon_sum, aes(x = Type,\n                               y = count_type)) +\n  geom_col() +\n  coord_flip()  ## flips the x and y axes\npokemon_sum <- pokemon_sum |> \n  mutate(Type_ordered = fct_reorder(.f = Type, .x = count_type))\nggplot(data = pokemon_sum, aes(x = Type_ordered,\n                               y = count_type)) +\n  geom_col() +\n  coord_flip()\nggplot(data = pokemon_sum, aes(x = Type_ordered,\n                               y = count_type)) +\n  geom_segment(aes(x = Type_ordered, xend = Type_ordered,\n                   y = 0, yend = count_type)) +\n  geom_point() +\n  coord_flip()\npokemon_long <- pokemon_long |>\n  filter(!is.na(Type)) |>\n  mutate(Type_Deford = fct_reorder(.f = Type, .x = Defense,\n                                   .fun = median))\nggplot(data = pokemon_long, aes(x = Type_Deford,\n                               y = Defense)) +\n  geom_boxplot() + \n  coord_flip()\npokemon_med <- pokemon_long |> group_by(Type_Deford) |>\n  summarise(med_def = median(Defense)) |>\n  mutate(Type_Deford = fct_reorder(.f = Type_Deford, .x = med_def,\n                                   .fun = median))\n\nggplot(data = pokemon_med, aes(x = med_def, y = Type_Deford)) +\n  geom_point()\nggplot(data = pokemon_med, aes(x = Type_Deford, y = med_def)) +\n  geom_segment(aes(xend = Type_Deford, y = 0, yend = med_def)) +\n  geom_point() +\n  coord_flip()\nmortality_df <- read_csv(here(\"data/gun_violence_us.csv\")) |>\n  mutate(region = factor(region))\nggplot(data = mortality_df,\n       aes(x = ownership_rate, y = mortality_rate, colour = region)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\nmortality_df <- mortality_df |>\n  mutate(region_2 = fct_reorder2(region,\n                                 .x = ownership_rate,\n                                 .y = mortality_rate))\nggplot(data = mortality_df,\n       aes(x = ownership_rate, y = mortality_rate, colour = region_2)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\nmortality_df <- mortality_df |>\n  mutate(region_3 = fct_relevel(region, c(\"South\", \"West\", \"MW\", \"NE\")))\nggplot(data = mortality_df,\n       aes(x = ownership_rate, y = mortality_rate, colour = region_3)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\nmod <- lm(mortality_rate ~ ownership_rate + region, data = mortality_df)\nmod2 <- lm(mortality_rate ~ ownership_rate + region_2, data = mortality_df)\nmod3 <- lm(mortality_rate ~ ownership_rate + region_3, data = mortality_df)\nsummary(mod)\nsummary(mod2)\nsummary(mod3)"
  },
  {
    "objectID": "09-ethics.html",
    "href": "09-ethics.html",
    "title": "8  Data Ethics",
    "section": "",
    "text": "Goals:"
  },
  {
    "objectID": "09-ethics.html#ethical-examples",
    "href": "09-ethics.html#ethical-examples",
    "title": "8  Data Ethics",
    "section": "8.1 Ethical Examples",
    "text": "8.1 Ethical Examples\nWe’ve tried to interweave issues of ethics throughout many examples used already in this course, but the purpose of this section is to put data ethics in direct focus.\nSome questions to consider for any data collected, especially data collected on human subjects:\n\nwho gets to use data and for what purposes?\nwho collected the data and does that organization have any conflicts of interest?\nis presentation of an analysis harmful to a particular person or group of people? Are there benefits of an analysis?\nhave the subjects of a data collection procedure been treated respectfully and have they given consent to their information being collected?\n\nWhen is consent needed and when is it not? For example, we have looked at data on professional athletes. Do they need to provide consent or is consent inherent in being in the spotlight?\nWe’ve also scraped data from SLU’s athletics website to look at data pertaining to some of you! Is that ethical? Is there a line you wouldn’t cross pertaining to data collected on named, individual people?\n\n\n\n8.1.1 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-9).\n\nRead Sections 8.1 - 8.3 in Modern Data Science with R. Then, write a one paragraph summary of the reading and how it might pertain to the way you use or interpret data.\nData Feminism is related to data ethics, though the two terms are certainly not synonymous. Recently, Catherine D’Ignazio and Lauren F. Klein published a book called Data Feminism https://datafeminism.io/ \n\nRead the following blog post on Data Feminism, focusing on the section on Missing Data. https://teachdatascience.com/datafem/ .\nPick one example from the bulleted list and write a 2 sentence explanation that explains why it might be important to acknowledge the missing data in an analysis.\n\nChoose 1 of the following two articles to read\n\n\nhttps://www.theguardian.com/world/2017/sep/08/ai-gay-gaydar-algorithm-facial-recognition-criticism-stanford  on the use of data in the LGBTQIA+ community\nhttps://towardsdatascience.com/5-steps-to-take-as-an-antiracist-data-scientist-89712877c214  on anti-racist data practices.\n\n\nFor the LGBTQIA+ article, write a two sentence summary for the side of the argument that research in facial recognition software to identify members of the LGBTQ+ community should not occur, even if this viewpoint isn’t your own.\n\nThen, write a two sentence summary for the side of the argument that research in facial recognition software to identify members of the LGBTQ+ community is okay as long as the results are used responsibly, even if this viewpoint isn’t your own.\n\nFor the anti-racist data science article, under Step 2, pick a News Article and read the first few paragraphs. Describe, in 2-3 sentences, what your article’s example of bias is and why the incidence of bias matters."
  },
  {
    "objectID": "09-ethics.html#data-privacy",
    "href": "09-ethics.html#data-privacy",
    "title": "8  Data Ethics",
    "section": "8.2 Data Privacy",
    "text": "8.2 Data Privacy\nRelated to data ethics is the idea of data privacy.\n\nWhat data is private and what data is public? For some examples, this may seem obvious, but for others (e.g. data on a government agency that collects data on people), the answer might not be as clear cut.\nIs anonymous data truly anonymous?\nWhat type of consent should be provided before collecting data on someone?\n\nWe will explore some of these issues in the following exercises.\n\n8.2.1 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-9).\n\n\n\n\n\n\nHow anonymous are SLU’s course evaluations? We will do an in-class activity to investigate this.\nSuppose that I collect data on students in this Data Science class. In each setting (a) through (d), suppose that I give you a data set with the following variables collected on each student in the class. Which option, if any, would it be ethically okay for me to share the data with all students in the class.\n\n\ncurrent grade and time spent on Canvas.\ncurrent grade, class year, and whether or not the student is a stat major\nfavorite R package, whether or not the student took STAT 213, whether or not the student took CS 140, and Major\nfavorite R package, whether or not the student took STAT 213, whether or not the student took CS 140, and current grade in the course"
  },
  {
    "objectID": "09-ethics.html#hypothesis-generation-vs.-confirmation",
    "href": "09-ethics.html#hypothesis-generation-vs.-confirmation",
    "title": "8  Data Ethics",
    "section": "8.3 Hypothesis Generation vs. Confirmation",
    "text": "8.3 Hypothesis Generation vs. Confirmation\nWe have focused on hypothesis generation for all data sets in this particular course. Read the following two articles that explain the difference between hypothesis generation and hypothesis confirmation:\nRead the following two very short articles, one from our textbook and one from another source:\n\nhttps://r4ds.had.co.nz/model-intro.html#hypothesis-generation-vs.-hypothesis-confirmation\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC6718169/\n\n\n8.3.1 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-9).\n\nExplain the difference between hypothesis generation and hypothesis confirmation.\nHow many times can you use a single observation for hypothesis generation? for hypothesis confirmation?\nWhich of the following questions, pertaining to someone’s fitness, sound more suitable to be answered with Hypothesis Exploration? Which with Hypothesis Confirmation?\n\n\nYou want to know if, on average, this person exercises more on weekends or more on weekdays, with no other questions of interest.\nYou want to look at general trends in the person’s step count and try to determine if various events influenced the step count.\nYou want to know if the person exercises more in winter or more in summer, and you would also like to investigate other seasonal trends.\n\nNote: Prediction is different from hypothesis confirmation, because you typically don’t really care which variables are associated with your response. You only want a model that gives the “best” predictions. Because of this, if your goal is prediction, you typically have a lot more freedom with how many times you can “use” a single observation. We will talk a little more about prediction later in the semester."
  },
  {
    "objectID": "09-ethics.html#chapexercise-9",
    "href": "09-ethics.html#chapexercise-9",
    "title": "8  Data Ethics",
    "section": "8.4 Chapter Exercises",
    "text": "8.4 Chapter Exercises\nThere are no chapter exercises for this chapter."
  },
  {
    "objectID": "09-ethics.html#solutions-9",
    "href": "09-ethics.html#solutions-9",
    "title": "8  Data Ethics",
    "section": "8.5 Exercise Solutions",
    "text": "8.5 Exercise Solutions\nThere are no exercise solutions for this chapter."
  },
  {
    "objectID": "08-import.html",
    "href": "08-import.html",
    "title": "9  Data Import",
    "section": "",
    "text": "Goals:"
  },
  {
    "objectID": "08-import.html#readr-to-read-in-data",
    "href": "08-import.html#readr-to-read-in-data",
    "title": "9  Data Import",
    "section": "9.1 readr to Read in Data",
    "text": "9.1 readr to Read in Data\nUp to now, we have mostly worked with data that was “R Ready”: meaning that it was in a nice .csv file that could be read into R easily with read_csv() from the readr package. We will begin by looking at some options in the read_csv() function and then move into formats other than .csv that data are commonly stored in.\n\n9.1.1 read_csv() Options\nThe mtcarsex.csv has observations on different car models with variables that include things like gas mileage, number of cylinders, etc. Read in the mtcarsex.csv data set with the following code. Then, examine the data set with head().\n\nlibrary(tidyverse)\nlibrary(here)\ncars_df <- read_csv(here(\"data/mtcarsex.csv\"))\nhead(cars_df)\n\n# A tibble: 6 × 11\n  This is a data s…¹ ...2  ...3  ...4  ...5  ...6  ...7  ...8  ...9  ...10 ...11\n  <chr>              <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr>\n1 \"I'm a na\\x95ve d… <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \n2 \"mpg\"              cyl   disp  hp    drat  wt    qsec  vs    am    gear  carb \n3  <NA>              <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \n4  <NA>              <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \n5 \"-999\"             6     160   110   3.9   2.62  16.46 0     1     4     4    \n6 \"21\"               6     160   110   3.9   2.875 17.02 0     1     4     4    \n# … with abbreviated variable name ¹​`This is a data set about cars.`\n\n\nWhat do you notice about the data set that seems odd? Open the .csv file with Excel or some other program to examine the data set outside of R.\nType in ?read_csv in the bottom-left window and look at some of the options in read_csv(). In particular, we will use the na and the skip arguments to fix up our reading.\nLet’s start with skip so that we aren’t reading in the first two rows of the data set:\n\ncars_df <- read_csv(here(\"data/mtcarsex.csv\"), skip = 2)\n## first two lines will be skipped\nhead(cars_df)\n\n# A tibble: 6 × 11\n     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1   NA      NA    NA    NA NA    NA     NA      NA    NA    NA    NA\n2   NA      NA    NA    NA NA    NA     NA      NA    NA    NA    NA\n3 -999       6   160   110  3.9   2.62  16.5     0     1     4     4\n4   21       6   160   110  3.9   2.88  17.0     0     1     4     4\n5   22.8     4   108    93  3.85  2.32  18.6     1     1     4     1\n6   21.4     6   258   110  3.08  3.22  19.4     1     0     3     1\n\n\nThat looks better, but there are still a couple of problems. What do you notice?\nGo the help and read about the na argument. Let’s add that as an option to fix the missing value issue.\n\ncars_df <- read_csv(here(\"data/mtcarsex.csv\"), na = c(\"NA\", \"-999\"), skip = 2)\nhead(cars_df)\n\n# A tibble: 6 × 11\n    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1  NA      NA    NA    NA NA    NA     NA      NA    NA    NA    NA\n2  NA      NA    NA    NA NA    NA     NA      NA    NA    NA    NA\n3  NA       6   160   110  3.9   2.62  16.5     0     1     4     4\n4  21       6   160   110  3.9   2.88  17.0     0     1     4     4\n5  22.8     4   108    93  3.85  2.32  18.6     1     1     4     1\n6  21.4     6   258   110  3.08  3.22  19.4     1     0     3     1\n\n\nNow look at the classes of each variable. Which classes look like they are incorrect?\nWe’ve talked about how to re-specify classes of variables using mutate() and the as.factor() or as.Date() or as.numeric() functions, but sometimes it’s easier just to respecify the class when we are reading in the data. Notice how, when we use read_csv(), R gives us a message about each of the column types. This is actually an argument in read_csv() called col_types. We can add a |> spec() piping statement after a read_csv() statement to tell R to print the col_types so that it’s easy for us to copy and paste it into read_csv() and change any classes.\n\nread_csv(here(\"data/mtcarsex.csv\"), na = c(\"NA\", \"-999\"), skip = 2) |>\n  spec()\n\nRows: 34 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (11): mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\ncols(\n  mpg = col_double(),\n  cyl = col_double(),\n  disp = col_double(),\n  hp = col_double(),\n  drat = col_double(),\n  wt = col_double(),\n  qsec = col_double(),\n  vs = col_double(),\n  am = col_double(),\n  gear = col_double(),\n  carb = col_double()\n)\n\n\nFor example, notice how cyl = col_double() is changed to cyl = col_factor() in the code chunk below:\n\ncars_df <- read_csv(here(\"data/mtcarsex.csv\"), na = c(NA, \"-999\"), skip = 2,\n  col_types = cols(\n  mpg = col_double(),\n  cyl = col_factor(),\n  disp = col_double(),\n  hp = col_double(),\n  drat = col_double(),\n  wt = col_double(),\n  qsec = col_double(),\n  vs = col_factor(),\n  am = col_double(),\n  gear = col_double(),\n  carb = col_double()\n))\n\nFinally, there are two rows with all missing values. These aren’t providing anything useful so we can slice() them out:\n\ncars_df <- read_csv(here(\"data/mtcarsex.csv\"), na = c(\"NA\", \"-999\"), skip = 2,\n  col_types = cols(\n  mpg = col_double(),\n  cyl = col_factor(),\n  disp = col_double(),\n  hp = col_double(),\n  drat = col_double(),\n  wt = col_double(),\n  qsec = col_double(),\n  vs = col_factor(),\n  am = col_double(),\n  gear = col_double(),\n  carb = col_double()\n)) |>\n  slice(-(1:2))\nhead(cars_df)\n\n# A tibble: 6 × 11\n    mpg cyl    disp    hp  drat    wt  qsec vs       am  gear  carb\n  <dbl> <fct> <dbl> <dbl> <dbl> <dbl> <dbl> <fct> <dbl> <dbl> <dbl>\n1  NA   6       160   110  3.9   2.62  16.5 0         1     4     4\n2  21   6       160   110  3.9   2.88  17.0 0         1     4     4\n3  22.8 4       108    93  3.85  2.32  18.6 1         1     4     1\n4  21.4 6       258   110  3.08  3.22  19.4 1         0     3     1\n5  NA   8       360   175  3.15  3.44  17.0 0         0     3     2\n6  18.1 6       225   105  2.76  3.46  20.2 1         0     3     1\n\n\nThere are many other possible file formats for data storage. For example, there is a data set called oscars.tsv, which is a tab-separated file. You can read it in with read_tsv() instead of read_csv().\n\noscars_df <- read_tsv(here(\"data/oscars.tsv\"))\nhead(oscars_df)\n\n# A tibble: 6 × 51\n  FilmN…¹ Oscar…² Durat…³ Rating Direc…⁴ Direc…⁵ Oscar…⁶ Genre…⁷ Genre…⁸ Genre…⁹\n  <chr>     <dbl>   <dbl>  <dbl> <chr>     <dbl>   <dbl> <chr>     <dbl>   <dbl>\n1 Crash      2006     113      4 Haggis        0       1 Drama         1       0\n2 Brokeb…    2006     134      4 Lee           0       0 Drama,…       1       0\n3 Capote     2006     114      4 Miller        0       0 Drama,…       1       1\n4 Good N…    2006      93      2 Clooney       0       0 Drama         1       0\n5 Munich     2006     164      4 Spielb…       0       0 Drama,…       1       0\n6 The De…    2007     151      4 Scorse…       0       1 Drama,…       1       0\n# … with 41 more variables: CountryName <chr>, ForeignandUSA <dbl>,\n#   ProductionName <chr>, ProductionCompany <dbl>, BudgetRevised <chr>,\n#   Budget <chr>, DomesticBoxOffice <dbl>, WorldwideRevised <dbl>,\n#   WorldwideBoxOffice <dbl>, DomesticPercent <dbl>, LimitedOpeningWnd <dbl>,\n#   LimitedTheaters <dbl>, LimitedAveragePThtr <dbl>, WideOpeningWkd <dbl>,\n#   WideTheaters <dbl>, WideTheaterAverage <dbl>, WidestTheaters <dbl>,\n#   Days <chr>, Rotten <dbl>, Metacritic <dbl>, IMDb <dbl>, …\n\n\nYou’ll be able to work with .txt files and Excel files in the Exercises. Check out https://rawgit.com/rstudio/cheatsheets/master/data-import.pdf for a data import cheatsheet.\nThe final issue that we will discuss in this section occurs when a data set has units within its cells. Consider the earlier example that we used in the reprex section:\n\ntest_df <- read_csv(here(\"data/parsedf.csv\"))\nhead(test_df)\n\n# A tibble: 3 × 2\n  x                   y\n  <chr>           <dbl>\n1 20,000 dollars      1\n2 40 dollars          2\n3 only 13 dollars     3\n\n\nThe parse_number() function is really useful if you just want the number (no commas, no units, etc.). The function is often paired with mutate() since we are creating a new variable:\n\ntest_df |> mutate(x2 = parse_number(x))\n\n# A tibble: 3 × 3\n  x                   y    x2\n  <chr>           <dbl> <dbl>\n1 20,000 dollars      1 20000\n2 40 dollars          2    40\n3 only 13 dollars     3    13\n\n\n\n\n9.1.2 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-8).\n\n* The birthdays.txt file has information on the birthdays of various animals on my Animal Crossing island. There are also columns for the Animal’s Name, Animal Type, and how long the animal has lived on the island (in weeks). Click on the file to open it to look at the format of the data.\n\nStart with the following code chunk and use the options of read_delim() to read in the data (?read_delim). The delim argument that’s already provided specifies that the delimiter (separator) that you’ll use is a -, as opposed to, for example, a , in a .csv file. Arguments that you may need to change include\n\nskip\ncol_names\nna\ntrim_ws\ncol_types\n\n\nlibrary(tidyverse)\ndf <- read_delim(here(\"data/birthdays.txt\"), delim = \" - \")\nhead(df)\n\n\n* Another common format for data to be stored in is an Excel file. Often, it’s easiest just to save the Excel file as a .csv file and read it in using read_csv(). But, sometimes this route can be difficult (for example, if your Excel file has thousands of sheets). To read in directly from Excel, you’ll need to install the readxl with install.packages(\"readxl\"). Once installed, load the package with library(readxl), and read in the first sheet evals_prof.xlsx data set, a similar data set as the one that will be used for Project 2, with the read_excel() function.\n* Now, read in the second sheet in the Excel file, using the help file for ?read_excel to change one of the arguments."
  },
  {
    "objectID": "08-import.html#data-scraping-with-rvest",
    "href": "08-import.html#data-scraping-with-rvest",
    "title": "9  Data Import",
    "section": "9.2 Data Scraping with rvest",
    "text": "9.2 Data Scraping with rvest\nSometimes, you might want data from a public website that isn’t provided in a file format. To obtain this data, you’ll need to use web scraping, a term which just means “getting data from a website.” The easiest way to do this in R is with the rvest package. Note that we could spend an entire semester talking about web scraping, but we will focus only on websites where the scraping of data is “easy” and won’t give us any major errors.\nGo to the following website and suppose that you wanted the table of gun violence statistics in R: https://en.wikipedia.org/wiki/Gun_violence_in_the_United_States_by_state. You could try copy-pasting the table into Excel and reading the data set in with read_excel(). Depending on the format of the table, that strategy may work but it may not. Another way is to scrape it directly with rvest. Additionally, if the website continually updates (standings for a sports league, enrollment data for a school, best-selling products for a company, etc.), then scraping is much more convenient, as you don’t need to continually copy-paste for updated data.\nIn the following code chunk, read_html() reads in the entire html file from the url provided while html_nodes() extracts only the tables on the website.\n\nlibrary(tidyverse)\nlibrary(rvest)\n\n## provide the URL and name it something (in this case, url).\nurl <- \"https://en.wikipedia.org/wiki/Gun_violence_in_the_United_States_by_state\"\n\n## convert the html code into something R can read\nh <- read_html(url)\n\n## grabs the tables\ntab <- h |> html_nodes(\"table\")\n\nYou’ll see that, for this example, there are 3 tables provided. The tables are stored in a list and we can reference the first table using [[1]], the second table using [[2]], etc. For the purposes of this class, we will figure out which of the 3 tables is the one we actually want using trial and error.\nThe html_table() function converts the table into a data.frame object.\n\ntest1 <- tab[[1]] |> html_table()\ntest2 <- tab[[2]] |> html_table()\ntest3 <- tab[[3]] |> html_table()\n\nhead(test1)\nhead(test2)\nhead(test3)\n\nWhich of the 3 tables is the one that we would want to use for an analysis on gun violence in the United States?\nAs another example, consider scraping data from SLU’s athletics page. In particular, suppose we want to do an analysis on SLU’s baseball team.\nGo to the following website to look at the table of data that we want to scrape: https://saintsathletics.com/sports/baseball/stats/2021.\nAfter looking at the website, use the following code to scrape the data set.\n\nurl <- \"https://saintsathletics.com/sports/baseball/stats/2021\"\nh <- read_html(url)\ntab <- h |> html_nodes(\"table\")\ntab\nobj <- tab[[1]] |> html_table(fill = TRUE)\nhead(obj)\ntail(obj)\nobj2 <- tab[[2]] |> html_table(fill = TRUE)\nhead(obj2)\ntail(obj2)\n\nThere’s now 72 different tables! See if you can figure out where the first few tables are coming from on the website.\n\n9.2.1 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-8).\n\nChoose a topic/person/place/etc. that interests you that has tables on Wikipedia and scrape the table that is related to that topic.\n* SLU keeps track of diversity of faculty through time and makes this data public on the following website: https://www.stlawu.edu/offices/institutional-research/faculty-diversity. Use rvest to scrape the data tables into R.\n\nHint: You may need to use an extra argument in html_table() like fill."
  },
  {
    "objectID": "08-import.html#json-files-with-jsonlite",
    "href": "08-import.html#json-files-with-jsonlite",
    "title": "9  Data Import",
    "section": "9.3 JSON Files with jsonlite",
    "text": "9.3 JSON Files with jsonlite\nA final common data format that we will discuss is JSON (JavaScript Object Notation). We will only cover the very basics of JSON data and use the jsonlite package in R to read in some .json files. JSON files are read in to R as a list object.\n\n9.3.1 Everything Working Well\nFirst, consider data from the mobile game Clash Royale. Install the jsonlite package and then use it to read in the json file with the function fromJSON():\n\n## install.packages(\"jsonlite\")\nlibrary(jsonlite)\ncr_cards <- fromJSON(here(\"data/clash_royale_card_info.json\"))\n\nYou should get a warning message, which we will investigate in class.\nNext, type View(cr_cards) in your console (bottom-left) window to look at the data. See if you can pull out the data set by clicking on some things in the View() window.\nThe following give a couple of ways to grab the data using code. The as_tibble() function converts a rectangular object into our familiar tibble.\nThe first option specifies the name of the table that’s in the JSON file (in this case, the name is \"cards\"):\n\nlibrary(tidyverse)\ncr_cards_flat <- cr_cards[[\"cards\"]]\ncr_cards_df <- as_tibble(cr_cards_flat)\nhead(cr_cards_df)\n\n# A tibble: 6 × 8\n  key     name      elixir type  rarity arena description                     id\n  <chr>   <chr>      <int> <chr> <chr>  <int> <chr>                        <int>\n1 knight  Knight         3 Troop Common     0 A tough melee fighter. The… 2.6 e7\n2 archers Archers        3 Troop Common     0 A pair of lightly armored … 2.60e7\n3 goblins Goblins        2 Troop Common     1 Three fast, unarmored mele… 2.60e7\n4 giant   Giant          5 Troop Rare       0 Slow but durable, only att… 2.60e7\n5 pekka   P.E.K.K.A      7 Troop Epic       4 A heavily armored, slow me… 2.60e7\n6 minions Minions        3 Troop Common     0 Three fast, unarmored flyi… 2.60e7\n\n\nThe second method uses the flatten() function from the purrr package, the only package in the core tidyverse that we do not talk about in detail in this class. There is also a different flatten() function in the jsonlite package. In the code below, we specify that we want to use flatten() from purrr with purrr::flatten(). If we wanted to use flatten() from jsonlite, we’d use jsonlite::flatten()\n\ncr_cards_flat2 <- purrr::flatten(cr_cards)\ncr_cards_df2 <- as_tibble(cr_cards_flat2)\nhead(cr_cards_df2)\n\n# A tibble: 6 × 8\n  key     name      elixir type  rarity arena description                     id\n  <chr>   <chr>      <int> <chr> <chr>  <int> <chr>                        <int>\n1 knight  Knight         3 Troop Common     0 A tough melee fighter. The… 2.6 e7\n2 archers Archers        3 Troop Common     0 A pair of lightly armored … 2.60e7\n3 goblins Goblins        2 Troop Common     1 Three fast, unarmored mele… 2.60e7\n4 giant   Giant          5 Troop Rare       0 Slow but durable, only att… 2.60e7\n5 pekka   P.E.K.K.A      7 Troop Epic       4 A heavily armored, slow me… 2.60e7\n6 minions Minions        3 Troop Common     0 Three fast, unarmored flyi… 2.60e7\n\n\nBoth methods give a tibble that we can then use our usual tidyverse tools ggplot2, dplyr, tidyr, etc. on.\n\n\n9.3.2 Things Aren’t Always So Easy\nNow let’s try to look at some animal crossing data that were obtained from https://github.com/jefflomacy/villagerdb. We first just want to look at the data from one individual villager (ace) in the file ace.json.\n\nacedata <- fromJSON(here(\"data/ace.json\"))\naceflat <- purrr::flatten(acedata)\nhead(aceflat)\n\n$gender\n[1] \"male\"\n\n$species\n[1] \"bird\"\n\n$birthday\n[1] \"3-13\"\n\n$ac\n$ac$personality\n[1] \"jock\"\n\n$ac$clothes\n[1] \"spade-shirt\"\n\n$ac$song\n[1] \"K.K. Parade\"\n\n$ac$phrase\n[1] \"ace\"\n\n\n$`afe+`\n$`afe+`$personality\n[1] \"jock\"\n\n$`afe+`$clothes\n[1] \"spade-shirt\"\n\n$`afe+`$song\n[1] \"K.K. Parade\"\n\n\n$name\n[1] \"Ace\"\n\n\nThings are now….more complicated. This example is just to show that it’s not always easy working with JSON data. Lists can be nested and that creates problems when trying to convert a deeply nested list into our “rectangular” format that’s easy to work with.\nThere’s also the added problem of reading in the .json files from all villagers at the same time We could do this with a for loop or a mapping function from purrr to download and read in the JSON files for all villagers. We won’t delve any more deeply into this, but there’s a lot more to all of the file formats that we’ve discussed this week, particularly web scraping and .json files.\n\n\n9.3.3 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-8).\n\n* Read in the pokedex.json file, a data set that has information on the 151 original Pokemon. Then, use the flatten() function from purrr to flatten the list.\n* Use as_tibble() to convert your flattened list to a tibble.\n* Use parse_number() with mutate() to tidy two of the variables in the data set.\n* Look at the type variable. What looks odd about it? What happens when you try to use it, either in a plot, or using a dplyr function?\n\nYou can unnest() the Type variable with the unnest() function from tidyr. We didn’t discuss this function but feel free to read about it with ?unnest\n\n\n\n\npokemon_unnest <- unnest(pokemon_df, cols = c(type))\n\n\nThere are 6 pokemon with a spawn_chance of 0. Figure out what these 6 pokemon are.\n\n\n\n\n\nFigure out what the 5 most common Pokemon types are in the first generation (you’ll need to use the unnest()-ed data set for this: why?)."
  },
  {
    "objectID": "08-import.html#chapexercise-8",
    "href": "08-import.html#chapexercise-8",
    "title": "9  Data Import",
    "section": "9.4 Chapter Exercises",
    "text": "9.4 Chapter Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-8).\n\nChoose a sports team at SLU, and go to that team’s website (by simply googling SLU name_of_sport). Scrape the data tables from the “Results” or “Statistics” section of this sport. After you scrape the data, tidy the data set. Then, choose one of the following options (different options might make more/less sense for different sports)\n\n(a). Summarise different team statistics, either numerically or graphically. Perhaps make some graphs showing different statistics through time.\n(b). Summarise different individual statistics, either numerically or graphically.\n(c). Ask and answer any other questions that make sense for the particular sport that you are looking at!\nNote: A few sports (men’s and women’s golf, for example), give results in PDF format. PDF format is generally a horrible way to record and share data, as it’s very difficult to read in to almost any program. Therefore, avoid sports with PDF results for the purposes of this exercise."
  },
  {
    "objectID": "08-import.html#solutions-8",
    "href": "08-import.html#solutions-8",
    "title": "9  Data Import",
    "section": "9.5 Exercise Solutions",
    "text": "9.5 Exercise Solutions\n\n9.5.1 readr S\n\n* The birthdays.txt file has information on the birthdays of various animals on my Animal Crossing island. There are also columns for the Animal’s Name, Animal Type, and how long the animal has lived on the island (in weeks). Click on the file to open it to look at the format of the data.\n\nStart with the following code chunk and use the options of read_delim() to read in the data (?read_delim). The delim argument that’s already provided specifies that the delimiter (separator) that you’ll use is a -, as opposed to, for example, a , in a .csv file. Arguments that you may to change include\n\nskip\ncol_names\nna\ntrim_ws\ncol_types\n\n\nlibrary(tidyverse)\ndf <- read_delim(here(\"data/birthdays.txt\"), delim = \" - \")\nhead(df)\n\n\nread_delim(here(\"data/birthdays.txt\"), delim = \"-\", skip = 4,\n  col_names = c(\"Birthday\", \"Name\",\n    \"Animal\", \"Island\"),\n  trim_ws = TRUE,\n  col_types = list(\n    col_character(), col_character(), col_character(), col_number()\n  ), na = c(\"N/A\", \"?\"))\n\n\n* Another common format for data to be stored in is an Excel file. Often, it’s easiest just to save the Excel file as a .csv file and read it in using read_csv(). But, sometimes this route can be difficult (for example, if your Excel file has thousands of sheets). To read in directly from Excel, you’ll need to install the readxl with install.packages(\"readxl\"). Once installed, load the package with library(readxl), and read in the first sheet evals_prof.xlsx data set with the read_excel() function.\n\n\n## install.packages(\"readxl\")\nlibrary(readxl)\nread_excel(here(\"data/evals_prof.xlsx\"))\n\n\n* Now, read in the second sheet, using the help file for ?read_excel to change one of the arguments.\n\n\nread_excel(here(\"data/evals_prof.xlsx\"), sheet = 2)\n\n\n\n9.5.2 rvest and Data Scraping S\n\n* SLU keeps track of diversity of faculty through time and makes this data public on the following website: https://www.stlawu.edu/ir/diversity/faculty. Use rvest to scrape the data tables into R.\n\n\nurl <- \"https://www.stlawu.edu/ir/diversity/faculty\"\nh <- read_html(url)\ntab <- h |> html_nodes(\"table\")\nobj <- tab[[1]] |> html_table(fill = TRUE)\nobj\n\n\n\n9.5.3 JSON with jsonlite S\n\n* Read in the pokedex.json file, a data set that has information on the 151 original Pokemon. Then, use the flatten() function from purrr to flatten the list.\n\n\nlibrary(jsonlite)\npokedex <- fromJSON(here(\"data/pokedex.json\"))\ndf <- purrr::flatten(pokedex)\n\n\n* Use as_tibble() to convert your flattened list to a tibble.\n\n\npokemon_df <- as_tibble(df)\n\n\n* Use parse_number() with mutate() to tidy two of the variables in the data set.\n\n\npokemon_df <- pokemon_df |> mutate(height = parse_number(height),\n                                    weight = parse_number(weight))\n\n\n* Look at the type variable. What looks odd about it? What happens when you try to use it, either in a plot, or using a dplyr function?\n\n\n## it's a variable of lists....this is happening because some \n## pokemon have more than one type.\n\n## most ggplot() and dplyr() functions won't work, or\n## won't work as you'd expect\n\nYou can unnest() the Type variable with the unnest() function from tidyr. We didn’t discuss this function but feel free to read about it with ?unnest\n\npokemon_unnest <- unnest(pokemon_df, cols = c(type))\n\n\n\n9.5.4 Chapter Exercises S"
  },
  {
    "objectID": "08-import.html#rcode-8",
    "href": "08-import.html#rcode-8",
    "title": "9  Data Import",
    "section": "9.6 Non-Exercise R Code",
    "text": "9.6 Non-Exercise R Code\n\nlibrary(tidyverse)\nlibrary(here)\ncars_df <- read_csv(here(\"data/mtcarsex.csv\"))\nhead(cars_df)\ncars_df <- read_csv(here(\"data/mtcarsex.csv\"), skip = 2)\n## first two lines will be skipped\nhead(cars_df)\ncars_df <- read_csv(here(\"data/mtcarsex.csv\"), na = c(\"NA\", \"-999\"), skip = 2)\nhead(cars_df)\ncars_df <- read_csv(here(\"data/mtcarsex.csv\"), na = c(NA, \"-999\"), skip = 2,\n  col_types = cols(\n  mpg = col_double(),\n  cyl = col_factor(),\n  disp = col_double(),\n  hp = col_double(),\n  drat = col_double(),\n  wt = col_double(),\n  qsec = col_double(),\n  vs = col_factor(),\n  am = col_double(),\n  gear = col_double(),\n  carb = col_double()\n))\ncars_df <- read_csv(here(\"data/mtcarsex.csv\"), na = c(\"NA\", \"-999\"), skip = 2,\n  col_types = cols(\n  mpg = col_double(),\n  cyl = col_factor(),\n  disp = col_double(),\n  hp = col_double(),\n  drat = col_double(),\n  wt = col_double(),\n  qsec = col_double(),\n  vs = col_factor(),\n  am = col_double(),\n  gear = col_double(),\n  carb = col_double()\n)) |>\n  slice(-(1:2))\nhead(cars_df)\noscars_df <- read_tsv(here(\"data/oscars.tsv\"))\nhead(oscars_df)\ntest_df <- read_csv(here(\"data/parsedf.csv\"))\nhead(test_df)\ntest_df |> mutate(x2 = parse_number(x))\nlibrary(tidyverse)\nlibrary(rvest)\n\n## provide the URL and name it something (in this case, url).\nurl <- \"https://en.wikipedia.org/wiki/Gun_violence_in_the_United_States_by_state\"\n\n## convert the html code into something R can read\nh <- read_html(url)\n\n## grabs the tables\ntab <- h |> html_nodes(\"table\")\ntest1 <- tab[[1]] |> html_table()\ntest2 <- tab[[2]] |> html_table()\ntest3 <- tab[[3]] |> html_table()\n\nhead(test1)\nhead(test2)\nhead(test3)\nurl <- \"https://saintsathletics.com/sports/baseball/stats/2021\"\nh <- read_html(url)\ntab <- h |> html_nodes(\"table\")\ntab\nobj <- tab[[1]] |> html_table(fill = TRUE)\nhead(obj)\ntail(obj)\nobj2 <- tab[[2]] |> html_table(fill = TRUE)\nhead(obj2)\ntail(obj2)\n## install.packages(\"jsonlite\")\nlibrary(jsonlite)\ncr_cards <- fromJSON(here(\"data/clash_royale_card_info.json\"))\nlibrary(tidyverse)\ncr_cards_flat <- cr_cards[[\"cards\"]]\ncr_cards_df <- as_tibble(cr_cards_flat)\nhead(cr_cards_df)\ncr_cards_flat2 <- purrr::flatten(cr_cards)\ncr_cards_df2 <- as_tibble(cr_cards_flat2)\nhead(cr_cards_df2)\nacedata <- fromJSON(here(\"data/ace.json\"))\naceflat <- purrr::flatten(acedata)\nhead(aceflat)"
  },
  {
    "objectID": "10-merging.html",
    "href": "10-merging.html",
    "title": "12  Merging with dplyr",
    "section": "",
    "text": "Goals:"
  },
  {
    "objectID": "10-merging.html#stacking-rows-and-appending-columns",
    "href": "10-merging.html#stacking-rows-and-appending-columns",
    "title": "12  Merging with dplyr",
    "section": "12.1 Stacking Rows and Appending Columns",
    "text": "12.1 Stacking Rows and Appending Columns\n\n12.1.1 Stacking with bind_rows()\nFirst, we will talk about combining two data sets by “stacking” them on top of each other to form one new data set. The bind_rows() function can be used for this purpose if the two data sets have identical column names.\nA common instance where this is useful is if two data sets come from the same source and have different locations or years, but the same exact column names.\nFor example, examine the following website and notice how there are .csv files given for each year of matches in the ATP (Association of (men’s) Tennis Professionals). https://github.com/JeffSackmann/tennis_atp.\nThen, read in the data sets, and look at how many columns each has.\n\nlibrary(tidyverse)\nlibrary(here)\natp_2019 <- read_csv(here(\"data/atp_matches_2019.csv\"))\natp_2018 <- read_csv(here(\"data/atp_matches_2018.csv\"))\nhead(atp_2019) \nhead(atp_2018)\n\nTo combine results from both data sets,\n\natp_df <- bind_rows(atp_2018, atp_2019)\n\nError in `bind_rows()`:\n! Can't combine `..1$winner_seed` <double> and `..2$winner_seed` <character>.\n\n\nWhat happens? Can you fix the error? Hint: run\n\nspec(atp_2018)\n\nto get the full column specifications and use your readr knowledge to change a couple of the column types. We also did not discuss this, but, when using the col_type argument in read_csv(), you don’t need to specify all of the column types. Just specifying the ones that you want to change works too. The following code forces the seed variables in the 2018 data set to be characters.\n\natp_2018 <- read_csv(here(\"data/atp_matches_2018.csv\"),\n                     col_types = cols(winner_seed = col_character(),\n                                      loser_seed = col_character()))\n\nWe can try combining the data sets now.\n\natp_df <- bind_rows(atp_2018, atp_2019)\natp_df\n\nDo a quick check to make sure the number of rows in atp_2018 plus the number of rows in atp_2019 equals the number of rows in atp_df.\nIt might seem a little annoying, but, by default bind_rows() will only combine two data sets by stacking rows if the data sets have identical column names and identical column classes, as we saw in the previous example.\nNow run the following and look at the output.\n\ndf_test2a <- tibble(xvar = c(1, 2))\ndf_test2b <- tibble(xvar = c(1, 2), y = c(5, 1))\nbind_rows(df_test2a, df_test2b)\n\n# A tibble: 4 × 2\n   xvar     y\n  <dbl> <dbl>\n1     1    NA\n2     2    NA\n3     1     5\n4     2     1\n\n\nIs this the behavior you would expect?\n\n\n\n12.1.2 Binding Columns with bind_cols()\nWe won’t spend much time talking about how to bind together columns because it’s generally a little dangerous.\nWe will use a couple of test data sets, df_test1a and df_test1b, to see it in action:\n\ndf_test1a <- tibble(xvar = c(1, 2), yvar = c(5, 1))\ndf_test1b <- tibble(x = c(1, 2), y = c(5, 1))\nbind_cols(df_test1a, df_test1b)\n\n# A tibble: 2 × 4\n   xvar  yvar     x     y\n  <dbl> <dbl> <dbl> <dbl>\n1     1     5     1     5\n2     2     1     2     1\n\n\nFor a larger data set, why might this be a dangerous way to combine data? What must you be sure of about the way the data was collected in order to combine data in this way?\n\n\n12.1.3 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-10).\n\n* Run the following and explain why R does not simply stack the rows. Then, fix the issue with the rename() function.\n\n\ndf_test1a <- tibble(xvar = c(1, 2), yvar = c(5, 1))\ndf_test1b <- tibble(x = c(1, 2), y = c(5, 1))\nbind_rows(df_test1a, df_test1b)\n\n# A tibble: 4 × 4\n   xvar  yvar     x     y\n  <dbl> <dbl> <dbl> <dbl>\n1     1     5    NA    NA\n2     2     1    NA    NA\n3    NA    NA     1     5\n4    NA    NA     2     1"
  },
  {
    "objectID": "10-merging.html#mutating-joins",
    "href": "10-merging.html#mutating-joins",
    "title": "12  Merging with dplyr",
    "section": "12.2 Mutating Joins",
    "text": "12.2 Mutating Joins\nIf the goal is to combine two data sets using some common variable(s) that both data sets have, we need different tools than simply stacking rows or appending columns. When merging together two or more data sets, we need to have a matching identification variable in each data set. This variable is commonly called a key. A key can be an identification number, a name, a date, etc, but must be present in both data sets.\nAs a simple first example, consider\n\nlibrary(tidyverse)\ndf1 <- tibble(name = c(\"Emily\", \"Miguel\", \"Tonya\"), fav_sport = c(\"Swimming\", \"Football\", \"Tennis\"))\ndf2 <- tibble(name = c(\"Tonya\", \"Miguel\", \"Emily\"),\n              fav_colour = c(\"Robin's Egg Blue\", \"Tickle Me Pink\", \"Goldenrod\"))\n\nOur goal is to combine the two data sets so that the people’s favorite sports and favorite colours are in one data set.\nIdentify the key in the example above. Why can we no longer use bind_cols() here?\n\n12.2.1 Keep All Rows of Data Set 1 with left_join()\nConsider the babynames R package, which has the following data sets:\n\nlifetables: cohort life tables for different sex and different year variables, starting at the year 1900.\nbirths: the number of births in the United States in each year, since 1909\nbabynames: popularity of different baby names per year and sex since the year 1880.\n\n\n##install.packages(\"babynames\")\nlibrary(babynames)\nlife_df <- babynames::lifetables\nbirth_df <- babynames::births\nbabynames_df <- babynames::babynames\nhead(babynames)\nhead(births)\nhead(lifetables)\n\nRead about each data set with ?babynames, ?births and ?lifetables.\nSuppose that you want to combine the births data set with the babynames data set, so that each row of babynames now has the total number of births for that year. We first need to identify the key in each data set that we will use for the joining. In this case, each data set has a year variable, and we can use left_join() to keep all observations in babynames_df, even for years that are not in the births_df data set.\n\ncombined_left <- left_join(babynames_df, birth_df, by = c(\"year\" = \"year\"))\nhead(combined_left)\n\n# A tibble: 6 × 6\n   year sex   name          n   prop births\n  <dbl> <chr> <chr>     <int>  <dbl>  <int>\n1  1880 F     Mary       7065 0.0724     NA\n2  1880 F     Anna       2604 0.0267     NA\n3  1880 F     Emma       2003 0.0205     NA\n4  1880 F     Elizabeth  1939 0.0199     NA\n5  1880 F     Minnie     1746 0.0179     NA\n6  1880 F     Margaret   1578 0.0162     NA\n\ntail(combined_left)\n\n# A tibble: 6 × 6\n   year sex   name       n       prop  births\n  <dbl> <chr> <chr>  <int>      <dbl>   <int>\n1  2017 M     Zyhier     5 0.00000255 3855500\n2  2017 M     Zykai      5 0.00000255 3855500\n3  2017 M     Zykeem     5 0.00000255 3855500\n4  2017 M     Zylin      5 0.00000255 3855500\n5  2017 M     Zylis      5 0.00000255 3855500\n6  2017 M     Zyrie      5 0.00000255 3855500\n\n\nWhy are births missing in head(combined_left) but not in tail(combined_left)?\n\n\n12.2.2 Keep All Rows of Data Set 2 with right_join()\nRecall from the accompanying handout that there is no need to ever use right_join() because it is the same as using a left_join() with the first two data set arguments switched:\n\n## these will always do the same exact thing\nright_join(babynames_df, birth_df, by = c(\"year\" = \"year\"))\n\n# A tibble: 1,839,952 × 6\n    year sex   name          n   prop  births\n   <dbl> <chr> <chr>     <int>  <dbl>   <int>\n 1  1909 F     Mary      19259 0.0523 2718000\n 2  1909 F     Helen      9250 0.0251 2718000\n 3  1909 F     Margaret   7359 0.0200 2718000\n 4  1909 F     Ruth       6509 0.0177 2718000\n 5  1909 F     Dorothy    6253 0.0170 2718000\n 6  1909 F     Anna       5804 0.0158 2718000\n 7  1909 F     Elizabeth  5176 0.0141 2718000\n 8  1909 F     Mildred    5054 0.0137 2718000\n 9  1909 F     Marie      4301 0.0117 2718000\n10  1909 F     Alice      4170 0.0113 2718000\n# … with 1,839,942 more rows\n\nleft_join(birth_df, babynames_df, by = c(\"year\" = \"year\"))\n\n# A tibble: 1,839,952 × 6\n    year  births sex   name          n   prop\n   <dbl>   <int> <chr> <chr>     <int>  <dbl>\n 1  1909 2718000 F     Mary      19259 0.0523\n 2  1909 2718000 F     Helen      9250 0.0251\n 3  1909 2718000 F     Margaret   7359 0.0200\n 4  1909 2718000 F     Ruth       6509 0.0177\n 5  1909 2718000 F     Dorothy    6253 0.0170\n 6  1909 2718000 F     Anna       5804 0.0158\n 7  1909 2718000 F     Elizabeth  5176 0.0141\n 8  1909 2718000 F     Mildred    5054 0.0137\n 9  1909 2718000 F     Marie      4301 0.0117\n10  1909 2718000 F     Alice      4170 0.0113\n# … with 1,839,942 more rows\n\n\nTherefore, it’s usually easier to just always use left_join() and ignore right_join() completely.\n\n\n12.2.3 Keep All Rows of Both Data Sets with full_join()\nIn addition to keeping any rows with a matching key in the other data frame, a full_join() will keep all rows in data set 1 that don’t have a matching key in data set 2, and will also keep all rows in data set 2 that don’t have a matching key in data set 1, filling in NA for missing values when necessary. For our example of merging babynames_df with birth_df,\n\nfull_join(babynames_df, birth_df, by = c(\"year\" = \"year\"))\n\n\n\n12.2.4 Keep Only Rows with Matching Keys with inner_join()\nWe can also keep only rows with matching keys with inner_join(). For this join, any row in data set 1 without a matching key in data set 2 is dropped, and any row in data set 2 without a matching key in data set 1 is also dropped.\n\ninner_join(babynames_df, birth_df, by = c(\"year\" = \"year\"))\n\n# A tibble: 1,839,952 × 6\n    year sex   name          n   prop  births\n   <dbl> <chr> <chr>     <int>  <dbl>   <int>\n 1  1909 F     Mary      19259 0.0523 2718000\n 2  1909 F     Helen      9250 0.0251 2718000\n 3  1909 F     Margaret   7359 0.0200 2718000\n 4  1909 F     Ruth       6509 0.0177 2718000\n 5  1909 F     Dorothy    6253 0.0170 2718000\n 6  1909 F     Anna       5804 0.0158 2718000\n 7  1909 F     Elizabeth  5176 0.0141 2718000\n 8  1909 F     Mildred    5054 0.0137 2718000\n 9  1909 F     Marie      4301 0.0117 2718000\n10  1909 F     Alice      4170 0.0113 2718000\n# … with 1,839,942 more rows\n\n\n\n\n\n12.2.5 Which xxxx_join()?\nWhich join function we use will depend on the context of the data and what questions you will be answering in your analysis. Most importantly, if you’re using a left_join(), right_join() or inner_join(), you’re potentially cutting out some data. It’s important to be aware of what data you’re omitting. For example, with the babynames and births data, we would want to keep a note that a left_join() removed all observations before 1909 from joined data set.\n\n\n12.2.6 The Importance of a Good Key\nThe key variable is very important for joining and is not always available in a “perfect” form. Recall the college majors data sets we have, called slumajors_df, which information on majors at SLU. Another data set, collegemajors_df, has different statistics on college majors nationwide. There’s lots of interesting variables in these data sets, but we’ll focus on the Major variable here. Read in and examine the two data sets with:\n\nslumajors_df <- read_csv(here(\"data/SLU_Majors_15_19.csv\"))\ncollegemajors_df <- read_csv(here(\"data/college-majors.csv\"))\nhead(slumajors_df)\n\n# A tibble: 6 × 3\n  Major                        nfemales nmales\n  <chr>                           <dbl>  <dbl>\n1 Anthropology                       34     15\n2 Art & Art History                  65     11\n3 Biochemistry                       14     11\n4 Biology                           162     67\n5 Business in the Liberal Arts      135    251\n6 Chemistry                          26     14\n\nhead(collegemajors_df)\n\n# A tibble: 6 × 12\n  Major   Total   Men Women Major…¹ Emplo…² Full_…³ Part_…⁴ Unemp…⁵ Median P25th\n  <chr>   <dbl> <dbl> <dbl> <chr>     <dbl>   <dbl>   <dbl>   <dbl>  <dbl> <dbl>\n1 PETROL…  2339  2057   282 Engine…    1976    1849     270      37 110000 95000\n2 MINING…   756   679    77 Engine…     640     556     170      85  75000 55000\n3 METALL…   856   725   131 Engine…     648     558     133      16  73000 50000\n4 NAVAL …  1258  1123   135 Engine…     758    1069     150      40  70000 43000\n5 CHEMIC… 32260 21239 11021 Engine…   25694   23170    5180    1672  65000 50000\n6 NUCLEA…  2573  2200   373 Engine…    1857    2038     264     400  65000 50000\n# … with 1 more variable: P75th <dbl>, and abbreviated variable names\n#   ¹​Major_category, ²​Employed, ³​Full_time, ⁴​Part_time, ⁵​Unemployed\n\n\nThe most logical key for joining these two data sets is Major, but joining the data sets won’t actually work. The following is an attempt at using Major as the key.\n\nleft_join(slumajors_df, collegemajors_df, by = c(\"Major\" = \"Major\"))\n\n# A tibble: 27 × 14\n   Major        nfema…¹ nmales Total   Men Women Major…² Emplo…³ Full_…⁴ Part_…⁵\n   <chr>          <dbl>  <dbl> <dbl> <dbl> <dbl> <chr>     <dbl>   <dbl>   <dbl>\n 1 Anthropology      34     15    NA    NA    NA <NA>         NA      NA      NA\n 2 Art & Art H…      65     11    NA    NA    NA <NA>         NA      NA      NA\n 3 Biochemistry      14     11    NA    NA    NA <NA>         NA      NA      NA\n 4 Biology          162     67    NA    NA    NA <NA>         NA      NA      NA\n 5 Business in…     135    251    NA    NA    NA <NA>         NA      NA      NA\n 6 Chemistry         26     14    NA    NA    NA <NA>         NA      NA      NA\n 7 Computer Sc…      21     47    NA    NA    NA <NA>         NA      NA      NA\n 8 Conservatio…      38     20    NA    NA    NA <NA>         NA      NA      NA\n 9 Economics        128    349    NA    NA    NA <NA>         NA      NA      NA\n10 English          131     54    NA    NA    NA <NA>         NA      NA      NA\n# … with 17 more rows, 4 more variables: Unemployed <dbl>, Median <dbl>,\n#   P25th <dbl>, P75th <dbl>, and abbreviated variable names ¹​nfemales,\n#   ²​Major_category, ³​Employed, ⁴​Full_time, ⁵​Part_time\n\n\nWhy did the collegemajors_df give only NA values when we tried to merge by major?\nThis example underscores the importance of having a key that matches exactly. Some, but not all, of the issues involved in joining these two data sets can be solved with functions in the stringr package (discussed in a few weeks). For example, the capitalization issue can be solved with the str_to_title() function, which converts that all-caps majors in collegemajors_df to majors where only the first letter of each word is capitalized:\n\ncollegemajors_df <- collegemajors_df |>\n  mutate(Major = str_to_title(Major))\nleft_join(slumajors_df, collegemajors_df)\n\nJoining, by = \"Major\"\n\n\n# A tibble: 27 × 14\n   Major     nfema…¹ nmales  Total    Men  Women Major…² Emplo…³ Full_…⁴ Part_…⁵\n   <chr>       <dbl>  <dbl>  <dbl>  <dbl>  <dbl> <chr>     <dbl>   <dbl>   <dbl>\n 1 Anthropo…      34     15     NA     NA     NA <NA>         NA      NA      NA\n 2 Art & Ar…      65     11     NA     NA     NA <NA>         NA      NA      NA\n 3 Biochemi…      14     11     NA     NA     NA <NA>         NA      NA      NA\n 4 Biology       162     67 280709 111762 168947 Biolog…  182295  144512   72371\n 5 Business…     135    251     NA     NA     NA <NA>         NA      NA      NA\n 6 Chemistry      26     14  66530  32923  33607 Physic…   48535   39509   15066\n 7 Computer…      21     47 128319  99743  28576 Comput…  102087   91485   18726\n 8 Conserva…      38     20     NA     NA     NA <NA>         NA      NA      NA\n 9 Economics     128    349 139247  89749  49498 Social…  104117   96567   25325\n10 English       131     54     NA     NA     NA <NA>         NA      NA      NA\n# … with 17 more rows, 4 more variables: Unemployed <dbl>, Median <dbl>,\n#   P25th <dbl>, P75th <dbl>, and abbreviated variable names ¹​nfemales,\n#   ²​Major_category, ³​Employed, ⁴​Full_time, ⁵​Part_time\n\n\nAs we can see, this solves the issue for some majors but others still have different naming conventions in the two data sets.\n\n\n12.2.7 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-10).\n\nExamine the following two joins that we’ve done, and explain why one resulting data set has fewer observations (rows) than the other.\n\n\nleft_join(babynames_df, birth_df, by = c(\"year\" = \"year\"))\n\n# A tibble: 1,924,665 × 6\n    year sex   name          n   prop births\n   <dbl> <chr> <chr>     <int>  <dbl>  <int>\n 1  1880 F     Mary       7065 0.0724     NA\n 2  1880 F     Anna       2604 0.0267     NA\n 3  1880 F     Emma       2003 0.0205     NA\n 4  1880 F     Elizabeth  1939 0.0199     NA\n 5  1880 F     Minnie     1746 0.0179     NA\n 6  1880 F     Margaret   1578 0.0162     NA\n 7  1880 F     Ida        1472 0.0151     NA\n 8  1880 F     Alice      1414 0.0145     NA\n 9  1880 F     Bertha     1320 0.0135     NA\n10  1880 F     Sarah      1288 0.0132     NA\n# … with 1,924,655 more rows\n\nleft_join(birth_df, babynames_df, by = c(\"year\" = \"year\"))\n\n# A tibble: 1,839,952 × 6\n    year  births sex   name          n   prop\n   <dbl>   <int> <chr> <chr>     <int>  <dbl>\n 1  1909 2718000 F     Mary      19259 0.0523\n 2  1909 2718000 F     Helen      9250 0.0251\n 3  1909 2718000 F     Margaret   7359 0.0200\n 4  1909 2718000 F     Ruth       6509 0.0177\n 5  1909 2718000 F     Dorothy    6253 0.0170\n 6  1909 2718000 F     Anna       5804 0.0158\n 7  1909 2718000 F     Elizabeth  5176 0.0141\n 8  1909 2718000 F     Mildred    5054 0.0137\n 9  1909 2718000 F     Marie      4301 0.0117\n10  1909 2718000 F     Alice      4170 0.0113\n# … with 1,839,942 more rows\n\n\n\nEvaluate whether the following statement is true or false: an inner_join() will always result in a data set with the same or fewer rows than a full_join().\nEvaluate whether the following statement is true or false: an inner_join() will always result in a data set with the same or fewer rows than a left_join().\nEvaluate whether the following statement is true or false: a left_join() will always result in a data set with the same number of rows as a semi_join() on the same two data sets."
  },
  {
    "objectID": "10-merging.html#filtering-joins",
    "href": "10-merging.html#filtering-joins",
    "title": "12  Merging with dplyr",
    "section": "12.3 Filtering Joins",
    "text": "12.3 Filtering Joins\nFiltering joins (semi_join() and anti_join()) are useful if you would only like to keep the variables in one data set, but you want to filter out observations by a variable in the second data set.\nConsider again the two data sets on men’s tennis matches in 2018 and in 2019.\n\natp_2019 <- read_csv(here(\"data/atp_matches_2019.csv\"))\natp_2018 <- read_csv(here(\"data/atp_matches_2018.csv\"))\natp_2019\n\n# A tibble: 2,781 × 49\n   tourney_id tourney_…¹ surface draw_…² tourn…³ tourn…⁴ match…⁵ winne…⁶ winne…⁷\n   <chr>      <chr>      <chr>     <dbl> <chr>     <dbl>   <dbl>   <dbl> <chr>  \n 1 2019-M020  Brisbane   Hard         32 A        2.02e7     300  105453 2      \n 2 2019-M020  Brisbane   Hard         32 A        2.02e7     299  106421 4      \n 3 2019-M020  Brisbane   Hard         32 A        2.02e7     298  105453 2      \n 4 2019-M020  Brisbane   Hard         32 A        2.02e7     297  104542 <NA>   \n 5 2019-M020  Brisbane   Hard         32 A        2.02e7     296  106421 4      \n 6 2019-M020  Brisbane   Hard         32 A        2.02e7     295  104871 <NA>   \n 7 2019-M020  Brisbane   Hard         32 A        2.02e7     294  105453 2      \n 8 2019-M020  Brisbane   Hard         32 A        2.02e7     293  104542 <NA>   \n 9 2019-M020  Brisbane   Hard         32 A        2.02e7     292  200282 7      \n10 2019-M020  Brisbane   Hard         32 A        2.02e7     291  106421 4      \n# … with 2,771 more rows, 40 more variables: winner_entry <chr>,\n#   winner_name <chr>, winner_hand <chr>, winner_ht <dbl>, winner_ioc <chr>,\n#   winner_age <dbl>, loser_id <dbl>, loser_seed <chr>, loser_entry <chr>,\n#   loser_name <chr>, loser_hand <chr>, loser_ht <dbl>, loser_ioc <chr>,\n#   loser_age <dbl>, score <chr>, best_of <dbl>, round <chr>, minutes <dbl>,\n#   w_ace <dbl>, w_df <dbl>, w_svpt <dbl>, w_1stIn <dbl>, w_1stWon <dbl>,\n#   w_2ndWon <dbl>, w_SvGms <dbl>, w_bpSaved <dbl>, w_bpFaced <dbl>, …\n\natp_2018\n\n# A tibble: 2,889 × 49\n   tourney_id tourney_…¹ surface draw_…² tourn…³ tourn…⁴ match…⁵ winne…⁶ winne…⁷\n   <chr>      <chr>      <chr>     <dbl> <chr>     <dbl>   <dbl>   <dbl>   <dbl>\n 1 2018-M020  Brisbane   Hard         32 A        2.02e7     271  105992      NA\n 2 2018-M020  Brisbane   Hard         32 A        2.02e7     272  111577      NA\n 3 2018-M020  Brisbane   Hard         32 A        2.02e7     273  104797      NA\n 4 2018-M020  Brisbane   Hard         32 A        2.02e7     275  200282      NA\n 5 2018-M020  Brisbane   Hard         32 A        2.02e7     276  111581      NA\n 6 2018-M020  Brisbane   Hard         32 A        2.02e7     277  104999       8\n 7 2018-M020  Brisbane   Hard         32 A        2.02e7     278  105238      NA\n 8 2018-M020  Brisbane   Hard         32 A        2.02e7     279  104547      NA\n 9 2018-M020  Brisbane   Hard         32 A        2.02e7     280  105051      NA\n10 2018-M020  Brisbane   Hard         32 A        2.02e7     282  111202      NA\n# … with 2,879 more rows, 40 more variables: winner_entry <chr>,\n#   winner_name <chr>, winner_hand <chr>, winner_ht <dbl>, winner_ioc <chr>,\n#   winner_age <dbl>, loser_id <dbl>, loser_seed <dbl>, loser_entry <chr>,\n#   loser_name <chr>, loser_hand <chr>, loser_ht <dbl>, loser_ioc <chr>,\n#   loser_age <dbl>, score <chr>, best_of <dbl>, round <chr>, minutes <dbl>,\n#   w_ace <dbl>, w_df <dbl>, w_svpt <dbl>, w_1stIn <dbl>, w_1stWon <dbl>,\n#   w_2ndWon <dbl>, w_SvGms <dbl>, w_bpSaved <dbl>, w_bpFaced <dbl>, …\n\n\n\n12.3.1 Filtering with semi_join()\nSuppose that we only want to keep matches in 2019 where the winning player had 10 or more wins in 2018. This might be useful if we want to not consider players in 2018 that only played in a couple of matches, perhaps because they got injured or perhaps because they received a special wildcard into the draw of only one event.\nTo accomplish this, we can first create a data set that has the names of all of the players that won 10 or more matches in 2018, using functions that we learned from dplyr earlier in the semester:\n\nwin10 <- atp_2018 |> group_by(winner_name) |>\n  summarise(nwin = n()) |> \n  filter(nwin >= 10)\nwin10\n\n# A tibble: 93 × 2\n   winner_name       nwin\n   <chr>            <int>\n 1 Adrian Mannarino    26\n 2 Albert Ramos        21\n 3 Alex De Minaur      29\n 4 Alexander Zverev    58\n 5 Aljaz Bedene        19\n 6 Andreas Seppi       24\n 7 Andrey Rublev       20\n 8 Benoit Paire        27\n 9 Borna Coric         40\n10 Cameron Norrie      19\n# … with 83 more rows\n\n\nNext, we apply semi_join(), which takes the names of two data sets (the second is the one that contains information about how the first should be “filtered”). The third argument gives the name of the key (winner_name) in this case.\n\ntennis_2019_10 <- semi_join(atp_2019, win10,\n                            by = c(\"winner_name\" = \"winner_name\"))\ntennis_2019_10$winner_name\n\nNote that this only keeps the matches in 2019 where the winner had 10 or more match wins in 2018. It drops any matches where the loser lost against someone who did not have 10 or more match wins in 2018. So this isn’t yet perfect and would take a little more thought into which matches we actually want to keep for a particular analysis.\n\n\n12.3.2 Filtering with anti_join()\nNow suppose that we want to only keep the matches in 2019 where the winning player did not have any wins in 2018. We might think of these players as “emerging players” in 2019, players who are coming back from an injury, etc.. To do this, we can use anti_join(), which only keeps the rows in the first data set that do not have a match in the second data set.\n\nnew_winners <- anti_join(atp_2019, atp_2018,\n                         by = c(\"winner_name\" = \"winner_name\")) \nnew_winners$winner_name\n\nWe can then examine how many wins each of these “new” (or perhaps previously injured) players had in 2019:\n\nnew_winners |> group_by(winner_name) |>\n  summarise(nwin = n()) |>\n  arrange(desc(nwin))\n\n# A tibble: 59 × 2\n   winner_name           nwin\n   <chr>                <int>\n 1 Christian Garin         32\n 2 Juan Ignacio Londero    22\n 3 Miomir Kecmanovic       22\n 4 Hugo Dellien            12\n 5 Attila Balazs            7\n 6 Cedrik Marcel Stebe      7\n 7 Janko Tipsarevic         7\n 8 Jannik Sinner            7\n 9 Soon Woo Kwon            7\n10 Gregoire Barrere         6\n# … with 49 more rows\n\n\nThe filtering join functions are useful if you want to filter out observations by some criterion in a different data set.\n\n\n12.3.3 Exercises\n\nExamine the following data sets (the first is df1 and the second is df2) and then, without running any code, answer the following questions.\n\n\n\n\n\n\n\n\n\n\nid\nxvar\n\n\n\n\nA\n1\n\n\nB\n2\n\n\nC\n3\n\n\nE\n1\n\n\nF\n2\n\n\n\n\n\n\n\n\n\n\n\n\nid\nyvar\n\n\n\n\nA\n2\n\n\nC\n1\n\n\nD\n2\n\n\nE\n1\n\n\nG\n1\n\n\nH\n4\n\n\n\n\n\n\nHow many rows would be in the data set from left_join(df1, df2, by = c(\"id\" = \"id\"))?\n\n\n\n\n\nHow many rows would be in the data set from left_join(df2, df1, by = c(\"id\" = \"id\"))?\n\n\n\n\n\nHow many rows would be in the data set from full_join(df1, df2, by = c(\"id\" = \"id\"))?\n\n\n\n\n\nHow many rows would be in the data set from inner_join(df1, df2, by = c(\"id\" = \"id\"))?\n\n\n\n\n\nHow many rows would be in the data set from semi_join(df1, df2, by = c(\"id\" = \"id\"))?\n\n\n\n\n\nHow many rows would be in the data set from anti_join(df1, df2, by = c(\"id\" = \"id\"))?\n\n\n\n\n\nExamine the following data sets (the first is df3 and the second is df4) and then, without running any code, answer the following questions. This question is a step up in challenge from Exercise 9 because a few of the levels of the id key have duplicates.\n\n\n\n\n\n\n\n\n\n\nid\nxvar\n\n\n\n\nA\n1\n\n\nA\n2\n\n\nC\n3\n\n\nC\n1\n\n\nF\n2\n\n\nF\n6\n\n\n\n\n\n\n\n\n\n\n\n\nid\nyvar\n\n\n\n\nA\n2\n\n\nB\n1\n\n\nC\n2\n\n\nD\n1\n\n\nD\n1\n\n\nD\n4\n\n\n\n\n\n\nHow many rows would be in the data set from left_join(df1, df2, by = c(\"id\" = \"id\"))?\nHow many rows would be in the data set from left_join(df2, df1, by = c(\"id\" = \"id\"))?\nHow many rows would be in the data set from full_join(df1, df2, by = c(\"id\" = \"id\"))?\nHow many rows would be in the data set from inner_join(df1, df2, by = c(\"id\" = \"id\"))?\nHow many rows would be in the data set from semi_join(df1, df2, by = c(\"id\" = \"id\"))?\nHow many rows would be in the data set from anti_join(df1, df2, by = c(\"id\" = \"id\"))?"
  },
  {
    "objectID": "10-merging.html#chapexercise-10",
    "href": "10-merging.html#chapexercise-10",
    "title": "12  Merging with dplyr",
    "section": "12.4 Chapter Exercises",
    "text": "12.4 Chapter Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-10).\n\n* Read in the gun violence data set, and suppose that you want to add a row to this data set that has the statistics on gun ownership and mortality rate in the District of Columbia (Washington D.C., which is in the NE region, has 16.7 deaths per 100,000 people, and a gun ownership rate of 8.7%). To do so, create a tibble() that has a single row representing D.C. and then combine your new tibble with the overall gun violence data set. Name this new data set all_df.\n\n\nlibrary(tidyverse)\nmortality_df <- read_csv(here(\"data/gun_violence_us.csv\"))\n\n\n\n\n\nExplain why each attempt at combining the D.C. data with the overall data doesn’t work or is incorrect.\n\n\ntest1 <- tibble(state = \"Washington D.C.\", mortality_rate = 16.7,\n                ownership_rate = 8.7, region = \"NE\")\nbind_rows(mortality_df, test1)\n\ntest2 <- tibble(state = \"Washington D.C.\", mortality_rate = 16.7,\n       ownership_rate = 0.087, region = NE)\n\nError in eval_tidy(xs[[j]], mask): object 'NE' not found\n\nbind_rows(mortality_df, test2)\n\nError in list2(...): object 'test2' not found\n\ntest3 <- tibble(state = \"Washington D.C.\", mortality_rate = \"16.7\",\n       ownership_rate = \"0.087\", region = \"NE\")\nbind_rows(mortality_df, test3)\n\nError in `bind_rows()`:\n! Can't combine `..1$mortality_rate` <double> and `..2$mortality_rate` <character>.\n\n\n\nExamine the following data sets that are in R’s base library on demographic statistics about the U.S. states and state abbreviations:\n\n\ndf1 <- as_tibble(state.x77)\ndf2 <- as_tibble(state.abb)\ndf1\n\n# A tibble: 50 × 8\n   Population Income Illiteracy `Life Exp` Murder `HS Grad` Frost   Area\n        <dbl>  <dbl>      <dbl>      <dbl>  <dbl>     <dbl> <dbl>  <dbl>\n 1       3615   3624        2.1       69.0   15.1      41.3    20  50708\n 2        365   6315        1.5       69.3   11.3      66.7   152 566432\n 3       2212   4530        1.8       70.6    7.8      58.1    15 113417\n 4       2110   3378        1.9       70.7   10.1      39.9    65  51945\n 5      21198   5114        1.1       71.7   10.3      62.6    20 156361\n 6       2541   4884        0.7       72.1    6.8      63.9   166 103766\n 7       3100   5348        1.1       72.5    3.1      56     139   4862\n 8        579   4809        0.9       70.1    6.2      54.6   103   1982\n 9       8277   4815        1.3       70.7   10.7      52.6    11  54090\n10       4931   4091        2         68.5   13.9      40.6    60  58073\n# … with 40 more rows\n\ndf2\n\n# A tibble: 50 × 1\n   value\n   <chr>\n 1 AL   \n 2 AK   \n 3 AZ   \n 4 AR   \n 5 CA   \n 6 CO   \n 7 CT   \n 8 DE   \n 9 FL   \n10 GA   \n# … with 40 more rows\n\n\nCombine the two data sets with bind_cols(). What are you assuming about the data sets in order to use this function?\n\n\n\n\n* Combine the columns of the states data set you made in Exercise 3 with the mortality data set without Washington D.C.\n\n\n\n\n\n* Use a join function to combine the mortality data set (all_df from Exercise 1) with D.C. with the states data set from Exercise 3 (states_df). For this exercise, keep the row with Washington D.C., having it take on NA values for any variable not observed in the states data.\n\n\n\n\n\n* Repeat Exercise 5, but now drop Washington D.C. in your merging process. Practice doing this with a join function (as opposed to slice()-ing it out explicitly).\n\n\n\n\n\n* Use semi_join() to create a subset of states_df that are in the NE region. Hint: You will need to filter all_df from Exercise 1 first to contain only states in the NE region.\n\n\n\n\n\n* Do the same thing as Exercise 7, but this time, use anti_join(). Hint: You’ll need to filter all_df in a different way to achieve this."
  },
  {
    "objectID": "10-merging.html#solutions-10",
    "href": "10-merging.html#solutions-10",
    "title": "12  Merging with dplyr",
    "section": "12.5 Exercise Solutions",
    "text": "12.5 Exercise Solutions\n\n12.5.1 bind_rows() and bind_cols() S\n\n* Run the following and explain why R does not simply stack the rows. Then, fix the issue with the rename() function.\n\n\ndf_test1a <- tibble(xvar = c(1, 2), yvar = c(5, 1))\ndf_test1b <- tibble(x = c(1, 2), y = c(5, 1))\nbind_rows(df_test1a, df_test1b)\n\n# A tibble: 4 × 4\n   xvar  yvar     x     y\n  <dbl> <dbl> <dbl> <dbl>\n1     1     5    NA    NA\n2     2     1    NA    NA\n3    NA    NA     1     5\n4    NA    NA     2     1\n\n\n\n## This doesn't stack rows because the columns are named differently\n## in the two data sets. If xvar is the same variable as x and \n## yvar is the same variable as y, then we can rename the columns in\n## one of the data sets:\n\ndf_test1a <- df_test1a |> rename(x = \"xvar\", y = \"yvar\")\nbind_rows(df_test1a, df_test1b)\n\n# A tibble: 4 × 2\n      x     y\n  <dbl> <dbl>\n1     1     5\n2     2     1\n3     1     5\n4     2     1\n\n\n\n\n12.5.2 Mutating Joins S\n\n\n12.5.3 Filtering Joins S\n\n\n12.5.4 Chapter Exercises S\n\n* Read in the gun violence data set, and suppose that you want to add a row to this data set that has the statistics on gun ownership and mortality rate in the District of Columbia (Washington D.C., which is in the NE region, has 16.7 deaths per 100,000 people, and a gun ownership rate of 8.7%). To do so, create a tibble() that has a single row representing D.C. and then combine your new tibble with the overall gun violence data set. Name this new data set all_df.\n\n\nlibrary(tidyverse)\nmortality_df <- read_csv(here(\"data/gun_violence_us.csv\"))\n\nRows: 50 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): state, region\ndbl (2): mortality_rate, ownership_rate\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\ndc_df <- tibble(state = \"Washington D.C.\", mortality_rate = 16.7,\n       ownership_rate = 0.087, region = \"NE\")\nall_df <- bind_rows(mortality_df, dc_df)\n\n\n* Combine the columns of the states data set you made in Section A Exercise 3 with the mortality data set without Washington D.C.\n\n\nbind_cols(mortality_df, states_df)\n\n\n* Use a join function to combine the mortality data set with D.C. with the states data set from Exercise 3. For this exercise, keep the row with Washington D.C., having it take on NA values for any variable not observed in the states data.\n\n\nleft_join(all_df, states_df, by = c(\"state\" = \"value\"))\n\n# A tibble: 51 × 12\n   state mortalit…¹ owner…² region Popul…³ Income Illit…⁴ Life …⁵ Murder HS Gr…⁶\n   <chr>      <dbl>   <dbl> <chr>    <dbl>  <dbl>   <dbl>   <dbl>  <dbl>   <dbl>\n 1 AL          16.7   0.489 South     3615   3624     2.1    69.0   15.1    41.3\n 2 AK          18.8   0.617 West       365   6315     1.5    69.3   11.3    66.7\n 3 AZ          13.4   0.323 West      2212   4530     1.8    70.6    7.8    58.1\n 4 AR          16.4   0.579 South     2110   3378     1.9    70.7   10.1    39.9\n 5 CA           7.4   0.201 West     21198   5114     1.1    71.7   10.3    62.6\n 6 CO          12.1   0.343 West      2541   4884     0.7    72.1    6.8    63.9\n 7 CT           4.9   0.166 NE        3100   5348     1.1    72.5    3.1    56  \n 8 DE          11.1   0.052 NE         579   4809     0.9    70.1    6.2    54.6\n 9 FL          11.5   0.325 South     8277   4815     1.3    70.7   10.7    52.6\n10 GA          13.7   0.316 South     4931   4091     2      68.5   13.9    40.6\n# … with 41 more rows, 2 more variables: Frost <dbl>, Area <dbl>, and\n#   abbreviated variable names ¹​mortality_rate, ²​ownership_rate, ³​Population,\n#   ⁴​Illiteracy, ⁵​`Life Exp`, ⁶​`HS Grad`\n\n## or\nfull_join(all_df, states_df, by = c(\"state\" = \"value\"))\n\n# A tibble: 51 × 12\n   state mortalit…¹ owner…² region Popul…³ Income Illit…⁴ Life …⁵ Murder HS Gr…⁶\n   <chr>      <dbl>   <dbl> <chr>    <dbl>  <dbl>   <dbl>   <dbl>  <dbl>   <dbl>\n 1 AL          16.7   0.489 South     3615   3624     2.1    69.0   15.1    41.3\n 2 AK          18.8   0.617 West       365   6315     1.5    69.3   11.3    66.7\n 3 AZ          13.4   0.323 West      2212   4530     1.8    70.6    7.8    58.1\n 4 AR          16.4   0.579 South     2110   3378     1.9    70.7   10.1    39.9\n 5 CA           7.4   0.201 West     21198   5114     1.1    71.7   10.3    62.6\n 6 CO          12.1   0.343 West      2541   4884     0.7    72.1    6.8    63.9\n 7 CT           4.9   0.166 NE        3100   5348     1.1    72.5    3.1    56  \n 8 DE          11.1   0.052 NE         579   4809     0.9    70.1    6.2    54.6\n 9 FL          11.5   0.325 South     8277   4815     1.3    70.7   10.7    52.6\n10 GA          13.7   0.316 South     4931   4091     2      68.5   13.9    40.6\n# … with 41 more rows, 2 more variables: Frost <dbl>, Area <dbl>, and\n#   abbreviated variable names ¹​mortality_rate, ²​ownership_rate, ³​Population,\n#   ⁴​Illiteracy, ⁵​`Life Exp`, ⁶​`HS Grad`\n\n\n\n* Repeat Exercise 5, but now drop Washington D.C. in your merging process. Practice doing this with a join function (as opposed to slice() ing it out explictly).\n\n\ninner_join(all_df, states_df, by = c(\"state\" = \"value\"))\n\n# A tibble: 50 × 12\n   state mortalit…¹ owner…² region Popul…³ Income Illit…⁴ Life …⁵ Murder HS Gr…⁶\n   <chr>      <dbl>   <dbl> <chr>    <dbl>  <dbl>   <dbl>   <dbl>  <dbl>   <dbl>\n 1 AL          16.7   0.489 South     3615   3624     2.1    69.0   15.1    41.3\n 2 AK          18.8   0.617 West       365   6315     1.5    69.3   11.3    66.7\n 3 AZ          13.4   0.323 West      2212   4530     1.8    70.6    7.8    58.1\n 4 AR          16.4   0.579 South     2110   3378     1.9    70.7   10.1    39.9\n 5 CA           7.4   0.201 West     21198   5114     1.1    71.7   10.3    62.6\n 6 CO          12.1   0.343 West      2541   4884     0.7    72.1    6.8    63.9\n 7 CT           4.9   0.166 NE        3100   5348     1.1    72.5    3.1    56  \n 8 DE          11.1   0.052 NE         579   4809     0.9    70.1    6.2    54.6\n 9 FL          11.5   0.325 South     8277   4815     1.3    70.7   10.7    52.6\n10 GA          13.7   0.316 South     4931   4091     2      68.5   13.9    40.6\n# … with 40 more rows, 2 more variables: Frost <dbl>, Area <dbl>, and\n#   abbreviated variable names ¹​mortality_rate, ²​ownership_rate, ³​Population,\n#   ⁴​Illiteracy, ⁵​`Life Exp`, ⁶​`HS Grad`\n\n## or\nleft_join(states_df, all_df, by = c(\"value\" = \"state\"))\n\n# A tibble: 50 × 12\n   Population Income Illiter…¹ Life …² Murder HS Gr…³ Frost   Area value morta…⁴\n        <dbl>  <dbl>     <dbl>   <dbl>  <dbl>   <dbl> <dbl>  <dbl> <chr>   <dbl>\n 1       3615   3624       2.1    69.0   15.1    41.3    20  50708 AL       16.7\n 2        365   6315       1.5    69.3   11.3    66.7   152 566432 AK       18.8\n 3       2212   4530       1.8    70.6    7.8    58.1    15 113417 AZ       13.4\n 4       2110   3378       1.9    70.7   10.1    39.9    65  51945 AR       16.4\n 5      21198   5114       1.1    71.7   10.3    62.6    20 156361 CA        7.4\n 6       2541   4884       0.7    72.1    6.8    63.9   166 103766 CO       12.1\n 7       3100   5348       1.1    72.5    3.1    56     139   4862 CT        4.9\n 8        579   4809       0.9    70.1    6.2    54.6   103   1982 DE       11.1\n 9       8277   4815       1.3    70.7   10.7    52.6    11  54090 FL       11.5\n10       4931   4091       2      68.5   13.9    40.6    60  58073 GA       13.7\n# … with 40 more rows, 2 more variables: ownership_rate <dbl>, region <chr>,\n#   and abbreviated variable names ¹​Illiteracy, ²​`Life Exp`, ³​`HS Grad`,\n#   ⁴​mortality_rate\n\n\n\n* Use semi_join() to create a subset of states_df that are in the NE region. Hint: You will need to filter all_df first to contain only states in the NE region.\n\n\nne_df <- all_df |> filter(region == \"NE\")\nsemi_join(states_df, ne_df, by = c(\"value\" = \"state\"))\n\n# A tibble: 10 × 9\n   Population Income Illiteracy `Life Exp` Murder `HS Grad` Frost  Area value\n        <dbl>  <dbl>      <dbl>      <dbl>  <dbl>     <dbl> <dbl> <dbl> <chr>\n 1       3100   5348        1.1       72.5    3.1      56     139  4862 CT   \n 2        579   4809        0.9       70.1    6.2      54.6   103  1982 DE   \n 3       1058   3694        0.7       70.4    2.7      54.7   161 30920 ME   \n 4       4122   5299        0.9       70.2    8.5      52.3   101  9891 MD   \n 5       5814   4755        1.1       71.8    3.3      58.5   103  7826 MA   \n 6        812   4281        0.7       71.2    3.3      57.6   174  9027 NH   \n 7       7333   5237        1.1       70.9    5.2      52.5   115  7521 NJ   \n 8      18076   4903        1.4       70.6   10.9      52.7    82 47831 NY   \n 9        931   4558        1.3       71.9    2.4      46.4   127  1049 RI   \n10        472   3907        0.6       71.6    5.5      57.1   168  9267 VT   \n\n\n\n* Do the same thing as Exercise 7, but this time, use anti_join(). Hint: You’ll need to filter all_df in a different way to achieve this.\n\n\nnotne_df <- all_df |> filter(region != \"NE\")\nanti_join(states_df, notne_df, by = c(\"value\" = \"state\"))\n\n# A tibble: 10 × 9\n   Population Income Illiteracy `Life Exp` Murder `HS Grad` Frost  Area value\n        <dbl>  <dbl>      <dbl>      <dbl>  <dbl>     <dbl> <dbl> <dbl> <chr>\n 1       3100   5348        1.1       72.5    3.1      56     139  4862 CT   \n 2        579   4809        0.9       70.1    6.2      54.6   103  1982 DE   \n 3       1058   3694        0.7       70.4    2.7      54.7   161 30920 ME   \n 4       4122   5299        0.9       70.2    8.5      52.3   101  9891 MD   \n 5       5814   4755        1.1       71.8    3.3      58.5   103  7826 MA   \n 6        812   4281        0.7       71.2    3.3      57.6   174  9027 NH   \n 7       7333   5237        1.1       70.9    5.2      52.5   115  7521 NJ   \n 8      18076   4903        1.4       70.6   10.9      52.7    82 47831 NY   \n 9        931   4558        1.3       71.9    2.4      46.4   127  1049 RI   \n10        472   3907        0.6       71.6    5.5      57.1   168  9267 VT"
  },
  {
    "objectID": "10-merging.html#rcode-10",
    "href": "10-merging.html#rcode-10",
    "title": "12  Merging with dplyr",
    "section": "12.6 Non-Exercise R Code",
    "text": "12.6 Non-Exercise R Code\n\nlibrary(tidyverse)\nlibrary(here)\natp_2019 <- read_csv(here(\"data/atp_matches_2019.csv\"))\natp_2018 <- read_csv(here(\"data/atp_matches_2018.csv\"))\nhead(atp_2019) \nhead(atp_2018)\nspec(atp_2018)\natp_2018 <- read_csv(here(\"data/atp_matches_2018.csv\"),\n                     col_types = cols(winner_seed = col_character(),\n                                      loser_seed = col_character()))\natp_df <- bind_rows(atp_2018, atp_2019)\natp_df\ndf_test2a <- tibble(xvar = c(1, 2))\ndf_test2b <- tibble(xvar = c(1, 2), y = c(5, 1))\nbind_rows(df_test2a, df_test2b)\ndf_test1a <- tibble(xvar = c(1, 2), yvar = c(5, 1))\ndf_test1b <- tibble(x = c(1, 2), y = c(5, 1))\nbind_cols(df_test1a, df_test1b)\nlibrary(tidyverse)\ndf1 <- tibble(name = c(\"Emily\", \"Miguel\", \"Tonya\"), fav_sport = c(\"Swimming\", \"Football\", \"Tennis\"))\ndf2 <- tibble(name = c(\"Tonya\", \"Miguel\", \"Emily\"),\n              fav_colour = c(\"Robin's Egg Blue\", \"Tickle Me Pink\", \"Goldenrod\"))\n##install.packages(\"babynames\")\nlibrary(babynames)\nlife_df <- babynames::lifetables\nbirth_df <- babynames::births\nbabynames_df <- babynames::babynames\nhead(babynames)\nhead(births)\nhead(lifetables)\ncombined_left <- left_join(babynames_df, birth_df, by = c(\"year\" = \"year\"))\nhead(combined_left)\ntail(combined_left)\n## these will always do the same exact thing\nright_join(babynames_df, birth_df, by = c(\"year\" = \"year\"))\nleft_join(birth_df, babynames_df, by = c(\"year\" = \"year\"))\nfull_join(babynames_df, birth_df, by = c(\"year\" = \"year\"))\ninner_join(babynames_df, birth_df, by = c(\"year\" = \"year\"))\nslumajors_df <- read_csv(here(\"data/SLU_Majors_15_19.csv\"))\ncollegemajors_df <- read_csv(here(\"data/college-majors.csv\"))\nhead(slumajors_df)\nhead(collegemajors_df)\nleft_join(slumajors_df, collegemajors_df, by = c(\"Major\" = \"Major\"))\ncollegemajors_df <- collegemajors_df |>\n  mutate(Major = str_to_title(Major))\nleft_join(slumajors_df, collegemajors_df)\natp_2019 <- read_csv(here(\"data/atp_matches_2019.csv\"))\natp_2018 <- read_csv(here(\"data/atp_matches_2018.csv\"))\natp_2019\natp_2018\nwin10 <- atp_2018 |> group_by(winner_name) |>\n  summarise(nwin = n()) |> \n  filter(nwin >= 10)\nwin10\ntennis_2019_10 <- semi_join(atp_2019, win10,\n                            by = c(\"winner_name\" = \"winner_name\"))\ntennis_2019_10$winner_name\nnew_winners <- anti_join(atp_2019, atp_2018,\n                         by = c(\"winner_name\" = \"winner_name\")) \nnew_winners$winner_name\nnew_winners |> group_by(winner_name) |>\n  summarise(nwin = n()) |>\n  arrange(desc(nwin))"
  },
  {
    "objectID": "11-lubridate.html",
    "href": "11-lubridate.html",
    "title": "13  Dates with lubridate",
    "section": "",
    "text": "Goals:"
  },
  {
    "objectID": "12-stringr.html",
    "href": "12-stringr.html",
    "title": "14  Text Data with tidytext and stringr",
    "section": "",
    "text": "Goals:"
  },
  {
    "objectID": "12-stringr.html#text-analysis",
    "href": "12-stringr.html#text-analysis",
    "title": "14  Text Data with tidytext and stringr",
    "section": "14.1 Text Analysis",
    "text": "14.1 Text Analysis\nBeyonce is a legend. For this example, we will work through a text analysis on lyrics from songs from Beyonce’s albums, utilizing functions from both stringr to parse strings and tidytext to convert text data into a tidy format. To begin, read in a data set of Beyonce’s lyrics:\n\nlibrary(tidyverse)\nlibrary(here)\nbeyonce <- read_csv(here(\"data/beyonce_lyrics.csv\"))\nhead(beyonce)\n\nWe will be most focused on the line variable, as each value for this variable contains a line from a Beyonce song. There’s other variables present as well, such as the song_name and the artist_name (the data set originally came from a data set with artists other than Beyonce).\nYou can look at the first 4 values of line with\n\nbeyonce$line[1:4]\n\n[1] \"If I ain't got nothing, I got you\"                       \n[2] \"If I ain't got something, I don't give a damn\"           \n[3] \"'Cause I got it with you\"                                \n[4] \"I don't know much about algebra, but I know 1+1 equals 2\"\n\n\nOur end goal is to construct a plot that shows the most popular words in Beyonce’s albums. This is much more challenging than it sounds because we will have to deal with the nuances of working with text data.\nThe tidytext package makes it a lot easier to work with text data in many regards. Let’s use the unnest_tokens() functions from tidytext to separate the lines into individual words. We’ll name this new data set beyonce_unnest:\n\nlibrary(tidytext)\nbeyonce_unnest <- beyonce |> unnest_tokens(output = \"word\", input = \"line\")\nbeyonce_unnest\n\n# A tibble: 164,740 × 6\n   song_id song_name artist_id artist_name song_line word   \n     <dbl> <chr>         <dbl> <chr>           <dbl> <chr>  \n 1   50396 1+1             498 Beyoncé             1 if     \n 2   50396 1+1             498 Beyoncé             1 i      \n 3   50396 1+1             498 Beyoncé             1 ain't  \n 4   50396 1+1             498 Beyoncé             1 got    \n 5   50396 1+1             498 Beyoncé             1 nothing\n 6   50396 1+1             498 Beyoncé             1 i      \n 7   50396 1+1             498 Beyoncé             1 got    \n 8   50396 1+1             498 Beyoncé             1 you    \n 9   50396 1+1             498 Beyoncé             2 if     \n10   50396 1+1             498 Beyoncé             2 i      \n# … with 164,730 more rows\n\n\nWe’ll want to make sure that either all words are capitalized or no words are capitalized, for consistency (remember that R is case-sensitive). To that end, we’ll modify the word variable and use stringr’s str_to_lower() to change all letters to lower-case:\n\nbeyonce_unnest <- beyonce_unnest |> mutate(word = str_to_lower(word))\n\nLet’s try counting up Beyonce’s most popular words from the data set we just made:\n\nbeyonce_unnest |> group_by(word) |>\n  summarise(n = n()) |>\n  arrange(desc(n))\n\n# A tibble: 6,469 × 2\n   word      n\n   <chr> <int>\n 1 you    7693\n 2 i      6669\n 3 the    4719\n 4 me     3774\n 5 to     3070\n 6 it     2999\n 7 a      2798\n 8 my     2676\n 9 and    2385\n10 on     2344\n# … with 6,459 more rows\n\n\nWhat’s the issue here?\nTo remedy this, we can use what are called stop words: words that are very common and carry little to no meaningful information. For example the, it, are, etc. are all stop words. We need to eliminate these from the data set before we continue on. Luckily, the tidytext package also provides a data set of common stop words in a data set named stop_words:\n\nhead(stop_words)\n\n# A tibble: 6 × 2\n  word      lexicon\n  <chr>     <chr>  \n1 a         SMART  \n2 a's       SMART  \n3 able      SMART  \n4 about     SMART  \n5 above     SMART  \n6 according SMART  \n\n\nLet’s join the Beyonce lyrics data set to the stop words data set and elminate any stop words:\n\nbeyonce_stop <- anti_join(beyonce_unnest, stop_words, by = c(\"word\" = \"word\"))\n\nThen, we can re-make the table with the stop words removed:\n\nbeyonce_sum <- beyonce_stop |> group_by(word) |>\n  summarise(n = n()) |>\n  arrange(desc(n)) |>\n  print(n = 25)\n\n# A tibble: 5,937 × 2\n   word        n\n   <chr>   <int>\n 1 love     1362\n 2 baby     1024\n 3 girl      592\n 4 wanna     564\n 5 hey       499\n 6 boy       494\n 7 yeah      491\n 8 feel      488\n 9 time      452\n10 uh        408\n11 halo      383\n12 check     366\n13 tonight   342\n14 girls     341\n15 ya        327\n16 run       325\n17 crazy     308\n18 world     301\n19 body      287\n20 ooh       281\n21 ladies    269\n22 top       241\n23 gotta     240\n24 beyoncé   238\n25 night     213\n# … with 5,912 more rows\n\nbeyonce_sum\n\n# A tibble: 5,937 × 2\n   word      n\n   <chr> <int>\n 1 love   1362\n 2 baby   1024\n 3 girl    592\n 4 wanna   564\n 5 hey     499\n 6 boy     494\n 7 yeah    491\n 8 feel    488\n 9 time    452\n10 uh      408\n# … with 5,927 more rows\n\n\nLooking through the list, there are still some stop words in there that were not picked up on in the stop_words data set. We will address these, as well as make a plot, in the exercises.\n\n14.1.1 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-12).\n\nLook at the remaining words. Do any of them look like stop words that were missed with the stop words from the tidytext package? Create a tibble with a few of the remaining stop words (like ooh, gotta, ya, uh, and yeah) not picked up by the tidytext package, and use a join function to drop these words from the data set.\nWith the new data set, construct a lollipop plot or a bar plot that shows the 20 most common words Beyonce uses, as well as the number of times each word is used.\nUse the wordcloud() function in the wordcloud library and the code below to make a wordcloud of Beyonce’s words.\n\n\n## install.packages(\"wordcloud\")\nlibrary(wordcloud)\n\nLoading required package: RColorBrewer\n\nbeyonce_small <- beyonce_sum |> filter(n > 50)\nwordcloud(beyonce_small$word, beyonce_small$n, \n          colors = brewer.pal(8, \"Dark2\"), scale = c(5, .2),\n          random.order = FALSE, random.color = FALSE)\n\nThen, use ?wordcloud to read about what the various arguments like random.order, scale, and random.color do.\nIf you want to delve into text data more, you’ll need to learn about regular expressions , or regexes. If interested, you can read more in the R4DS textbook. Starting out is not too bad, but learning about escaping special characters in R can be much more challenging!\nWe analyzed a short text data set, but, you can imagine extending this type of analysis to things like:\n\nsong lyrics, if you have the lyrics to all of the songs from an artist https://rpubs.com/RosieB/taylorswiftlyricanalysis\nbook analysis, if you have the text of an entire book or series of books\ntv analysis, if you have the scripts to all episodes of a tv show\n\nIf you were doing one of these analyses, there are lots of cool functions in tidytext to help you out! We will do one more example, this time looking at Donald Trump’s twitter account in 2016."
  },
  {
    "objectID": "12-stringr.html#basic-sentiment-analysis",
    "href": "12-stringr.html#basic-sentiment-analysis",
    "title": "14  Text Data with tidytext and stringr",
    "section": "14.2 Basic Sentiment Analysis",
    "text": "14.2 Basic Sentiment Analysis\nWe will use a provided .qmd file to replicate a sentiment analysis on Trump’s twitter account from 2016. This analysis was used in conjunction with a major news story that hypothesized that Trump himself wrote tweets from an Android device while his campaign staff wrote tweets for him from an iPhone device. We will investigate what properties of his tweets led the author to believe this.\nThe .qmd file used for this is posted on Canvas. We will see more uses of stringr for this particular analysis. For this entire section, you should be able to follow along and understand what each line of code is doing. However, unlike all previous sections, you will not be expected to do a sentiment analysis on your own."
  },
  {
    "objectID": "12-stringr.html#introduction-to-stringr",
    "href": "12-stringr.html#introduction-to-stringr",
    "title": "14  Text Data with tidytext and stringr",
    "section": "14.3 Introduction to stringr",
    "text": "14.3 Introduction to stringr\nIn the previous examples, the string data that we had consisted primarily of words. The tools in tidytext make working with data consisting of words not too painful. However, some data exists as strings that are not words. For a non-trivial example, consider data sets obtained from https://github.com/JeffSackmann/tennis_MatchChartingProject, a repository for professional tennis match charting put together by Jeff Sackmann. Some of the following code was modified from a project completed by James Wolpe in a data visualization course.\nFrom this repository, I have put together a data set on one particular tennis match to make it a bit easier for us to get started. The match I have chosen is the 2021 U.S. Open Final between Daniil Medvedev and Novak Djokovic. Why this match? This was arguably the most important match of Djokovic’s career: if he won, he would win all four grand slams in a calendar year. I don’t like Djokovic and he lost so looking back at the match brings me joy. Read in the data set with:\n\nlibrary(here)\nlibrary(tidyverse)\nmed_djok_df <- read_csv(here(\"data/med_djok.csv\"))\n\nRows: 182 Columns: 46\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (19): point, Serving, match_id, Pts, Gm#, 1st, 2nd, Notes, Player 1, Pla...\ndbl (19): Pt, Set1, Set2, Gm1, Gm2, TbSet, TB?, Svr, Ret, 1stSV, 2ndSV, 1stI...\nlgl  (8): TBpt, isAce, isUnret, isRallyWinner, isForced, isUnforced, isDoubl...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(med_djok_df)\n\n# A tibble: 6 × 46\n  point    Serving match…¹    Pt  Set1  Set2   Gm1   Gm2 Pts   `Gm#` TbSet `TB?`\n  <chr>    <chr>   <chr>   <dbl> <dbl> <dbl> <dbl> <dbl> <chr> <chr> <dbl> <dbl>\n1 4f2d@    ND      202109…     1     0     0     0     0 0-0   1 (1)     1     0\n2 6d       ND      202109…     2     0     0     0     0 15-0  1 (2)     1     0\n3 6b29f3b… ND      202109…     3     0     0     0     0 15-15 1 (3)     1     0\n4 4b28f1f… ND      202109…     4     0     0     0     0 30-15 1 (4)     1     0\n5 5b37b3b… ND      202109…     5     0     0     0     0 40-15 1 (5)     1     0\n6 6f28f1f… ND      202109…     6     0     0     0     0 40-30 1 (6)     1     0\n# … with 34 more variables: TBpt <lgl>, Svr <dbl>, Ret <dbl>, `1st` <chr>,\n#   `2nd` <chr>, Notes <chr>, `1stSV` <dbl>, `2ndSV` <dbl>, `1stIn` <dbl>,\n#   `2ndIn` <dbl>, isAce <lgl>, isUnret <lgl>, isRallyWinner <lgl>,\n#   isForced <lgl>, isUnforced <lgl>, isDouble <lgl>, PtWinner <dbl>,\n#   isSvrWinner <dbl>, rallyCount <dbl>, `Player 1` <chr>, `Player 2` <chr>,\n#   `Pl 1 hand` <chr>, `Pl 2 hand` <chr>, Gender <chr>, Date <dbl>,\n#   Tournament <chr>, Round <lgl>, Time <chr>, Court <chr>, Surface <chr>, …\n\n\nThe observations of the data set correspond to points played (so there is one row per point). There are a ton of variables in this data set, but the most important variable is the first variable, point, which contains a string with information about the types of shots that were played during the point. The coding of the point variable includes:\n\n4 for serve out wide, 5 for serve into the body, and 6 for a serve “down the t (center)”.\nf for forehand stroke, b for backhand stroke.\n1 to a right-hander’s forehand side, 2 for down the middle of the court, and 3 to a right-hander’s backhand side.\nd for a ball hit deep, w for a ball hit wide, and n for a ball hit into the net\n@ symbol at the end if the point ended in an unforced error\nand there’s lots of other numbers and symbols that correspond to other things (volleys, return depths, hitting the top of the net, etc.)\n\nFor example, Djokovic served the 7th point of the match, which has a point value of 4f18f1f2b3b2f1w@. This reads that\n\n4: Djokovic served out wide,\nf18: Medvedev hit a forehand cross-court to Djokovic’s forehand side\nf1: Djokovic hit a forehand cross-court to Medvedev’s forehand side\nf2: Medvedev hit a forehand to the center of the court\nb3: Djokovic hit a backhand to Medvedev’s backhand side\nb2: Medvedev hit a backhand to the center of the court\nf1w@: Djokovic hit a forehand to Medvedev’s forehand side, but the shot landed wide and was recorded as an unforced error.\n\nClearly, there is a lot of data encoded in the point variable. We are going to introduce stringr by answering a relatively simple question: what are the serving patterns of Medvedev and Djokovic during this match?\n\n14.3.1 Regular Expressions\nA regex, or regular expression, is a string used to identify particular patterns in string data. Regular expressions are used in many languages (so, if you google something about a regular expression, you do not need to limit yourself to just looking at resources pertaining to R).\nRegex’s can be used with the functions in the stringr package. The functions in the stringr package begin with str_(), much like the functions in forcats began with fct_(). We will first focus on the str_detect() function, which detects whether a particular regex is present in a string variable. str_detect() takes the name of the string as its first argument and the regex as the second argument. For example,\n\nstr_detect(med_djok_df$point, pattern = \"f\")\n\n  [1]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n [13]  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n [25]  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE\n [37]  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE\n [49] FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE FALSE\n [61]  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n [73] FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE\n [85]  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n [97]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n[109] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE  TRUE\n[121]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE\n[133]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE\n[145] FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE\n[157]  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE\n[169] FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[181] FALSE FALSE\n\n\nreturns a TRUE if the letter f appears anywhere in the string and a FALSE if it does not. So, we can examine how many points a forehand was hit in the Medvedev Djokovic match. As a second example,\n\nstr_detect(med_djok_df$point, pattern = \"d@\")\n\nreturns TRUE if d@ appears in a string and FALSE if not. Note that d@ must appear together and in that order to return a TRUE. This lets us examine how many points a ball is hit deep and is recorded an unforced error. It looks like\n\nsum(str_detect(med_djok_df$point, pattern = \"d@\"))\n\n[1] 21\n\n\npoints ended in an unforced error where the ball was hit deep,\n\nsum(str_detect(med_djok_df$point, pattern = \"w@\"))\n\n[1] 19\n\n\npoints ended in an unforced error where the ball was hit wide, and\n\nsum(str_detect(med_djok_df$point, pattern = \"n@\"))\n\n[1] 22\n\n\npoints ended in an unforced error where the ball was hit into the net.\n\n\n14.3.2 stringr Functions with dplyr\nWe can combine the stringr functions with dplyr functions that we already know and love. For example, if we are only interested in points the end in an unforced error (so points that have the @ symbol in them), we can filter out the points that don’t have an @:\n\nmed_djok_df |> filter(str_detect(point, pattern = \"@\") == TRUE)\n\n# A tibble: 63 × 46\n   point   Serving match…¹    Pt  Set1  Set2   Gm1   Gm2 Pts   `Gm#` TbSet `TB?`\n   <chr>   <chr>   <chr>   <dbl> <dbl> <dbl> <dbl> <dbl> <chr> <chr> <dbl> <dbl>\n 1 4f2d@   ND      202109…     1     0     0     0     0 0-0   1 (1)     1     0\n 2 6b29f3… ND      202109…     3     0     0     0     0 15-15 1 (3)     1     0\n 3 5b37b3… ND      202109…     5     0     0     0     0 40-15 1 (5)     1     0\n 4 4f18f1… ND      202109…     7     0     0     0     0 40-40 1 (7)     1     0\n 5 5b28f1… ND      202109…     8     0     0     0     0 40-AD 1 (8)     1     0\n 6 6b27f2… DM      202109…    13     0     0     0     1 40-15 2 (5)     1     0\n 7 6b38y1… ND      202109…    14     0     0     0     2 0-0   3 (1)     1     0\n 8 5b28f1… ND      202109…    16     0     0     0     2 0-30  3 (3)     1     0\n 9 6f38b3… ND      202109…    17     0     0     0     2 15-30 3 (4)     1     0\n10 5b1w@   ND      202109…    28     0     0     1     3 30-0  5 (3)     1     0\n# … with 53 more rows, 34 more variables: TBpt <lgl>, Svr <dbl>, Ret <dbl>,\n#   `1st` <chr>, `2nd` <chr>, Notes <chr>, `1stSV` <dbl>, `2ndSV` <dbl>,\n#   `1stIn` <dbl>, `2ndIn` <dbl>, isAce <lgl>, isUnret <lgl>,\n#   isRallyWinner <lgl>, isForced <lgl>, isUnforced <lgl>, isDouble <lgl>,\n#   PtWinner <dbl>, isSvrWinner <dbl>, rallyCount <dbl>, `Player 1` <chr>,\n#   `Player 2` <chr>, `Pl 1 hand` <chr>, `Pl 2 hand` <chr>, Gender <chr>,\n#   Date <dbl>, Tournament <chr>, Round <lgl>, Time <chr>, Court <chr>, …\n\n\nWe can then use mutate() with case_when() to create a variable corresponding to error type and then summarise() the error types made from the two players.\n\nmed_djok_df |> filter(str_detect(point, pattern = \"@\") == TRUE) |>\n  mutate(error_type = case_when(str_detect(point, pattern = \"d@\") ~ \"deep error\",\n                                   str_detect(point, pattern = \"w@\") ~ \"wide error\",\n            str_detect(point, pattern = \"n@\") ~ \"net error\")) |>\n  group_by(PtWinner, error_type) |>\n  summarise(n_errors = n())\n\n`summarise()` has grouped output by 'PtWinner'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 7 × 3\n# Groups:   PtWinner [2]\n  PtWinner error_type n_errors\n     <dbl> <chr>         <int>\n1        1 deep error        9\n2        1 net error         6\n3        1 wide error       10\n4        2 deep error       12\n5        2 net error        16\n6        2 wide error        9\n7        2 <NA>              1\n\n\nIn the output above, a PtWinner of 1 corresponds to points that Djokovic won (and therefore points where Medvedev made the unforced error) while a PtWinner of 2 corresponds to points that Medvedev won (and therefore points where Djokovic made the unforced error). So we see that, in this match, Djokovic had more unforced errors overall. Medvedev’s unforced errors tended to be deep or wide while the highest proportion of Djokovic’s unforced errors were balls that went into the net.\nWe will explore our original “service patterns” question in the exercises. To close out this section, we will just emphasize that we have done a very simple introduction into regexes. These can get very cumbersome, especially as the patterns you want to extract get more complicated. Consider the examples below.\n\nDetect which points are aces, which are coded in the variable as *. Regexes have “special characters, like \\, *, ., which, if present in the variable need to be”escaped” with a backslash. But, the backslash is a special character, so it needs to be escaped too: so we need two \\\\ in front of * to pull the points with a *.\n\n\nstr_detect(med_djok_df$point, pattern = \"\\\\*\")\n\n\n\nDetect which points start with a 4 using ^ to denote “at the beginning”:\n\n\nstr_detect(med_djok_df$point, pattern = \"^4\")\n\n\n\nDetect which points end with an @ using $ to denote “at the end” (this is safer than what we did in the code above, where we just assumed that @ did not appear anywhere else in the string except at the end).\n\n\nstr_detect(med_djok_df$point, pattern = \"@$\")\n\n * Extract all of the forehand shots that were hit with str_extract_all(). The regex here says to extract anything with an f followed by any number of digits before another non-digit symbol.\n\nstr_extract_all(med_djok_df$point, pattern = \"f[:digit:]+\")\n\nThe purpose of these examples is just to show that things can get complicated with strings. For the purposes of assessment in this course, you are only responsible for the relatively simple cases discussed earlier in the section and in the exercises.\n\n\n14.3.3 Exercises\n\nUse str_detect() and dplyr functions to create a variable for serve_location that is either \"wide\" if the point starts with a 4, \"body\" if the point starts with a 5, and \"down the center\" if the point starts with a 6.\nUse dplyr functions and the Serving variable to count the number of serve locations for each player. (i.e. for how many points did Medvedev hit a serve out wide?).\nUse dplyr functions, the Serving variable, and the isSrvWinner variable to find the proportion of points each player won for each of their serving locations (i.e. if Medvedev won 5 points while serving out wide and lost 3 points, the proportion would be 5 / 8 = 0.625).\n\nNote that the isSrvWinner variable is coded as a 1 if the serving player won the point and 0 if the serving player lost the point.\n\nThe letters v, z, i, and k denote volleys (of different types). Use str_detect() and dplyr functions to figure out the proportion of points where a volley was hit."
  },
  {
    "objectID": "12-stringr.html#chapexercise-12",
    "href": "12-stringr.html#chapexercise-12",
    "title": "14  Text Data with tidytext and stringr",
    "section": "14.4 Chapter Exercises",
    "text": "14.4 Chapter Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-12).\nThere are no Chapter Exercises."
  },
  {
    "objectID": "12-stringr.html#solutions-12",
    "href": "12-stringr.html#solutions-12",
    "title": "14  Text Data with tidytext and stringr",
    "section": "14.5 Exercise Solutions",
    "text": "14.5 Exercise Solutions\n\n14.5.1 Text Analysis S\nThere are no solutions.\n\n\n14.5.2 Basic Sentiment Analysis S\nThere are no solutions.\n\n\n14.5.3 Introduction to stringr S\nThere are no solutions.\n\n\n14.5.4 Chapter Exercises S\nThere are no solutions."
  },
  {
    "objectID": "12-stringr.html#rcode-12",
    "href": "12-stringr.html#rcode-12",
    "title": "14  Text Data with tidytext and stringr",
    "section": "14.6 Non-Exercise R Code",
    "text": "14.6 Non-Exercise R Code\n\nlibrary(tidyverse)\nlibrary(here)\nbeyonce <- read_csv(here(\"data/beyonce_lyrics.csv\"))\nhead(beyonce)\nbeyonce$line[1:4]\nlibrary(tidytext)\nbeyonce_unnest <- beyonce |> unnest_tokens(output = \"word\", input = \"line\")\nbeyonce_unnest\nbeyonce_unnest <- beyonce_unnest |> mutate(word = str_to_lower(word))\nbeyonce_unnest |> group_by(word) |>\n  summarise(n = n()) |>\n  arrange(desc(n))\nhead(stop_words)\nbeyonce_stop <- anti_join(beyonce_unnest, stop_words, by = c(\"word\" = \"word\"))\nbeyonce_sum <- beyonce_stop |> group_by(word) |>\n  summarise(n = n()) |>\n  arrange(desc(n)) |>\n  print(n = 25)\nbeyonce_sum\n## install.packages(\"wordcloud\")\nlibrary(wordcloud)\nbeyonce_small <- beyonce_sum |> filter(n > 50)\nwordcloud(beyonce_small$word, beyonce_small$n, \n          colors = brewer.pal(8, \"Dark2\"), scale = c(5, .2),\n          random.order = FALSE, random.color = FALSE)\nlibrary(here)\nlibrary(tidyverse)\nmed_djok_df <- read_csv(here(\"data/med_djok.csv\"))\nhead(med_djok_df)\nstr_detect(med_djok_df$point, pattern = \"f\")\nstr_detect(med_djok_df$point, pattern = \"d@\")\nsum(str_detect(med_djok_df$point, pattern = \"d@\"))\nsum(str_detect(med_djok_df$point, pattern = \"w@\"))\nsum(str_detect(med_djok_df$point, pattern = \"n@\"))\nmed_djok_df |> filter(str_detect(point, pattern = \"@\") == TRUE)\nmed_djok_df |> filter(str_detect(point, pattern = \"@\") == TRUE) |>\n  mutate(error_type = case_when(str_detect(point, pattern = \"d@\") ~ \"deep error\",\n                                   str_detect(point, pattern = \"w@\") ~ \"wide error\",\n            str_detect(point, pattern = \"n@\") ~ \"net error\")) |>\n  group_by(PtWinner, error_type) |>\n  summarise(n_errors = n())\nstr_detect(med_djok_df$point, pattern = \"\\\\*\")\nstr_detect(med_djok_df$point, pattern = \"^4\")\nstr_detect(med_djok_df$point, pattern = \"@$\")\nstr_extract_all(med_djok_df$point, pattern = \"f[:digit:]+\")"
  },
  {
    "objectID": "13-knn.html",
    "href": "13-knn.html",
    "title": "12  Predictive Modeling with knn",
    "section": "",
    "text": "Goals"
  },
  {
    "objectID": "13-knn.html#introduction-to-classification",
    "href": "13-knn.html#introduction-to-classification",
    "title": "12  Predictive Modeling with knn",
    "section": "12.1 Introduction to Classification",
    "text": "12.1 Introduction to Classification\nk-nearest neighbors (or knn) is an introductory supervised machine learning algorithm, most commonly used as a classification algorithm. Classification refers to prediction of a categorical response variable with two or more categories. For example, for a data set with SLU students, we might be interested in predicting whether or not each student graduates in four years (so the response has two categories: graduates in 4 years or doesn’t). We might want to classify this response based on various student characteristics like anticipated major, GPA, standardized test scores, etc. knn can also be used to predict a quantitative response, but we’ll focus on categorical responses throughout this section.\nIf you’ve had STAT 213, you might try to draw some parallels to knn and classification using logistic regression. Note, however, that logistic regression required the response to have two levels while knn can classify a response variable that has two or more levels.\nTo introduce this, we will be using pokemon_full.csv data. Pokemon have different Types: we will use Type as a categorical response that we are interested in predicting. For simplicity, we will only use Pokemon’s primary type and we will only use 4 different types:\n\nset.seed(1119)\nlibrary(tidyverse)\nlibrary(here)\npokemon <- read_csv(here(\"data/pokemon_full.csv\")) |>\n  filter(Type %in% c(\"Steel\", \"Dark\", \"Fire\", \"Ice\"))\n\nOur goal is to develop a k-nearest-neighbors model that is able to classify/predict Pokemon Type from a set of predictors, like Pokemon HP, Attack, Defense, etc.\n\n12.1.1 Training and Test Data\nIn order to develop our knn model (note that we still haven’t discussed what knn actually is yet!), we first need to discuss terms that applies to almost all predictive/classification modeling: training and test data. A training data set is a subset of the full data set used to fit various models. For the example below, the training data set will be just 15 observations for pedagogical purposes. More commonly, the training data set will contain 50%-80% of the observations in the full data set.\nA test data set consists of the remaining 20%-50% of the observations not in the training data set. A test data set is used to assess different the performances of various models that were fit using the training data set. Why do we need to do this division? Using the full data set for both training a model and testing that model is “cheating:” the model will perform better than it should because we are using each observation twice: once for fitting and once for testing. Having a separate test data set that wasn’t used to fit the model gives the model a more “fair” test, as these observations are supposed to be new data that the model hasn’t yet seen.\nThe following code uses that slice_sample() function to randomly select 15 observations to be in the training data set. anti_join() then makes a test data set without the 15 pokemon in the training data set.\n\ntrain_sample <- pokemon |>\n  slice_sample(n = 15)\ntest_sample <- anti_join(pokemon, train_sample)\n\ntrain_sample |> head()\n\n# A tibble: 6 × 14\n   ...1 Name      Type     HP Attack Defense Speed SpAtk SpDef Generat…¹ Legen…²\n  <dbl> <chr>     <chr> <dbl>  <dbl>   <dbl> <dbl> <dbl> <dbl>     <dbl> <lgl>  \n1   491 Darkrai   Dark     70     90      90   125   135    90         4 TRUE   \n2   136 Flareon   Fire     65    130      60    65    95   110         1 FALSE  \n3   571 Zoroark   Dark     60    105      60   105   120    60         5 FALSE  \n4   221 Piloswine Ice     100    100      80    50    60    60         2 FALSE  \n5   668 Pyroar    Fire     86     68      72   106   109    66         6 FALSE  \n6   262 Mightyena Dark     70     90      70    70    60    60         3 FALSE  \n# … with 3 more variables: height <dbl>, weight <dbl>, base_experience <dbl>,\n#   and abbreviated variable names ¹​Generation, ²​Legendary\n\ntest_sample |> head()\n\n# A tibble: 6 × 14\n   ...1 Name       Type     HP Attack Defense Speed SpAtk SpDef Genera…¹ Legen…²\n  <dbl> <chr>      <chr> <dbl>  <dbl>   <dbl> <dbl> <dbl> <dbl>    <dbl> <lgl>  \n1     4 Charmander Fire     39     52      43    65    60    50        1 FALSE  \n2     5 Charmeleon Fire     58     64      58    80    80    65        1 FALSE  \n3    37 Vulpix     Fire     38     41      40    65    50    65        1 FALSE  \n4    38 Ninetales  Fire     73     76      75   100    81   100        1 FALSE  \n5    58 Growlithe  Fire     55     70      45    60    70    50        1 FALSE  \n6    59 Arcanine   Fire     90    110      80    95   100    80        1 FALSE  \n# … with 3 more variables: height <dbl>, weight <dbl>, base_experience <dbl>,\n#   and abbreviated variable names ¹​Generation, ²​Legendary\n\n\nThe ideas of a training data set and test data set are pervasive in predictive and classification models, including models not related to knn. Note that we are going to do this method because it’s the simplest: if you wanted to take this a step further, you’d repeat the training and test process 5 or 10 times, using what’s known as k-fold cross-validation.\n\n\n12.1.2 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-13).\n\nExplain what anti_join() joins on when by isn’t specified and why not specifying a by argument works for this example."
  },
  {
    "objectID": "13-knn.html#knn-introduction",
    "href": "13-knn.html#knn-introduction",
    "title": "12  Predictive Modeling with knn",
    "section": "12.2 knn Introduction",
    "text": "12.2 knn Introduction\n\n12.2.1 knn with k = 1 and 1 Predictor\nSuppose that we have just those 15 pokemon in our training data set. We want to predict Type from just one predictor, Defense. Below is a plot that shows the defenses of the 15 pokemon in our training data set, and has points coloured by Type and with different shapes for Type.\n\nggplot(data = train_sample, aes(x = Defense, y = 1, colour = Type, shape = Type)) +\n  geom_point(size = 4) +  theme(axis.title.y=element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())\n\n\n\n\nWe see from the plot that Steel type Pokemon tend to have pretty high defense values. Now suppose that we want to predict the Type for one of the Pokemon in our test data set, Dialga. We know that Dialga has a Defense stat of 120: the plot below shows Dialga marked with a large black X.\n\ndialga <- test_sample |> slice(63)\nggplot(data = train_sample, aes(x = Defense, y = 1, colour = Type, shape = Type)) +\n  geom_point(size = 4) +  theme(axis.title.y=element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks.y=element_blank()) +\n  geom_point(data = dialga, colour = \"black\", shape = 4, size = 7)\n\n\n\n\nWhat would your prediction for Dialga be? Why? According to knn with k = 1, we would predict Dialga to be Fire type. k = 1 means that we are using the 1st nearest neighbor: in this case the point that is closest to Dialga is a green triangle, corresponding to a Fire type Pokemon.\n\n\n12.2.2 knn with k > 1 and One Predictor\nBut, we might not necessarily want to predict the response value based on the single nearest neighbor. Dialga is also near many purple plus signs: should those factor in at all? We can extend knn to different values for k. For example, \\(k = 3\\) looks at the 3 nearest neighbors, and assigns a prediction as the category that appears the most among those 3 nearest neighbors.\nUsing k = 3, what would the prediction for Dialga be? Why?\n\n\n12.2.3 knn with k > 1 and More Than One Predictor\nWe can increase the number of predictors in our knn model as well. We can generally include as many predictors as we would like, but visualizing becomes challenging with more than 2 predictors and nearly impossible with more than 3 predictors. For the case of two predictors, suppose that we want to use Defense and Speed as our predictors for Type. Dialga, the Pokemon we want to predict for, is again marked with a large black X.\n\nggplot(data = train_sample, aes(x = Defense, y = Speed, colour = Type, shape = Type)) +\n  geom_point(size = 3) +\n  geom_point(data = dialga, colour = \"black\", shape = 4, size = 5)\n\n\n\n\nFor \\(k = 1\\), we would predict the Dialga is Steel, as the closest point is the purple + sign in the top-left corner of the graph. For \\(k = 3\\), what Type would you predict for Dialga? For this question, it’s a little hard to tell which three points are closest to Dialga without computing the distances numerically, which is something we will let R do with the knn() function.\n\n\n12.2.4 Scaling Predictor Variables before Using knn\nIn general, we want to scale any quantitative predictors when using knn because it relies on distances between points in its predictions. This is easiest to see with an example. Suppose, in our Pokemon example, that we want to use height and weight as our predictors in the knn model. We just have 2 observations in our training data set: a Dark Type pokemon with a height of 15 centimeters and a weight of 505 pounds, and a Fire Type Pokemon with a height of 9 centimeters and a weight of 250 pounds.\n\ntrain_tiny <- train_sample |> slice(1:2)\nnewobs <- tibble(height = 15, weight = 350, Type = \"Unknown\")\nggplot(data = train_tiny, aes(x = height, y = weight, shape = Type)) +\n  geom_point(size = 5, aes(colour = Type)) + xlim(c(7, 17)) + ylim(c(200, 550)) +\n  geom_point(data = newobs, shape = 4, size = 10)\n\n\n\n\nOn the plot is also given a Pokemon in our test data set that we wish to predict the Type of, marked with a black X. Upon visual inspection, with k = 1, it looks like we would classify this pokemon as Dark. However, the units of weight and height are on very different scales. We will compute the actual distances in class to see if the conclusion from the calculation matches with the visual conclusion.\n\nTo get around this issue, it is customary to scale all quantitative predictors before applying knn. One method of doing this is applying\n\\[\nscaled_x = \\frac{x - min(x)}{max(x) - min(x)}\n\\]\nFor example, scaling weight for the 15 original pokemon:\n\ntrain_sample |> select(weight) |> head()\n\n# A tibble: 6 × 1\n  weight\n   <dbl>\n1    505\n2    250\n3    811\n4    558\n5    815\n6    370\n\n\nputs all weights between 0 and 1:\n\ntrain_sample |> mutate(weight_s = (weight - min(weight)) / \n                          (max(weight) - min(weight))) |>\n  select(weight_s) |>\n  head()\n\n# A tibble: 6 × 1\n  weight_s\n     <dbl>\n1   0.187 \n2   0.0835\n3   0.312 \n4   0.209 \n5   0.314 \n6   0.132 \n\n\nIf we do the same with height, then the variables will contribute more “equally” to the distance metric used in knn.\nThe code below scales all numeric variables in a data set, using the across() function. across() applies a transformation to every column in a data set that satisfies the condition given in the where argument.\n\n## ?across\nlibrary(pander)\ntrain_sample |>\n  mutate(across(where(is.numeric), ~ (.x - min(.x)) /\n                                 (max(.x) - min(.x)))) |>\n  slice(1:3)\n\n# A tibble: 3 × 14\n   ...1 Name    Type     HP Attack Defense Speed SpAtk SpDef Generation Legend…¹\n  <dbl> <chr>   <chr> <dbl>  <dbl>   <dbl> <dbl> <dbl> <dbl>      <dbl> <lgl>   \n1 0.720 Darkrai Dark  0.417  0.444     0.4 1     1     0.658        0.6 TRUE    \n2 0.193 Flareon Fire  0.333  0.889     0.1 0.368 0.619 0.921        0   FALSE   \n3 0.838 Zoroark Dark  0.25   0.611     0.1 0.789 0.857 0.263        0.8 FALSE   \n# … with 3 more variables: height <dbl>, weight <dbl>, base_experience <dbl>,\n#   and abbreviated variable name ¹​Legendary\n\n\n\n\n12.2.5 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-13).\n\n* Consider again the toy example with just two observations in the training data set and unscaled weight and height as predictors.\n\n\nggplot(data = train_tiny, aes(x = height, y = weight, shape = Type)) +\n  geom_point(size = 5, aes(colour = Type)) + xlim(c(7, 17)) +\n  ylim(c(200, 550)) +\n  geom_point(data = newobs, shape = 4, size = 10)\n\n\n\n\nThe actual (height, weight) coordinates of the Fire pokemon are (9, 250), the actual coordinates of the Dark pokemon are (15, 505), and the actual coordinates of the test pokemon are (15, 350). We mentioned that, visually, the pokemon looks “closer” to the Dark type pokemon. Verify that this is not actually the case by computing the actual distances numerically. Recall that the formula for the distance between two points at (\\(x1, y1\\)) and (\\(x2, y2\\)) is:\n\\[\n\\sqrt{(x_1 - x_2) ^ 2 + (y1 - y2) ^ 2}\n\\]\nYou’ll need to use this formula twice: once for the distance between the fire pokemon and the test pokemon and once for the distance between the dark pokemon and the test pokemon.\n\n\n\n\n* After scaling according to the formula in this section, the coordinates (height, weight) of the Fire pokemon are (0, 0) and the coordinates of the Dark pokemon are (1, 1). (Since there are only two observations, the formula doesn’t give any output between 0 and 1 for this tiny example). The scaled coordinates for the test pokemon are (1, 0.39). Verify that, after scaling, the test pokemon is “closer” to the Dark type pokemon by numerically computing distances.\n\n\n\n\n\nConsider again the example with 15 pokemon in the training data set and a single predictor, Defense.\n\n\nggplot(data = train_sample, aes(x = Defense, y = 1, colour = Type, shape = Type)) +\n  geom_point(size = 4) +  theme(axis.title.y=element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks.y=element_blank()) +\n  geom_point(data = dialga, colour = \"black\", shape = 4, size = 7)\n\n\n\n\nWith k = 2, there is a tie between Fire and Steel. Come up with a way in which you might break ties in a knn algorithm.\n\nExplain what knn would use as a prediction for all test observations if k equals the number of observations in the training data set.\nWhat are some advantages for making k smaller and what are some advantages for making k larger?"
  },
  {
    "objectID": "13-knn.html#choosing-predictors-and-k",
    "href": "13-knn.html#choosing-predictors-and-k",
    "title": "12  Predictive Modeling with knn",
    "section": "12.3 Choosing Predictors and k",
    "text": "12.3 Choosing Predictors and k\nWe now know how knn classifies observations in the test data set, but how do we choose which predictors should be used by the knn algorithm? And how do we choose the number of neighbors, k? We want to measure how “good” models with different predictors and different k’s do, but we first need to define what “good” means.\nMuch of the “choosing predictors” part will be trial and error by evaluating different models with a criterion that we will talk about in the next section. However, it is always helpful to explore the data set with graphics to get us to a good starting point. A scatterplot matrix is a useful exploratory tool. The following is a scatterplot matrix with the response variable, Type, and just three candidate predictors, HP, Attack, and Defense, created with the GGally (“g-g-ally”) package.\n\n## install.packages(\"GGally\")\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\n\nAttaching package: 'GGally'\n\n\nThe following object is masked from 'package:pander':\n\n    wrap\n\nggpairs(data = train_sample, columns = c(4, 5, 6, 3), \n        lower = list(combo = wrap(ggally_facethist, bins = 15)))\n\n\n\n\nThe lower argument changes the number of bins in the faceted histograms in the bottom row. You can mostly ignore this.\nThe columns argument is important: it allows you to specify which columns you want to look at. I prefer putting the response, Type (column 3) in the last slot.\nWe can examine this to see which variables seem to have a relationship with Type. Where would we want to look for this?\n\n12.3.1 The Confusion Matrix\nBut, we still need a metric to evaluate models with different predictors. One definition of a “good” model in the classification context is a model that has a high proportion of correct predictions in the test data set. This should make some intuitive sense, as we would hope that a “good” model correctly classifies most Dark pokemon as Dark, most Fire pokemon as Fire, etc.\nIn order to examine the performance of a particular model, we’ll create a confusion matrix that shows the results of the model’s classification on observations in the test data set. Note that in STAT 213, we didn’t call this a confusion matrix; we instead called this a classification table.\nThe following video explains confusion matrices in more detail and should also cement the ideas of training and test data. https://www.youtube.com/watch?v=Kdsp6soqA7o.\n\n\n12.3.2 Using knn in R\nTo make a confusion matrix for a model using the pokemon data set, we first need to obtain predictions from a model. We’ll use the class library to fit a knn model to the pokemon data. Note that, instead of having 15 Pokemon in our training data set, we now have 70 pokemon to give a more reasonable number. The test set has the remaining 50 pokemon.\nThe following code chunk sets a seed so that we all get the same training and test samples, scales all numeric variables in the pokemon data set, and then randomly selects 70 pokemon to be in the training sample.\n\nlibrary(tidyverse)\nset.seed(11232020) ## run this line so that you get the same \n## results as I do!\n\n## scale the quantitative predictors\npokemon_scaled <- pokemon |>\n    mutate(across(where(is.numeric), ~ (.x - min(.x)) /\n                                 (max(.x) - min(.x)))) \n\ntrain_sample_2 <- pokemon_scaled |>\n  slice_sample(n = 70)\ntest_sample_2 <- anti_join(pokemon_scaled, train_sample_2)\n\nJoining, by = c(\"...1\", \"Name\", \"Type\", \"HP\", \"Attack\", \"Defense\", \"Speed\",\n\"SpAtk\", \"SpDef\", \"Generation\", \"Legendary\", \"height\", \"weight\",\n\"base_experience\")\n\n\nThe first knn model we will investigate will have HP, Attack, Defense, and Speed as predictors. The class library can fit knn models with a knn() function but requires the training and test data sets to have only the predictors that we want to use to fit the model. The knn() function also requires the response variable, Type, to be given as a vector.\n\n## install.packages(\"class\")\nlibrary(class)\n\n## create a data frame that only has the predictors\n## that we will use\ntrain_small <- train_sample_2 |> select(HP, Attack, Defense, Speed)\ntest_small <- test_sample_2 |> select(HP, Attack, Defense, Speed)\n\n## put our response variable into a vector\ntrain_cat <- train_sample_2$Type\ntest_cat <- test_sample_2$Type\n\nNow that the data has been prepared for the knn() function in the class library, we fit the model with 9 nearest neighbors. The arguments to knn() are\n\ntrain, a data set with the training data that contains only the predictors we want to use (and not other predictors or the response).\ntest, a data set with the test data that contains only the predictors we want to use (and not other predictors or the response).\ncl, a vector of the response variable for the training data.\nk, the number of nearest neighbors.\n\n\n## fit the knn model with 9 nearest neighbors\nknn_mod <- knn(train = train_small, test = test_small,\n               cl = train_cat, k = 9)\nknn_mod\n\n [1] Ice   Fire  Fire  Fire  Fire  Fire  Steel Fire  Ice   Fire  Fire  Fire \n[13] Fire  Ice   Ice   Steel Ice   Dark  Ice   Fire  Steel Fire  Fire  Ice  \n[25] Fire  Ice   Steel Fire  Fire  Ice   Dark  Fire  Fire  Fire  Dark  Ice  \n[37] Ice   Fire  Ice   Fire  Fire  Fire  Fire  Fire  Fire  Fire  Fire  Fire \n[49] Ice   Fire \nLevels: Dark Fire Ice Steel\n\n\nThe output of knn_mod gives the predicted categories for the test sample. We can compare the predictions from the knn model with the actual pokemon Types in the test sample with table(), which makes a confusion matrix:\n\ntable(knn_mod, test_cat) \n\n       test_cat\nknn_mod Dark Fire Ice Steel\n  Dark     0    3   0     0\n  Fire     6   13   7     4\n  Ice      5    5   2     1\n  Steel    0    1   0     3\n\n\nThe columns of the confusion matrix give the actual Pokemon types in the test data while the rows give the predicted types from our knn model. The above table tells us that there were 0 pokemon that were Dark type that our knn model correctly classified as Dark. There were 6 pokemon that were Dark type that our knn model incorrectly classified as Fire. There were 5 pokemon that were Dark type and that our knn model incorrectly classified as Ice. In other words, correct predictions appear on the diagonal, while incorrect predictions appear on the off-diagonal.\nOne common metric used to assess overall model performance is the model’s classification rate, which is computed as the number of correct classifications divided by the total number of observations in the test data set. In this case, our classification rate is\n\n(0 + 13 + 2 + 3) / 50\n\n[1] 0.36\n\n\nCode to automatically obtain the classification rate from a confusion matrix is\n\ntab <- table(knn_mod, test_cat) \nsum(diag(tab)) / sum(tab)\n\n[1] 0.36\n\n\nWhat does diag() seem to do in the code above?\n\n\n12.3.3 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-13).\n\nChange the predictors used or change k to improve the classification rate of the model with k = 9 and Attack, Defense, HP, and Speed as predictors."
  },
  {
    "objectID": "13-knn.html#chapexercise-13",
    "href": "13-knn.html#chapexercise-13",
    "title": "12  Predictive Modeling with knn",
    "section": "12.4 Chapter Exercises",
    "text": "12.4 Chapter Exercises\nThere will be no chapter exercises for this section. Instead, we’ll devote some in-class time for you to begin work on your final project."
  },
  {
    "objectID": "13-knn.html#solutions-13",
    "href": "13-knn.html#solutions-13",
    "title": "12  Predictive Modeling with knn",
    "section": "12.5 Exercise Solutions",
    "text": "12.5 Exercise Solutions\n\n12.5.1 Introduction to Classification S\n\n\n12.5.2 knn Introduction S\n\n* Consider again the toy example with just two observations in the training data set and unscaled weight and height as predictors.\n\n\nggplot(data = train_tiny, aes(x = height, y = weight, shape = Type)) +\n  geom_point(size = 5, aes(colour = Type)) + xlim(c(7, 17)) +\n  ylim(c(200, 550)) +\n  geom_point(data = newobs, shape = 4, size = 10)\n\n\n\n\nThe actual (height, weight) coordinates of the Fire pokemon are (9, 250), the actual coordinates of the Dark pokemon are (15, 505), and the actual coordinates of the test pokemon are (15, 350). We mentioned that, visually, the pokemon looks “closer” to the Dark type pokemon. Verify that this is not the case by computing the actual distances numerically.\n\n\n\n\n* After scaling according to the formula in this section, the coordinates (height, weight) of the Fire pokemon are (0, 0) and the coordinates of the Dark pokemon are (1, 1). (Since there are only two observations, the formula doesn’t give any output between 0 and 1 for this tiny example). The scaled coordinates for the test pokemon are (1, 0.39). Verify that, after scaling, the test pokemon is “closer” to the Dark type pokemon bu numerically computing distances.\n\n\n\n\n\n\n12.5.3 Choosing Predictors and k S"
  },
  {
    "objectID": "13-knn.html#rcode-13",
    "href": "13-knn.html#rcode-13",
    "title": "12  Predictive Modeling with knn",
    "section": "12.6 Non-Exercise R Code",
    "text": "12.6 Non-Exercise R Code\n\nset.seed(1119)\nlibrary(tidyverse)\nlibrary(here)\npokemon <- read_csv(here(\"data/pokemon_full.csv\")) |>\n  filter(Type %in% c(\"Steel\", \"Dark\", \"Fire\", \"Ice\"))\ntrain_sample <- pokemon |>\n  slice_sample(n = 15)\ntest_sample <- anti_join(pokemon, train_sample)\n\ntrain_sample |> head()\ntest_sample |> head()\nggplot(data = train_sample, aes(x = Defense, y = 1, colour = Type, shape = Type)) +\n  geom_point(size = 4) +  theme(axis.title.y=element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())\ndialga <- test_sample |> slice(63)\nggplot(data = train_sample, aes(x = Defense, y = 1, colour = Type, shape = Type)) +\n  geom_point(size = 4) +  theme(axis.title.y=element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks.y=element_blank()) +\n  geom_point(data = dialga, colour = \"black\", shape = 4, size = 7)\nggplot(data = train_sample, aes(x = Defense, y = Speed, colour = Type, shape = Type)) +\n  geom_point(size = 3) +\n  geom_point(data = dialga, colour = \"black\", shape = 4, size = 5)\ntrain_tiny <- train_sample |> slice(1:2)\nnewobs <- tibble(height = 15, weight = 350, Type = \"Unknown\")\nggplot(data = train_tiny, aes(x = height, y = weight, shape = Type)) +\n  geom_point(size = 5, aes(colour = Type)) + xlim(c(7, 17)) + ylim(c(200, 550)) +\n  geom_point(data = newobs, shape = 4, size = 10)\ntrain_sample |> select(weight) |> head()\ntrain_sample |> mutate(weight_s = (weight - min(weight)) / \n                          (max(weight) - min(weight))) |>\n  select(weight_s) |>\n  head()\n## ?across\nlibrary(pander)\ntrain_sample |>\n  mutate(across(where(is.numeric), ~ (.x - min(.x)) /\n                                 (max(.x) - min(.x)))) |>\n  slice(1:3)\n## install.packages(\"GGally\")\nlibrary(GGally)\nggpairs(data = train_sample, columns = c(4, 5, 6, 3), \n        lower = list(combo = wrap(ggally_facethist, bins = 15)))\nlibrary(tidyverse)\nset.seed(11232020) ## run this line so that you get the same \n## results as I do!\n\n## scale the quantitative predictors\npokemon_scaled <- pokemon |>\n    mutate(across(where(is.numeric), ~ (.x - min(.x)) /\n                                 (max(.x) - min(.x)))) \n\ntrain_sample_2 <- pokemon_scaled |>\n  slice_sample(n = 70)\ntest_sample_2 <- anti_join(pokemon_scaled, train_sample_2)\n## install.packages(\"class\")\nlibrary(class)\n\n## create a data frame that only has the predictors\n## that we will use\ntrain_small <- train_sample_2 |> select(HP, Attack, Defense, Speed)\ntest_small <- test_sample_2 |> select(HP, Attack, Defense, Speed)\n\n## put our response variable into a vector\ntrain_cat <- train_sample_2$Type\ntest_cat <- test_sample_2$Type\n## fit the knn model with 9 nearest neighbors\nknn_mod <- knn(train = train_small, test = test_small,\n               cl = train_cat, k = 9)\nknn_mod\ntable(knn_mod, test_cat) \n(0 + 13 + 2 + 3) / 50\ntab <- table(knn_mod, test_cat) \nsum(diag(tab)) / sum(tab)"
  },
  {
    "objectID": "15-connections.html",
    "href": "15-connections.html",
    "title": "16  Connections to STAT 113, STAT 213, and CS 140",
    "section": "",
    "text": "In this section, we discuss how what we have learned in this data science course connects to some concepts that you learned about in STAT 113. As a quick refresher, a few concepts that you learned about in STAT 113 are:\n\nexploring data through numerical and graphical summaries. The connection to what we have been doing in this class is fairly straightforward: we’ve learned a lot about how to actually compute those numerical summaries and make appropriate graphics for potentially messy data.\nexplaining what sampling distributions are and how they relate to confidence intervals and hypothesis tests. This topic is probably the least connected to what we have learned so far in this class.\nconducting hypothesis tests and creating confidence intervals to answer questions of interest. We will focus on this third general objective in this section.\n\nThe example we will use is an experiment designed to assess the effects of race and sex on whether or not an employee received a callback for a job. In order to conduct the experiment, researchers randomly assigned names to resumes with each name associated with a particular race and gender, sent the resumes to employers, and recorded whether or not the resume received a callback. In addition to race, sex, and whether or not an employee received a callback, a few more variables were collected, like resume quality, whether or not the applicant had computer skills, years of experience, etc. A 1 for the received_callback indicates that the applicant received a callback.\nYou may recall this example from STAT 113: we used it to introduce a chi-square test of association. In that example and others like it, an appropriate graphic and summary statistics were provided. Here, we create these ourselves.\nThe data set is called resume in the openintro package: you’ll need to install this package with install.packages(\"openintro\"). Then, load in the data with\n\nlibrary(openintro)\n\nLoading required package: airports\n\n\nLoading required package: cherryblossom\n\n\nLoading required package: usdata\n\nresume\n\n# A tibble: 4,870 × 30\n   job_ad_id job_city job_indu…¹ job_t…² job_f…³ job_e…⁴ job_o…⁵ job_r…⁶ job_r…⁷\n       <dbl> <chr>    <chr>      <chr>     <dbl>   <dbl> <chr>     <dbl>   <dbl>\n 1       384 Chicago  manufactu… superv…      NA       1 unknown       1       0\n 2       384 Chicago  manufactu… superv…      NA       1 unknown       1       0\n 3       384 Chicago  manufactu… superv…      NA       1 unknown       1       0\n 4       384 Chicago  manufactu… superv…      NA       1 unknown       1       0\n 5       385 Chicago  other_ser… secret…       0       1 nonpro…       1       0\n 6       386 Chicago  wholesale… sales_…       0       1 private       0       0\n 7       386 Chicago  wholesale… sales_…       0       1 private       0       0\n 8       385 Chicago  other_ser… secret…       0       1 nonpro…       1       0\n 9       386 Chicago  wholesale… sales_…       0       1 private       0       0\n10       386 Chicago  wholesale… sales_…       0       1 private       0       0\n# … with 4,860 more rows, 21 more variables: job_req_education <dbl>,\n#   job_req_min_experience <chr>, job_req_computer <dbl>,\n#   job_req_organization <dbl>, job_req_school <chr>, received_callback <dbl>,\n#   firstname <chr>, race <chr>, gender <chr>, years_college <int>,\n#   college_degree <dbl>, honors <int>, worked_during_school <int>,\n#   years_experience <int>, computer_skills <int>, special_skills <int>,\n#   volunteer <int>, military <int>, employment_holes <int>, …\n\n\n\n\nOur goal is to assess whether there is evidence of racial discrimination in the study. In other words, are the variables race and received_callback associated?\nPrepare. Let’s start by writing the null and alternative hypotheses.\n\\(H_0:\\) There is no association between race and received_callback.\n\\(H_a:\\) There is an association between race and received_callback.\nNext, we can construct a summary graphic. One graphic to explore two categorical variables is a stacked bar plot.\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.2      ✔ forcats 0.5.1 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nresume_sum <- resume |> \n  mutate(received_callback = received_callback) |>\n           group_by(race, received_callback) |>\n  summarise(count = n())\n\n`summarise()` has grouped output by 'race'. You can override using the\n`.groups` argument.\n\nggplot(data = resume_sum, aes(x = race, y = count)) +\n  geom_col(aes(fill = received_callback)) +\n  scale_fill_viridis_c()\n\n\n\n\nWhat do you notice about the recieved_callback variable scale? How could we fix that?\n\nresume <- resume |>\n  mutate(received_callback = as.factor(received_callback))\nresume_sum <- resume |> \n           group_by(race, received_callback) |>\n  summarise(count = n())\n\n`summarise()` has grouped output by 'race'. You can override using the\n`.groups` argument.\n\nggplot(data = resume_sum, aes(x = race, y = count)) +\n  geom_col(aes(fill = received_callback)) +\n  scale_fill_viridis_d()\n\n\n\n\nWe might also want to generate a two-way table:\n\nresume |> group_by(race, received_callback) |>\n  summarise(count = n()) |>\n  pivot_wider(names_from = race,\n              values_from = count)\n\n`summarise()` has grouped output by 'race'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 2 × 3\n  received_callback black white\n  <fct>             <int> <int>\n1 0                  2278  2200\n2 1                   157   235\n\n\nCheck: The two assumptions for the test are independence of observations and that all expected counts are larger than 5. We don’t have time to discuss these in detail but we will assume that they are satisfied here.\nCalculate: We next want to calculate a p-value for the hypothesis test. The core tidyverse packages do not offer functionality for hypothesis testing. Instead, there are some functions in base R that perform the various tests. You may have used the lm() function in STAT 213 to perform hypothesis testing in the regression context. t.test() and chisq.test() are a couple of other functions that can perform a one and two-sample t-test (t.test()) or a chi-square goodness-of-fit test and chi-square test of association chisq.test(). The arguments to chisq.test() for a test of association are two vectors. Because the arguments are not data.frames, we need to specify the appropriate vectors directly with resume$race and resume$received_callback.\n\nchisq.test(x = resume$race, y = resume$received_callback)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  resume$race and resume$received_callback\nX-squared = 16.449, df = 1, p-value = 4.998e-05\n\n\nThe output of chisq.test() gives a p-value of 0.00004998 with a chi-square statistic of 16.449 and 1 degree of freedom.\nConclude. Finally, we write a conclusion in context of the problem.\nThere is strong evidence that race and callback are associated. The graph shows that white applicants receive a callback more often than black applicants do and the hypothesis test shows that this is statistically significant.\n\n\n\nIn addition to carrying out the steps of a statistical hypothesis test, we can also use the skills we have learned in this course to provide further information about the study. Some questions we might answer include:\n\nwhat is the distribution of job types job_type and job industries job_industry in the study?\ndo some of the first names (firstname) used have more bias than other first names?\nwhat other variables are associated with whether or not the applicant received a callback?\n\nTo answer the question about the distribution of job types and job industries used in the study, we can make a simple bar plot:\n\nggplot(data = resume, aes(x = fct_rev(fct_infreq(job_type)))) +\n  geom_bar() +\n  coord_flip() +\n  labs(x = \"Job Type\")\n\n\n\n\nIn the code, fct_infreq() orders the levels of job_type from the highest count/frequency to the lowest. fct_rev() reverses the order so that, on the resulting bar plot, the level with the highest count appears first.\n\nggplot(data = resume, aes(x = fct_rev(fct_infreq(job_industry)))) +\n  geom_bar() +\n  coord_flip() +\n  labs(x = \"Job Industry\")\n\n\n\n\nTo answer the question about whether some first names are more biased than others, we might make a graph of the proportion of resumes that received a callback for each first name.\n\nresume_firstname <- resume |>\n  group_by(firstname) |>\n  summarise(propcallback = mean(received_callback == \"1\"),\n            gender = unique(gender),\n            race = unique(race)) |>\n  arrange(desc(propcallback)) |>\n  unite(\"gender_race\", c(gender, race))\n\nggplot(data = resume_firstname, aes(x = gender_race, y = propcallback)) +\n  geom_point()\n\n\n\n\nWe can the label the name with the lowest callback rate and the name with the highest callback rate.\n\nlibrary(ggrepel)\nlabel_df <- resume_firstname |> \n  filter(propcallback == max(propcallback) |\n           propcallback == min(propcallback))\n\nggplot(data = resume_firstname, aes(x = gender_race, y = propcallback)) +\n  geom_point() +\n  geom_label_repel(data = label_df, aes(label = firstname))\n\n\n\n\n\n\n\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-15).\n\nConstruct a graphic or make a table that explores whether one of the other variables in the data set is associated with whether the applicant receives a callback for the job. Other variables include gender, years_college, college_degree, honors, worked_during_school, years_experience, computer_skills, special_skills, volunteer, military, employment_holes, and resume_quality.\nConstruct a graphic or make a table that explores one of the other variables in the data set is associated with whether the applicant receives a callback for the job. If your variable in Exercise 1 was categorical, choose a quantitative variable for this exercise. If your variable in Exercise 1 was quantitative, choose a categorical variable for this exercise.\nFor the categorical variable that you chose, conduct a Chi-square test of association to see if there is statistical evidence that the variable is associated with received_callback. In your test, (a), write the null and alternative hypotheses, run the test in chisq.test() and make a note of whether or not you get a warning about assumptions for the test, and write a conclusion in context of the problem.\nWrite a short, one paragraph summary on your major take-aways from this section."
  },
  {
    "objectID": "15-connections.html#stat-213",
    "href": "15-connections.html#stat-213",
    "title": "16  Connections to STAT 113, STAT 213, and CS 140",
    "section": "16.2 STAT 213",
    "text": "16.2 STAT 213\nMuch of the same concepts in connecting STAT 113 to this course hold for connecting STAT 213 with this course. We can still use what we have learned to explore a data set, conduct a hypothesis test, and perform further analysis and exploration on the data set.\nFor this section, however, we will focus more on a tidy approach to modeling. In particular, we will use the broom package to return tibbles with model summary information that we can then use for further analysis, plotting, or presentation.\nWe will use the coffee_ratings data set, which contains observations on ratings of various coffees throughout the world. The data was obtained from the Github account (https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-07-07/readme.md).\nA description of each variable in the data set is given below.\n\ntotal_cup_points, the score of the coffee by a panel of experts (our response variable for this section)\nspecies, the species of the coffee bean (Arabica or Robusta)\naroma, aroma (smell) grade\nflavor, flavor grade\naftertaste, aftertaste grade\nacidity, acidity grade\nbody, body grade\nbalance, balance grade\nuniformity, uniformity grade\nclean_cup, clean cup grade\nsweetness, sweetness grade\nmoisture, moisture grade\ncategory_one_defects, count of category one defects\nquakers, quakers\ncategory_two_defects, the number of category two defects\n\n\n16.2.1 broom Package Functions\nThe broom package consists of three primary functions: tidy(), glance(), and augment().\ntidy()\ntidy() is analagous to summary() for a linear model object. Let’s start by fitting a linear model with lm() with total_cup_points as the response and species, aroma, flavor, sweetness, and moisture as predictors.\nRead in the data, load the broom package (and install it with install.packages(\"broom\")), and fit the model with\n\nlibrary(broom)\nlibrary(here)\n\nhere() starts at /Users/highamm/Desktop/ds234_quarto\n\ncoffee_df <- read_csv(here(\"data/coffee_ratings.csv\"))\n\nRows: 1339 Columns: 43\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (24): species, owner, country_of_origin, farm_name, lot_number, mill, ic...\ndbl (19): total_cup_points, number_of_bags, aroma, flavor, aftertaste, acidi...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncoffee_mod <- lm(total_cup_points ~ species + aroma + flavor +\n                   sweetness + moisture,\n   data = coffee_df)\n\nIn STAT 213, you likely used summary() to look at the model output:\n\nsummary(coffee_mod)\n\n\nCall:\nlm(formula = total_cup_points ~ species + aroma + flavor + sweetness + \n    moisture, data = coffee_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.5132 -0.3705  0.0726  0.5610  5.5844 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     7.04039    0.77377   9.099  < 2e-16 ***\nspeciesRobusta  2.85365    0.26861  10.624  < 2e-16 ***\naroma           1.95188    0.14575  13.392  < 2e-16 ***\nflavor          5.09440    0.14042  36.281  < 2e-16 ***\nsweetness       2.23956    0.06553  34.173  < 2e-16 ***\nmoisture       -1.88033    0.67368  -2.791  0.00533 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.168 on 1333 degrees of freedom\nMultiple R-squared:  0.8891,    Adjusted R-squared:  0.8887 \nF-statistic:  2137 on 5 and 1333 DF,  p-value: < 2.2e-16\n\n\nHowever, there are a few inconveniences involving summary(). First, it’s just not that nice to look at: the output isn’t formatted in a way that is easy to look at. Second, it can be challenging to pull items from the summary output with code. For example, if you want to pull the p-value for moisture, you would need to write something like:\n\nsummary(coffee_mod)$coefficients[\"moisture\", 4]\n\n[1] 0.005327594\n\n\ntidy() is an alternative that puts the model coefficients, standard errors, t-stats, and p-values in a tidy tibble:\n\ntidy(coffee_mod)\n\n# A tibble: 6 × 5\n  term           estimate std.error statistic   p.value\n  <chr>             <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)        7.04    0.774       9.10 3.23e- 19\n2 speciesRobusta     2.85    0.269      10.6  2.31e- 25\n3 aroma              1.95    0.146      13.4  1.82e- 38\n4 flavor             5.09    0.140      36.3  4.73e-201\n5 sweetness          2.24    0.0655     34.2  2.41e-184\n6 moisture          -1.88    0.674      -2.79 5.33e-  3\n\n\nThe advantage of this format of output is that we can now use other tidyverse functions on the output. To pull the p-values,\n\ntidy(coffee_mod) |> select(p.value)\n\n# A tibble: 6 × 1\n    p.value\n      <dbl>\n1 3.23e- 19\n2 2.31e- 25\n3 1.82e- 38\n4 4.73e-201\n5 2.41e-184\n6 5.33e-  3\n\n\nor, to grab the output for a particular variable of interest:\n\ntidy(coffee_mod) |> filter(term == \"aroma\")\n\n# A tibble: 1 × 5\n  term  estimate std.error statistic  p.value\n  <chr>    <dbl>     <dbl>     <dbl>    <dbl>\n1 aroma     1.95     0.146      13.4 1.82e-38\n\n\nglance()\nglance() puts some model summary statistics into a tidy tibble. For example, if we run\n\nglance(coffee_mod)\n\n# A tibble: 1 × 12\n  r.squ…¹ adj.r…² sigma stati…³ p.value    df logLik   AIC   BIC devia…⁴ df.re…⁵\n    <dbl>   <dbl> <dbl>   <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>   <int>\n1   0.889   0.889  1.17   2137.       0     5 -2105. 4224. 4260.   1818.    1333\n# … with 1 more variable: nobs <int>, and abbreviated variable names\n#   ¹​r.squared, ²​adj.r.squared, ³​statistic, ⁴​deviance, ⁵​df.residual\n\n\nyou should notice a lot of statistics that you are familiar with from STAT 213, including r.squared, adj.r.squared, sigma (the residual standard error), statistic (the overall F-statistic), AIC, and BIC.\naugment()\naugment() is my personal favourite of the three. The function returns a tibble that contains all of the variables used to fit the model appended with commonly used diagnostic statistics like the fitted values (.fitted), cook’s distance (.cooksd), .hat values for leverage, and residuals (.resid).\n\naugment(coffee_mod)\n\n# A tibble: 1,339 × 12\n   total_c…¹ species aroma flavor sweet…² moist…³ .fitted  .resid    .hat .sigma\n       <dbl> <chr>   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>  <dbl>\n 1      90.6 Arabica  8.67   8.83   10       0.12    91.1 -0.537  0.0112    1.17\n 2      89.9 Arabica  8.75   8.67   10       0.12    90.5 -0.538  0.0103    1.17\n 3      89.8 Arabica  8.42   8.5    10       0       89.2  0.577  0.00745   1.17\n 4      89   Arabica  8.17   8.58   10       0.11    88.9  0.114  0.00784   1.17\n 5      88.8 Arabica  8.25   8.5    10       0.12    88.6  0.214  0.00669   1.17\n 6      88.8 Arabica  8.58   8.42   10       0.11    88.9 -0.0411 0.00726   1.17\n 7      88.8 Arabica  8.42   8.5    10       0.11    89.0 -0.216  0.00655   1.17\n 8      88.7 Arabica  8.25   8.33    9.33    0.03    86.4  2.25   0.00728   1.17\n 9      88.4 Arabica  8.67   8.67    9.33    0.03    89.0 -0.550  0.0122    1.17\n10      88.2 Arabica  8.08   8.58   10       0.1     88.7 -0.479  0.00826   1.17\n# … with 1,329 more rows, 2 more variables: .cooksd <dbl>, .std.resid <dbl>,\n#   and abbreviated variable names ¹​total_cup_points, ²​sweetness, ³​moisture\n\n\naugment() the data set makes it really easy to do things like:\n\nfilter() the data set to examine values with high cook’s distance that might be influential\n\n\naugment_df <- augment(coffee_mod)\naugment_df |> filter(.cooksd > 1)\n\n# A tibble: 1 × 12\n  total_cup_p…¹ species aroma flavor sweet…² moist…³ .fitted .resid  .hat .sigma\n          <dbl> <chr>   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>  <dbl> <dbl>  <dbl>\n1             0 Arabica     0      0       0    0.12    6.81  -6.81 0.430   1.14\n# … with 2 more variables: .cooksd <dbl>, .std.resid <dbl>, and abbreviated\n#   variable names ¹​total_cup_points, ²​sweetness, ³​moisture\n\n\nWe see right away that there is a potentially influential observation with 0 total_cup_points. Examining this variable further, we see that it is probably a data entry error that can be removed from the data.\n\nggplot(data = coffee_df, aes(x = total_cup_points)) +\n  geom_histogram(bins = 15, fill = \"white\", colour = \"black\")\n\n\n\n\nWe could also find observations with high leverage\n\naugment_df |> filter(.hat > 0.2)\n\n# A tibble: 2 × 12\n  total_cup_p…¹ species aroma flavor sweet…² moist…³ .fitted .resid  .hat .sigma\n          <dbl> <chr>   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>  <dbl> <dbl>  <dbl>\n1          59.8 Arabica   7.5   6.67    1.33    0.1    58.4    1.38 0.223   1.17\n2           0   Arabica   0     0       0       0.12    6.81  -6.81 0.430   1.14\n# … with 2 more variables: .cooksd <dbl>, .std.resid <dbl>, and abbreviated\n#   variable names ¹​total_cup_points, ²​sweetness, ³​moisture\n\n\nor observations that are outliers:\n\naugment_df |> filter(.std.resid > 3 | .std.resid < -3)\n\n# A tibble: 25 × 12\n   total_cu…¹ species aroma flavor sweet…² moist…³ .fitted .resid    .hat .sigma\n        <dbl> <chr>   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>  <dbl>   <dbl>  <dbl>\n 1       82.8 Arabica  8.08   8.17   10       0.12    86.6  -3.85 0.00359   1.16\n 2       82.4 Arabica  5.08   7.75   10       0.11    78.6   3.79 0.111     1.16\n 3       82.3 Arabica  7.75   8.08    6.67    0.11    78.1   4.27 0.0421    1.16\n 4       80.7 Arabica  7.67   7.5     6.67    0       75.2   5.51 0.0347    1.16\n 5       80   Arabica  7.58   7.75   10       0       83.7  -3.71 0.00388   1.16\n 6       79.9 Arabica  7.83   7.67   10       0       83.8  -3.87 0.00365   1.16\n 7       79.2 Arabica  7.17   7.42    6.67    0.1     73.6   5.58 0.0337    1.16\n 8       78.6 Arabica  7.92   7.58   10       0.1     83.3  -4.74 0.00232   1.16\n 9       78.3 Arabica  7.17   6.08   10       0.11    74.2   4.13 0.0199    1.16\n10       77.6 Arabica  7.58   7.67    6       0.12    74.1   3.46 0.0518    1.16\n# … with 15 more rows, 2 more variables: .cooksd <dbl>, .std.resid <dbl>, and\n#   abbreviated variable names ¹​total_cup_points, ²​sweetness, ³​moisture\n\n\nFinally, we can use our ggplot2 skills to construct plots like a residuals versus fitted values plot (filtering out the outlying observation first):\n\nggplot(data = augment_df |> filter(.fitted > 25), aes(x = .fitted, y = .resid)) +\n  geom_point() \n\n\n\n\n\n\n16.2.2 Exploring Concepts\nWe can also use our data science skills to explore concepts that you may or may not have found difficult in STAT 213. In this section, we will use plotting to help interpret coefficient estimates in models with two quantitative predictors: one with an interaction and one without an interaction.\nFirst, consider a model with flavor and moisture as predictors on the coffee data set that removes the observation with a total_cup_points rating of 0:\n\\(Y = \\beta_0 + \\beta_1 flavor + \\beta_2 aroma + \\epsilon\\)\n\ncoffee_noout <- coffee_df |> filter(total_cup_points > 0)\ncoffee_fa <- lm(total_cup_points ~ flavor + aroma,\n   data = coffee_noout)\ncoffee_fa |> tidy()\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)    29.8      1.01      29.6  1.79e-148\n2 flavor          5.55     0.177     31.4  1.33e-162\n3 aroma           1.39     0.191      7.27 6.11e- 13\n\n\nYou might recall that, you can interpret the estimate for \\(\\hat{\\beta}_1\\) as something like:\n“For a one point increase in flavor grade, we expect average coffee score to increase by 5.55 points, if aroma is held constant.”\n(or, if aroma does not change, or if aroma is fixed).\nBut, what if someone asked you, what does that mean, “if aroma is held constant.” Instead of using words to explain, we can use our ggplot2 skills to construct a plot. The plot that we are going to create is going to show the fitted model for 5 distinct values of aroma: the minimum value, Q1, the median, Q3, and the maximum (though these values can also be chosen arbitrarily). We will make use of the modelr package to gather predictions to make the lines in our plot. First, we are creating a grid of values for predictions:\n\n## install.packages(\"modelr\")\nlibrary(modelr)\n\n\nAttaching package: 'modelr'\n\n\nThe following object is masked from 'package:broom':\n\n    bootstrap\n\ngrid_vals <- coffee_noout |>\n  modelr::data_grid(\n    flavor = quantile(flavor),\n    aroma = quantile(aroma)\n  )\ngrid_vals\n\n# A tibble: 25 × 2\n   flavor aroma\n    <dbl> <dbl>\n 1   6.08  5.08\n 2   6.08  7.42\n 3   6.08  7.58\n 4   6.08  7.75\n 5   6.08  8.75\n 6   7.33  5.08\n 7   7.33  7.42\n 8   7.33  7.58\n 9   7.33  7.75\n10   7.33  8.75\n# … with 15 more rows\n\n\nWe then gather these predictions in a data set, and use that data set to make the lines in our plot. The plot will show total_cup_points on the y-axis, flavor on the x-axis, and have coloured lines for a few different values of aroma that we specified in the grid function:\n\ngrid <- grid_vals |>\n  modelr::gather_predictions(coffee_fa)\ngrid\n\n# A tibble: 25 × 4\n   model     flavor aroma  pred\n   <chr>      <dbl> <dbl> <dbl>\n 1 coffee_fa   6.08  5.08  70.7\n 2 coffee_fa   6.08  7.42  73.9\n 3 coffee_fa   6.08  7.58  74.1\n 4 coffee_fa   6.08  7.75  74.4\n 5 coffee_fa   6.08  8.75  75.8\n 6 coffee_fa   7.33  5.08  77.6\n 7 coffee_fa   7.33  7.42  80.9\n 8 coffee_fa   7.33  7.58  81.1\n 9 coffee_fa   7.33  7.75  81.3\n10 coffee_fa   7.33  8.75  82.7\n# … with 15 more rows\n\n\n\nggplot(data = coffee_noout, aes(x = flavor, y = total_cup_points)) +\n  geom_point(alpha = 0.2) +\n  geom_line(data = grid, aes(colour = fct_rev(factor(aroma)),\n                             y = pred), size = 1.2) +\n  scale_colour_viridis_d() +\n  labs(colour = \"Aroma\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nFrom this model, we might more easily be able to explain that slope interpretation earlier: for a fixed coloured line of aroma, if flavor increases by one point, we expect coffee rating to increase by 5.55 points, on average.\nBut, we can really see the power of modelr and plotting models if we fit more complicated models. For example, next, we will fit a model with an interaction between flavor and aroma. You may recall that an interaction allows the association between one predictor (flavor) and the response (total_cup_points) to change depending on the value of the other predictor (aroma).\n\\(Y = \\beta_0 + \\beta_1 flavor + \\beta_2 aroma + \\beta_3 flavor * aroma + \\epsilon\\)\n\ncoffee_fa_int <- lm(total_cup_points ~ flavor + aroma + flavor:aroma,\n   data = coffee_noout)\ncoffee_fa_int |> tidy()\n\n# A tibble: 4 × 5\n  term         estimate std.error statistic  p.value\n  <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    -38.5     13.4       -2.88 4.02e- 3\n2 flavor          14.6      1.77       8.24 4.06e-16\n3 aroma           10.5      1.79       5.88 5.26e- 9\n4 flavor:aroma    -1.21     0.235     -5.13 3.34e- 7\n\n\nYou may also recall that, in a model with an interaction between two quantitative variables, the coefficient estimates are not interpretable on their own. We cannot say something like “For a one point increase in flavor, we expect average coffee rating to increase by 14.6 points, as long as aroma and the interaction between flavor and aroma is held constant” because it does not make sense to talk about an increase in one unit of flavor while also holding the interaction fixed.\nSo, for interpreting the model output, we can again use the modelr package and out ggplot2 knowledge to construct a plot that shows the relationship between total_cup_points and flavor for a few different values of aroma:\n\ngrid <- coffee_noout |>\n  modelr::data_grid(\n    flavor = quantile(flavor),\n    aroma = quantile(aroma), \n  ) |>\n  modelr::gather_predictions(coffee_fa_int)\nggplot(data = coffee_noout, aes(x = flavor, y = total_cup_points)) +\n  geom_point(alpha = 0.2) +\n  geom_line(data = grid, aes(colour = fct_rev(factor(aroma)), y = pred), size = 1.2) +\n  scale_colour_viridis_d() +\n  labs(colour = \"Aroma\") +\n  theme_minimal()\n\n\n\n# grid <- coffee_noout |>\n#   modelr::data_grid(\n#     flavor = quantile(aroma),\n#     aroma = quantile(flavor), \n#   ) |>\n#   modelr::gather_predictions(coffee_fa_int)\n# coffee_noout <- coffee_noout |> mutate(flavor_cut = cut(flavor, breaks = 5))\n# ggplot(data = coffee_noout, aes(x = aroma, y = total_cup_points)) +\n#   geom_point(alpha = 0.2, aes(colour = flavor_cut)) +\n#   geom_smooth(aes(colour = flavor_cut), method = \"lm\", fullrange = TRUE, se = FALSE) +\n#   scale_colour_viridis_d() +\n#   theme_minimal()\n\nFrom the plot, we can understand the nature of the interaction a little better. It looks like, no matter what the value of aroma, flavor and total_cup_points have a positive association. But, for larger values of aroma, the slope is smaller while, for smaller values of aroma, the slope is larger. The model says that, when aroma is small, we would expect a larger increase in total_cup_points (on average) for a one point increase in flavor while, when aroma is large, we would expect a smaller increase in total_cup_points (on average) for a one point increase in flavor.\n\n\n16.2.3 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-15).\n\nAdd a couple of more predictors to the linear model that we fit earlier. Then, use glance() to obtain some model fit statistics. Which model is “better” according to some of the metrics you learned about in STAT 213?\nFor one of your fitted models, construct a histogram of the residuals to assess the normality assumption (using ggplot2 and augment()).\nMake a table of the 5 coffees that have the highest predicted coffee rating, according to one of your models.\nRe-examine the interaction model plot that had total_cup_points on the y-axis, flavor on the x-axis, and coloured lines for aroma. Change the plotso that, aroma is on the x-axis and the five coloured lines correspond to the quantiles of flavor. This allows us to explore how the association between total_cup_points and aroma changes for different values of flavor.\nWrite a short, one paragraph summary on your major take-aways from this section."
  },
  {
    "objectID": "15-connections.html#cs-140",
    "href": "15-connections.html#cs-140",
    "title": "16  Connections to STAT 113, STAT 213, and CS 140",
    "section": "16.3 CS 140",
    "text": "16.3 CS 140\nIn this section, we will repeat a couple of topics from CS 140, which is in Python, in R. In particular we will,\n\nwrite our own function. The syntax for doing so in R is very similar to Python.\nperform iteration to repeat a similar task multiple times.\n\nTo start, suppose that we are interested in scraping some hitting data on SLU’s baseball team from the web address https://saintsathletics.com/sports/baseball/stats/2022. After we have the hitting data, we want to create a statistic for each player’s weighted on-base-average (wOBA). Information on what the wOBA is can be found here: https://www.mlb.com/glossary/advanced-stats/weighted-on-base-average. Some of the following code was modified from a project completed by Jack Sylvia in a data visualization course.\nCode to do such a task is given in the following chunk.\n\nlibrary(tidyverse)\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nurl_SLU <- \"https://saintsathletics.com/sports/baseball/stats/2022\"\ntab_SLU <- read_html(url_SLU) |> html_nodes(\"table\")\nSLU_Hitting <- tab_SLU[[1]] |> html_table(fill = TRUE) |>\n  head(-2) |>\n  select(-23) |>\n  mutate(wOBA = (0.69 * BB + 0.72 * HBP + 0.89 * (H-`2B`-`3B`-`HR`) + 1.27 * `2B` + 1.62 * `3B` + 2.10 * HR) / (AB + BB + SF + HBP))\n\nWe can make sure that the statistic was calculated with:\n\nSLU_Hitting |> select(wOBA, everything()) |> arrange(desc(wOBA))\n\n# A tibble: 20 × 23\n    wOBA   `#` Player      AVG   OPS `GP-GS`    AB     R     H  `2B`  `3B`    HR\n   <dbl> <int> <chr>     <dbl> <dbl> <chr>   <int> <int> <int> <int> <int> <int>\n 1 0.514     7 \"Brinker… 0.556 1.16  5-1         9     3     5     0     0     0\n 2 0.497    25 \"Liberat… 0.379 1.16  25-19      66    19    25     8     0     4\n 3 0.46      1 \"Verrast… 0.5   1.1   4-0         2     1     1     0     0     0\n 4 0.452    13 \"Butler,… 0.325 1.08  35-35     126    31    41     9     5     7\n 5 0.433     6 \"Clark, … 0.367 1.02  29-19      79    24    29     5     2     3\n 6 0.425    11 \"Circell… 0.252 1.00  35-33     111    27    28     9     0    11\n 7 0.412     9 \"Burke, … 0.308 0.885 6-4        13     4     4     1     0     0\n 8 0.391    30 \"Watson,… 0.318 0.853 35-33     110    29    35     6     0     1\n 9 0.365    19 \"Delaney… 0.33  0.817 34-34     115    18    38     5     1     1\n10 0.358     8 \"Forgion… 0.244 0.799 27-27      86    22    21     4     0     4\n11 0.356     5 \"Desjard… 0.284 0.741 28-20      67    23    19     1     0     0\n12 0.347    37 \"Goretti… 0.268 0.771 32-32      97    18    26     6     3     0\n13 0.345     3 \"Haun, H… 0     0.5   11-0        2     1     0     0     0     0\n14 0.335    18 \"Federic… 0.275 0.733 25-7       40    13    11     3     0     0\n15 0.309    23 \"Connor,… 0.286 0.661 6-0         7     2     2     0     0     0\n16 0.294    22 \"Courtwr… 0.256 0.63  20-10      39     7    10     1     0     0\n17 0.292    20 \"Courtwr… 0.281 0.646 27-17      64     6    18     2     0     0\n18 0.285    41 \"Comerfo… 0.222 0.582 31-21      72    16    16     0     0     0\n19 0.154    24 \"Colange… 0.095 0.295 15-3       21     6     2     0     0     0\n20 0        36 \"Bolduc,… 0     0     2-0         2     0     0     0     0     0\n# … with 11 more variables: RBI <int>, TB <int>, `SLG%` <dbl>, BB <int>,\n#   HBP <int>, SO <int>, GDP <int>, `OB%` <dbl>, SF <int>, SH <int>,\n#   `SB-ATT` <chr>\n\n\n\n16.3.1 Functions\nNow, suppose that we might want to repeat the scraping and calculation of wOBA for other years at SLU or for other teams. We could, of course, obtain the new URL address and copy and paste the code that we used above, replacing the old URL address with the new one. This would be a reasonable thing to do if we only wanted to do this for one other url. But, if we wanted to do this for 10, 20, 50, 1000, urls, we might consider writing a function to scrape the data and calculate the wOBA.\nThe format of a function in R is:\n\nname_of_function <- function(argument1, argument2, ....) {\n  body_of_function ## performs various tasks with the arguments\n  \n  return(output) ## tells the function what to return\n}\n\nWe have used functions throughout the entire semester, but they have always been functions that others have defined and are imported into R through packages. As we expand our toolbox, we might encounter situations where we want to write our own specialized functions for performing tasks that are not covered by functions that others have written.\nBefore we get back to our example, let’s write a very simple function, called get_sum_squares, that computes the sum of squares from a numeric vector argument named x_vec. A sum of squares function would take each number in x_vec, square it, and then add the numbers up.\n\nget_sum_squares <- function(x_vec) {\n  \n  sum_of_squares <- sum(x_vec ^ 2)\n  \n  return(sum_of_squares)\n}\n\nNow, let’s test our function on the numeric vector c(2, 4, 1)\n\nget_sum_squares(x_vec = c(2, 4, 1))\n\n[1] 21\n\n\nNow, we will move back to our example. We want to write a function called get_hitting_data that takes a url_name, scrapes the data from that url, and calculates the wOBA from the variables that were scraped. Note that our function will only work on urls that contain a data table formatted with the various baseball statistics as column names.\nTo create this function, we can simply copy and paste the code above and replace the SLU url web address with the argument url_name in the body of the function.\n\nget_hitting_data <- function(url_name) {\n  \n  tab <- read_html(url_name) |> html_nodes(\"table\")\n  \n  hitting <- tab[[1]] |> html_table(fill = TRUE) |>\n    head(-2) |>\n    select(-23) |>\n    mutate(wOBA = (0.69 * BB + 0.72 * HBP + 0.89 *\n                     (H- `2B` - `3B` - `HR`) +\n                     1.27 * `2B` + 1.62 * `3B` + 2.10 * HR) / \n             (AB + BB + SF + HBP),\n           url_name = url_name)\n  \n  return(hitting)\n}\n\nWe can then test our function on the SLU url:\n\nget_hitting_data(url_name = \"https://saintsathletics.com/sports/baseball/stats/2022\")\n\n# A tibble: 20 × 24\n     `#` Player      AVG   OPS `GP-GS`    AB     R     H  `2B`  `3B`    HR   RBI\n   <int> <chr>     <dbl> <dbl> <chr>   <int> <int> <int> <int> <int> <int> <int>\n 1     6 \"Clark, … 0.367 1.02  29-19      79    24    29     5     2     3    23\n 2    19 \"Delaney… 0.33  0.817 34-34     115    18    38     5     1     1    24\n 3    13 \"Butler,… 0.325 1.08  35-35     126    31    41     9     5     7    36\n 4    30 \"Watson,… 0.318 0.853 35-33     110    29    35     6     0     1    20\n 5     5 \"Desjard… 0.284 0.741 28-20      67    23    19     1     0     0     9\n 6    20 \"Courtwr… 0.281 0.646 27-17      64     6    18     2     0     0    13\n 7    37 \"Goretti… 0.268 0.771 32-32      97    18    26     6     3     0    14\n 8    11 \"Circell… 0.252 1.00  35-33     111    27    28     9     0    11    35\n 9     8 \"Forgion… 0.244 0.799 27-27      86    22    21     4     0     4    17\n10    41 \"Comerfo… 0.222 0.582 31-21      72    16    16     0     0     0     6\n11     7 \"Brinker… 0.556 1.16  5-1         9     3     5     0     0     0     2\n12     1 \"Verrast… 0.5   1.1   4-0         2     1     1     0     0     0     1\n13    25 \"Liberat… 0.379 1.16  25-19      66    19    25     8     0     4    20\n14     9 \"Burke, … 0.308 0.885 6-4        13     4     4     1     0     0     2\n15    23 \"Connor,… 0.286 0.661 6-0         7     2     2     0     0     0     1\n16    18 \"Federic… 0.275 0.733 25-7       40    13    11     3     0     0     8\n17    22 \"Courtwr… 0.256 0.63  20-10      39     7    10     1     0     0     5\n18    24 \"Colange… 0.095 0.295 15-3       21     6     2     0     0     0     2\n19     3 \"Haun, H… 0     0.5   11-0        2     1     0     0     0     0     0\n20    36 \"Bolduc,… 0     0     2-0         2     0     0     0     0     0     0\n# … with 12 more variables: TB <int>, `SLG%` <dbl>, BB <int>, HBP <int>,\n#   SO <int>, GDP <int>, `OB%` <dbl>, SF <int>, SH <int>, `SB-ATT` <chr>,\n#   wOBA <dbl>, url_name <chr>\n\n\n\n\n16.3.2 Iteration\nNow suppose that we want to use our function to scrape the 2022 baseball statistics for all teams in the Liberty League. There are 10 teams in total. The websites for each team’s statistics as well as the school name is given in the tibble below:\n\nschool_df <- tibble(school_name = c(\"SLU\", \"Clarkson\", \"Rochester\", \"RIT\", \"Ithaca\", \"Skidmore\", \"RPI\", \"Union\", \"Bard\", \"Vassar\"),\n                    hitting_web_url = c(\"https://saintsathletics.com/sports/baseball/stats/2022\",\n                 \"https://clarksonathletics.com/sports/baseball/stats/2022\", \n                 \"https://uofrathletics.com/sports/baseball/stats/2022\",\n                 \"https://ritathletics.com/sports/baseball/stats/2022\",\n                 \"https://athletics.ithaca.edu/sports/baseball/stats/2022\",\n                 \"https://skidmoreathletics.com/sports/baseball/stats/2022\",\n                 \"https://rpiathletics.com/sports/baseball/stats/2022\",\n                 \"https://unionathletics.com/sports/baseball/stats/2022\",\n                 \"https://bardathletics.com/sports/baseball/stats/2022\",\n                 \"https://www.vassarathletics.com/sports/baseball/stats/2022\"))\nschool_df\n\nOne option we have to obtain the hitting statistics for all 10 teams and calculating the wOBA (assuming that the tables are structured the same way on each web page) would be to apply our function 10 times and then bind together the results. The first three applications of the function are shown below.\n\nget_hitting_data(url_name = \"https://saintsathletics.com/sports/baseball/stats/2022\")\nget_hitting_data(url_name = \"https://clarksonathletics.com/sports/baseball/stats/2022\")\nget_hitting_data(url_name = \"https://uofrathletics.com/sports/baseball/stats/2022\")\n\nFor just 10 teams, this approach is certainly doable but is a bit annoying. And, what if we wanted to do this type of calculation for a league with more teams, such as the MLB (Major League Baseball)? Or, for multiple years for each team?\nA better approach is to use iteration and write code to repeatedly scrape the data from each website and calculate the wOBA statistic with our function. In CS 140, the primary form of iteration you used was probably a for loop. for loops in R have very similar syntax to for loops in Python. However, in general, for loops are clunky, can take up a lot of lines of code, and can be difficult to read.\nFor this section, we will instead focus on a functional programming approach to iteration through the map() function family in purrr. purrr is part of the core tidyverse so the package gets loaded in with library(tidyverse). The map() function has two arguments: the first is a vector or a list and the second is a function. map() applies the function in the second argument to each element of the vector or list in the first argument. For example, consider applying the get_sum_squares function we wrote earlier to a list of vectors:\n\nnum_list <- list(vec1 = c(1, 4, 5), vec2 = c(9, 8, 3, 5), vec3 = 1)\nmap(num_list, get_sum_squares)\n\n$vec1\n[1] 42\n\n$vec2\n[1] 179\n\n$vec3\n[1] 1\n\n\nThe output is a list of sums of squares calculated with our get_sum_squares() function.\nTo apply the map() approach to iteration to our baseball web urls, we will create an object called url_vec that has the urls of each school.\n\nurl_vec <- school_df$hitting_web_url\n\nThen, we apply map() with the first argument being the url_vec and the second argument being the function get_hitting_data() that we wrote earlier.\n\nhitting_list <- map(url_vec, get_hitting_data)\nhitting_list\n\nScraping and performing the wOBA calculation will take a few seconds. The output is a list of 10 tibbles. The bind_rows() function that we used to stack rows of different data frames or tibbles can also be used to stack rows of data frames or tibbles given in a list. We apply the function to the scraped data and then add the name of the school to the data frame with a left_join():\n\nhitting_ll <- hitting_list |> bind_rows() |>\n  left_join(school_df, by = c(\"url_name\" = \"hitting_web_url\"))\nhitting_ll\n\n# A tibble: 213 × 25\n     `#` Player      AVG   OPS `GP-GS`    AB     R     H  `2B`  `3B`    HR   RBI\n   <int> <chr>     <dbl> <dbl> <chr>   <int> <int> <int> <int> <int> <int> <int>\n 1     6 \"Clark, … 0.367 1.02  29-19      79    24    29     5     2     3    23\n 2    19 \"Delaney… 0.33  0.817 34-34     115    18    38     5     1     1    24\n 3    13 \"Butler,… 0.325 1.08  35-35     126    31    41     9     5     7    36\n 4    30 \"Watson,… 0.318 0.853 35-33     110    29    35     6     0     1    20\n 5     5 \"Desjard… 0.284 0.741 28-20      67    23    19     1     0     0     9\n 6    20 \"Courtwr… 0.281 0.646 27-17      64     6    18     2     0     0    13\n 7    37 \"Goretti… 0.268 0.771 32-32      97    18    26     6     3     0    14\n 8    11 \"Circell… 0.252 1.00  35-33     111    27    28     9     0    11    35\n 9     8 \"Forgion… 0.244 0.799 27-27      86    22    21     4     0     4    17\n10    41 \"Comerfo… 0.222 0.582 31-21      72    16    16     0     0     0     6\n# … with 203 more rows, and 13 more variables: TB <int>, `SLG%` <dbl>,\n#   BB <int>, HBP <int>, SO <int>, GDP <int>, `OB%` <dbl>, SF <int>, SH <int>,\n#   `SB-ATT` <chr>, wOBA <dbl>, url_name <chr>, school_name <chr>\n\n\nWith this data set, we can now do things like figure out the top 3 hitters from each team, according to the wOBA metric:\n\nhitting_ll |> group_by(school_name) |>\n  arrange(desc(wOBA)) |>\n  slice(1:3) |>\n  select(Player, school_name, wOBA)\n\n# A tibble: 30 × 3\n# Groups:   school_name [10]\n   Player                                                          schoo…¹  wOBA\n   <chr>                                                           <chr>   <dbl>\n 1 \"Toby, Jared\\r\\n                                              … Bard    0.488\n 2 \"Dumper, Sam\\r\\n                                              … Bard    0.475\n 3 \"Myers, Jordan\\r\\n                                            … Bard    0.431\n 4 \"Cantor, Danny\\r\\n                                            … Clarks… 0.805\n 5 \"Price, Grant\\r\\n                                             … Clarks… 0.475\n 6 \"Doyle, Caleb\\r\\n                                             … Clarks… 0.473\n 7 \"Shirley, Buzz\\r\\n                                            … Ithaca  0.524\n 8 \"Fabbo, Louis\\r\\n                                             … Ithaca  0.465\n 9 \"Fabian, Matt\\r\\n                                             … Ithaca  0.428\n10 \"Blackall, Patrick \\r\\n                                       … RIT     0.399\n# … with 20 more rows, and abbreviated variable name ¹​school_name\n\n\nor find the players on each team with the most at bats AB:\n\nhitting_ll |> group_by(school_name) |>\n  arrange(desc(AB)) |>\n  slice(1:3) |>\n  select(Player, school_name, AB)\n\n# A tibble: 30 × 3\n# Groups:   school_name [10]\n   Player                                                          schoo…¹    AB\n   <chr>                                                           <chr>   <int>\n 1 \"Toby, Jared\\r\\n                                              … Bard      107\n 2 \"Myers, Jordan\\r\\n                                            … Bard      107\n 3 \"Luscher, Alex\\r\\n                                            … Bard      101\n 4 \"Brouillette, Colby\\r\\n                                       … Clarks…   127\n 5 \"Wilson, Kent\\r\\n                                             … Clarks…   126\n 6 \"Doyle, Caleb\\r\\n                                             … Clarks…   103\n 7 \"Pedersen, Connor\\r\\n                                         … Ithaca    207\n 8 \"Cutaia, Nicholas\\r\\n                                         … Ithaca    180\n 9 \"Merod, Gil\\r\\n                                               … Ithaca    169\n10 \"Reilly, Chris\\r\\n                                            … RIT       152\n# … with 20 more rows, and abbreviated variable name ¹​school_name\n\n\n\n\n16.3.3 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-15).\nThe code below scrapes data from a wikipedia page listing the billboard end of year “hot 100” songs for the year 2021\n\nlibrary(rvest)\nlibrary(tidyverse)\n\nyear_scrape <- 2021\nurl <- paste0(\"https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_\", year_scrape)\n\n## convert the html code into something R can read\nbillboard_tab <- read_html(url) |> html_nodes(\"table\")\n\n## grabs the tables\nbillboard_df <- billboard_tab[[1]] |> html_table() |>\n  mutate(year = year_scrape)\nbillboard_df\n\n# A tibble: 100 × 4\n     No. Title                                `Artist(s)`                   year\n   <int> <chr>                                <chr>                        <dbl>\n 1     1 \"\\\"Levitating\\\"\"                     Dua Lipa                      2021\n 2     2 \"\\\"Save Your Tears\\\"\"                The Weeknd and Ariana Grande  2021\n 3     3 \"\\\"Blinding Lights\\\"\"                The Weeknd                    2021\n 4     4 \"\\\"Mood\\\"\"                           24kGoldn featuring Iann Dior  2021\n 5     5 \"\\\"Good 4 U\\\"\"                       Olivia Rodrigo                2021\n 6     6 \"\\\"Kiss Me More\\\"\"                   Doja Cat featuring SZA        2021\n 7     7 \"\\\"Leave the Door Open\\\"\"            Silk Sonic (Bruno Mars and …  2021\n 8     8 \"\\\"Drivers License\\\"\"                Olivia Rodrigo                2021\n 9     9 \"\\\"Montero (Call Me by Your Name)\\\"\" Lil Nas X                     2021\n10    10 \"\\\"Peaches\\\"\"                        Justin Bieber featuring Dan…  2021\n# … with 90 more rows\n\n\n\nWrap the code above in a function that scrapes data from Wikipedia for a user-provided year_scrape argument.\nCreate either a vector of the years 2014 through 2021 or a list of the years 2014 through 2021. Use your vector or list, along with the function you wrote in Exercise 1 and the map() function, to scrape data tables from each year.\nCombine the data frames you scraped in Exercise 2 with bind_rows() and use the combined data set to figure out which artist appears the highest number of times in the billboard hot 100 list within the years 2014 through 2021. To save time, once you have the data frames combined you should be able to do something like:\n\n\ncombined_df |> group_by(`Artist(s)`) |>\n  summarise(n_appear = n()) |>\n  arrange(desc(n_appear))\n\n\n* Note that your solution to Exercise 3 is likely imperfect because of songs that feature another musical artist. Why would such songs present a problem in counting the number of songs for each artist?\nWrite a short, one paragraph summary on your major take-aways from this section."
  },
  {
    "objectID": "15-connections.html#chapexercise-15",
    "href": "15-connections.html#chapexercise-15",
    "title": "16  Connections to STAT 113, STAT 213, and CS 140",
    "section": "16.4 Chapter Exercises",
    "text": "16.4 Chapter Exercises\nThere are no chapter exercises for this section on connections to STAT 113, STAT 213, and CS 140."
  },
  {
    "objectID": "15-connections.html#solutions-15",
    "href": "15-connections.html#solutions-15",
    "title": "16  Connections to STAT 113, STAT 213, and CS 140",
    "section": "16.5 Exercise Solutions",
    "text": "16.5 Exercise Solutions\n\n16.5.1 STAT 113 S\n\n\n16.5.2 STAT 213 S\n\n\n16.5.3 CS 140 S\n\n* Note that your solution to Exercise 3 is likely imperfect because of songs that feature another musical artist. Why would such songs present a problem in counting the number of songs for each artist?\n\nWhen we group_by() musical artist and use our counting function n(), the artist by themselves and the artist featuring the other musician would be counted separately in our table."
  },
  {
    "objectID": "15-connections.html#rcode-15",
    "href": "15-connections.html#rcode-15",
    "title": "16  Connections to STAT 113, STAT 213, and CS 140",
    "section": "16.6 Non-Exercise R Code",
    "text": "16.6 Non-Exercise R Code\n\nlibrary(openintro)\nresume\nlibrary(tidyverse)\nresume_sum <- resume |> \n  mutate(received_callback = received_callback) |>\n           group_by(race, received_callback) |>\n  summarise(count = n())\nggplot(data = resume_sum, aes(x = race, y = count)) +\n  geom_col(aes(fill = received_callback)) +\n  scale_fill_viridis_c()\nresume <- resume |>\n  mutate(received_callback = as.factor(received_callback))\nresume_sum <- resume |> \n           group_by(race, received_callback) |>\n  summarise(count = n())\nggplot(data = resume_sum, aes(x = race, y = count)) +\n  geom_col(aes(fill = received_callback)) +\n  scale_fill_viridis_d()\nresume |> group_by(race, received_callback) |>\n  summarise(count = n()) |>\n  pivot_wider(names_from = race,\n              values_from = count)\nchisq.test(x = resume$race, y = resume$received_callback)\nggplot(data = resume, aes(x = fct_rev(fct_infreq(job_type)))) +\n  geom_bar() +\n  coord_flip() +\n  labs(x = \"Job Type\")\nggplot(data = resume, aes(x = fct_rev(fct_infreq(job_industry)))) +\n  geom_bar() +\n  coord_flip() +\n  labs(x = \"Job Industry\")\nresume_firstname <- resume |>\n  group_by(firstname) |>\n  summarise(propcallback = mean(received_callback == \"1\"),\n            gender = unique(gender),\n            race = unique(race)) |>\n  arrange(desc(propcallback)) |>\n  unite(\"gender_race\", c(gender, race))\n\nggplot(data = resume_firstname, aes(x = gender_race, y = propcallback)) +\n  geom_point()\nlibrary(ggrepel)\nlabel_df <- resume_firstname |> \n  filter(propcallback == max(propcallback) |\n           propcallback == min(propcallback))\n\nggplot(data = resume_firstname, aes(x = gender_race, y = propcallback)) +\n  geom_point() +\n  geom_label_repel(data = label_df, aes(label = firstname))\nlibrary(broom)\nlibrary(here)\ncoffee_df <- read_csv(here(\"data/coffee_ratings.csv\"))\ncoffee_mod <- lm(total_cup_points ~ species + aroma + flavor +\n                   sweetness + moisture,\n   data = coffee_df)\nsummary(coffee_mod)\nsummary(coffee_mod)$coefficients[\"moisture\", 4]\ntidy(coffee_mod)\ntidy(coffee_mod) |> select(p.value)\ntidy(coffee_mod) |> filter(term == \"aroma\")\nglance(coffee_mod)\naugment(coffee_mod)\naugment_df <- augment(coffee_mod)\naugment_df |> filter(.cooksd > 1)\nggplot(data = coffee_df, aes(x = total_cup_points)) +\n  geom_histogram(bins = 15, fill = \"white\", colour = \"black\")\naugment_df |> filter(.hat > 0.2)\naugment_df |> filter(.std.resid > 3 | .std.resid < -3)\nggplot(data = augment_df |> filter(.fitted > 25), aes(x = .fitted, y = .resid)) +\n  geom_point() \ncoffee_noout <- coffee_df |> filter(total_cup_points > 0)\ncoffee_fa <- lm(total_cup_points ~ flavor + aroma,\n   data = coffee_noout)\ncoffee_fa |> tidy()\n## install.packages(\"modelr\")\nlibrary(modelr)\ngrid_vals <- coffee_noout |>\n  modelr::data_grid(\n    flavor = quantile(flavor),\n    aroma = quantile(aroma)\n  )\ngrid_vals\ngrid <- grid_vals |>\n  modelr::gather_predictions(coffee_fa)\ngrid\nggplot(data = coffee_noout, aes(x = flavor, y = total_cup_points)) +\n  geom_point(alpha = 0.2) +\n  geom_line(data = grid, aes(colour = fct_rev(factor(aroma)),\n                             y = pred), size = 1.2) +\n  scale_colour_viridis_d() +\n  labs(colour = \"Aroma\") +\n  theme_minimal()\ncoffee_fa_int <- lm(total_cup_points ~ flavor + aroma + flavor:aroma,\n   data = coffee_noout)\ncoffee_fa_int |> tidy()\ngrid <- coffee_noout |>\n  modelr::data_grid(\n    flavor = quantile(flavor),\n    aroma = quantile(aroma), \n  ) |>\n  modelr::gather_predictions(coffee_fa_int)\nggplot(data = coffee_noout, aes(x = flavor, y = total_cup_points)) +\n  geom_point(alpha = 0.2) +\n  geom_line(data = grid, aes(colour = fct_rev(factor(aroma)), y = pred), size = 1.2) +\n  scale_colour_viridis_d() +\n  labs(colour = \"Aroma\") +\n  theme_minimal()\n\n# grid <- coffee_noout |>\n#   modelr::data_grid(\n#     flavor = quantile(aroma),\n#     aroma = quantile(flavor), \n#   ) |>\n#   modelr::gather_predictions(coffee_fa_int)\n# coffee_noout <- coffee_noout |> mutate(flavor_cut = cut(flavor, breaks = 5))\n# ggplot(data = coffee_noout, aes(x = aroma, y = total_cup_points)) +\n#   geom_point(alpha = 0.2, aes(colour = flavor_cut)) +\n#   geom_smooth(aes(colour = flavor_cut), method = \"lm\", fullrange = TRUE, se = FALSE) +\n#   scale_colour_viridis_d() +\n#   theme_minimal()\nlibrary(tidyverse)\nlibrary(rvest)\n\nurl_SLU <- \"https://saintsathletics.com/sports/baseball/stats/2022\"\ntab_SLU <- read_html(url_SLU) |> html_nodes(\"table\")\nSLU_Hitting <- tab_SLU[[1]] |> html_table(fill = TRUE) |>\n  head(-2) |>\n  select(-23) |>\n  mutate(wOBA = (0.69 * BB + 0.72 * HBP + 0.89 * (H-`2B`-`3B`-`HR`) + 1.27 * `2B` + 1.62 * `3B` + 2.10 * HR) / (AB + BB + SF + HBP))\nSLU_Hitting |> select(wOBA, everything()) |> arrange(desc(wOBA))\nget_sum_squares <- function(x_vec) {\n  \n  sum_of_squares <- sum(x_vec ^ 2)\n  \n  return(sum_of_squares)\n}\nget_sum_squares(x_vec = c(2, 4, 1))\nget_hitting_data <- function(url_name) {\n  \n  tab <- read_html(url_name) |> html_nodes(\"table\")\n  \n  hitting <- tab[[1]] |> html_table(fill = TRUE) |>\n    head(-2) |>\n    select(-23) |>\n    mutate(wOBA = (0.69 * BB + 0.72 * HBP + 0.89 *\n                     (H- `2B` - `3B` - `HR`) +\n                     1.27 * `2B` + 1.62 * `3B` + 2.10 * HR) / \n             (AB + BB + SF + HBP),\n           url_name = url_name)\n  \n  return(hitting)\n}\nget_hitting_data(url_name = \"https://saintsathletics.com/sports/baseball/stats/2022\")\nschool_df <- tibble(school_name = c(\"SLU\", \"Clarkson\", \"Rochester\", \"RIT\", \"Ithaca\", \"Skidmore\", \"RPI\", \"Union\", \"Bard\", \"Vassar\"),\n                    hitting_web_url = c(\"https://saintsathletics.com/sports/baseball/stats/2022\",\n                 \"https://clarksonathletics.com/sports/baseball/stats/2022\", \n                 \"https://uofrathletics.com/sports/baseball/stats/2022\",\n                 \"https://ritathletics.com/sports/baseball/stats/2022\",\n                 \"https://athletics.ithaca.edu/sports/baseball/stats/2022\",\n                 \"https://skidmoreathletics.com/sports/baseball/stats/2022\",\n                 \"https://rpiathletics.com/sports/baseball/stats/2022\",\n                 \"https://unionathletics.com/sports/baseball/stats/2022\",\n                 \"https://bardathletics.com/sports/baseball/stats/2022\",\n                 \"https://www.vassarathletics.com/sports/baseball/stats/2022\"))\nschool_df\nget_hitting_data(url_name = \"https://saintsathletics.com/sports/baseball/stats/2022\")\nget_hitting_data(url_name = \"https://clarksonathletics.com/sports/baseball/stats/2022\")\nget_hitting_data(url_name = \"https://uofrathletics.com/sports/baseball/stats/2022\")\nnum_list <- list(vec1 = c(1, 4, 5), vec2 = c(9, 8, 3, 5), vec3 = 1)\nmap(num_list, get_sum_squares)\nurl_vec <- school_df$hitting_web_url\nhitting_list <- map(url_vec, get_hitting_data)\nhitting_list\nhitting_ll <- hitting_list |> bind_rows() |>\n  left_join(school_df, by = c(\"url_name\" = \"hitting_web_url\"))\nhitting_ll\nhitting_ll |> group_by(school_name) |>\n  arrange(desc(wOBA)) |>\n  slice(1:3) |>\n  select(Player, school_name, wOBA)\nhitting_ll |> group_by(school_name) |>\n  arrange(desc(AB)) |>\n  slice(1:3) |>\n  select(Player, school_name, AB)"
  },
  {
    "objectID": "16-sql.html",
    "href": "16-sql.html",
    "title": "14  Introduction to SQL with dbplyr",
    "section": "",
    "text": "Goals:\nAll of the dplyr functions we’ve used (both the ones from early in the semester and from the xxxx_join() family more recently) have corresponding components in SQL. SQL stands for Structured Query Language and is a very common language used with databases. Compared to dplyr, in general, SQL code is much harder to read, as SQL isn’t designed specifically for data analysis like dplyr is. In this section, we will introduce databases and give a brief introduction to SQL for analyzing data from a database."
  },
  {
    "objectID": "16-sql.html#what-is-a-database",
    "href": "16-sql.html#what-is-a-database",
    "title": "14  Introduction to SQL with dbplyr",
    "section": "14.1 What is a Database",
    "text": "14.1 What is a Database\nThe R for Data Science textbook defines a database as “a collection of data frames,” each called a database table. There a few key differences between a data frame (what we’ve been using the entire semester) and a database table. They are summarised from R for Data Science here as:\n\na database table can be larger and is stored on disk while a data frame is stored in memory so their size is more limited.\na database table usually indices while data frames do not.\nmany , but not all, data base tables are “row-oriented” while tidy data frames are “column-oriented.”\n\nDatabases are run through Database Management Systems. The R for Data Science textbook divides Database Management Systems into 3 types:\n\nclient-server like PostgreSQL and SQL Server\nCloud-based like Amazon’s Redshift\nIn-process like SQLite\n\nWe won’t really discuss these any further, but an advanced course in database systems through the CS department would give more information about Database Management Systems (and databases in general).\nHow to connect to a database from R depends on the type of database management system. There is an R package for most major Database Management Systems. For our purposes, because how to connect to a Database management system depends so heavily on the type, we will focus on a database management system that is contained in the R package duckdb.\nWe also need a database interface to connect to the database tables in duckdb in the DBI package.\nNote: This section on connecting to a database management systems may be confusing, particularly if you do not have a computer science background. But don’t let that derail your learning for this rest of this chapter, which will consist of primarily of R code from here on! The take-home message is that we need a way to connect to the system within R. It’s challenging to give specific directions because the connection depends on the type of system, so we are avoiding most of that by connecting to a database management system in the duckdb R package using functions from the DBI package.\nhttps://r4ds.hadley.nz/databases.html SQL is short from Structured Query Language. We first load in the duckdb and DBI libraries and make a connection to the database management system, which we will name con:\n\nlibrary(DBI)\nlibrary(duckdb)\ncon <- DBI::dbConnect(duckdb::duckdb())\n\nWe can type in con to see what it stores:\n\ncon\n\n<duckdb_connection b4000 driver=<duckdb_driver 9c690 dbdir=':memory:' read_only=FALSE>>\n\n\nWe’ve created a brand-new database, so we can next add some data tables with the duckdb_read_csv() function. Compared to read_csv() from the readr package, duckdb_read_csv() has a couple of extra arguments: a conn argument giving the database management connection and a name argument giving the name that we want to give to the data table:\n\nlibrary(here)\n\nhere() starts at /Users/highamm/Desktop/datascience_quarto\n\nduckdb_read_csv(conn = con, name = \"tennis2018\", \n                files = here(\"data/atp_matches_2018.csv\"))\nduckdb_read_csv(conn = con, name = \"tennis2019\", \n                files = here(\"data/atp_matches_2019.csv\"))\n\nThe doListTables() function lists the names of the data tables in the database we just created:\n\ndbListTables(con)\n\n[1] \"tennis2018\" \"tennis2019\"\n\n\nAnd, dbExistsTable() can be used to examine whether or not a data table exists in the current database:\n\ndbExistsTable(con, \"tennis2019\")\n\n[1] TRUE\n\ndbExistsTable(con, \"tennis2020\")\n\n[1] FALSE\n\n\nNote that, in many practical situations, the data tables will already exist in the database you are working with, so the step of duckdb_read_csv() would not be necessary.\nTo use raw SQL code and query the database that we just created, we can create a string of SQL code, name it sql, and pass it to the dbGetQuery() function. We also load in the tidyverse package here to use the as_tibble() function to convert the data.frame to a tibble.\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.2      ✔ forcats 0.5.1 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nsql <- \"\n  SELECT surface, winner_name, loser_name, w_ace, l_ace, minutes\n  FROM tennis2019 \n  WHERE minutes > 240\n\"\ndbGetQuery(con, sql)|>\n  as_tibble()\n\n# A tibble: 30 × 6\n   surface winner_name           loser_name            w_ace l_ace minutes\n   <chr>   <chr>                 <chr>                 <int> <int>   <int>\n 1 Hard    Joao Sousa            Guido Pella              19    18     241\n 2 Hard    Jeremy Chardy         Ugo Humbert              29    20     244\n 3 Hard    Roberto Bautista Agut Andy Murray               7    19     249\n 4 Hard    Joao Sousa            Philipp Kohlschreiber    28    20     258\n 5 Hard    Alex Bolt             Gilles Simon             11    14     244\n 6 Hard    Milos Raonic          Stan Wawrinka            39    28     241\n 7 Hard    Marin Cilic           Fernando Verdasco         8    27     258\n 8 Hard    Kei Nishikori         Pablo Carreno Busta      15     5     305\n 9 Hard    Frances Tiafoe        David Goffin              3    11     244\n10 Clay    Alexander Zverev      John Millman             17     0     248\n# … with 20 more rows\n\n\n\n14.1.1 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-16).\n\n* Though we do not know SQL code, we can probably figure out what the code above is doing. Which matches are being returned from our query?\nWhat is the dplyr equivalent function to WHERE in the SQL code above? What is the dplyr equivalent function to SELECT in the SQL code above?"
  },
  {
    "objectID": "16-sql.html#dbplyr-a-database-version-of-dplyr",
    "href": "16-sql.html#dbplyr-a-database-version-of-dplyr",
    "title": "14  Introduction to SQL with dbplyr",
    "section": "14.2 dbplyr: A Database Version of dplyr",
    "text": "14.2 dbplyr: A Database Version of dplyr\ndbplyr is a package that will allow us to continue to write dplyr-style code to query databases instead of writing native SQL, as in the code-chunk above.\nWe begin by loading in the package and creating a database table object with the tbl() function. In this case, we create a database table with the tennis2019 data and name it tennis_db:\n\nlibrary(dbplyr)\n\n\nAttaching package: 'dbplyr'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    ident, sql\n\ntennis_db <- tbl(con, \"tennis2019\")\ntennis_db\n\n# Source:   table<tennis2019> [?? x 49]\n# Database: DuckDB 0.3.5-dev1410 [root@Darwin 21.6.0:R 4.2.1/:memory:]\n   tourney_id tourney_…¹ surface draw_…² tourn…³ tourn…⁴ match…⁵ winne…⁶ winne…⁷\n   <chr>      <chr>      <chr>     <int> <chr>     <int>   <int>   <int> <chr>  \n 1 2019-M020  Brisbane   Hard         32 A        2.02e7     300  105453 2      \n 2 2019-M020  Brisbane   Hard         32 A        2.02e7     299  106421 4      \n 3 2019-M020  Brisbane   Hard         32 A        2.02e7     298  105453 2      \n 4 2019-M020  Brisbane   Hard         32 A        2.02e7     297  104542 <NA>   \n 5 2019-M020  Brisbane   Hard         32 A        2.02e7     296  106421 4      \n 6 2019-M020  Brisbane   Hard         32 A        2.02e7     295  104871 <NA>   \n 7 2019-M020  Brisbane   Hard         32 A        2.02e7     294  105453 2      \n 8 2019-M020  Brisbane   Hard         32 A        2.02e7     293  104542 <NA>   \n 9 2019-M020  Brisbane   Hard         32 A        2.02e7     292  200282 7      \n10 2019-M020  Brisbane   Hard         32 A        2.02e7     291  106421 4      \n# … with more rows, 40 more variables: winner_entry <chr>, winner_name <chr>,\n#   winner_hand <chr>, winner_ht <int>, winner_ioc <chr>, winner_age <dbl>,\n#   loser_id <int>, loser_seed <chr>, loser_entry <chr>, loser_name <chr>,\n#   loser_hand <chr>, loser_ht <int>, loser_ioc <chr>, loser_age <dbl>,\n#   score <chr>, best_of <int>, round <chr>, minutes <int>, w_ace <int>,\n#   w_df <int>, w_svpt <int>, w_1stIn <int>, w_1stWon <int>, w_2ndWon <int>,\n#   w_SvGms <int>, w_bpSaved <int>, w_bpFaced <int>, l_ace <int>, l_df <int>, …\n\n\nExamine the print for tennis_db, which should look similar to the print for a tibble or data.frame. Let’s use some dplyr code to obtain only the matches that lasted longer than 240 minutes and keep only a few of the columns. We will name the result tennis_query1:\n\ntennis_query1 <- tennis_db |> \n  filter(minutes > 240) |> \n  select(minutes, winner_name, loser_name, minutes, tourney_name)\ntennis_query1\n\n# Source:   SQL [?? x 4]\n# Database: DuckDB 0.3.5-dev1410 [root@Darwin 21.6.0:R 4.2.1/:memory:]\n   minutes winner_name           loser_name            tourney_name   \n     <int> <chr>                 <chr>                 <chr>          \n 1     241 Joao Sousa            Guido Pella           Australian Open\n 2     244 Jeremy Chardy         Ugo Humbert           Australian Open\n 3     249 Roberto Bautista Agut Andy Murray           Australian Open\n 4     258 Joao Sousa            Philipp Kohlschreiber Australian Open\n 5     244 Alex Bolt             Gilles Simon          Australian Open\n 6     241 Milos Raonic          Stan Wawrinka         Australian Open\n 7     258 Marin Cilic           Fernando Verdasco     Australian Open\n 8     305 Kei Nishikori         Pablo Carreno Busta   Australian Open\n 9     244 Frances Tiafoe        David Goffin          Miami Masters  \n10     248 Alexander Zverev      John Millman          Roland Garros  \n# … with more rows\n\n\nWe should note that the result is still a database object: it’s not our “usual” tibble. One major difference between the database object and the usual tibble is that our tennis_query1 does not tell us how many rows are in the data (see the ?? and the specification with more rows). The code that we wrote is not actually looking in the entire data set for matches that are longer than 240 minutes: it is saving time by only performing our query on part of the database table. This is very useful behaviour for database tables that are very, very large, where code might take a long time to run.\nIf we want to obtain the result of our query as a tibble, we can use the collect() function:\n\ntennis_query1 |>\n  collect()\n\n# A tibble: 30 × 4\n   minutes winner_name           loser_name            tourney_name   \n     <int> <chr>                 <chr>                 <chr>          \n 1     241 Joao Sousa            Guido Pella           Australian Open\n 2     244 Jeremy Chardy         Ugo Humbert           Australian Open\n 3     249 Roberto Bautista Agut Andy Murray           Australian Open\n 4     258 Joao Sousa            Philipp Kohlschreiber Australian Open\n 5     244 Alex Bolt             Gilles Simon          Australian Open\n 6     241 Milos Raonic          Stan Wawrinka         Australian Open\n 7     258 Marin Cilic           Fernando Verdasco     Australian Open\n 8     305 Kei Nishikori         Pablo Carreno Busta   Australian Open\n 9     244 Frances Tiafoe        David Goffin          Miami Masters  \n10     248 Alexander Zverev      John Millman          Roland Garros  \n# … with 20 more rows\n\n\nThe result is a tibble that we can now use any R functions on (not just functions from dplyr and a few other packages).\nThe show_query() function can be used on our tennis_query1 to give the SQL code that was executed:\n\ntennis_query1 |>\n  show_query()\n\n<SQL>\nSELECT \"minutes\", \"winner_name\", \"loser_name\", \"tourney_name\"\nFROM \"tennis2019\"\nWHERE (\"minutes\" > 240.0)\n\n\nTo get a better idea about what SQL code looks like, let’s make one more query with dplyr code and use the show_query() function to give the native SQL:\n\nmedvedev_query <- tennis_db |>\n  pivot_longer(c(winner_name, loser_name), names_to = \"win_loss\",\n               values_to = \"player\") |>\n  filter(player == \"Daniil Medvedev\") |>\n  group_by(win_loss) |>\n  summarise(win_loss_count = n())\nmedvedev_query\n\n# Source:   SQL [2 x 2]\n# Database: DuckDB 0.3.5-dev1410 [root@Darwin 21.6.0:R 4.2.1/:memory:]\n  win_loss    win_loss_count\n  <chr>                <dbl>\n1 winner_name             59\n2 loser_name              21\n\nshow_query(medvedev_query)\n\n<SQL>\nSELECT \"win_loss\", COUNT(*) AS \"win_loss_count\"\nFROM (\n  (\n    SELECT\n      \"tourney_id\",\n      \"tourney_name\",\n      \"surface\",\n      \"draw_size\",\n      \"tourney_level\",\n      \"tourney_date\",\n      \"match_num\",\n      \"winner_id\",\n      \"winner_seed\",\n      \"winner_entry\",\n      \"winner_hand\",\n      \"winner_ht\",\n      \"winner_ioc\",\n      \"winner_age\",\n      \"loser_id\",\n      \"loser_seed\",\n      \"loser_entry\",\n      \"loser_hand\",\n      \"loser_ht\",\n      \"loser_ioc\",\n      \"loser_age\",\n      \"score\",\n      \"best_of\",\n      \"round\",\n      \"minutes\",\n      \"w_ace\",\n      \"w_df\",\n      \"w_svpt\",\n      \"w_1stIn\",\n      \"w_1stWon\",\n      \"w_2ndWon\",\n      \"w_SvGms\",\n      \"w_bpSaved\",\n      \"w_bpFaced\",\n      \"l_ace\",\n      \"l_df\",\n      \"l_svpt\",\n      \"l_1stIn\",\n      \"l_1stWon\",\n      \"l_2ndWon\",\n      \"l_SvGms\",\n      \"l_bpSaved\",\n      \"l_bpFaced\",\n      \"winner_rank\",\n      \"winner_rank_points\",\n      \"loser_rank\",\n      \"loser_rank_points\",\n      'winner_name' AS \"win_loss\",\n      \"winner_name\" AS \"player\"\n    FROM \"tennis2019\"\n  )\n  UNION ALL\n  (\n    SELECT\n      \"tourney_id\",\n      \"tourney_name\",\n      \"surface\",\n      \"draw_size\",\n      \"tourney_level\",\n      \"tourney_date\",\n      \"match_num\",\n      \"winner_id\",\n      \"winner_seed\",\n      \"winner_entry\",\n      \"winner_hand\",\n      \"winner_ht\",\n      \"winner_ioc\",\n      \"winner_age\",\n      \"loser_id\",\n      \"loser_seed\",\n      \"loser_entry\",\n      \"loser_hand\",\n      \"loser_ht\",\n      \"loser_ioc\",\n      \"loser_age\",\n      \"score\",\n      \"best_of\",\n      \"round\",\n      \"minutes\",\n      \"w_ace\",\n      \"w_df\",\n      \"w_svpt\",\n      \"w_1stIn\",\n      \"w_1stWon\",\n      \"w_2ndWon\",\n      \"w_SvGms\",\n      \"w_bpSaved\",\n      \"w_bpFaced\",\n      \"l_ace\",\n      \"l_df\",\n      \"l_svpt\",\n      \"l_1stIn\",\n      \"l_1stWon\",\n      \"l_2ndWon\",\n      \"l_SvGms\",\n      \"l_bpSaved\",\n      \"l_bpFaced\",\n      \"winner_rank\",\n      \"winner_rank_points\",\n      \"loser_rank\",\n      \"loser_rank_points\",\n      'loser_name' AS \"win_loss\",\n      \"loser_name\" AS \"player\"\n    FROM \"tennis2019\"\n  )\n) \"q01\"\nWHERE (\"player\" = 'Daniil Medvedev')\nGROUP BY \"win_loss\"\n\n\nThe show_query() shows the native SQL code for a pivot: yikes! Remember that SQL was not designed for data analysis, so it doesn’t always look pretty. We’ll do one more simpler query:\n\nover20aces <- tennis_db |> filter(w_ace > 20) |>\n  select(w_ace, winner_name) |>\n  group_by(winner_name) |>\n  summarise(nmatch = n()) |>\n  arrange(desc(nmatch))\nover20aces\n\n# Source:     SQL [?? x 2]\n# Database:   DuckDB 0.3.5-dev1410 [root@Darwin 21.6.0:R 4.2.1/:memory:]\n# Ordered by: desc(nmatch)\n   winner_name        nmatch\n   <chr>               <dbl>\n 1 John Isner             15\n 2 Reilly Opelka          14\n 3 Milos Raonic           10\n 4 Sam Querrey             9\n 5 Nick Kyrgios            8\n 6 Alexander Bublik        7\n 7 Ivo Karlovic            6\n 8 Jan Lennard Struff      4\n 9 Jo-Wilfried Tsonga      4\n10 Alexander Zverev        4\n# … with more rows\n\nover20aces |> show_query()\n\n<SQL>\nSELECT \"winner_name\", COUNT(*) AS \"nmatch\"\nFROM (\n  SELECT \"w_ace\", \"winner_name\"\n  FROM \"tennis2019\"\n  WHERE (\"w_ace\" > 20.0)\n) \"q01\"\nGROUP BY \"winner_name\"\nORDER BY \"nmatch\" DESC\n\n\nCan you match some of the SQL code with the corresponding dplyr functions used?\n\n14.2.1 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-16).\n\n* Obtain the distribution of the surface variable by making a table of the total number of matches played on each surface in the 2019 season using dplyr functions on tennis_db. Then, use show_query() to show the corresponding SQL code.\nCreate a new variable that is the difference in the winner_rank_points and loser_rank_points using a dplyr function. Then, have your query return only the column you just created, the winner_name column, and the loser_name column. Use the show_query() function to show the corresponding SQL code.\nPerform a query of your choosing on tennis_db and use the show_query() function to show the corresponding SQL code."
  },
  {
    "objectID": "16-sql.html#sql",
    "href": "16-sql.html#sql",
    "title": "14  Introduction to SQL with dbplyr",
    "section": "14.3 SQL",
    "text": "14.3 SQL\nThe purpose of this section is to explore SQL syntax a little more, focusing on its connections to dplyr. Knowing dplyr is quite helpful in learning this SQL syntax because, while the syntax differs, the concepts are quite similar. Much of the text in this section is paraphrased from the R for Data Science textbook.\nThere are five core components of an SQL query. The two most basic are a SELECT statement (similar to select(), and, as discussed below, mutate() and summarise()) and a FROM statement (similar to the data argument). Using the show_query() function directly on tennis_db shows an SQL query that SELECTs all columns (denoted by the *), FROM the tennis2019 database.\n\ntennis_db |> show_query()\n\n<SQL>\nSELECT *\nFROM \"tennis2019\"\n\n\nThe WHERE and ORDER BY statements control which rows are returned (similar to filter()) and in what order those rows get returned (similar to arrange()):\n\ntennis_db |> filter(winner_hand == \"L\") |>\n  arrange(desc(tourney_date)) |>\n  show_query()\n\n<SQL>\nSELECT *\nFROM \"tennis2019\"\nWHERE (\"winner_hand\" = 'L')\nORDER BY \"tourney_date\" DESC\n\n\nFinally, GROUP BY is used for aggregation (similar to the dplyr group_by() and summarise() combination).\n\ntennis_db |>\n  group_by(winner_name) |>\n  summarise(meanace = mean(w_ace, na.rm = TRUE)) |>\n  show_query()\n\n<SQL>\nSELECT \"winner_name\", AVG(\"w_ace\") AS \"meanace\"\nFROM \"tennis2019\"\nGROUP BY \"winner_name\"\n\n\nIn the above code chunk, remove the na.rm = TRUE argument and run the query. What do you learn?\nThe SQL syntax must always follow the order SELECT, FROM, WHERE, GROUP BY, ORDER BY, even though the operations can be performed in a different order than what is specified. This is one aspect that makes SQL harder to pick up than something like dplyr, where we specify what we want done in the order that we want.\nBelow we give a little more detail about the 5 operations.\nSELECT: SELECT covers a lot of dplyr functions. In the code below, we explore how it is used in SQL to choose which columns get returned, rename columns, and create new variables:\n\nSELECT to choose which columns to return:\n\n\ntennis_db |> select(1:4) |> show_query()\n\n<SQL>\nSELECT \"tourney_id\", \"tourney_name\", \"surface\", \"draw_size\"\nFROM \"tennis2019\"\n\n\n\nSELECT to rename columns:\n\n\ntennis_db |> rename(tournament = tourney_name) |>\n  show_query()\n\n<SQL>\nSELECT\n  \"tourney_id\",\n  \"tourney_name\" AS \"tournament\",\n  \"surface\",\n  \"draw_size\",\n  \"tourney_level\",\n  \"tourney_date\",\n  \"match_num\",\n  \"winner_id\",\n  \"winner_seed\",\n  \"winner_entry\",\n  \"winner_name\",\n  \"winner_hand\",\n  \"winner_ht\",\n  \"winner_ioc\",\n  \"winner_age\",\n  \"loser_id\",\n  \"loser_seed\",\n  \"loser_entry\",\n  \"loser_name\",\n  \"loser_hand\",\n  \"loser_ht\",\n  \"loser_ioc\",\n  \"loser_age\",\n  \"score\",\n  \"best_of\",\n  \"round\",\n  \"minutes\",\n  \"w_ace\",\n  \"w_df\",\n  \"w_svpt\",\n  \"w_1stIn\",\n  \"w_1stWon\",\n  \"w_2ndWon\",\n  \"w_SvGms\",\n  \"w_bpSaved\",\n  \"w_bpFaced\",\n  \"l_ace\",\n  \"l_df\",\n  \"l_svpt\",\n  \"l_1stIn\",\n  \"l_1stWon\",\n  \"l_2ndWon\",\n  \"l_SvGms\",\n  \"l_bpSaved\",\n  \"l_bpFaced\",\n  \"winner_rank\",\n  \"winner_rank_points\",\n  \"loser_rank\",\n  \"loser_rank_points\"\nFROM \"tennis2019\"\n\n\n\nSELECT to create a new variable\n\n\ntennis_db |> mutate(prop_first_won = w_1stIn / w_1stWon) |>\n  select(prop_first_won, winner_name) |>\n  show_query()\n\n<SQL>\nSELECT \"w_1stIn\" / \"w_1stWon\" AS \"prop_first_won\", \"winner_name\"\nFROM \"tennis2019\"\n\n\n\nSELECT to create a new variable that is a summary:\n\n\ntennis_db |> summarise(mean_length = mean(minutes)) |>\n  show_query()\n\nWarning: Missing values are always removed in SQL aggregation functions.\nUse `na.rm = TRUE` to silence this warning\nThis warning is displayed once every 8 hours.\n\n\n<SQL>\nSELECT AVG(\"minutes\") AS \"mean_length\"\nFROM \"tennis2019\"\n\n\n\nGROUP BY: GROUP BY covers aggregation in a similar way as dplyr’s group_by() function:\n\ntennis_db |> group_by(winner_name) |>\n  summarise(meanlength = mean(minutes)) |>\n  show_query()\n\n<SQL>\nSELECT \"winner_name\", AVG(\"minutes\") AS \"meanlength\"\nFROM \"tennis2019\"\nGROUP BY \"winner_name\"\n\n\nWHERE: WHERE is used for filter(), though SQL uses different Boolean operators than R (for example, & becomes AND, | becomes or).\n\ntennis_db |> filter(winner_age > 35 | loser_age > 35) |>\n  show_query()\n\n<SQL>\nSELECT *\nFROM \"tennis2019\"\nWHERE (\"winner_age\" > 35.0 OR \"loser_age\" > 35.0)\n\n\nORDER BY: ORDER BY is used for arrange(). This one is quite straightforward:\n\ntennis_db |> arrange(desc(winner_rank_points)) |>\n  show_query()\n\n<SQL>\nSELECT *\nFROM \"tennis2019\"\nORDER BY \"winner_rank_points\" DESC\n\n\nSQL also has corresponding syntax for the xxxx_join() family of functions, but we do not have time to discuss this in detail. Note that we have really just scratched the surface of SQL. There are entire courses devoted to learning SQL syntax and more about databases in general. If you ever do find yourself in a situation where you need to learn SQL, either for a course or for a job, you should have a major head-start with your dplyr knowledge!\n\n14.3.1 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-16).\nIn much of this section, we have created code with dplyr and seen how that code translates to SQL. In these exercises, you will intead be given SQL code and asked to write dplyr code that achieves the same thing.\n\n* Examine the SQL code below and write equivalent dplyr code.\n\n\nSELECT * \nFROM \"tennis2019\"\nWHERE (\"tourney_name\" = 'Wimbledon')\n\n\nExamine the SQL code below and write equivalent dplyr code.\n\n\nSELECT \"winner_name\", \"loser_name\", \"w_ace\", \"l_ace\", \"w_ace\" - \"l_ace\" AS \"ace_diff\"\nFROM \"tennis2019\"\nORDER BY \"ace_diff\" DESC\n\n\nExamine the SQL code below and write equivalent dplyr code.\n\n\nSELECT \"tourney_name\", AVG(\"minutes\") AS \"mean_min\"\nFROM \"tennis2019\"\nGROUP BY \"tourney_name\""
  },
  {
    "objectID": "16-sql.html#chapexercise-16",
    "href": "16-sql.html#chapexercise-16",
    "title": "14  Introduction to SQL with dbplyr",
    "section": "14.4 Chapter Exercises",
    "text": "14.4 Chapter Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-16).\n\nRun the following code:\n\n\ntennis_db |> slice(1000:1005) |>\n  show_query()\n\nMake a hypothesis about why a function like slice() is not compatible with dbplyr.\n\n* Try to run a function from lubridate or forcats on tennis_db with mutate(). Does the function work? Did you expect it to work?\nRun the following code and write how the ! is translated to SQL.\n\n\ntennis_db |> filter(winner_name != \"Daniil Medvedev\") |>\n  show_query()\n\n<SQL>\nSELECT *\nFROM \"tennis2019\"\nWHERE (\"winner_name\" != 'Daniil Medvedev')\n\n\n\nRun the following code and write how the %in% symbol is translated to SQL.\n\n\ntennis_db |>\n  filter(winner_name %in% c(\"Daniil Medvedev\", \"Dominic Thiem\")) |>\n  show_query()\n\n<SQL>\nSELECT *\nFROM \"tennis2019\"\nWHERE (\"winner_name\" IN ('Daniil Medvedev', 'Dominic Thiem'))"
  },
  {
    "objectID": "16-sql.html#solutions-16",
    "href": "16-sql.html#solutions-16",
    "title": "14  Introduction to SQL with dbplyr",
    "section": "14.5 Exercise Solutions",
    "text": "14.5 Exercise Solutions\n\n14.5.1 What is a Database S\n\n* Though we do not know SQL code, we can probably figure out what the code above is doing. Which matches are being returned from our query?\n\nThe code is keeping any matches that are longer than 240 minutes. It is also getting rid of all of the columns except for those specified in SELECT.\n\n\n14.5.2 dbplyr: A Database Version of dplyr S\n\n* Obtain the distribution of the surface variable by making a table of the total number of matches played on each surface in the 2019 season using dplyr functions on tennis_db. Then, use show_query() to show the corresponding SQL code.\n\n\ntennis_db |> group_by(surface) |> summarise(nmatch = n())\n\n# Source:   SQL [3 x 2]\n# Database: DuckDB 0.3.5-dev1410 [root@Darwin 21.6.0:R 4.2.1/:memory:]\n  surface nmatch\n  <chr>    <dbl>\n1 Hard      1626\n2 Clay       828\n3 Grass      327\n\ntennis_db |> group_by(surface) |> summarise(nmatch = n()) |>\n  show_query()\n\n<SQL>\nSELECT \"surface\", COUNT(*) AS \"nmatch\"\nFROM \"tennis2019\"\nGROUP BY \"surface\"\n\n\n\n\n14.5.3 SQL S\n\n* Examine the SQL code below and write equivalent dplyr code.\n\n\nSELECT * \nFROM \"tennis2019\"\nWHERE (\"tourney_name\" = 'Wimbledon')\n\n\ntennis_db |>\n  filter(tourney_name == \"Wimbledon\")\n\n# Source:   SQL [?? x 49]\n# Database: DuckDB 0.3.5-dev1410 [root@Darwin 21.6.0:R 4.2.1/:memory:]\n   tourney_id tourney_…¹ surface draw_…² tourn…³ tourn…⁴ match…⁵ winne…⁶ winne…⁷\n   <chr>      <chr>      <chr>     <int> <chr>     <int>   <int>   <int> <chr>  \n 1 2019-540   Wimbledon  Grass       128 G        2.02e7     100  104925 1      \n 2 2019-540   Wimbledon  Grass       128 G        2.02e7     101  106045 <NA>   \n 3 2019-540   Wimbledon  Grass       128 G        2.02e7     102  104919 <NA>   \n 4 2019-540   Wimbledon  Grass       128 G        2.02e7     103  128034 <NA>   \n 5 2019-540   Wimbledon  Grass       128 G        2.02e7     104  200000 19     \n 6 2019-540   Wimbledon  Grass       128 G        2.02e7     105  144895 <NA>   \n 7 2019-540   Wimbledon  Grass       128 G        2.02e7     106  104719 <NA>   \n 8 2019-540   Wimbledon  Grass       128 G        2.02e7     107  200005 <NA>   \n 9 2019-540   Wimbledon  Grass       128 G        2.02e7     108  106421 11     \n10 2019-540   Wimbledon  Grass       128 G        2.02e7     109  200615 <NA>   \n# … with more rows, 40 more variables: winner_entry <chr>, winner_name <chr>,\n#   winner_hand <chr>, winner_ht <int>, winner_ioc <chr>, winner_age <dbl>,\n#   loser_id <int>, loser_seed <chr>, loser_entry <chr>, loser_name <chr>,\n#   loser_hand <chr>, loser_ht <int>, loser_ioc <chr>, loser_age <dbl>,\n#   score <chr>, best_of <int>, round <chr>, minutes <int>, w_ace <int>,\n#   w_df <int>, w_svpt <int>, w_1stIn <int>, w_1stWon <int>, w_2ndWon <int>,\n#   w_SvGms <int>, w_bpSaved <int>, w_bpFaced <int>, l_ace <int>, l_df <int>, …\n\n## check query:\ntennis_db |>\n  filter(tourney_name == \"Wimbledon\") |>\n  show_query()\n\n<SQL>\nSELECT *\nFROM \"tennis2019\"\nWHERE (\"tourney_name\" = 'Wimbledon')\n\n\n\n\n14.5.4 Chapter Exercises S\n\n* Try to run a function from lubridate or forcats on tennis_db with mutate(). Does the function work? Did you expect it to work?\n\n\ntennis_db |> mutate(tourney_name_reorder = fct_reorder(tourney_name, \n                                                       draw_size))\n\nThe result is an error. Only functions compatible with the dbplyr package can be used on a database table. Functions specific to R, like those in lubridate and forcats cannot work until we collect() the database table into a data frame or tibble:\n\ntennis_db |> collect() |>\n  mutate(tourney_name_reorder = fct_reorder(tourney_name, \n                                                       draw_size))\n\n# A tibble: 2,781 × 50\n   tourney_id tourney_…¹ surface draw_…² tourn…³ tourn…⁴ match…⁵ winne…⁶ winne…⁷\n   <chr>      <chr>      <chr>     <int> <chr>     <int>   <int>   <int> <chr>  \n 1 2019-M020  Brisbane   Hard         32 A        2.02e7     300  105453 2      \n 2 2019-M020  Brisbane   Hard         32 A        2.02e7     299  106421 4      \n 3 2019-M020  Brisbane   Hard         32 A        2.02e7     298  105453 2      \n 4 2019-M020  Brisbane   Hard         32 A        2.02e7     297  104542 <NA>   \n 5 2019-M020  Brisbane   Hard         32 A        2.02e7     296  106421 4      \n 6 2019-M020  Brisbane   Hard         32 A        2.02e7     295  104871 <NA>   \n 7 2019-M020  Brisbane   Hard         32 A        2.02e7     294  105453 2      \n 8 2019-M020  Brisbane   Hard         32 A        2.02e7     293  104542 <NA>   \n 9 2019-M020  Brisbane   Hard         32 A        2.02e7     292  200282 7      \n10 2019-M020  Brisbane   Hard         32 A        2.02e7     291  106421 4      \n# … with 2,771 more rows, 41 more variables: winner_entry <chr>,\n#   winner_name <chr>, winner_hand <chr>, winner_ht <int>, winner_ioc <chr>,\n#   winner_age <dbl>, loser_id <int>, loser_seed <chr>, loser_entry <chr>,\n#   loser_name <chr>, loser_hand <chr>, loser_ht <int>, loser_ioc <chr>,\n#   loser_age <dbl>, score <chr>, best_of <int>, round <chr>, minutes <int>,\n#   w_ace <int>, w_df <int>, w_svpt <int>, w_1stIn <int>, w_1stWon <int>,\n#   w_2ndWon <int>, w_SvGms <int>, w_bpSaved <int>, w_bpFaced <int>, …"
  },
  {
    "objectID": "16-sql.html#rcode-16",
    "href": "16-sql.html#rcode-16",
    "title": "14  Introduction to SQL with dbplyr",
    "section": "14.6 Non-Exercise R Code",
    "text": "14.6 Non-Exercise R Code\n\nlibrary(DBI)\nlibrary(duckdb)\ncon <- DBI::dbConnect(duckdb::duckdb())\ncon\nlibrary(here)\nduckdb_read_csv(conn = con, name = \"tennis2018\", \n                files = here(\"data/atp_matches_2018.csv\"))\nduckdb_read_csv(conn = con, name = \"tennis2019\", \n                files = here(\"data/atp_matches_2019.csv\"))\ndbListTables(con)\ndbExistsTable(con, \"tennis2019\")\ndbExistsTable(con, \"tennis2020\")\nlibrary(tidyverse)\n\nsql <- \"\n  SELECT surface, winner_name, loser_name, w_ace, l_ace, minutes\n  FROM tennis2019 \n  WHERE minutes > 240\n\"\ndbGetQuery(con, sql)|>\n  as_tibble()\nlibrary(dbplyr)\ntennis_db <- tbl(con, \"tennis2019\")\ntennis_db\ntennis_query1 <- tennis_db |> \n  filter(minutes > 240) |> \n  select(minutes, winner_name, loser_name, minutes, tourney_name)\ntennis_query1\ntennis_query1 |>\n  collect()\ntennis_query1 |>\n  show_query()\nmedvedev_query <- tennis_db |>\n  pivot_longer(c(winner_name, loser_name), names_to = \"win_loss\",\n               values_to = \"player\") |>\n  filter(player == \"Daniil Medvedev\") |>\n  group_by(win_loss) |>\n  summarise(win_loss_count = n())\nmedvedev_query\nshow_query(medvedev_query)\nover20aces <- tennis_db |> filter(w_ace > 20) |>\n  select(w_ace, winner_name) |>\n  group_by(winner_name) |>\n  summarise(nmatch = n()) |>\n  arrange(desc(nmatch))\nover20aces\n\nover20aces |> show_query()\ntennis_db |> show_query()\ntennis_db |> filter(winner_hand == \"L\") |>\n  arrange(desc(tourney_date)) |>\n  show_query()\ntennis_db |>\n  group_by(winner_name) |>\n  summarise(meanace = mean(w_ace, na.rm = TRUE)) |>\n  show_query()\ntennis_db |> select(1:4) |> show_query()\ntennis_db |> rename(tournament = tourney_name) |>\n  show_query()\ntennis_db |> mutate(prop_first_won = w_1stIn / w_1stWon) |>\n  select(prop_first_won, winner_name) |>\n  show_query()\ntennis_db |> summarise(mean_length = mean(minutes)) |>\n  show_query()\ntennis_db |> group_by(winner_name) |>\n  summarise(meanlength = mean(minutes)) |>\n  show_query()\ntennis_db |> filter(winner_age > 35 | loser_age > 35) |>\n  show_query()\ntennis_db |> arrange(desc(winner_rank_points)) |>\n  show_query()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DATA / STAT 234",
    "section": "",
    "text": "Welcome! This is the course materials website for DATA / STAT 234. Though we will use other sources at times, we will use materials on this site most heavily.\n\n\nInstructor Information\n\nProfessor: Matt Higham\nOffice: Bewkes 123\nEmail: mhigham@stlawu.edu\nSemester: Fall 2022\nSections:\n\nMW 2:30 - 4:00\n\nOffice Hours: 15 minute slots bookable at my calendly page.\n\nNote that you must book a time for office hours at least 12 hours in advance to guarantee that I am present and available at that time.\n\n\nCourse Materials\n\nSTAT 234 Materials Bundle. This will be our primary source of materials.\nTextbooks (only used as references):\n\nModern Data Science with R by Baumer, Kaplan, and Horton, found here in a free online version.\nR for Data Science by Grolemund and Wickham, found here in a free online version.\n\nComputer with Internet access.\n\n\n\n\n\n\nWelcome to STAT 234! The overall purpose of this course is learn the data science skills necessary to complete large-scale data analysis projects. The tool that we will be using to achieve this goal is the statistical software language R. We will work with a wide variety of interesting data sets throughout the semester to build our R skills. In particular, we will focus on the Data Analysis Life Cycle (Grolemund and Wickham 2020):\n\nWe will put more emphasis on the Import, Tidy, Transform, Visualize, and Communicate parts of the cycle, as an introduction to Modeling part is covered in STAT 213.\n\n\nWe will use the statistical software R to construct graphs and analyze data. A few notes:\n\nR and RStudio are both free to use.\nWe will primarily be using the SLU R Studio server at first: rstudio.stlawu.local:8787.\nAdditionally, we will be using RMarkdown for data analysis reports. Note: It’s always nice to start assignments and projects as early as possible, but this is particularly important to do for assignments and projects involving R. It’s no fun to try and figure out why code is not working at the last minute. If you start early enough though, you will have plenty of time to seek help and therefore won’t waste a lot of time on a coding error.\n\n\n\n\n\n\n\n\nImport data of a few different types into R for analysis.\nTidy data into a form that can be more easily visualized, summarised, and modeled.\nTransform, Wrangle, and Visualize variables in a data set to assess patterns in the data.\nCommunicate the results of your analysis to a target audience with a written report, or, possibly an oral presentation.\nPractice reproducible statistical practices through the use of Quarto for data analysis projects.\nExplain why it is ethically important to consider the context that a data set comes in.\nDevelop the necessary skills to be able to ask and answer future data analysis questions on your own, either using R or another program, such as Python.\n\nTo paraphrase the R for Data Science textbook, about 80% of the skills necessary to do a complete data analysis project can be learned through coursework in classes like this one. But, 20% of any particular project will involve learning new things that are specific to that project. Achieving Goal # 6 will allow you to learn this extra 20% on your own.\n\n\n\n\n\nThe components to your grade are described below:\n\nModules\n\nEach week, you will submit a 60-point Module, consisting of the following:\n\nan Exercise Set (10 points): Due Mondays and usually from our STAT 234 Materials Bundle. Exercises are graded for completion only and solutions are typically provided after you submit to Canvas. Collaboration is allowed.\na Take-Home Quiz (20 points): Due Wednesdays. Take-Home quizzes are graded for correctness. Collaboration is allowed.\nan In-Class Quiz (30 points): Given on Wednesdays at the start of class. In-Class quizzes are graded for correctness. Collaboration is not allowed. Some questions on the in-class quiz will be based on exercises we complete in class, the exercise sets you complete, and the take-home quiz questions.\n\nThere are 13 modules in total. In three modules, you will complete a 50-point Project instead of the two quizzes. The Project which will have some tasks for you to complete for a particular data set. The lowest module will be dropped from your grade so the total number of points available is 12 * 60 = 720 points.\nAdditionally, for one module, you are permitted to take the in-class quiz as a take-home and turn it in the following Monday. For this quiz, you must let me know that you are doing this before grades are posted, and you are not permitted to collaborate with anyone.\nFinally, if you choose to take the (optional) in-person final exam (described below), the score that you earn on that final will replace your second and third-lowest module scores.\n\nClass\n\nClass participation will be assessed three times throughout the semester in a 20 point rubric for a total of 60 points. The rubric used will be shared on the first day of class.\n\nFinal Project\n\nThere is one final project, worth 100 points. The primary purpose of the final project is to give you an opportunity to assemble topics throughout the course into one coherent data analysis. You will be able to choose the data set you use for your final project, so you might begin thinking about a particular topic or data set you are interested in exploring. The final project will be presented in a format to be decided later in the semester.\n\nFinal Exam\n\nThere is an optional Final Exam, worth 120 points total. You must be on campus for our final exam time to take the final exam.\nThere are two options for the final exam:\n\nOption 1: Skip the final exam and assign the average percentage of your 12 highest modules to be your percentage score for the 120 point final. For example, suppose your 13 module scores are: 60, 60, 59, 58, 57, 53, 53, 53, 53, 45, 43, 30, 0. Then, you would drop your lowest score (the 0) and your score for the final would be: (60 + 60 + 59 + 58 + 57 + 53 + 53 + 53 + 53 + 45 + 43 + 30 + 30) / 720 * 120 = 109 / 120 points.\nOption 2: Take the final exam. Your score will be used for the 120 points devoted to the final exam. Additionally, if your final exam score is better than your 2nd and 3rd lowest module grades, the will be replaced with your final exam score. For example, suppose your 13 module scores are: 60, 60, 59, 58, 57, 53, 53, 53, 53, 45, 43, 30, 0. You take the final exam and score a 110 / 120. Then, your score for the final is 110 / 120 points and your new module scores would be: 60, 60, 59, 58, 57, 53, 53, 53, 53, 45, 55, 55, 0. The 0 would still be dropped as your lowest module score.\n\n\n\n\n720 points for Modules\n60 points for Class Participation\n100 points for the Final Project\n120 points for the (optional) in-person Final Exam\n\nPoints add up to 1000 so your grade at the end of the semester will be the number of points you’ve earned across all categories divided by 1000.\n\n\n\nThe following is a rough grading scale. I reserve the right to make any changes to the scale if necessary.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrade\n4.0\n3.75\n3.5\n3.25\n3.0\n2.75\n2.5\n2.25\n2.0\n1.75\n1.5\n1.25\n1.0\n0.0\n\n\n\n\nPoints\n950-1000\n920-949\n890-919\n860-889\n830-859\n810-829\n770-809\n750-769\n720-749\n700-719\n670-699\n640-669\n600-639\n0-599\n\n\n\n\n\n\n\n\n\n\n\nCollaboration with your classmates on exercises, take-home quizzes, and projects is encouraged, but you must follow these guidelines:\n\nyou must state the name(s) of who you collaborated with at the top of each assessment.\nall work must be your own. This means that you should never send someone your code via email or let someone directly type code off of your screen. Instead, you can talk about strategies for solving problems and help or ask someone about a coding error.\nyou may use the Internet and StackExchange, but you also should not copy paste code directly from the website, without citing that you did so.\nthis isn’t a rule, but keep in mind that collaboration is not permitted on quizzes, exams, and very limited collaboration will be permitted on the final project. Therefore, when working with someone, make sure that you are both really learning so that you both can have success on the non-collaborative assessments.\n\n\n\n\n\n\nDiversity encompasses differences in age, colour, ethnicity, national origin, gender, physical or mental ability, religion, socioeconomic background, veteran status, sexual orientation, and marginalized groups. The interaction of different human characteristics brings about a positive learning environment. Diversity is both respected and valued in this classroom.\n\n\n\n\n\nIf you have a specific learning profile, medical and or mental health condition and need accommodations, please be sure to contact the Student Accessibility Services Office right away so they can help you get the accommodations you require. If you need to use any accommodations in this class, please meet with your instructor early and provide them with your Individualized Educational Accommodation Plan (IEAP) letter so you can have the best possible experience this semester.\nAlthough not required, your instructor would like to know of any accommodations that are needed at least 10 days before a quiz or test. Please be proactive and set up an appointment to meet with someone from the Student Accessibility Services Office.\nColor-Vision Deficiency: If you are Color-Vision Deficient, the Student Accessibility Services office has on loan glasses for students who are color vision deficient. Please contact the office to make an appointment.\nFor more specific information about setting up an appointment with Student Accessibility Services please see the listed options below:\n\nTelephone: 315.229.5537\nEmail: studentaccessibility@stlawu.edu\n\nFor further information about Student Accessibility Services you can check the website at: https://www.stlawu.edu/student-accessibility-services\n\n\n\n\n\nAcademic dishonesty will not be tolerated. Any specific policies for this course are supplementary to the\nHonor Code. According to the St. Lawrence University Academic Honor Policy,\n\nIt is assumed that all work is done by the student unless the instructor/mentor/employer gives specific permission for collaboration.\nCheating on examinations and tests consists of knowingly giving or using or attempting to use unauthorized assistance during examinations or tests.\nDishonesty in work outside of examinations and tests consists of handing in or presenting as original work which is not original, where originality is required.\n\nClaims of ignorance and academic or personal pressure are unacceptable as excuses for academic dishonesty. Students must learn what constitutes one’s own work and how the work of others must be acknowledged.\nFor more information, refer to www.stlawu.edu/acadaffairs/academic_honor_policy.pdf.\nTo avoid academic dishonesty, it is important that you follow all directions and collaboration rules and ask for clarification if you have any questions about what is acceptable for a particular assignment or exam. If I suspect academic dishonesty, a score of zero will be given for the entire assignment in which the academic dishonesty occurred for all individuals involved and Academic Honor Council will be notified. If a pattern of academic dishonesty is found to have occurred, a grade of 0.0 for the entire course can be given.\nIt is important to work in a way that maximizes your learning. Be aware that students who rely too much on others for the homework and projects tend to do poorly on the quizzes and exams.\nPlease note that in addition the above, any assignments in which your score is reduced due to academic dishonesty will not be dropped according to the quiz policy e.g., if you receive a zero on a quiz because of academic dishonesty, it will not be dropped from your grade.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopics\n\n\n\n\n0\n8/24\nIntroduction to R, R Studio\n\n\n1\n8/29\nGraphics with ggplot2\n\n\n2\n9/5\nData Wrangling and Transformation with dplyr\n\n\n3\n9/12\nCommunication with Quarto and ggplot2\n\n\n4\n9/19\nSoft Skills and Workflow\n\n\n5\n9/26*\nData Tidying with tidyr\n\n\n6\n10/3\nBase R\n\n\n\n\n\n\n\n7\n10/10\nFactors with forcats and Data Ethics\n\n\n8\n10/17*\nData Import with readr, jsonlite, rvest, and tibble\n\n\n9\n10/24\nData Merging with dplyr\n\n\n10\n10/31*\nDates and Times with lubridate\n\n\n11\n11/7\nText Data with tidytext and stringr\n\n\n12\n11/14\nIntro to Statistical/Machine Learning with knn\n\n\n\n\n\n\n\n13\n11/21\nThanksgiving Break\n\n\n14\n11/28\nConnections to STAT and CS\n\n\n14\n12/5\nDatabases and SQL with dbplyr\n\n\n\n\nThe three projects are tentatively scheduled to be due on September 28, October 19, and November 2, though these are subject to change."
  },
  {
    "objectID": "05-comm.html#reproducibility",
    "href": "05-comm.html#reproducibility",
    "title": "5  Communication with Quarto",
    "section": "5.1 Reproducibility",
    "text": "5.1 Reproducibility\nWe’ve been using Quarto for a while now, but have not yet talked about any of its features or how to do anything except insert a new code chunk. By the end of this section, we want to be able to use some of the Quarto options to make a nice-looking document (so that you can implement some of these options in your first mini-project).\nReproducibility is a concept that has recently gained popularity in the sciences for describing analyses that another researcher is able to repeat. That is, an analysis is reproducible if you provide enough information that the person sitting next to you can obtain identical results as long as they follow your procedures. An analysis is not reproducible if this isn’t the case. Quarto makes it easy for you to make your analysis reproducible for a couple of reasons:\n\nan Quarto file will not render unless all of your code runs, meaning that you won’t accidentally give someone code that doesn’t work.\nQuarto combines the “coding” steps with the “write-up” steps into one coherent document that contains the code, all figures and tables, and any explanations.\n\n\n5.1.1 R Scripts vs. Quarto\nWe’ve been using Quarto for the entirety of this course. But, you may have noticed that when you go to File -> New File to open a new Quarto Document file, there are a ton of other options. The first option is R Script. Go ahead and open a new R Script file now.\nThe file you open should be completely blank. An R Script is a file that reads only R code. It cannot have any text in it at all, unless that text is commented out with a #. For example, you could copy and paste all of the code that is inside a code chunk in a .qmd file to the .R file and run it line by line.\nSo, what are the advantages and disadvantages of using an R Script file compared to using an Quarto file? Let’s start with the advantages of Quarto. Quarto allows you to fully integrate text explanations of the code and results, the actual tables and figures themselves, and the code to make those tables and figures in one cohesive document. As we will see, if using R Scripts to write-up an analysis in Word, there is a lot of copy-pasting involved of results. For this reason, using Quarto often results in more reproducible analyses.\nThe advantage of an R Script would be in a situation where you really aren’t presenting results to anyone and you also don’t need any text explanations. This often occurs in two situations. (1) There are a lot of data preparation steps. In this case, you would typically complete all of these data prep steps in an R script and then write the resulting clean data to a .csv that you’d import in an Quarto file. (2) What you’re doing is complicated statistically. If this is the case, then the code is much more of a focus than the text or creating figures so you’d use an R Script.\nWe will “demo” a reproducible analysis in class.\n\n\n5.1.2 Spell-Checking\nIf using Quarto for communication, you probably want to utilize its spell-check feature. Go to Edit -> Check Spelling, and you’ll be presented with a spell-checker that lets you change the spelling of any words you may have misspelled.\n\n\n5.1.3 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-5).\n\nWhat’s the difference between R and Quarto?\n\n\n\n\n\nWhy is an Quarto analysis more reproducible than the base R script analysis?\n\n\n\n\n\nWhy is an Quarto analysis easier to make more reproducible than an analysis with Excel?\n\n\n\n\n\nYour friend Chaz is doing a data analysis project in Excel to compare the average GPA of student athletes with the average GPA of non-student athletes. He has two variables: whether or not a student is a student athlete and GPA. He decides that a two-sample t-test is an appropriate procedure for this data (recall from Intro Stat that this procedure is appropriate for comparing a quantitative response (GPA) across two groups). Here are the steps of his analysis.\n\n\nHe writes the null and alternative hypotheses in words and in statistical notation.\nHe uses Excel to make a set of side-by-side boxplots. He changes the labels and the limits on the y-axis using Point-and-Click Excel operations.\nFrom his boxplots, he see that there are 3 outliers in the non-athlete group. These three students have GPAs of 0 because they were suspended for repeatedly refusing to wear masks indoors. Chaz decides that these 3 students should be removed from the analysis because, if they had stayed enrolled, their GPAs would have been different than 0. He deletes these 3 rows in Excel.\nChaz uses the t.test function in Excel to run the test. He writes down the degrees of freedom, the T-stat, and the p-value.\nChaz copies his graph to Word and writes a conclusion in context of the problem.\n\nState 2 aspects of Chaz’s analysis that are not reproducible."
  },
  {
    "objectID": "11-lubridate.html#converting-variables-to-date",
    "href": "11-lubridate.html#converting-variables-to-date",
    "title": "13  Dates with lubridate",
    "section": "13.1 Converting Variables to <date>",
    "text": "13.1 Converting Variables to <date>\nThe lubridate package is built to easily work with Date objects and DateTime objects. R does not actually have a class that stores Time objects (unless you install a separate package). Dates tend to be much more common than Times, so, we will primarily focus on Dates, but most functions we will see have easy extensions to Times.\nTo begin, install the lubridate package, and load the package with library(). The today() function prints today’s date while now() prints today’s date and time. These can sometimes be useful in other contexts, but we will just run the code to see how R stores dates and date-times.\n\nlibrary(tidyverse)\nlibrary(lubridate)\ntoday()\n\n[1] \"2023-02-15\"\n\nnow()\n\n[1] \"2023-02-15 13:22:37 EST\"\n\n\nThis first section will deal with how to convert a variable in R to be a Date. We will use a data set that has the holidays of Animal Crossing from January to April. The columns in this data set are:\n\nHoliday, the name of the holiday and\nvarious other columns with different date formats\n\nRead in the data set with\n\nlibrary(here)\nholiday_df <- read_csv(here(\"data/animal_crossing_holidays.csv\"))\nholiday_df\n\n# A tibble: 6 × 10\n  Holiday         Date1     Date2     Date3 Date4 Date5 Month  Year   Day Month2\n  <chr>           <chr>     <chr>     <chr> <chr> <chr> <dbl> <dbl> <dbl> <chr> \n1 New Year's Day  1-Jan-20  Jan-1-20… 1/1/… 1/1/… 2020…     1  2020     1 Janua…\n2 Groundhog Day   2-Feb-20  Feb-2-20… 2/2/… 2/2/… 2020…     2  2020     2 Febru…\n3 Valentine's Day 14-Feb-20 Feb-14-2… 2/14… 2020… 2020…     2  2020    14 Febru…\n4 Shamrock Day    17-Mar-20 Mar-17-2… 3/17… 2020… 2020…     3  2020    17 March \n5 Bunny Day       12-Apr-20 Apr-12-2… 4/12… 12/4… 2020…     4  2020    12 April \n6 Earth Day       22-Apr-20 Apr-22-2… 4/22… 2020… 2020…     4  2020    22 April \n\n\nWhich columns were specified as Dates? In this example, none of the columns have the <date> specification: all of the date columns are read in as character variables.\n\n13.1.1 From <chr> to <date>\nWe will use the dmy() series of functions in lubridate to convert character variables to dates. We will typically pair this new function with a mutate() statement: much like the forcats functions, we are almost always creating a new variable.\nThere are a series of dmy()-type variables, each corresponding to a different Day-Month-Year order.\n\ndmy() is used to parse a date from a character vector that has the day first, month second, and year last.\nymd() is used to parse a date that has year first, month second, and date last\nydm() is used to parse a date that has year first, day second, and month last,….\n\nand dym(), mdy(), and myd() work similarly. lubridate is usually “smart” and picks up dates in all kinds of different formats (e.g. it can pick up specifying October as the month and Oct as the month and 10 as the month).\nLet’s try it out on Date1 and Date2:\n\nholiday_df |> mutate(Date_test = dmy(Date1)) |>\n  select(Date_test, everything())\n\n# A tibble: 6 × 11\n  Date_test  Holiday      Date1 Date2 Date3 Date4 Date5 Month  Year   Day Month2\n  <date>     <chr>        <chr> <chr> <chr> <chr> <chr> <dbl> <dbl> <dbl> <chr> \n1 2020-01-01 New Year's … 1-Ja… Jan-… 1/1/… 1/1/… 2020…     1  2020     1 Janua…\n2 2020-02-02 Groundhog D… 2-Fe… Feb-… 2/2/… 2/2/… 2020…     2  2020     2 Febru…\n3 2020-02-14 Valentine's… 14-F… Feb-… 2/14… 2020… 2020…     2  2020    14 Febru…\n4 2020-03-17 Shamrock Day 17-M… Mar-… 3/17… 2020… 2020…     3  2020    17 March \n5 2020-04-12 Bunny Day    12-A… Apr-… 4/12… 12/4… 2020…     4  2020    12 April \n6 2020-04-22 Earth Day    22-A… Apr-… 4/22… 2020… 2020…     4  2020    22 April \n\nholiday_df |> mutate(Date_test = mdy(Date2)) |>\n  select(Date_test, everything())\n\n# A tibble: 6 × 11\n  Date_test  Holiday      Date1 Date2 Date3 Date4 Date5 Month  Year   Day Month2\n  <date>     <chr>        <chr> <chr> <chr> <chr> <chr> <dbl> <dbl> <dbl> <chr> \n1 2020-01-01 New Year's … 1-Ja… Jan-… 1/1/… 1/1/… 2020…     1  2020     1 Janua…\n2 2020-02-02 Groundhog D… 2-Fe… Feb-… 2/2/… 2/2/… 2020…     2  2020     2 Febru…\n3 2020-02-14 Valentine's… 14-F… Feb-… 2/14… 2020… 2020…     2  2020    14 Febru…\n4 2020-03-17 Shamrock Day 17-M… Mar-… 3/17… 2020… 2020…     3  2020    17 March \n5 2020-04-12 Bunny Day    12-A… Apr-… 4/12… 12/4… 2020…     4  2020    12 April \n6 2020-04-22 Earth Day    22-A… Apr-… 4/22… 2020… 2020…     4  2020    22 April \n\n\nA Reminder: Why do <date> objects even matter? Compare the following two plots: one made where the date is in <chr> form and the other where date is in its appropriate <date> form.\n\nggplot(data = holiday_df, aes(x = Date1, y = Holiday)) +\n  geom_point()\n\n\n\nholiday_df <- holiday_df |> mutate(Date_test_plot = dmy(Date1)) |>\n  select(Date_test_plot, everything())\nggplot(data = holiday_df, aes(x = Date_test_plot, y = Holiday)) +\n  geom_point()\n\n\n\n\nIn which plot does the ordering on the x-axis make more sense?\n\n\n13.1.2 Making a <date> variable from Date Components\nAnother way to create a Date object is to assemble it with make_date() from a month, day, and year components, each stored in a separate column:\n\nholiday_df |> mutate(Date_test2 = make_date(year = Year,\n                                             month = Month,\n                                             day = Day)) |>\n  select(Date_test2, everything())\n\n# A tibble: 6 × 12\n  Date_test2 Date_test…¹ Holiday Date1 Date2 Date3 Date4 Date5 Month  Year   Day\n  <date>     <date>      <chr>   <chr> <chr> <chr> <chr> <chr> <dbl> <dbl> <dbl>\n1 2020-01-01 2020-01-01  New Ye… 1-Ja… Jan-… 1/1/… 1/1/… 2020…     1  2020     1\n2 2020-02-02 2020-02-02  Ground… 2-Fe… Feb-… 2/2/… 2/2/… 2020…     2  2020     2\n3 2020-02-14 2020-02-14  Valent… 14-F… Feb-… 2/14… 2020… 2020…     2  2020    14\n4 2020-03-17 2020-03-17  Shamro… 17-M… Mar-… 3/17… 2020… 2020…     3  2020    17\n5 2020-04-12 2020-04-12  Bunny … 12-A… Apr-… 4/12… 12/4… 2020…     4  2020    12\n6 2020-04-22 2020-04-22  Earth … 22-A… Apr-… 4/22… 2020… 2020…     4  2020    22\n# … with 1 more variable: Month2 <chr>, and abbreviated variable name\n#   ¹​Date_test_plot\n\n\nBut, when Month is stored as a character (e.g. February) instead of a number (e.g. 2), problems arise with the make_date() function:\n\nholiday_df |> mutate(Date_test2 = make_date(year = Year,\n                                             month = Month2,\n                                             day = Day)) |>\n  select(Date_test2, everything())\n\nWarning in make_date(year = Year, month = Month2, day = Day): NAs introduced by\ncoercion\n\n\n# A tibble: 6 × 12\n  Date_test2 Date_test…¹ Holiday Date1 Date2 Date3 Date4 Date5 Month  Year   Day\n  <date>     <date>      <chr>   <chr> <chr> <chr> <chr> <chr> <dbl> <dbl> <dbl>\n1 NA         2020-01-01  New Ye… 1-Ja… Jan-… 1/1/… 1/1/… 2020…     1  2020     1\n2 NA         2020-02-02  Ground… 2-Fe… Feb-… 2/2/… 2/2/… 2020…     2  2020     2\n3 NA         2020-02-14  Valent… 14-F… Feb-… 2/14… 2020… 2020…     2  2020    14\n4 NA         2020-03-17  Shamro… 17-M… Mar-… 3/17… 2020… 2020…     3  2020    17\n5 NA         2020-04-12  Bunny … 12-A… Apr-… 4/12… 12/4… 2020…     4  2020    12\n6 NA         2020-04-22  Earth … 22-A… Apr-… 4/22… 2020… 2020…     4  2020    22\n# … with 1 more variable: Month2 <chr>, and abbreviated variable name\n#   ¹​Date_test_plot\n\n\nSo the make_date() function requires a specific format for the year, month, and day columns. It may take a little pre-processing to put a particular data set in that format.\n\n\n13.1.3 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-11).\n\n* What’s the issue with trying to convert Date4 to a <date> form? You may want to investigate Date4 further to answer this question.\n\n\nholiday_df |> mutate(Date_test = ymd(Date4)) |>\n  select(Date_test, everything())\n\nWarning: 3 failed to parse.\n\n\n# A tibble: 6 × 12\n  Date_test  Date_test…¹ Holiday Date1 Date2 Date3 Date4 Date5 Month  Year   Day\n  <date>     <date>      <chr>   <chr> <chr> <chr> <chr> <chr> <dbl> <dbl> <dbl>\n1 2001-01-20 2020-01-01  New Ye… 1-Ja… Jan-… 1/1/… 1/1/… 2020…     1  2020     1\n2 2002-02-20 2020-02-02  Ground… 2-Fe… Feb-… 2/2/… 2/2/… 2020…     2  2020     2\n3 NA         2020-02-14  Valent… 14-F… Feb-… 2/14… 2020… 2020…     2  2020    14\n4 NA         2020-03-17  Shamro… 17-M… Mar-… 3/17… 2020… 2020…     3  2020    17\n5 2012-04-20 2020-04-12  Bunny … 12-A… Apr-… 4/12… 12/4… 2020…     4  2020    12\n6 NA         2020-04-22  Earth … 22-A… Apr-… 4/22… 2020… 2020…     4  2020    22\n# … with 1 more variable: Month2 <chr>, and abbreviated variable name\n#   ¹​Date_test_plot\n\n\n\n* Practice converting Date3 and Date5 to <date> variables with lubridate functions."
  },
  {
    "objectID": "11-lubridate.html#functions-for-date-variables",
    "href": "11-lubridate.html#functions-for-date-variables",
    "title": "13  Dates with lubridate",
    "section": "13.2 Functions for <date> Variables",
    "text": "13.2 Functions for <date> Variables\nOnce an object is in the <date> format, there are some special functions in lubridate that can be used on that date variable. To investigate some of these functions, we will pull stock market data from Yahoo using the quantmod package. Install the package, and run the following code, which gets stock market price data on Apple, Nintendo, Chipotle, and the S & P 500 Index from 2011 to now. Note that you have the ability to understand all of the code below, but we will skip over this code for now to focus more on the new information in this section (information about date functions).\n\n## install.packages(\"quantmod\")\nlibrary(quantmod)\n\nstart <- ymd(\"2011-01-01\")\nend <- ymd(\"2021-5-19\")\ngetSymbols(c(\"AAPL\", \"NTDOY\", \"CMG\", \"SPY\"), src = \"yahoo\",\n           from = start, to = end)\n\n[1] \"AAPL\"  \"NTDOY\" \"CMG\"   \"SPY\"  \n\ndate_tib <- as_tibble(index(AAPL)) |>\n  rename(start_date = value)\napp_tib <- as_tibble(AAPL)\nnint_tib <- as_tibble(NTDOY)\nchip_tib <- as_tibble(CMG)\nspy_tib <- as_tibble(SPY)\nall_stocks <- bind_cols(date_tib, app_tib, nint_tib, chip_tib, spy_tib)\n\nstocks_long <- all_stocks |>\n  select(start_date, AAPL.Adjusted, NTDOY.Adjusted,\n                      CMG.Adjusted, SPY.Adjusted) |>\n  pivot_longer(2:5, names_to = \"Stock_Type\", values_to = \"Price\") |>\n  mutate(Stock_Type = fct_recode(Stock_Type,\n                                 Apple = \"AAPL.Adjusted\",\n                                 Nintendo = \"NTDOY.Adjusted\",\n                                 Chipotle = \"CMG.Adjusted\",\n                                 `S & P 500` = \"SPY.Adjusted\"\n                                 ))\ntail(stocks_long)\n\n# A tibble: 6 × 3\n  start_date Stock_Type  Price\n  <date>     <fct>       <dbl>\n1 2021-05-17 Chipotle   1332. \n2 2021-05-17 S & P 500   405. \n3 2021-05-18 Apple       124. \n4 2021-05-18 Nintendo     14.1\n5 2021-05-18 Chipotle   1325. \n6 2021-05-18 S & P 500   401. \n\n\nYou’ll have a chance in the Exercises to choose your own stocks to investigate. For now, I’ve made a data set with three variables:\n\nstart_date, the opening date for the stock market\nStock_Type, a factor with 4 levels: Apple, Nintendo, Chipotle, and S & P 500\nPrice, the price of the stock?\n\nFirst, let’s make a line plot that shows how the S & P 500 has changed over time:\n\nstocks_sp <- stocks_long |> filter(Stock_Type == \"S & P 500\")\nggplot(data = stocks_sp, aes(x = start_date, y = Price)) +\n  geom_line()\n\n\n\n\nBut, there’s other information that we can get from the start_date variable. We might be interested in things like day of the week, monthly trends, or yearly trends. To extract variables like “weekday” and “month” from a <date> variable, there are a series of functions that are fairly straightforward to use. We will discuss the year() month(), mday(), yday(), and wday() functions.\n\n13.2.1 year(), month(), and mday()\nThe functions year(), month(), and mday() can grab the year, month, and day of the month, respectively, from a <date> variable. Like the forcats functions, these will almost always be paired with a mutate() statement because they will create a new variable:\n\nstocks_long |> mutate(year_stock = year(start_date))\nstocks_long |> mutate(month_stock = month(start_date))\nstocks_long |> mutate(day_stock = mday(start_date))\n\n\n\n13.2.2 yday() and wday()\nThe yday() function grabs the day of the year from a <date> object. For example,\n\ntest <- mdy(\"November 4, 2020\")\nyday(test)\n\n[1] 309\n\n\nreturns 309, indicating that November 4th is the 309th day of the year 2020. Using this function in a mutate() statement creates a new variable that has yday for each observation:\n\nstocks_long |> mutate(day_in_year = yday(start_date))\n\n# A tibble: 10,444 × 4\n   start_date Stock_Type  Price day_in_year\n   <date>     <fct>       <dbl>       <dbl>\n 1 2011-01-03 Apple       10.0            3\n 2 2011-01-03 Nintendo     7.34           3\n 3 2011-01-03 Chipotle   224.             3\n 4 2011-01-03 S & P 500  101.             3\n 5 2011-01-04 Apple       10.1            4\n 6 2011-01-04 Nintendo     7.1            4\n 7 2011-01-04 Chipotle   222.             4\n 8 2011-01-04 S & P 500  101.             4\n 9 2011-01-05 Apple       10.2            5\n10 2011-01-05 Nintendo     6.92           5\n# … with 10,434 more rows\n\n\nFinally, the function wday() grabs the day of the week from a <date>. By default, wday() puts the day of the week as a numeric, but I find this confusing, as I can’t ever remember whether a 1 means Sunday or a 1 means Monday. Adding, label = TRUE creates the weekday variable as Sunday, Monday, Tuesday, etc.:\n\nstocks_long |> mutate(day_of_week = wday(start_date))\nstocks_long |> mutate(day_of_week = wday(start_date,\n                                          label = TRUE, abbr = FALSE))\n\nPossible uses for these functions are:\n\nyou want to look at differences between years (with year())\nyou want to look at differences between months (with month())\nyou want to look at differences between days of the week (with wday())\nyou want to see whether there are yearly trends within years (with yday())\n\nNote: Working with times is extremely similar to working with dates. Instead of ymd(), mdy(), etc., you tack on a few extra letters to specify the order that the hour, minute, and seconds appear in the variable: ymd_hms() converts a character vector that has the order year, month, day, hour, minute, second to a <datetime>.\nAdditionally, the functions hour(), minute(), and second() grab the hour, minute, and second from a <datetime> variable.\nNote on Complications: Things can get complicated, especially if you start to consider things like time duration. The reason is that our time system is inherently confusing. Consider how the following might affect an analysis involving time duration:\n\ntime zones\nleap years (not all years have the same number of days)\ndiffering number of days in a given month\ndaylight saving time (not all days have the same number of hours)\n\n\n\n13.2.3 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-11).\n\nThe month() function gives the numbers corresponding to each month by default. Type ?month and figure out which argument you would need to change to get the names (January, February, etc.) instead of the month numbers. What about the abbreviations (Jan, Feb, etc.) of each month instead of the month numbers? Try making the changes in the mutate() statement below.\n\n\nstocks_long |> mutate(month_stock = month(start_date))"
  },
  {
    "objectID": "11-lubridate.html#chapexercise-11",
    "href": "11-lubridate.html#chapexercise-11",
    "title": "13  Dates with lubridate",
    "section": "13.3 Chapter Exercises",
    "text": "13.3 Chapter Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-11).\nThe truncated argument to ymd(), dmy(), mdy(), etc. will allow R to parse dates that aren’t actually complete. For example,\n\nlibrary(lubridate)\nymd(\"2019\", truncated = 2)\n\n[1] \"2019-01-01\"\n\n\nparses 2019 to be January 1, 2019 when the month and day are missing. The 2 means that the last two parts of the date (in this case, month and day) are allowed to be missing. Similarly,\n\ndmy(\"19-10\", truncated = 1)\n\n[1] \"0000-10-19\"\n\n\ntruncates the year (which is given as 0000). The truncate function is usually most useful in the context of the first example with a truncated month and/or day.\nExamine the ds_google.csv, which contains\n\nMonth, the year and month from 2004 to now\nData_Science, the relative popularity of data science (Google keeps how it calculates “popularity” as somewhat of a mystery but it is likely based off of the number of times people search for the term “Data Science”)\n\n\nlibrary(tidyverse)\nlibrary(lubridate)\nds_df <- read_csv(here(\"data/ds_google.csv\"))\nds_df\n\n# A tibble: 202 × 2\n   Month   Data_Science\n   <chr>          <dbl>\n 1 2004-01           14\n 2 2004-02            8\n 3 2004-03           16\n 4 2004-04           11\n 5 2004-05            5\n 6 2004-06            8\n 7 2004-07            7\n 8 2004-08            9\n 9 2004-09           13\n10 2004-10           11\n# … with 192 more rows\n\n\n\n* Use a lubridate function with the truncated option to convert the Month variable to be in the <date> format.\n\n\n\n\n\n* Make a plot of the popularity of Data Science through Time. Add a smoother to your plot. What patterns do you notice?\n\n\n\n\nThe data was obtained from Google Trends: Google Trends. Google Trends is incredibly cool to explore, even without R.\n\n* On Google Trends, Enter in a search term, and change the Time dropdown menu to be 2004-present. Then, enter in a second search term that you want to compare. You can also change the country if you want to (or, you can keep the country as United States).\n\nMy search terms will be “super smash” and “animal crossing”, but yours should be something that interests you!\nIn the top-right window of the graph, you should click the down arrow to download the data set. Delete the first two rows of your data set (either in Excel or R), read in the data set, and change the date variable so that it’s in a Date format.\n\n\n\n\n* Make a plot of your Popularity variables through time. Hint: Does the data set need to be tidied at all first?\n\n\n\n\n\n* Using your data set that explored a variable or two from 2004 through now, make a table of the average popularity for each year. Hint: You’ll need a lubridate function to extract the year variable from the date object.\n* Clear your search and now enter a search term that you’d like to investigate for the past 90 days. Mine will be “Pittsburgh Steelers” but, again, yours should be something that interests you.\n\nAgain, click the download button again and read in the data to R. Convert the date variable to be in <date> format.\n\n\n\n\n* Make a plot of your popularity variable through time, adding a smoother.\n\n\n\n\n\nUsing your data set that explored a variable from the past 90 days, construct a table that compares the average popularity on each day of the week (Monday through Saturday).\nExamine the ds_df data set again, the data set on data science in Google Trends, and suppose that you have an observation for each day of every year (not just one observation per month). You want to look at whether data science is more popular on certain days of the week. Explain why the following strategy wouldn’t really work that well.\n\n\ncreate a weekday variable with wday()\nuse summarise() and group_by() to find the average popularity for each day of the week\n\n\nUse the code in the tutorial section on the Stocks data to get a data frame on stock prices for a couple of different stocks that interest you. The start and end date that you use are completely up to you.\n\nExplore the stock data that you chose, constructing a line plot of the price through time, as well as any graphs or summaries that show interesting patterns across years, months, days, days of the week, etc.\n\nUse the lag() function to create a new variable that is the previous day’s stock price. Can you predict the current stock price based on the previous day’s stock price accurately? Why or why not? Use either graphical or numerical evidence."
  },
  {
    "objectID": "11-lubridate.html#solutions-11",
    "href": "11-lubridate.html#solutions-11",
    "title": "13  Dates with lubridate",
    "section": "13.4 Exercise Solutions",
    "text": "13.4 Exercise Solutions\n\n13.4.1 Converting Variables to <date> S\n\n* What’s the issue with trying to convert Date4 to a <date> form?\n\n\nholiday_df |> mutate(Date_test = ymd(Date4)) |>\n  select(Date_test, everything())\n\nWarning: 3 failed to parse.\n\n\n# A tibble: 6 × 12\n  Date_test  Date_test…¹ Holiday Date1 Date2 Date3 Date4 Date5 Month  Year   Day\n  <date>     <date>      <chr>   <chr> <chr> <chr> <chr> <chr> <dbl> <dbl> <dbl>\n1 2001-01-20 2020-01-01  New Ye… 1-Ja… Jan-… 1/1/… 1/1/… 2020…     1  2020     1\n2 2002-02-20 2020-02-02  Ground… 2-Fe… Feb-… 2/2/… 2/2/… 2020…     2  2020     2\n3 NA         2020-02-14  Valent… 14-F… Feb-… 2/14… 2020… 2020…     2  2020    14\n4 NA         2020-03-17  Shamro… 17-M… Mar-… 3/17… 2020… 2020…     3  2020    17\n5 2012-04-20 2020-04-12  Bunny … 12-A… Apr-… 4/12… 12/4… 2020…     4  2020    12\n6 NA         2020-04-22  Earth … 22-A… Apr-… 4/22… 2020… 2020…     4  2020    22\n# … with 1 more variable: Month2 <chr>, and abbreviated variable name\n#   ¹​Date_test_plot\n\n\n\n## Date4 has two __different__ formats, \n## which creates problems for `lubridate` functions\n\n\n* Practice converting Date3 and Date5 to date objects with lubridate functions.\n\n\nholiday_df |> mutate(Date_test = mdy(Date3)) |>\n  select(Date_test, everything())\n\n# A tibble: 6 × 12\n  Date_test  Date_test…¹ Holiday Date1 Date2 Date3 Date4 Date5 Month  Year   Day\n  <date>     <date>      <chr>   <chr> <chr> <chr> <chr> <chr> <dbl> <dbl> <dbl>\n1 2020-01-01 2020-01-01  New Ye… 1-Ja… Jan-… 1/1/… 1/1/… 2020…     1  2020     1\n2 2020-02-02 2020-02-02  Ground… 2-Fe… Feb-… 2/2/… 2/2/… 2020…     2  2020     2\n3 2020-02-14 2020-02-14  Valent… 14-F… Feb-… 2/14… 2020… 2020…     2  2020    14\n4 2020-03-17 2020-03-17  Shamro… 17-M… Mar-… 3/17… 2020… 2020…     3  2020    17\n5 2020-04-12 2020-04-12  Bunny … 12-A… Apr-… 4/12… 12/4… 2020…     4  2020    12\n6 2020-04-22 2020-04-22  Earth … 22-A… Apr-… 4/22… 2020… 2020…     4  2020    22\n# … with 1 more variable: Month2 <chr>, and abbreviated variable name\n#   ¹​Date_test_plot\n\nholiday_df |> mutate(Date_test = ymd(Date5)) |>\n  select(Date_test, everything())\n\n# A tibble: 6 × 12\n  Date_test  Date_test…¹ Holiday Date1 Date2 Date3 Date4 Date5 Month  Year   Day\n  <date>     <date>      <chr>   <chr> <chr> <chr> <chr> <chr> <dbl> <dbl> <dbl>\n1 2020-01-01 2020-01-01  New Ye… 1-Ja… Jan-… 1/1/… 1/1/… 2020…     1  2020     1\n2 2020-02-02 2020-02-02  Ground… 2-Fe… Feb-… 2/2/… 2/2/… 2020…     2  2020     2\n3 2020-02-14 2020-02-14  Valent… 14-F… Feb-… 2/14… 2020… 2020…     2  2020    14\n4 2020-03-17 2020-03-17  Shamro… 17-M… Mar-… 3/17… 2020… 2020…     3  2020    17\n5 2020-04-12 2020-04-12  Bunny … 12-A… Apr-… 4/12… 12/4… 2020…     4  2020    12\n6 2020-04-22 2020-04-22  Earth … 22-A… Apr-… 4/22… 2020… 2020…     4  2020    22\n# … with 1 more variable: Month2 <chr>, and abbreviated variable name\n#   ¹​Date_test_plot\n\n\n\n\n13.4.2 Functions for <date> Variables S\n\n\n13.4.3 Chapter Exercises S\n\n* Use a lubridate function with the truncated option to convert the Month variable to be in the <date> format.\n\n\nds_df <- ds_df |> mutate(Month = ymd(Month, truncated = 1))\nds_df\n\n\n* Make a plot of the popularity of Data Science through Time. Add a smoother to your plot. What patterns do you notice?\n\n\nggplot(data = ds_df, aes(x = Month, y = Data_Science)) +\n  geom_line() +\n  geom_smooth(se = FALSE)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n## it's like super popular!!!!\n\n\n* On Google Trends, Enter in a search term, and change the Time dropdown menu to be 2004-present. Then, enter in a second search term that you want to compare. You can also change the country if you want to (or, you can keep the country as United States).\n\nMy search terms will be “super smash” and “animal crossing”, but yours should be something that interests you!\nIn the top-right window of the graph, you should click the down arrow to download the data set. Delete the first two rows of your data set (either in Excel or R), read in the data set, and change the date variable so that it’s in a Date format.\n\nvideogame_df <- read_csv(here(\"data/smash_animal_crossing.csv\"))\n\nRows: 203 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Month\ndbl (2): super_smash, animal_crossing\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nvideogame_df <- videogame_df |> mutate(date = ymd(Month, truncated = 1))\n\n\n* Make a plot of your Popularity variables through time. Hint: Does the data set need to be tidied at all first?\n\n\nvideogame_long <- videogame_df |>\n  pivot_longer(cols = c(\"super_smash\", \"animal_crossing\"),\n                              names_to = \"game\",\n                              values_to = \"popularity\")\nggplot(data = videogame_long, aes(x = date, \n                                  y = popularity,\n                                  colour = game)) +\n  geom_line() +\n  scale_colour_viridis_d(begin = 0, end = 0.9)\n\n\n\n\n\n* Using your data set that explored a variable or two from 2004 through now, make a table of the average popularity for each year. Hint: You’ll need a lubridate function to extract the year variable from the date object.\n* Clear your search and now enter a search term that you’d like to investigate for the past 90 days. Mine will be “pittsburgh steelers” but, again, yours should be something that interests you.\n\nAgain, click the download button again and read in the data to R. Convert the date variable to be in <date> format.\n\nsteelers_df <- read_csv(here(\"data/steelers.csv\"))\n\nRows: 91 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Day\ndbl (1): Steelers\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsteelers_df <- steelers_df |> mutate(day_var = mdy(Day))\n\n\n* Make a plot of your popularity variable through time, adding a smoother.\n\n\nggplot(data = steelers_df, aes(x = day_var, y = Steelers)) +\n  geom_smooth() + \n  geom_line() +\n  labs(y = \"Popularity\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'"
  },
  {
    "objectID": "11-lubridate.html#rcode-11",
    "href": "11-lubridate.html#rcode-11",
    "title": "13  Dates with lubridate",
    "section": "13.5 Non-Exercise R Code",
    "text": "13.5 Non-Exercise R Code\n\nlibrary(tidyverse)\nlibrary(lubridate)\ntoday()\nnow()\nlibrary(here)\nholiday_df <- read_csv(here(\"data/animal_crossing_holidays.csv\"))\nholiday_df\nholiday_df |> mutate(Date_test = dmy(Date1)) |>\n  select(Date_test, everything())\nholiday_df |> mutate(Date_test = mdy(Date2)) |>\n  select(Date_test, everything())\nggplot(data = holiday_df, aes(x = Date1, y = Holiday)) +\n  geom_point()\nholiday_df <- holiday_df |> mutate(Date_test_plot = dmy(Date1)) |>\n  select(Date_test_plot, everything())\nggplot(data = holiday_df, aes(x = Date_test_plot, y = Holiday)) +\n  geom_point()\nholiday_df |> mutate(Date_test2 = make_date(year = Year,\n                                             month = Month,\n                                             day = Day)) |>\n  select(Date_test2, everything())\nholiday_df |> mutate(Date_test2 = make_date(year = Year,\n                                             month = Month2,\n                                             day = Day)) |>\n  select(Date_test2, everything())\n## install.packages(\"quantmod\")\nlibrary(quantmod)\n\nstart <- ymd(\"2011-01-01\")\nend <- ymd(\"2021-5-19\")\ngetSymbols(c(\"AAPL\", \"NTDOY\", \"CMG\", \"SPY\"), src = \"yahoo\",\n           from = start, to = end)\n\ndate_tib <- as_tibble(index(AAPL)) |>\n  rename(start_date = value)\napp_tib <- as_tibble(AAPL)\nnint_tib <- as_tibble(NTDOY)\nchip_tib <- as_tibble(CMG)\nspy_tib <- as_tibble(SPY)\nall_stocks <- bind_cols(date_tib, app_tib, nint_tib, chip_tib, spy_tib)\n\nstocks_long <- all_stocks |>\n  select(start_date, AAPL.Adjusted, NTDOY.Adjusted,\n                      CMG.Adjusted, SPY.Adjusted) |>\n  pivot_longer(2:5, names_to = \"Stock_Type\", values_to = \"Price\") |>\n  mutate(Stock_Type = fct_recode(Stock_Type,\n                                 Apple = \"AAPL.Adjusted\",\n                                 Nintendo = \"NTDOY.Adjusted\",\n                                 Chipotle = \"CMG.Adjusted\",\n                                 `S & P 500` = \"SPY.Adjusted\"\n                                 ))\ntail(stocks_long)\nstocks_sp <- stocks_long |> filter(Stock_Type == \"S & P 500\")\nggplot(data = stocks_sp, aes(x = start_date, y = Price)) +\n  geom_line()\nstocks_long |> mutate(year_stock = year(start_date))\nstocks_long |> mutate(month_stock = month(start_date))\nstocks_long |> mutate(day_stock = mday(start_date))\ntest <- mdy(\"November 4, 2020\")\nyday(test)\nstocks_long |> mutate(day_in_year = yday(start_date))\nstocks_long |> mutate(day_of_week = wday(start_date))\nstocks_long |> mutate(day_of_week = wday(start_date,\n                                          label = TRUE, abbr = FALSE))"
  },
  {
    "objectID": "01-intro.html#intro-to-r-and-r-studio",
    "href": "01-intro.html#intro-to-r-and-r-studio",
    "title": "2  Getting Started with R and R Studio",
    "section": "2.1 Intro to R and R Studio",
    "text": "2.1 Intro to R and R Studio\nR is a statistical computing software used by many statisticians as well as professionals in other fields, such as biology, ecology, business, and psychology. The goal of Week 0 is to provide basic familiarity with R and Quarto, which we will be using for the entire semester.\n\n2.1.1 Installing R and R Studio\nThe R Studio server is a computer that is set-up to carry out any R-based analyses for students with remote access to the computer (in our case, through SLU Login credentials). It might be helpful to think about the server as a large machine with no keyboard and no screen: it’s only purpose is to execute the code. You may have used the R Studio server in a different stats course. The server does have some benefits, such as\n\nusing the server ensures that we are all using the same version of R. In theory, if one person gets an error, then everyone should get that same error.\ninstalling R and R Studio on your personal device is much easier after you’ve had some experience using it through the server.\nyou don’t need a computer that is capable of running R to use the server (you can use a tablet or a Chromebook since the server does all of the actual computation).\n\nWe, however, will move away from the server and install R and R Studio on our own devices. Though the server does have some advantages, there are also some disadvantages:\n\nyou won’t have your SLU login forever, so, if you wanted to use R post graduation, you’d need to know how to install it.\nyou haven’t had experience installing R packages. This is quite easy to do, but I’ve installed all necessary R packages on the server for us so you haven’t had to worry about this step.\nthe server requires good Internet access and also has the potential to crash.\n\nIn this next section, we will work on installing R and R Studio to your personal laptop. The following videos provide instructions on how to install R and R Studio to your laptop computer. It will be easiest if you complete all of these steps consecutively in one sitting.\n Watch and follow along with a video on installing R. \n Watch and follow along with a video on installing R Studio. \n Watch and follow along with a video on installing R packages and changing other options. \n\n\n2.1.2 Relevant Websites\n\nInstall R: http://lib.stat.cmu.edu/R/CRAN/\nInstall R Studio (the free option): https://www.rstudio.com/products/rstudio/download/\n\n\n\n2.1.3 Creating an R Project\nAfter you have both R and R Studio installed, open R Studio in your Applications. Create a new folder on your Desktop (or some other place that is easy for you to access and remember). Make sure the folder name has no spaces in it.\nThen, in R Studio, create an R Project by Clicking File -> New Project -> Existing Directory. Navigate to the DATA234 folder you made, and click Create Project. You should see a new window of R Studio open up.\nNext, we want to put some data in a folder in the same folder that has our newly created R Project. Download the data.zip file from Canvas (in Resources) and move the zip file to the folder with your R project (you can drag and drop from downloads, copy/paste from downloads, or move it with whatever method you usually move files with). Clicking the data.zip file (either in a window or in the bottom-left window of R Studio) will created a data folder that has data sets we want to use throughout the semester.\nFinally, we want to create a new Quarto file by clicking File -> New File -> Quarto Document. You can give your new Quarto file a title if you want, and then click okay.\nWe are also going to change one option routinely in our Quarto files. Change the first few lines of the file to be something like:\n---\ntitle: \"Your Title\"\nauthor: \"Your Name\"\nformat: \n  html:\n    self-contained: true\n---\nNote the self-contained: true option that we added. This ensures that all figures, images, tables, etc. are contained in the one .html file, which is important because, for quizzes and exercises, you will typically only turn in the .html file.\nBefore moving on, click the Render button in the top-left window at the top of the menu bar. Make sure that the file renders to a pretty-looking .html file. The newly rendered .html file can now be found in your folder with your R project."
  },
  {
    "objectID": "01-intro.html#what-are-r-r-studio-and-quarto",
    "href": "01-intro.html#what-are-r-r-studio-and-quarto",
    "title": "2  Getting Started with R and R Studio",
    "section": "2.2 What are R, R Studio, and Quarto?",
    "text": "2.2 What are R, R Studio, and Quarto?\nThe distinction between the 3 will become more clear later on. For now,\n\nR is a statistical coding software used heavily for data analysis and statistical procedures.\nR Studio is a nice IDE (Integrated Development Environment) for R that has a lot of convenient features. Think of this as just a convenient User Interface.\nQuarto allows users to mix regular Microsoft-Word-style text with code. The .qmd file ending denotes an Quarto file. Quarto has many options that we will use heavily throughout the semester, but there’s no need to worry about these now.\n\n\n2.2.1 R Packages and the tidyverse\nYou can think of R packages as add-ons to R that let you do things that R on its own would not be able to do. If you’re in to video games, you can think of R packages as extra Downloadable Content (DLC). But, unlike most gaming DLC, R packages are always free and we will make very heavy use of R packages.\nThe tidyverse is a series of R packages that are useful for data science. In the order that we will encounter them in this class, the core tidyverse packages are:\n\nggplot2 for plotting data\ndplyr for data wrangling and summarizing\ntidyr for data tidying and reshaping\nreadr for data import\ntibble for how data is stored\nstringr for text data\nforcats for factor (categorical) data\npurrr, for functional programming, the only one of these core 8 that we won’t get to use\n\nWe will use packages outside of the core tidyverse as well, but the tidyverse is the main focus.\n\n\n\n2.2.2 Installing R Packages\nOn the R Studio server, either myself or one of the other statistics faculty members have installed all packages that we’ve needed to use on the server globally. However, if you want to use a package that isn’t installed on the server, or, you want to use a package using R Studio on your own personal computer, you need to install it first.\nInstallation only needs to happen once (or until you upgrade R, which usually doesn’t happen too often), whereas the package needs to be loaded with library() every time you open R. The analogy of a lightbulb might be helpful. You only need to screw in the lightbulb into a socket once, but, every time you want the lightbulb to provide light, you need to flip the light switch.\nIn the lightbulb analogy, what does putting the lightbulb into the socket correspond to? What does flipping the light switch correspond to?\nNow that you have R on your own computer, you’ll need to install all packages that you want to use (but, remember that you just need to install each package once). Try installing the tidyverse package, a collection of many useful data science packages, with:\n\ninstall.packages(\"tidyverse\")"
  },
  {
    "objectID": "01-intro.html#putting-code-in-a-.qmd-file",
    "href": "01-intro.html#putting-code-in-a-.qmd-file",
    "title": "2  Getting Started with R and R Studio",
    "section": "2.3 Putting Code in a .qmd File",
    "text": "2.3 Putting Code in a .qmd File\nThe first thing that we will do that involves code is to load a package into R with the library() function. A package is just an R add-on that lets you do more than you could with just R on its own. Load the tidyverse package into R by typing and running the library(tidyverse) line. To create a code chunk, click Insert -> R. Within this code chunk, type in library(tidyverse) and run the code by either\n\nClicking the “Run” button in the menu bar of the top-left window of R Studio or\n(Recommended) Clicking “Command + Enter” on a Mac or “Control + Enter” on a PC.\n\nNote that all code appears in grey boxes surrounded by three backticks while normal text has a different colour background with no backticks.\n\nlibrary(tidyverse)\n\nWhen you run the previous line, some text will appear in the bottom-left window. We won’t worry too much about what this text means now, but we also won’t ignore it completely. You should be able to spot the 8 core tidyverse packages listed above as well as some numbers that follow each package. The numbers correspond to the package version. There’s some other things too, but as long as this text does not start with “Error:”, you’re good to go!\nCongrats on running your first line of code for this class! This particular code isn’t particularly exciting because it doesn’t really do anything that we can see.\nWe have run R code using an R chunk. In your R chunk, on a new line, try typing in a basic calculation, like 71 + 9 or 4 / 3, them run the line and observe the result.\nSo, that still wasn’t super exciting. R can perform basic calculations, but you could just use a calculator or Excel for that. In order to look at things that are a bit more interesting, we need some data."
  },
  {
    "objectID": "01-intro.html#alcohol-data-example",
    "href": "01-intro.html#alcohol-data-example",
    "title": "2  Getting Started with R and R Studio",
    "section": "2.4 Alcohol Data Example",
    "text": "2.4 Alcohol Data Example\nWe will be looking at two data sets just to get a little bit of a preview of things we will be working on for the rest of the semester. Important: Do not worry about understanding what the following code is doing at this point. There will be plenty of time to understand this in the weeks ahead. The purpose of this section is just to get used to using R: there will be more detailed explanations and exercises about the functions used and various options in the coming weeks. In particular, the following code uses the ggplot2, dplyr, and tidyr packages, which we will cover in detail throughout the first ~ 3-4 weeks of this course.\nData for this first part was obtained from fivethirtyeight at Five Thirty Eight GitHub page.\nThe first step is to read the data set into R. Though you have already downloaded alcohol.csv in the data zip, we still need to load it into R. Check to make sure the alcohol.csv is in the data folder in your bottom-right hand window. The following code can be copied to an R code chunk to read in the data:\n\nread_csv(\"data/alcohol.csv\")\n\nNote that we do not need the full file extension if we have the data set in an R project.\nDid something show up in your console window? If so, great! If not, make sure that the data set is in the data folder and that you have an R project set up.\nWe would like to name our data set something so that we could easily reference it later, so name your data set using the <- operator, as in\n\nalcohol_data <- read_csv(\"data/alcohol.csv\")\n\nYou can name your data set whatever you want to (with a few restrictions). I’ve named it alcohol_data. Now, if you run the line of code above where you name the data set, and run alcohol_data, you should see the data set appear:\n\nalcohol_data\n\n# A tibble: 193 × 5\n   country           beer_servings spirit_servings wine_servings total_litres_…¹\n   <chr>                     <dbl>           <dbl>         <dbl>           <dbl>\n 1 Afghanistan                   0               0             0             0  \n 2 Albania                      89             132            54             4.9\n 3 Algeria                      25               0            14             0.7\n 4 Andorra                     245             138           312            12.4\n 5 Angola                      217              57            45             5.9\n 6 Antigua & Barbuda           102             128            45             4.9\n 7 Argentina                   193              25           221             8.3\n 8 Armenia                      21             179            11             3.8\n 9 Australia                   261              72           212            10.4\n10 Austria                     279              75           191             9.7\n# … with 183 more rows, and abbreviated variable name\n#   ¹​total_litres_of_pure_alcohol\n\n\nWhat’s in this data set? We see a few variables on the columns:\n\ncountry: the name of the country\nbeer_servings: the average number of beer servings per person per year\nspirit_servings: the average number of spirit (hard alcohol) servings per person per year\nwine_servings: the average number of wine servings per person per year\ntotal_litres_of_pure_alcohol: the average total litres of pure alcohol consumed per person per year.\n\nOne goal of this class is for you to be able to pose questions about a data set and then use the tools we will learn to answer those questions. For example, we might want to know what the distribution of total litres of alcohol consumed per person looks like across countries. To do this, we can make a plot with the ggplot2 package, one of the packages that automatically loads with tidyverse. We might start by constructing the following plot. Reminder: the goal of this is not for everyone to understand the code in this plot, so don’t worry too much about that.\n\nggplot(data = alcohol_data,\n       mapping = aes(total_litres_of_pure_alcohol)) +\n  geom_histogram(colour = \"black\", fill = \"white\", bins = 15)\n\n\n\n\nI now want to see where the United States (USA) falls on this distribution by drawing a red vertical line for the total litres of alcohol consumed in the United States. To do so, I’ll first use the filter() function in the dplyr package (again, we will learn about that function in detail later). Copy and paste the following lines of code into a new R chunk. Then, run the lines.\n\nsmall_df <- alcohol_data |> filter(country == \"USA\")\nggplot(data = alcohol_data,\n       mapping = aes(total_litres_of_pure_alcohol)) +\n  geom_histogram(colour = \"black\", fill = \"white\", bins = 15) +\n  geom_vline(data = small_df,\n             aes(xintercept = total_litres_of_pure_alcohol),\n             colour = \"red\")\n\nIt looks like there are some countries that consume little to no alcohol. We might want to know what these countries are:\n\nalcohol_data |> filter(total_litres_of_pure_alcohol == 0)\n\n# A tibble: 13 × 5\n   country          beer_servings spirit_servings wine_servings total_litres_o…¹\n   <chr>                    <dbl>           <dbl>         <dbl>            <dbl>\n 1 Afghanistan                  0               0             0                0\n 2 Bangladesh                   0               0             0                0\n 3 North Korea                  0               0             0                0\n 4 Iran                         0               0             0                0\n 5 Kuwait                       0               0             0                0\n 6 Libya                        0               0             0                0\n 7 Maldives                     0               0             0                0\n 8 Marshall Islands             0               0             0                0\n 9 Mauritania                   0               0             0                0\n10 Monaco                       0               0             0                0\n11 Pakistan                     0               0             0                0\n12 San Marino                   0               0             0                0\n13 Somalia                      0               0             0                0\n# … with abbreviated variable name ¹​total_litres_of_pure_alcohol\n\n\nIt looks like there are 13 countries in the data set that consume no alcohol. Note that, in the chunk above, we have to use in total_litres_of_pure_alcohol as the variable name because this is the name of the variable in the data set. Even something like spelling litres in the American English liters (total_liters_of_pure_alcohol) would throw an error because this isn’t the exact name of the variable in the data set. This is something that can be very aggravating when you are first learning any coding language.\nNow suppose that we want to know the 3 countries that consume the most beer, the 3 countries that consume the most spirits, and the 3 countries that consume the most wine per person. If you’re a trivia person, you can form some guesses. Without cheating, I am going to guess (Germany, USA, and UK) for beer, (Spain, Italy, and USA) for wine, and (Russia, Poland, and Lithuania) for spirits. Let’s do beer first!\n\nalcohol_data |> mutate(rankbeer = rank(desc(beer_servings))) |>\n  arrange(rankbeer) |> \n  filter(rankbeer <= 3)\n\nLet’s do the same thing for Wine and Spirits:\n\nalcohol_data |> mutate(rankwine = rank(desc(wine_servings))) |>\n  arrange(rankwine) |> \n  filter(rankwine <= 3)\n\nalcohol_data |> mutate(rankspirits = rank(desc(spirit_servings))) |>\n  arrange(rankspirits) |> \n  filter(rankspirits <= 3)\n\nFinally, suppose that I want to know which country consumes the most wine relative to their beer consumption? Let’s first look at this question graphically. I need to tidy the data first with the pivot_longer() function from the tidyr package:\n\nonecountry_df <- alcohol_data |> \n  filter(country == \"Denmark\")\n\nlibrary(ggrepel)\nggplot(data = alcohol_data,\n       mapping = aes(x = beer_servings, y = wine_servings)) + \n  geom_point(alpha = 0.5) +\n  geom_label_repel(data = onecountry_df, aes(label = country),\n    colour = \"purple\") +\n  geom_point(data = onecountry_df, colour = \"purple\",\n             size = 2.5, shape = 1) +\n  geom_abline(aes(slope = 1, intercept = 0), alpha = 0.3)\n\n\n\n\nThe x-axis corresponds to beer servings while the y-axis corresponds to wine servings. A reference line is given so with countries above the line consuming more wine than beer. We will get into how to make a plot like this later: for now, copy the code chunk and change the labeled point so that it corresponds to a country that interests you (other than Denmark). We might be able to better answer the original question numerically by computing the wine to beer ratio for each country and then ordering from the largest ratio to the smallest ratio:\n\nalcohol_data |>\n  mutate(wbratio = wine_servings / beer_servings) |>\n  arrange(desc(wbratio)) |>\n  select(country, beer_servings, wine_servings, wbratio)\n\n# A tibble: 193 × 4\n   country             beer_servings wine_servings wbratio\n   <chr>                       <dbl>         <dbl>   <dbl>\n 1 Cook Islands                    0            74  Inf   \n 2 Qatar                           1             7    7   \n 3 Montenegro                     31           128    4.13\n 4 Timor-Leste                     1             4    4   \n 5 Syria                           5            16    3.2 \n 6 France                        127           370    2.91\n 7 Georgia                        52           149    2.87\n 8 Italy                          85           237    2.79\n 9 Equatorial Guinea              92           233    2.53\n10 Sao Tome & Principe            56           140    2.5 \n# … with 183 more rows\n\n\nWhy is one of the ratios Inf?\n\n2.4.1 Exercises\n\nWhat is the shape of the distribution of total alcohol consumption? Left-skewed, right-skewed, or approximately symmetric? Unimodal or multimodal?\nIn the histogram of total alcohol consumption, pick a country other than the USA that interests you. See if you can change the code in the chunk that made the histogram so that the red vertical line is drawn for the country that interests you.\n\nHint: Use the View() function to look at the alcohol data set by typing View(alcohol_data) in your bottom-left window to help you see which countries are in the data set.\n\nView(alcohol_data)\n\nNote: careful about capitalization: R is case sensitive so USA is different than usa.\n\nIn the histogram of total alcohol consumption, change the fill colour of the bins in the histogram above: what should be changed in the code chunk?\nIn the spirit rankings, why do you think only 2 countries showed up instead of 3? Can you do any investigation as to why this is the case?\nIn the rankings code, what if you wanted to look at the top 5 countries instead of the top 3? See if you could change the code.\nChange the wine to beer ratio code example to find the countries with the highest beer to wine consumption (instead of wine to beer consumption)."
  },
  {
    "objectID": "01-intro.html#athlete-data-example",
    "href": "01-intro.html#athlete-data-example",
    "title": "2  Getting Started with R and R Studio",
    "section": "2.5 Athlete Data Example",
    "text": "2.5 Athlete Data Example\nSecondly, we will look at a data set on the top 100 highest paid athletes in 2014. The athletesdata was obtained from https://github.com/ali-ce/datasets data set has information on the following variables from the 100 highest paid athletes of 2014, according to Forbes (pay includes both salary and endorsements):\n\nName (name of the athlete)\nRank (where the athlete ranks, with 1 being the highest paid)\nSport (the sport the athlete plays)\nendorsements (money from sponsorships from companies)\ntotalpay (in millions in the year of 2014, salary + endorsements)\nsalary (money from tournaments or contract salary)\nage of athlete in 2014\nGender (Male or Female)\n\nWe will first read in the data set below and name it athletes. We can then use the head() function to look at the first few rows of the data set.\n\nathletes <- read_csv(\"data/athletesdata.csv\")\nhead(athletes)\n\n# A tibble: 6 × 9\n   ...1 Name               Rank Sport      endorse…¹ total…² salary   age Gender\n  <dbl> <chr>             <dbl> <chr>          <dbl>   <dbl>  <dbl> <dbl> <chr> \n1     1 Aaron Rodgers        55 Football     7500000  2.2 e7 1.45e7    31 Male  \n2     2 Adam Scott           95 Golf         9000000  1.77e7 8.7 e6    34 Male  \n3     3 Adrian Gonzalez      60 Baseball      400000  2.15e7 2.11e7    32 Male  \n4     4 Alex Rodriguez       48 Baseball      300000  2.29e7 2.26e7    39 Male  \n5     5 Alfonso Soriano      93 Baseball       50000  1.80e7 1.8 e7    38 Male  \n6     6 Amar'e Stoudemire    27 Basketball   5000000  2.67e7 2.17e7    32 Male  \n# … with abbreviated variable names ¹​endorsements, ²​totalpay\n\n\nThere are many different interesting questions to answer with this data set. First, we might be interested in the relationship between athlete age and salary for the top 100 athletes. Recall from an earlier stat course that one appropriate graphic to examine this relationship is a scatterplot:\n\nggplot(data = athletes, mapping = aes(x = age, y = salary)) + \n  geom_point() +\n  geom_smooth(se = FALSE)\n\n\n\n\nDo you see anything strange with the scatterplot? What do you think the y-axis tick labels of 2.5e+07, 5.0e+07, etc. mean?\nNow let’s see if we can count the number of athletes in the Top 100 that are in my personal favourite sport, Tennis:\n\nathletes |> group_by(Sport) |>\n  summarise(counts = n()) |>\n  filter(Sport == \"Tennis\")\n\n# A tibble: 1 × 2\n  Sport  counts\n  <chr>   <int>\n1 Tennis      6\n\n\nIt looks like there are 6 athletes: we can see who they are and sort them by their Rank with:\n\nathletes |>\n  filter(Sport == \"Tennis\") |>\n  arrange(Rank)\n\n# A tibble: 6 × 9\n   ...1 Name             Rank Sport  endorsements totalpay   salary   age Gender\n  <dbl> <chr>           <dbl> <chr>         <dbl>    <dbl>    <dbl> <dbl> <chr> \n1    82 Roger Federer       7 Tennis     52000000 56200000  4200000    33 Male  \n2    78 Rafael Nadal        9 Tennis     30000000 44500000 14500000    28 Male  \n3    72 Novak Djokovic     17 Tennis     21000000 33100000 12100000    27 Male  \n4    64 Maria Sharapova    34 Tennis     22000000 24400000  2400000    27 Female\n5    60 Li Na              41 Tennis     18000000 23600000  5600000    32 Female\n6    89 Serena Williams    55 Tennis     11000000 22000000 11000000    33 Female\n\n\nFinally, let’s see if we can compare the ratio of endorsements (from commercials and products) to salary of professional athletes in the Top 100 in 2 sports: Football (referring to American Football) and Basketball. Recall from an earlier Stat class that we might want to use side-by-side boxplots to make this comparison since we have one categorical variable (Sport Type) and one quantitative variable (Ratio of Endorsements to Salary).\n\nfootball_basketball <- athletes |>\n  filter(Sport == \"Football\" | Sport == \"Basketball\")\n\nggplot(data = football_basketball,\n       aes(x = Sport, y = endorsements / salary)) + \n  geom_boxplot() +\n  labs(y = \"Endorsements / Salary\")\n\n\n\n\nIn the graph an endorsements / salary ratio of 1 indicates that the person makes half of their overall pay from endorsements and half of their overall pay from salary.\nWhich sport looks like it tends to receive a larger proportion of their overall pay from endorsements for athletes in the top 100?\n\n\n2.5.1 Exercises\n\nInstead of looking at the relationship between age and salary in the top 100 athletes of 2014, change the plot to look at the relationship between age and endorsements. What would you change in the code above? Try it!\nPick a Sport other than Tennis and see if you can count the number of athletes in the top 100 in that sport as well as sort them by Rank. Careful: not all sports will have athletes in the Top 100.\n\nHow many athletes are in the top 100 in the sport that you chose?\n\nIn the endorsements / salary example, change one of the sports to the sport of your choice and make a comparison. Which sport tends to receive a larger proportion of their overall pay from endorsements.\nWhat qualification might you want to make about your statement in the previous exercise? (Is this a random sample of athletes from each sport? Why does that matter?).\nIn the side-by-side boxplots comparing the endorsements to salary ratio of two different sports, I’ve changed the y-axis label above to be Endorsements / Salary using the labs(y = \"Endorsements / Salary\") statement. Try changing the x-axis label to something else. What do you think you would need to add to the plot?"
  },
  {
    "objectID": "01-intro.html#finishing-up-common-errors-in-r",
    "href": "01-intro.html#finishing-up-common-errors-in-r",
    "title": "2  Getting Started with R and R Studio",
    "section": "2.6 Finishing Up: Common Errors in R",
    "text": "2.6 Finishing Up: Common Errors in R\nWe will now talk a little bit about getting errors in R and what can be done to correct some common errors.\nYou may have encountered some errors by this point in the document. Let’s go over a few common errors as well as discuss how to comment your code.\n\nA missing parenthesis: any open parenthesis ( needs to close ). Try running the following code chunk without fixing anything.\n\n\nggplot(data = athletes, aes(x = Sport, y = salary) + \n  geom_boxplot()\n\nNotice in your bottom-left window that the > symbol that starts a line changes to a +. This is generally bad!! It means that you forgot to close a parenthesis ) or a quote (' or \"). No code will run since R thinks you are still trying to type something into a function. To fix this issue, click your cursor into the bottom-left window and press Esc. Then, try to find the error in the code chunk.\n\nCan you find the missing closing parenthesis above?\n\n\nMissing Comma. Try running the following code chunk without fixing anything.\n\n\nggplot(data = athletes aes(x = Sport, y = salary)) + \n  geom_boxplot()\n\nR gives you an “Error: unexpected symbol in ….”. Oftentimes, this means that there is a missing comma or that you spelled a variable name incorrectly.\n\nCan you find the missed comma above?\n\n\nCapitalization Issues\n\n\nathletes |> filter(sport == \"Tennis\")\n\nIn the original data set, the variable Sport is capitalized. Not capitalizing it means that R won’t be able to find it and proclaims that “object sport not found”.\n\nForgetting Quotes. Character strings need to have quotation marks around them. We will discuss more of this later, but graph labels and titles need to have quotes around them since they don’t directly refer to columns or rows in our data set:\n\n\nggplot(data = athletes, aes(x = Sport, y = endorsements)) + \n  geom_boxplot() + xlab(Popularity Measure)\n\nThe error for forgetting quotes is typically an “Unexpected Symbol” though this error is also given for other issues.\n\nWhere are the quotes missing in the code chunk above?\n\n\nFinally, you can add a comment to a code chunk with the # symbol (I always use double ## for some reason though). This allows you to type a comment into a code chunk that isn’t code:\n\n## this is a comment\n## this calculation might be useful later\n7 * 42\n\n[1] 294\n\n\nComments are most useful for longer code chunks, as they allow you to remember why you did something. They also tell someone whom you’ve shared your code with why you did something.\nSave this file by clicking File -> Save or by using the keyboard shortcut Command + s (or Control + s on a PC). Render this file by clicking the Render button in the top-left window. You should see a .html file pop up, if there are no errors in your code!"
  },
  {
    "objectID": "01-intro.html#chapexercise-1",
    "href": "01-intro.html#chapexercise-1",
    "title": "2  Getting Started with R and R Studio",
    "section": "2.7 Chapter Exercises",
    "text": "2.7 Chapter Exercises\nNote: Usually, exercises will ask you to write code on your own using the week’s chapter as a reference. However, for this initial chapter, we will do something a little different.\nOpen a new .qmd file (File -> New File -> Quarto Document -> OK) and delete the text explaining what Quarto is. Make sure that your Quarto document is self-contained by using something like the following in the first few lines of the file:\n---\ntitle: \"Your Title\"\nauthor: \"Your Name\"\nformat: \n  html:\n    self-contained: true\n---\nThen, complete the following exercises.\nExercise 1. Read the very short paper at https://joss.theoj.org/papers/10.21105/joss.01686 on an Introduction to the tidyverse, and answer the questions below in your Quarto file. I’m imagining this whole exercise should only take you ~ 20-25 minutes.\nAnswer the following questions by typing answers in your .qmd document. You should not need to make any new code chunks, as the questions don’t ask you to do any coding!\n\nWhat are the two major areas that the tidyverse doesn’t provide tools for?\nHow do the authors define “tidy”?\nWhat does it mean for the tidyverse to be “human-centred”?\nIn about 2 sentences, describe the data science “cycle” given in the diagram at the top of page 3.\n\nExercise 2. You may continue to use the same .qmd file to answer these questions. For each question, type your answer on a new line, with a line space between your answers. All of these questions should be answered outside of code chunks since your answers will all be text, not code.\n\nWhat is your name and what is your class year (first-year, sophomore, junior, senior)?\nWhat is/are your major(s) and minor(s), either actual or intended?\nWhy are you taking this course? (Major requirement?, Minor requirement?, recommended by advisor or student?, exploring the field?, etc.). If you are taking it for a major or minor requirement, why did you decide to major or minor in statistics or data science?\nIn what semester and year did you take STAT 113 and who was your professor?\nHave you taken STAT 213? Have you taken CS 140?\nWhat is your hometown: city, state, country?\nDo you play a sport on campus? If so, what sport? If not, what is an activity that you do on or off-campus?\nWhat is your favorite TV show or movie or band/musical artist?\nTell me something about yourself.\nTake a look at the learning outcomes listed on the syllabus. Which are you most excited for and why?\nWhat are your expectations for this class and/or what do you hope to gain from this class?\nTake a moment to scroll through the advice from students who took this course in the Fall semester of 2021. What is one piece of advice that you hope to apply to our course this semester?\n\n\nRender your .qmd file into an .html file and submit your rendered .html file to Canvas. If your file won’t render, then submit the .qmd file instead. To submit either file, you first need to get the file off of the server and onto your computer so that you can upload it to Canvas.\nNice work: we will dive into ggplot() in the ggplot2 package next!"
  },
  {
    "objectID": "01-intro.html#solutions-1",
    "href": "01-intro.html#solutions-1",
    "title": "2  Getting Started with R and R Studio",
    "section": "2.8 Exercise Solutions",
    "text": "2.8 Exercise Solutions\nIn most sections, some exercise solutions will be posted at the end of the section. However, because R is brand new, we will do all of the coding exercises as a class for this first section."
  },
  {
    "objectID": "04-tidyr.html#what-is-tidy-data",
    "href": "04-tidyr.html#what-is-tidy-data",
    "title": "7  Tidying with tidyr",
    "section": "7.1 What is Tidy Data?",
    "text": "7.1 What is Tidy Data?\nR usually (but not always) works best when your data is in tidy form. A tidy data set has a few characteristics. Note that you should already be quite familiar with tidy data because, up to this point, all of the data sets we have used in this class (and probably most of the data sets that you see in STAT 113 an all of the data sets that you may have seen in STAT 213) are tidy. This definition of tidy data is taken from R for Data Science:\n\nevery variable in the data set is stored in its own column\nevery case in the data set is stored in its own row\neach value of a variable is stored in one cell\nvalues in the data set should not contain units\nthere should not be any table headers or footnotes\n\nWe will begin by focusing on the first characteristic: every variable in a the data set should be stored in its own column (and correspondingly, number 3: each value of a variable should be stored in one cell)."
  },
  {
    "objectID": "04-tidyr.html#separate-and-unite-columns",
    "href": "04-tidyr.html#separate-and-unite-columns",
    "title": "7  Tidying with tidyr",
    "section": "7.2 separate() and unite() Columns",
    "text": "7.2 separate() and unite() Columns\nIn a fresh .qmd file (File -> New File -> Quarto) that is in your Notes project, copy and paste the following code into an R chunk:\n\nlibrary(tidyverse)\nlibrary(here)\npolls <- read_csv(here(\"data/rcp-polls.csv\"), na = \"--\")\npolls\n\n# A tibble: 7 × 8\n  Poll                   Date       Sample   MoE Clint…¹ Trump…² Johns…³ Stein…⁴\n  <chr>                  <chr>      <chr>  <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 Monmouth               7/14 - 7/… 688 LV   3.7      45      43       5       1\n2 CNN/ORC                7/13 - 7/… 872 RV   3.5      42      37      13       5\n3 ABC News/Wash Post     7/11 - 7/… 816 RV   4        42      38       8       5\n4 NBC News/Wall St. Jrnl 7/9 - 7/13 1000 …   3.1      41      35      11       6\n5 Economist/YouGov       7/9 - 7/11 932 RV   4.5      40      37       5       2\n6 Associated Press-GfK   7/7 - 7/11 837 RV  NA        40      36       6       2\n7 McClatchy/Marist       7/5 - 7/9  1053 …   3        40      35      10       5\n# … with abbreviated variable names ¹​`Clinton (D)`, ²​`Trump (R)`,\n#   ³​`Johnson (L)`, ⁴​`Stein (G)`\n\n\nSuppose that you wanted to know what the average sample size of the polls was. Using dplyr functions,\n\npolls |> summarise(meansample = mean(Sample))\n\nWhat warning do you get? Why?\nYou would get a similar warning (or sometimes an error) any time that you want to try to use Sample size in plotting or summaries. The issue is that the Sample column actually contains two variables so the data set is not tidy.\n\n7.2.1 separate() a Column\nLet’s separate() the two variables into Sample_size and Sample_type:\n\npolls |>\n  separate(col = Sample, into = c(\"Sample_size\", \"Sample_type\"), \n           sep = \" \")\n\n# A tibble: 7 × 9\n  Poll               Date  Sampl…¹ Sampl…²   MoE Clint…³ Trump…⁴ Johns…⁵ Stein…⁶\n  <chr>              <chr> <chr>   <chr>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 Monmouth           7/14… 688     LV        3.7      45      43       5       1\n2 CNN/ORC            7/13… 872     RV        3.5      42      37      13       5\n3 ABC News/Wash Post 7/11… 816     RV        4        42      38       8       5\n4 NBC News/Wall St.… 7/9 … 1000    RV        3.1      41      35      11       6\n5 Economist/YouGov   7/9 … 932     RV        4.5      40      37       5       2\n6 Associated Press-… 7/7 … 837     RV       NA        40      36       6       2\n7 McClatchy/Marist   7/5 … 1053    RV        3        40      35      10       5\n# … with abbreviated variable names ¹​Sample_size, ²​Sample_type, ³​`Clinton (D)`,\n#   ⁴​`Trump (R)`, ⁵​`Johnson (L)`, ⁶​`Stein (G)`\n\n\nThe arguments to separate() are fairly easy to learn:\n\ncol is the name of the column in the data set you want to separate.\ninto is the name of the new columns. These could be anything you want, and are entered in as a vector (with c() to separate the names)\nsep is the character that you want to separate the column by. In this case, the sample size and sample type were separated by whitespace, so our sep = \" \", white space.\n\nThe sep argument is the newest piece of information here. Note that even using sep = \"\" will produce an error (there is not a space now, so R doesn’t know what to separate by).\n\npolls |>\n  separate(col = Sample, into = c(\"Sample_size\", \"Sample_type\"), \n           sep = \"\")\n\nSimilarly, we would like the Date column to be separated into a poll start date and a poll end date:\n\npolls_sep <- polls |>\n  separate(col = Date, into = c(\"Start\", \"End\"),\n           sep = \" - \")\n\nWhy should you use \" - \" as the separator instead of \"-\"? Try using \"-\" if you aren’t sure: you shouldn’t get an error but something should look off.\nWhat happened to Sample? Why is it back to its un-separated form?\n\n\n7.2.2 unite() Columns\nunite() is the “opposite” of separate(): use it when one variable is stored across multiple columns, but each row still represents a single case. The need to use unite() is less common than separate(). In our current data set, there is no need to use it at all. But, for the sake of seeing an example, let’s separate the Start date into month and day and then use unite() to re-unite those columns:\n\npolls_sillytest <- polls_sep |>\n  separate(col = Start, into = c(\"Start_month\", \"Start_day\"), \n           sep = \"/\")\npolls_sillytest\n\n# A tibble: 7 × 10\n  Poll        Start…¹ Start…² End   Sample   MoE Clint…³ Trump…⁴ Johns…⁵ Stein…⁶\n  <chr>       <chr>   <chr>   <chr> <chr>  <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 Monmouth    7       14      7/16  688 LV   3.7      45      43       5       1\n2 CNN/ORC     7       13      7/16  872 RV   3.5      42      37      13       5\n3 ABC News/W… 7       11      7/14  816 RV   4        42      38       8       5\n4 NBC News/W… 7       9       7/13  1000 …   3.1      41      35      11       6\n5 Economist/… 7       9       7/11  932 RV   4.5      40      37       5       2\n6 Associated… 7       7       7/11  837 RV  NA        40      36       6       2\n7 McClatchy/… 7       5       7/9   1053 …   3        40      35      10       5\n# … with abbreviated variable names ¹​Start_month, ²​Start_day, ³​`Clinton (D)`,\n#   ⁴​`Trump (R)`, ⁵​`Johnson (L)`, ⁶​`Stein (G)`\n\n\nThis situation could occur in practice: the date variable is in multiple columns: one for month and one for day (and if there are multiple years, there could be a third for year). We would use unite() to combine these two columns into a single Date, called New_start_date:\n\npolls_sillytest |>\n  unite(\"New_start_date\", c(Start_month, Start_day),\n        sep = \"/\")\n\n# A tibble: 7 × 9\n  Poll                New_s…¹ End   Sample   MoE Clint…² Trump…³ Johns…⁴ Stein…⁵\n  <chr>               <chr>   <chr> <chr>  <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 Monmouth            7/14    7/16  688 LV   3.7      45      43       5       1\n2 CNN/ORC             7/13    7/16  872 RV   3.5      42      37      13       5\n3 ABC News/Wash Post  7/11    7/14  816 RV   4        42      38       8       5\n4 NBC News/Wall St. … 7/9     7/13  1000 …   3.1      41      35      11       6\n5 Economist/YouGov    7/9     7/11  932 RV   4.5      40      37       5       2\n6 Associated Press-G… 7/7     7/11  837 RV  NA        40      36       6       2\n7 McClatchy/Marist    7/5     7/9   1053 …   3        40      35      10       5\n# … with abbreviated variable names ¹​New_start_date, ²​`Clinton (D)`,\n#   ³​`Trump (R)`, ⁴​`Johnson (L)`, ⁵​`Stein (G)`\n\n\nNote how unite() just switches around the first two arguments of separate(). Argument 1 is now the name of the new column and Argument 2 is the names of columns in the data set that you want to combine.\nWe have also used the c() function in separate() and unite(). While c() is a very general R function and isn’t specific to tidy data, this is the first time that we’re seeing it in this course. c() officially stands for concatenate, but, in simpler terms, c() combines two or more “things”, separated by a comma.\n\nc(1, 4, 2)\n\n[1] 1 4 2\n\nc(\"A\", \"A\", \"D\")\n\n[1] \"A\" \"A\" \"D\"\n\n\nThis is useful if a function argument expects two or more “things”: for example, in separate(), the into argument requires two column names for this example. Those column names must be specified by combining the names together with c().\n\n\n7.2.3 Column Names and rename()\nYou might have noticed that the columns with percentage of votes for Clinton, Trump, etc. are surrounded by backticks ` ` when you print polls or polls_sep:\n\npolls_sep\n\n# A tibble: 7 × 9\n  Poll                  Start End   Sample   MoE Clint…¹ Trump…² Johns…³ Stein…⁴\n  <chr>                 <chr> <chr> <chr>  <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 Monmouth              7/14  7/16  688 LV   3.7      45      43       5       1\n2 CNN/ORC               7/13  7/16  872 RV   3.5      42      37      13       5\n3 ABC News/Wash Post    7/11  7/14  816 RV   4        42      38       8       5\n4 NBC News/Wall St. Jr… 7/9   7/13  1000 …   3.1      41      35      11       6\n5 Economist/YouGov      7/9   7/11  932 RV   4.5      40      37       5       2\n6 Associated Press-GfK  7/7   7/11  837 RV  NA        40      36       6       2\n7 McClatchy/Marist      7/5   7/9   1053 …   3        40      35      10       5\n# … with abbreviated variable names ¹​`Clinton (D)`, ²​`Trump (R)`,\n#   ³​`Johnson (L)`, ⁴​`Stein (G)`\n\n\nThis happens because the column names have a space in them (this also would occur if the columns started with a number or had odd special characters in them). Then, any time you want to reference a variable, you need the include the backticks:\n\npolls_sep |>\n  summarise(meanclinton = mean(Clinton (D))) ## throws an error\npolls_sep |>\n  summarise(meanclinton = mean(`Clinton (D)`)) ## backticks save the day!\n\nHaving variable names with spaces doesn’t technically violate any principle of tidy data, but it can be quite annoying. Always using backticks can be a huge pain. We can rename variables easily with rename(), which just takes a series of new_name = old_name arguments.\n\npolls_new <- polls_sep |>\n  rename(Clinton_D = `Clinton (D)`, Trump_R = `Trump (R)`,\n         Johnson_L = `Johnson (L)`, Stein_G = `Stein (G)`)\npolls_new\n\n# A tibble: 7 × 9\n  Poll                  Start End   Sample   MoE Clint…¹ Trump_R Johns…² Stein_G\n  <chr>                 <chr> <chr> <chr>  <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 Monmouth              7/14  7/16  688 LV   3.7      45      43       5       1\n2 CNN/ORC               7/13  7/16  872 RV   3.5      42      37      13       5\n3 ABC News/Wash Post    7/11  7/14  816 RV   4        42      38       8       5\n4 NBC News/Wall St. Jr… 7/9   7/13  1000 …   3.1      41      35      11       6\n5 Economist/YouGov      7/9   7/11  932 RV   4.5      40      37       5       2\n6 Associated Press-GfK  7/7   7/11  837 RV  NA        40      36       6       2\n7 McClatchy/Marist      7/5   7/9   1053 …   3        40      35      10       5\n# … with abbreviated variable names ¹​Clinton_D, ²​Johnson_L\n\n\nrename() can also be very useful if you have variable names that are very long to type out. rename() is actually from dplyr, not tidyr, but we didn’t have a need for it with any of the dplyr data sets.\n\n\n7.2.4 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-4).\nThe MLB salary data set contains salaries on all 862 players in Major League Baseball in 2016. The data set was obtained from http://www.usatoday.com/sports/mlb/salaries/2016/player/all/\n\nRead in the data using the following code chunk and write a sentence or two that explains why the data set is not tidy.\n\n\nlibrary(tidyverse)\nlibrary(here)\nbaseball_df <- read_csv(here(\"data/mlb2016.csv\"))\nhead(baseball_df)\n\n# A tibble: 6 × 7\n  Name             Team  POS   Salary       Years        Total.Value   Avg.Ann…¹\n  <chr>            <chr> <chr> <chr>        <chr>        <chr>         <chr>    \n1 Clayton Kershaw  LAD   SP    $ 33,000,000 7 (2014-20)  $ 215,000,000 $ 30,714…\n2 Zack Greinke     ARI   SP    $ 31,799,030 6 (2016-21)  $ 206,500,000 $ 34,416…\n3 David Price      BOS   SP    $ 30,000,000 7 (2016-22)  $ 217,000,000 $ 31,000…\n4 Miguel Cabrera   DET   1B    $ 28,000,000 10 (2014-23) $ 292,000,000 $ 29,200…\n5 Justin Verlander DET   SP    $ 28,000,000 7 (2013-19)  $ 180,000,000 $ 25,714…\n6 Yoenis Cespedes  NYM   CF    $ 27,328,046 3 (2016-18)  $ 75,000,000  $ 25,000…\n# … with abbreviated variable name ¹​Avg.Annual\n\n\n\n\n\n\n* Tidy the data set just so that\n\n\nDuration of the salary contract (currently given in the Year column) is in its own column\nthe year range (also currently given in the Year column) is split into a variable called Start and a variable called End year that give the start and end years of the contract. You can still have special characters for now (like ( and )) in the start and end year.\n\n\n\n\n\nYou should have received a warning message. What does this message mean? See if you can figure it out by typing View(baseball_df) in your console window and scrolling down to some of the rows that the warning mentions: 48, 59, 60, etc.\n\n\n\n\n\nWe won’t learn about parse_number() until readr, but the function is straightforward enough to mention here. It’s useful when you have extra characters in the values of a numeric variable (like a $ or a (), but you just want to grab the actual number:\n\n\nbaseball_df <- baseball_df |>\n  mutate(Salary = parse_number(Salary),\n         Total.Value = parse_number(Total.Value),\n         Avg.Annual = parse_number(Avg.Annual),\n         Start = parse_number(Start),\n         End = parse_number(End))\n\nRun the code above so that the parsing is saved to baseball_df.\n\n* Using a function from dplyr. fix the End variable that you created so that, for example, the first observation is 2020 instead of just 20.\n* tidyr is extremely useful, but it’s not glamorous. What you end up with is a data set that ggplot2 and dplyr can use to do cool things. So, let’s do something with our tidy data set to make all that tidying a little worth it before moving on. Make a graphic that investigates how player Salary compares for different POS.\n* State the reason why making that plot would not have worked before we tidied the data set."
  },
  {
    "objectID": "04-tidyr.html#reshaping-with-pivot_",
    "href": "04-tidyr.html#reshaping-with-pivot_",
    "title": "7  Tidying with tidyr",
    "section": "7.3 Reshaping with pivot_()",
    "text": "7.3 Reshaping with pivot_()\nWe will continue to use the polling data set to introduce the pivoting functions and data reshaping. To make sure that we are all working with the same data set, run the following line of code:\n\npolls_clean <- polls |>\n  separate(col = Sample, into = c(\"Sample_size\", \"Sample_type\"), \n           sep = \" \")  |>\n  separate(col = Date, into = c(\"Start\", \"End\"),\n           sep = \" - \") |> \n  rename(Clinton_D = `Clinton (D)`, Trump_R = `Trump (R)`,\n         Johnson_L = `Johnson (L)`, Stein_G = `Stein (G)`)\npolls_clean\n\n# A tibble: 7 × 10\n  Poll         Start End   Sampl…¹ Sampl…²   MoE Clint…³ Trump_R Johns…⁴ Stein_G\n  <chr>        <chr> <chr> <chr>   <chr>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 Monmouth     7/14  7/16  688     LV        3.7      45      43       5       1\n2 CNN/ORC      7/13  7/16  872     RV        3.5      42      37      13       5\n3 ABC News/Wa… 7/11  7/14  816     RV        4        42      38       8       5\n4 NBC News/Wa… 7/9   7/13  1000    RV        3.1      41      35      11       6\n5 Economist/Y… 7/9   7/11  932     RV        4.5      40      37       5       2\n6 Associated … 7/7   7/11  837     RV       NA        40      36       6       2\n7 McClatchy/M… 7/5   7/9   1053    RV        3        40      35      10       5\n# … with abbreviated variable names ¹​Sample_size, ²​Sample_type, ³​Clinton_D,\n#   ⁴​Johnson_L\n\n\nThe data set polls_clean still isn’t tidy!! The candidate variable is spread out over 4 different columns and the values in each of these 4 columns actually represent 1 variable: poll percentage.\nThinking about data “tidyness” using the definitions above can sometimes be a little bit confusing. In practice, oftentimes we will usually realize that a data set is untidy when we go to do something that should be super simple but that something turns out to not be super simple at all when the data is in its current form.\nFor example, one thing we might want to do is to make a plot that has poll Start time on the x-axis, polling numbers on the y-axis, and has candidates represented by different colours. For this small data set, we might not see any trends through time, but you could imagine this graph would be quite useful if we had polling numbers through June, July, August, September, etc.\nTake a moment to think about how you would make this graph in ggplot2: what is your x-axis variable? What variable are you specifying for the y-axis? For the colours?\nA first attempt in making a graph would be:\n\nggplot(data = polls_clean, aes(x = Start, y = Clinton_D)) + \n  geom_point(aes(colour = ....??????????))\n\nAnd we’re stuck. It’s certainly not impossible to make the graph with the data in its current form (keep adding geom_point() and re-specifying the aesthetics, then manually specify colours, then manually specify a legend), but it’s definitely a huge pain.\nThis is where pivot_longer() can help! https://www.youtube.com/watch?v=8w3wmQAMoxQ\n\n7.3.1 pivot_longer() to Gather Columns\npivot_longer() “pivots” the data set so that is has more rows (hence the “longer”) by collapsing multiple columns into two columns. One new column is a “key” column, which is the new variable containing the old data set’s column names. The second new column is a “value” column, which is the new variable containing the old data set’s values for each of the old data set’s column names. It’s easier to see this with an example. We know from our plotting exercise above that we’d really like a candidate variable to colour by and a poll_percent variable for the y-axis of our plot. So, we can use pivot_longer() to make these two columns:\n\npolls_clean |>\n  pivot_longer(cols = c(Clinton_D, Trump_R, Johnson_L, Stein_G),\n               names_to = \"candidate\", values_to = \"poll_percent\")\n\n# A tibble: 28 × 8\n   Poll               Start End   Sample_size Sample_type   MoE candid…¹ poll_…²\n   <chr>              <chr> <chr> <chr>       <chr>       <dbl> <chr>      <dbl>\n 1 Monmouth           7/14  7/16  688         LV            3.7 Clinton…      45\n 2 Monmouth           7/14  7/16  688         LV            3.7 Trump_R       43\n 3 Monmouth           7/14  7/16  688         LV            3.7 Johnson…       5\n 4 Monmouth           7/14  7/16  688         LV            3.7 Stein_G        1\n 5 CNN/ORC            7/13  7/16  872         RV            3.5 Clinton…      42\n 6 CNN/ORC            7/13  7/16  872         RV            3.5 Trump_R       37\n 7 CNN/ORC            7/13  7/16  872         RV            3.5 Johnson…      13\n 8 CNN/ORC            7/13  7/16  872         RV            3.5 Stein_G        5\n 9 ABC News/Wash Post 7/11  7/14  816         RV            4   Clinton…      42\n10 ABC News/Wash Post 7/11  7/14  816         RV            4   Trump_R       38\n# … with 18 more rows, and abbreviated variable names ¹​candidate, ²​poll_percent\n\n\npivot_longer() has three important arguments:\n\ncols, the names of the columns that you want to PIVOT!\nnames_to, the name of the new variable that will have the old column names (anything you want it to be!)\nvalues_to, the name of the new variable that will have the old column values (anything you want it to be!)\n\nWhat happens when you omit names_to and values_to arguments? Give it a try!\nNow we can make our plot using Week 1 ggplot functions. But don’t forget to give a name to our new “long” data set first!\n\npolls_long <- polls_clean |>\n  pivot_longer(cols = c(Clinton_D, Trump_R, Johnson_L, Stein_G),\n               names_to = \"candidate\", values_to = \"poll_percent\")\n\n## ignore as.Date for now....we will get to dates later!\nggplot(data = polls_long,\n       aes(x = as.Date(Start, \"%m/%d\"), y = poll_percent,\n           colour = candidate)) +\n  geom_point() + xlab(\"Poll Start Date\")\n\n\n\n\n\n\n7.3.2 pivot_wider() to Spread to Multiple Columns\nThe “opposite” of pivot_longer() is pivot_wider(). We need to use pivot_wider() when one case is actually spread across multiple rows. Again, I typically will realize there is an issue with untidy data when I go to do something that should be simple and it’s not.\nLet’s examine some airline safety data that fivethirtyeight used in their Should Travelers Avoid Flying Airlines that Have Had Crashes in the Past? story: https://fivethirtyeight.com/features/should-travelers-avoid-flying-airlines-that-have-had-crashes-in-the-past/ . The raw data can be found here.\n\nlibrary(here)\nairlines <- read_csv(here(\"data/airline-safety.csv\"))\nhead(airlines)\n\n# A tibble: 6 × 8\n  airline               avail_…¹ incid…² fatal…³ fatal…⁴ incid…⁵ fatal…⁶ fatal…⁷\n  <chr>                    <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 Aer Lingus              3.21e8       2       0       0       0       0       0\n2 Aeroflot*               1.20e9      76      14     128       6       1      88\n3 Aerolineas Argentinas   3.86e8       6       0       0       1       0       0\n4 Aeromexico*             5.97e8       3       1      64       5       0       0\n5 Air Canada              1.87e9       2       0       0       2       0       0\n6 Air France              3.00e9      14       4      79       6       2     337\n# … with abbreviated variable names ¹​avail_seat_km_per_week,\n#   ²​`incidents 1985_1999`, ³​`fatal_accidents 1985_1999`,\n#   ⁴​`fatalities 1985_1999`, ⁵​`incidents 2000_2014`,\n#   ⁶​`fatal_accidents 2000_2014`, ⁷​`fatalities 2000_2014`\n\n\nThe data set contains the following columns:\n\nairline, the name of the airline\navail_seat_km_per_week, the available seat kilometers flown each week\nincidents 1985_1999, the number of incidents between 1985 and 1999\nfatal_accidents 1985_1999, the number of fatal accidents between 1985 and 1999\nfatalities 1985_1999, the number of fatalities between 1985 and 1999\nincidents 2000_2014\nfatal_accidents 2000_2014\nfatalities 2000_2014\n\nThere’s a whole lot of mess in this data set: we really want a variable for year that has two values (1985-1999 and 2000-2014). Sometimes it’s tough to know where to even start, but one strategy is to draw a sketch of a data frame that you’d like to end with. For example, we think that we want a data set with the following columns: airline, available seat km, years, incidents, fatal accidents, and fatalities. So, our sketch might look something like:\n\n\n\nairline\navail\nyears\nincidents\nfatalacc\nfatalities\n\n\n\n\nairline1\n1009391\n1985-1999\n0\n1\n2\n\n\nairline1\n1009391\n2000-2014\n9\n1\n1\n\n\nairline2\n2141\n1985-1999\n2\n0\n0\n\n\n\netc.\nLet’s start with pivot_longer() to see if we can get year to be its own variable (We know that a year variable, which is what we want, will make more rows so pivot_longer() seems like a good place to start):\n\nairlines |>\n  pivot_longer(c(3, 4, 5, 6, 7, 8), names_to = \"type_year\",\n  values_to = \"total_num\") \n\n# A tibble: 336 × 4\n   airline    avail_seat_km_per_week type_year                 total_num\n   <chr>                       <dbl> <chr>                         <dbl>\n 1 Aer Lingus              320906734 incidents 1985_1999               2\n 2 Aer Lingus              320906734 fatal_accidents 1985_1999         0\n 3 Aer Lingus              320906734 fatalities 1985_1999              0\n 4 Aer Lingus              320906734 incidents 2000_2014               0\n 5 Aer Lingus              320906734 fatal_accidents 2000_2014         0\n 6 Aer Lingus              320906734 fatalities 2000_2014              0\n 7 Aeroflot*              1197672318 incidents 1985_1999              76\n 8 Aeroflot*              1197672318 fatal_accidents 1985_1999        14\n 9 Aeroflot*              1197672318 fatalities 1985_1999            128\n10 Aeroflot*              1197672318 incidents 2000_2014               6\n# … with 326 more rows\n\n\nInstead of giving pivot_longer() names of variables, we gave it the column numbers instead. So c(3, 4, 5, 6, 7, 8) corresponds to the 3rd, 4th, …., 8th columns in the data set. That didn’t quite give us a year variable, but we should be excited to see an opportunity to take advantage of separate():\n\nairlines |> pivot_longer(c(3, 4, 5, 6, 7, 8), names_to = \"type_year\",\n                          values_to = \"total_num\") |>\n  separate(type_year, into = c(\"type\", \"year\"), sep = \" \")\n\n# A tibble: 336 × 5\n   airline    avail_seat_km_per_week type            year      total_num\n   <chr>                       <dbl> <chr>           <chr>         <dbl>\n 1 Aer Lingus              320906734 incidents       1985_1999         2\n 2 Aer Lingus              320906734 fatal_accidents 1985_1999         0\n 3 Aer Lingus              320906734 fatalities      1985_1999         0\n 4 Aer Lingus              320906734 incidents       2000_2014         0\n 5 Aer Lingus              320906734 fatal_accidents 2000_2014         0\n 6 Aer Lingus              320906734 fatalities      2000_2014         0\n 7 Aeroflot*              1197672318 incidents       1985_1999        76\n 8 Aeroflot*              1197672318 fatal_accidents 1985_1999        14\n 9 Aeroflot*              1197672318 fatalities      1985_1999       128\n10 Aeroflot*              1197672318 incidents       2000_2014         6\n# … with 326 more rows\n\n\nIs this the format that we want the data set to be in? Depending on the task, it could be. But, we also might want each of the accident types to be its own variable. That is, we might want to collapse the data set to have a variable for incidents, a variable for fatal_accidents, and a variable for fatalities. If so, we want to add more columns to the data set, so we need to use pivot_wider().\n\n## name the long data set\nairlines_long <- airlines |>\n  pivot_longer(c(3, 4, 5, 6, 7, 8), names_to = \"type_year\",\n               values_to = \"total_num\") |>\n  separate(type_year, into = c(\"type\", \"year\"), sep = \" \")\n\n## use pivot_wider() to create variables for incidents, fatalities, and\n## fatal_accidents:\nairlines_long |> pivot_wider(names_from = type,\n                              values_from = total_num)\n\n# A tibble: 112 × 6\n   airline               avail_seat_km_per_week year     incid…¹ fatal…² fatal…³\n   <chr>                                  <dbl> <chr>      <dbl>   <dbl>   <dbl>\n 1 Aer Lingus                         320906734 1985_19…       2       0       0\n 2 Aer Lingus                         320906734 2000_20…       0       0       0\n 3 Aeroflot*                         1197672318 1985_19…      76      14     128\n 4 Aeroflot*                         1197672318 2000_20…       6       1      88\n 5 Aerolineas Argentinas              385803648 1985_19…       6       0       0\n 6 Aerolineas Argentinas              385803648 2000_20…       1       0       0\n 7 Aeromexico*                        596871813 1985_19…       3       1      64\n 8 Aeromexico*                        596871813 2000_20…       5       0       0\n 9 Air Canada                        1865253802 1985_19…       2       0       0\n10 Air Canada                        1865253802 2000_20…       2       0       0\n# … with 102 more rows, and abbreviated variable names ¹​incidents,\n#   ²​fatal_accidents, ³​fatalities\n\n\npivot_wider() has two main arguments:\n\nnames_from, the column in the old data set that will provide the names of the new columns and\nvalues_from, the column in the old data set that will provide the values that fill in the new columns\n\nWe will see more examples of pivot_wider() and pivot_longer() in the Exercises. Note that tidy data isn’t necessarily always better: you might find cases where you need to “untidy” the data by using pivot_longer() or pivot_wider(). However, most functions in R (and in other languages) work best with tidy data.\nThere are a few more topics to discuss in tidying data. We have not yet discussed the 4th or 5th characteristics of tidy data (cells should not contain units and there should be no headers or footers), but these are usually dealt with when we read in the data. Therefore, these issues will be covered when we discuss readr.\n\n\n7.3.3 Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-4).\n\nOnce you have a handle on data science terminology, it’s not too difficult to transfer what you’ve learned to a different language. For example, students in computer science might be more familiar with Python. Google something like “pivot from wide to long in python” to find help on achieving the equivalent of pivot_longer() in Python.\n\n\n\n\nThe UBSprices2 data set contains information on prices of common commodities in cities throughout the world in the years 2003 and 2009. The three commodities in the data set are Rice (1 kg worth), Bread (1 kg worth), and a Big Mac https://media1.giphy.com/media/Fw5LicDKem6nC/source.gif\n\nprices_df <- read_csv(here(\"data/UBSprices2.csv\"))\n\n\n* Convert the data set to a tidier form so that there is a year variable and a commodity variable that has 3 values: \"bigmac\", \"bread\", and \"rice\"\n\nHint: At some point, you will need to separate the commodity from the year in, for example, bread2009. But, you’ll notice this is different than our other uses of separate() because there is no “-” or ” ” or “/” to use as a separating character. Look at the help for separate() and scroll down to the sep argument to see if you can figure out this issue. The first code chunk below shows the solution for this particular issue in case you only get stuck on this part while the second code chunk shows the entire solution.\n\nseparate(name_of_variable, into = c(\"newname1\", \"newname2\"), sep = -4)\n\n\n* Convert your data set from the previous exercise so that commodity is split up into 3 variables: bigmac price, rice price and bread price.\nIn which data set would it be easiest to make a line plot with year on the x-axis and price of rice on the y-axis with lines for each city? In which data set would it be easiest to make a line chart with 3 lines, one for each type of commodity, for the city of Amsterdam?\n\nIf you have time, make these plots!\n\n\n\nNew Data:\nThe under5mortality.csv file contains data on mortality for people under the age of 5 in countries around the world (mortality in deaths per 1000 people). The data come from https://www.gapminder.org/data/. The data set is extremely wide in its current form, having a column for each year in the data set. Read in the data set with\n\nmortality_df <- read_csv(here(\"data/under5mortality.csv\"))\nhead(mortality_df)\n\n# A tibble: 6 × 217\n  Under …¹ `1800` `1801` `1802` `1803` `1804` `1805` `1806` `1807` `1808` `1809`\n  <chr>     <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n1 Abkhazia    NA     NA     NA     NA     NA     NA     NA     NA     NA     NA \n2 Afghani…   469.   469.   469.   469.   469.   469.   470.   470.   470.   470.\n3 Akrotir…    NA     NA     NA     NA     NA     NA     NA     NA     NA     NA \n4 Albania    375.   375.   375.   375.   375.   375.   375.   375.   375.   375.\n5 Algeria    460.   460.   460.   460.   460.   460.   460.   460.   460.   460.\n6 America…    NA     NA     NA     NA     NA     NA     NA     NA     NA     NA \n# … with 206 more variables: `1810` <dbl>, `1811` <dbl>, `1812` <dbl>,\n#   `1813` <dbl>, `1814` <dbl>, `1815` <dbl>, `1816` <dbl>, `1817` <dbl>,\n#   `1818` <dbl>, `1819` <dbl>, `1820` <dbl>, `1821` <dbl>, `1822` <dbl>,\n#   `1823` <dbl>, `1824` <dbl>, `1825` <dbl>, `1826` <dbl>, `1827` <dbl>,\n#   `1828` <dbl>, `1829` <dbl>, `1830` <dbl>, `1831` <dbl>, `1832` <dbl>,\n#   `1833` <dbl>, `1834` <dbl>, `1835` <dbl>, `1836` <dbl>, `1837` <dbl>,\n#   `1838` <dbl>, `1839` <dbl>, `1840` <dbl>, `1841` <dbl>, `1842` <dbl>, …\n\n\n\n* Notice that there are 217 columns (at the top of the print out of the header, 217 is the second number). When we use tidyr, we aren’t going to want to type out c(2, 3, 4, 5, .....) all the way up to 217! R has short-hand notation that we can use with :. For example, type in 4:9 in your console window. Use this notation to tidy the mortality_df data set.\n\nNote: You’ll need to add something to your pivot_longer() function to convert the variable Year to numeric. We haven’t talked too much about variable types yet so, after your values_to = \"Mortality\" statement, add , names_transform = list(Year = as.numeric), making sure you have a second ) to close the pivot_longer() function.\n\n\n\n\nMake a line plot to look at the overall under 5 mortality trends for each country.\n\n\n\n\n\nWhat is the overall trend in under 5 mortality? Does every single country follow this trend? What looks strange about the plot, specifically about the data collected before 1900?\n\n\n\n\n\nWrite two short paragraphs about an article found on https://www.r-bloggers.com/. The most important thing for this exercise is to pick an article that interests you. There are many to choose from, with multiple posts being put up each day. For the purposes of this assignment though, find an article where the author actually provides code (most have this but there are a few that are more “big-picture” views of certain topics).\n\nIn the first paragraph, answer the following: (a) What is the main purpose of the blog post? (b) What data is/are the author(s) using? (c) What are the main findings? (d) Why was it important for the author to have data in their main findings?\nIn the second paragraph, discuss (a) Any code that you see that we have explicitly seen in class, (b) Any code that you see that we have not explicitly seen in class, and (c) anything else you find interesting about your article. Then, copy and paste the URL."
  },
  {
    "objectID": "04-tidyr.html#chapexercise-4",
    "href": "04-tidyr.html#chapexercise-4",
    "title": "7  Tidying with tidyr",
    "section": "7.4 Chapter Exercises",
    "text": "7.4 Chapter Exercises\nExercises marked with an * indicate that the exercise has a solution at the end of the chapter at @ref(solutions-4).\nWe will use nfl salary data obtained from FiveThirtyEight that were originally obtained from Spotrac.com.\nThe data set has the top 100 paid players for each year for each position from 2011 through 2018, broken down by player position. For those unfamiliar with American football, the positions in the data set are Quarterback, Running Back, Wide Receiver, Tight End, and Offensive Lineman for offense, Cornerback, Defensive Lineman, Linebacker, and Safety for Defense, and a separate category for Special Teams players that includes punters and kickers. You can review a summary of player positions  here.\nWe are interested in how salaries compare for the top 100 players in each position and on how salaries have changed through time for each position.\nRead in the data set with\n\nnfl_df <- read_csv(here(\"data/nfl_salary.csv\"))\n\n\nUse the head() functions to look at the data, and then explain why this data set is not in tidy form.\n\n\n\n\n\n* Use a function in tidyr to make the data tidy, and give your tidy data set a new name.\n* To your data set in the previous exercise, add a ranking variable that ranks the salaries within each player position so that the highest paid players in each position all receive a 1, the second highest paid players receive a 2, etc. Compare your results for the default way that R uses to break ties between two salaries that are the same and using ties.method = \"first\".\n\nHint: See Exercise 4 in @ref(exercise-3-3) for another example on how to do this.\n\n* Find the maximum salary for each player position in each year. Then, create two different line graphs that shows how the maximum salary has changed from 2011 to 2018 for each position. For one line graph, make the colours of the lines different for each position. For the second line graph, facet by position. Which graph do you like better?\n* The maximum salary is very dependent on one specific player. Make the same graph, but plot the average salary of the top 20 players in each position of each year. What do you notice? Any interesting patterns for any of the positions? If you’re a fan of football, provide a guess as to why one of the positions has had their salary plateau in recent years.\n* Sometimes for graphs involving cost or salary, we want to take into account the inflation rate. Google what the inflation rate was between 2011 and 2018 (Google something like “inflation rate from 2011 to 2018” and you should be able to find something). Adjust all of the 2011 salaries for inflation so that they are comparable to the 2018 salaries. Then, make a similar line plot as above but ignore all of the years between 2012 and 2017 (so your line plot will just have 2 points per position).\n\nAfter adjusting for inflation, how many positions have average higher salaries for the top 20 players in that position?\n\nConstruct a graph that shows how much salary decreases moving from higher ranked players to lower ranked players for each position in the year 2018. Why do you think the depreciation is so large for Quarterbacks?"
  },
  {
    "objectID": "04-tidyr.html#solutions-4",
    "href": "04-tidyr.html#solutions-4",
    "title": "7  Tidying with tidyr",
    "section": "7.5 Exercise Solutions",
    "text": "7.5 Exercise Solutions\n\n7.5.1 What is Tidy Data? S\n\n\n7.5.2 separate() and unite() S\n\n* Tidy the data set just so that\n\n\nDuration of the salary contract (currently given in the Year column) is in its own column\nthe year range (also currently given in the Year column) is split into a variable called Start and a variable called End year that give the start and end years of the contract. You can still have special characters for now (like ( and )) in the start and end year.\n\n\nbaseball_df <- baseball_df |>\n  separate(Years, into = c(\"Duration\", \"Range\"), sep = \" \") |>\n  separate(Range, into = c(\"Start\", \"End\"), sep = \"-\")\n\n\n* Using a function from dplyr. fix the End variable that you created so that, for example, the first observation is 2020 instead of just 20.\n\n\nbaseball_df <- baseball_df |> mutate(End = End + 2000)\n\nThis is a somewhat lazy way to do this and could get you into trouble (what if one of the years was in the 1990s?) But, it’s safe for this particular data set.\n\n* tidyr is extremely useful, but it’s not glamorous. What you end up with is a data set that ggplot2 and dplyr can use to do cool things. So, let’s do something with our tidy data set to make all that tidying a little worth it before moving on. Make a graphic that investigates how player Salary compares for different POS.\n\n\nggplot(data = baseball_df, aes(x = POS, y = Salary)) +\n  geom_boxplot()\nggplot(data = baseball_df, aes(x = Salary, colour = POS)) + \n  geom_freqpoly() ## boxplots look better in this case\n\n\n* State the reason why making that plot would not have worked before we tidied the data set.\n\nThe Salary variable had a dollar sign in it so ggplot would not have known how to plot it.\n\n\n7.5.3 pivot_() S\n\n* Convert the data set to a tidier form so that there is a year variable and a commodity variable that has 3 values: \"bigmac\", \"bread\", and \"rice\"\n\nHint: At some point, you will need to separate the commodity from the year in, for example, bread2009. But, you’ll notice this is different than our other uses of separate() because there is no “-” or ” ” or “/” to use as a separating character. Look at the help for separate() and scroll down to the sep argument to see if you can figure out this issue. The first code chunk below shows the solution for this particular issue in case you only get stuck on this part while the second code chunk shows the entire solution.\n\nseparate(name_of_variable, into = c(\"newname1\", \"newname2\"), sep = -4)\n\n\nprices_long <- prices_df |> pivot_longer(cols = c(2, 3, 4, 5, 6, 7),\n  names_to = \"commod_year\", values_to = \"price\") |>\n  separate(col = \"commod_year\", into = c(\"commodity\", \"year\"), sep = -4)\nhead(prices_long)\n\n# A tibble: 6 × 4\n  city      commodity year  price\n  <chr>     <chr>     <chr> <dbl>\n1 Amsterdam bigmac    2009     19\n2 Amsterdam bread     2009     10\n3 Amsterdam rice      2009     11\n4 Amsterdam bigmac    2003     16\n5 Amsterdam bread     2003      9\n6 Amsterdam rice      2003      9\n\n\n\n* Convert your data set from the previous exercise so that commodity is split up into 3 variables: bigmac price, rice price and bread price.\n\n\nprices_wide <- prices_long |>\n  pivot_wider(names_from = commodity, values_from = price)\nhead(prices_wide)\n\n# A tibble: 6 × 5\n  city      year  bigmac bread  rice\n  <chr>     <chr>  <dbl> <dbl> <dbl>\n1 Amsterdam 2009      19    10    11\n2 Amsterdam 2003      16     9     9\n3 Athens    2009      30    13    27\n4 Athens    2003      21    12    19\n5 Auckland  2009      19    19    13\n6 Auckland  2003      19    19     9\n\n\n\n* Notice that there are 217 columns (at the top of the print out of the header, 217 is the second number). When we use tidyr, we aren’t going to want to type out c(2, 3, 4, 5, .....) all the way up to 217! R has short-hand notation that we can use with :. For example, type in 4:9 in your console window. Use this notation to tidy the mortality_df data set.\n\n\nmortality_long <- mortality_df |>\n  pivot_longer(cols = 2:217, names_to = \"Year\",\n               values_to = \"Mortality\",\n               names_transform = list(Year = as.numeric))\n\nYou’ll need to add something to your pivot_longer() function to convert the variable Year to numeric. We haven’t talked too much about variable types yet so, after your values_to = \"Mortality\" statement, add , names_transform = list(Year = as.numeric), making sure you have a second ) to close the pivot_longer() function.\n\n\n7.5.4 Chapter Exercises S\n\n* Use a function in tidyr to make the data tidy, and give your tidy data set a new name.\n\n\nnfl_long <- nfl_df |>\n  pivot_longer(c(2, 3, 4, 5, 6, 7, 8, 9, 10, 11),\n               names_to = \"position\", values_to = \"salary\")\nnfl_long\n\n# A tibble: 8,000 × 3\n    year position            salary\n   <dbl> <chr>                <dbl>\n 1  2011 Cornerback        11265916\n 2  2011 Defensive Lineman 17818000\n 3  2011 Linebacker        16420000\n 4  2011 Offensive Lineman 15960000\n 5  2011 Quarterback       17228125\n 6  2011 Running Back      12955000\n 7  2011 Safety             8871428\n 8  2011 Special Teamer     4300000\n 9  2011 Tight End          8734375\n10  2011 Wide Receiver     16250000\n# … with 7,990 more rows\n\n\n\n* To your data set in the previous exercise, add a ranking variable that ranks the salaries within each player position in each year so that the highest paid players in each position in each year all receive a 1, the second highest paid players receive a 2, etc. Compare your results for the default way that R uses to break ties between two salaries that are the same and using ties.method = \"first\".\n\nHint: See Exercise 4 in @ref(exercise-3-3) for another example on how to do this.\n\nnfl_long_default <- nfl_long |> group_by(position, year) |>\n  mutate(rank = rank(desc(salary)))\n\nnfl_long <- nfl_long |> group_by(position, year) |>\n  mutate(rank = rank(desc(salary), ties.method = \"first\"))\n\nThe first ranking code allows observations that have the same ranking get their rankings averaged together (e.g. two observations tied for 5th would get a ranking of (5 + 6) / 2 = 5.5).\nIn the second ranking method, the first observation in the data set gets the “higher” rank.\n\n* Find the maximum salary for each player position in each year. Then, create two different line graphs that shows how the maximum salary has changed from 2011 to 2018 for each position. For one line graph, make the colours of the lines different for each position. For the second line graph, facet by position. Which graph do you like better?\n\n\nnfl_max <- nfl_long |> group_by(position, year) |>\n  summarise(maxsal = max(salary, na.rm = TRUE))\n\n`summarise()` has grouped output by 'position'. You can override using the\n`.groups` argument.\n\nggplot(data = nfl_max,\n  aes(x = year, y = maxsal, group = position, colour = position)) +\n  geom_line()\n\n\n\nggplot(data = nfl_max, aes(x = year, y = maxsal)) +\n  geom_line() +\n  facet_wrap( ~ position)\n\n\n\n\nWith this number of levels, I personally prefer the faceted graph for its cleaner look.\n\n* The maximum salary is very dependent on one specific player. Make the same graph, but plot the average salary of the top 20 players in each position of each year. What do you notice? Any interesting patterns for any of the positions? If you’re a fan of football, provide a guess as to why one of the positions has had their salary plateau in recent years.\n\n\nnfl_rank <- nfl_long |> filter(rank <= 20) |>\n  group_by(position, year) |>\n  summarise(mean20 = mean(salary, na.rm = TRUE))\n\n`summarise()` has grouped output by 'position'. You can override using the\n`.groups` argument.\n\nggplot(data = nfl_rank, aes(x = year, y = mean20)) +\n  geom_line() + \n  facet_wrap( ~ position)\n\n\n\n\nRunning backs haven’t had much of a salary increase whereas all other offensive positions have had a large salary increase. There are many plausible explanations for why this is the case. One is that the NFL is much more of a “passing league” now than it was decades ago.\n\n* Sometimes for graphs involving cost or salary, we want to take into account the inflation rate. Google what the inflation rate was between 2011 and 2018 (Google something like “inflation rate from 2011 to 2018” and you should be able to find something). Adjust all of the 2011 salaries for inflation so that they are comparable to the 2018 salaries. Then, make a similar line plot as above but ignore all of the years between 2012 and 2017 (so your line plot will just have 2 points per position).\n\nAfter adjusting for inflation, how many positions have average higher salaries for the top 20 players in that position?\n\n## 11.6% from 2011 to 2018.\nnfl_inf <- nfl_long |>\n  mutate(salary_inf = if_else(year == 2011,\n                              true = salary * 1.116,\n                              false = salary)) |> \n  filter(year == 2011 | year == 2018) |> \n  filter(rank <= 20) |> group_by(position, year) |>\n  summarise(mean20 = mean(salary, na.rm = TRUE)) \n\n`summarise()` has grouped output by 'position'. You can override using the\n`.groups` argument.\n\nggplot(data = nfl_inf, aes(x = year, y = mean20)) +\n  geom_line() +\n  geom_point() +\n  facet_wrap( ~ position)\n\n\n\n\nAll positions have higher salaries, even after adjusting for inflation, except perhaps running backs (it’s too hard to tell from the graph)."
  },
  {
    "objectID": "04-tidyr.html#rcode-4",
    "href": "04-tidyr.html#rcode-4",
    "title": "7  Tidying with tidyr",
    "section": "7.6 Non-Exercise R Code",
    "text": "7.6 Non-Exercise R Code\n\nlibrary(tidyverse)\nlibrary(here)\npolls <- read_csv(here(\"data/rcp-polls.csv\"), na = \"--\")\npolls\npolls |> summarise(meansample = mean(Sample))\npolls |>\n  separate(col = Sample, into = c(\"Sample_size\", \"Sample_type\"), \n           sep = \" \")\npolls_sep <- polls |>\n  separate(col = Date, into = c(\"Start\", \"End\"),\n           sep = \" - \")\npolls_sillytest <- polls_sep |>\n  separate(col = Start, into = c(\"Start_month\", \"Start_day\"), \n           sep = \"/\")\npolls_sillytest\npolls_sillytest |>\n  unite(\"New_start_date\", c(Start_month, Start_day),\n        sep = \"/\")\nc(1, 4, 2)\nc(\"A\", \"A\", \"D\")\npolls_sep\npolls_new <- polls_sep |>\n  rename(Clinton_D = `Clinton (D)`, Trump_R = `Trump (R)`,\n         Johnson_L = `Johnson (L)`, Stein_G = `Stein (G)`)\npolls_new\npolls_clean <- polls |>\n  separate(col = Sample, into = c(\"Sample_size\", \"Sample_type\"), \n           sep = \" \")  |>\n  separate(col = Date, into = c(\"Start\", \"End\"),\n           sep = \" - \") |> \n  rename(Clinton_D = `Clinton (D)`, Trump_R = `Trump (R)`,\n         Johnson_L = `Johnson (L)`, Stein_G = `Stein (G)`)\npolls_clean\npolls_clean |>\n  pivot_longer(cols = c(Clinton_D, Trump_R, Johnson_L, Stein_G),\n               names_to = \"candidate\", values_to = \"poll_percent\")\npolls_long <- polls_clean |>\n  pivot_longer(cols = c(Clinton_D, Trump_R, Johnson_L, Stein_G),\n               names_to = \"candidate\", values_to = \"poll_percent\")\n\n## ignore as.Date for now....we will get to dates later!\nggplot(data = polls_long,\n       aes(x = as.Date(Start, \"%m/%d\"), y = poll_percent,\n           colour = candidate)) +\n  geom_point() + xlab(\"Poll Start Date\")\nlibrary(here)\nairlines <- read_csv(here(\"data/airline-safety.csv\"))\nhead(airlines)\nairlines |>\n  pivot_longer(c(3, 4, 5, 6, 7, 8), names_to = \"type_year\",\n  values_to = \"total_num\") \nairlines |> pivot_longer(c(3, 4, 5, 6, 7, 8), names_to = \"type_year\",\n                          values_to = \"total_num\") |>\n  separate(type_year, into = c(\"type\", \"year\"), sep = \" \")\n## name the long data set\nairlines_long <- airlines |>\n  pivot_longer(c(3, 4, 5, 6, 7, 8), names_to = \"type_year\",\n               values_to = \"total_num\") |>\n  separate(type_year, into = c(\"type\", \"year\"), sep = \" \")\n\n## use pivot_wider() to create variables for incidents, fatalities, and\n## fatal_accidents:\nairlines_long |> pivot_wider(names_from = type,\n                              values_from = total_num)"
  }
]
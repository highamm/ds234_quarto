[["index.html", "STAT 234: Data Science 1 Syllabus and Course Information 1.1 General Information 1.2 Course Information 1.3 General Course Outcomes 1.4 How You Will Be Assessed 1.5 Tentative Schedule", " STAT 234: Data Science Matt Higham 2021-08-09 1 Syllabus and Course Information 1.1 General Information Instructor Information Professor: Matt Higham Office: Bewkes 123 Email: mhigham@stlawu.edu Semester: Fall 2021 Office Hours: Tuesday 1:30 - 3:30 Wednesday 2:30 - 3:30 Friday 10:00 - 11:00 other times by appointment all in-person, unless otherwise requested Sections: MW 8:50 - 10:20 Course Materials STAT 234 Materials Bundle. This will be our primary source of materials. Textbooks (only used as references): Modern Data Science with R by Baumer, Kaplan, and Horton, found here in a free online version. R for Data Science by Grolemund and Wickham, found here in a free online version. Computer with Internet access. 1.2 Course Information Welcome to STAT 234! The overall purpose of this course is learn the data science skills necessary to complete large-scale data analysis projects. The tool that we will be using to achieve this goal is the statistical software language R. We will work with a wide variety of interesting data sets throughout the semester to build our R skills. In particular, we will focus on the Data Analysis Life Cycle (Grolemund and Wickham 2020): We will put more emphasis on the Import, Tidy, Transform, Visualize, and Communicate parts of the cycle, as an introduction to Modeling part is covered in STAT 213. 1.3 General Course Outcomes Import data of a few different types into R for analysis. Tidy data into a form that can be more easily visualized, summarised, and modeled. Transform, Wrangle, and Visualize variables in a data set to assess patterns in the data. Communicate the results of your analysis to a target audience with a written report, or, possibly an oral presentation. Practice reproducible statistical practices through the use of R Markdown for data analysis projects. Explain why it is ethically important to consider the context that a data set comes in. Develop the necessary skills to be able to ask and answer future data analysis questions on your own, either using R or another program, such as Python. To paraphrase the R for Data Science textbook, about 80% of the skills necessary to do a complete data analysis project can be learned through coursework in classes like this one. But, 20% of any particular project will involve learning new things that are specific to that project. Achieving Goal # 6 will allow you to learn this extra 20% on your own. 1.3.1 Use of R and RStudio We will use the statistical software R to construct graphs and analyze data. A few notes: R and RStudio are both free to use. We will primarily be using the SLU R Studio server at first: Link to R Studio Server. Additionally, we will be using RMarkdown for data analysis reports. Note: It’s always nice to start assignments and projects as early as possible, but this is particularly important to do for assignments and projects involving R. It’s no fun to try and figure out why code is not working at the last minute. If you start early enough though, you will have plenty of time to seek help and therefore won’t waste a lot of time on a coding error. 1.4 How You Will Be Assessed The components to your grade are described below: Class Class participation will be assessed three times throughout the semester in a 20 point rubric for a total of 60 points. Additionally, there will be a 10-point “share something interesting you found with the class” assignment on very Wednesday, where, two students will volunteer to….share something interesting that they found with the data set we were working with with the rest of the class. The rubric used will be shared on the first day of class, and more information about the Wednesday 10 points will also be given on the first day of class. Exercises There are about 14 sets of weekly exercises that often require you to read some of the sections in the STAT 234 Materials Bundle first. These are worth either 10 or 5 points, depending on the length of the exercises, for a total of 100 points. Most weeks toward the beginning of the semester will be 10 point weeks because we won’t have any projects to work on. Exercises are graded for completion only: for many exercises, the solutions are provided in our course materials. Quizzes There will be 10 Quizzes, each worth 20 points for a total of 180 points with one dropped quiz. The purpose of the quizzes are for you to practice what you’ve learned for the week in a short, concise format. Quizzes will consist of two parts: (1) a take-home component and (2) an in-class component. The take-home component should take about 15 minutes. You are allowed use any course materials and you are allowed to work with other students in this course, as long as you list the names of those students at the top of your quiz. The in-class component will be 5 minutes. You will be asked to do a simple task with pen and paper, without using any course notes or materials. Mini Projects There are 3 mini-projects scattered throughout the semester that are worth 60 points each. Each mini-project will have some prescriptive tasks and questions that you will investigate as well as a section where you come up with and subsequently answer your own questions relating to the data set. In order to get experience with oral presentations of results, each student will give a short oral presentation on 1 mini-project. Use of R Markdown is required for this presentation (as opposed to PowerPoint or Prezi). More details will be given later in the semester. Midterm Exams There will be two midterm exams, each worth 150 points. More information will be given about these later. Final Project There is one final project, worth 150 points. The primary purpose of the final project is to give you an opportunity to assemble topics throughout the course into one coherent data analysis. More information about the final project will be given later. There will be no Final Exam for this course. 1.4.1 Breakdown 70 points for Class 100 points for Exercises 180 points for Quizzes 180 points for Mini-Projects + 20 points for Presentation 150 points for each of two Midterm Exams 150 points for Final Project Points add up to 1000 so your grade at the end of the semester will be the number of points you’ve earned across all categories divided by 1000. The tutorials should help you complete the exercises sets, which should help you to do well on the quizzes, which should help you complete the mini-projects, which should help you to do well on the midterm exams. Then, everything together should help you create an awesome final project! 1.4.2 Grading Scale The following is a rough grading scale. I reserve the right to make any changes to the scale if necessary. Grade 4.0 3.75 3.5 3.25 3.0 2.75 2.5 2.25 2.0 1.75 1.5 1.25 1.0 0.0 Points 950-1000 920-949 890-919 860-889 830-859 810-829 770-809 750-769 720-749 700-719 670-699 640-669 600-639 0-599 1.4.3 Rules for Collaboration Collaboration with your classmates on handouts, tutorials, and projects is encouraged, but you must follow these guidelines: you must state the name(s) of who you collaborated with at the top of each assessment. all work must be your own. This means that you should never send someone your code via email or let someone directly type code off of your screen. Instead, you can talk about strategies for solving problems and help or ask someone about a coding error. you may use the Internet and StackExchange, but you also should not copy paste code directly from the website, without citing that you did so. this isn’t a rule, but keep in mind that collaboration is not permitted on quizzes, exams, and very limited collaboration will be permitted on the final project. Therefore, when working with someone, make sure that you are both really learning so that you both can have success on the non-collaborative assessments. 1.4.4 Diversity Statement Diversity encompasses differences in age, colour, ethnicity, national origin, gender, physical or mental ability, religion, socioeconomic background, veteran status, sexual orientation, and marginalized groups. The interaction of different human characteristics brings about a positive learning environment. Diversity is both respected and valued in this classroom. 1.4.5 Accessibility Statement If you have a learning difference/disability or other health impairment and need accommodations please be sure to contact the Student Accessibility Services Office right away so they can help you get the accommodations you require. If you need to use any accommodations in this class, please meet with your instructor early and provide them with your Individualized Educational Accommodation Plan (IEAP) letter so you can have the best possible experience this semester. Although not required, your instructor would like to know of any accommodations that are needed at least 10 days before a quiz or test. Please be proactive and set up an appointment to meet with someone from the Student Accessibility Services Office. Color-Vision Deficiency: If you are Color-Vision Deficient, the Student Accessibility Services office has on loan glasses for students who are color vision deficient. Please contact the office to make an appointment. For more specific information about setting up an appointment with Student Accessibility Services please see the listed options below: Telephone: 315.229.5537 Email: studentaccessibility@stlawu.edu For further information about Student Accessibility Services you can check the website at: https://www.stlawu.edu/student-accessibility-services 1.4.6 Academic Dishonesty Academic dishonesty will not be tolerated. Any specific policies for this course are supplementary to the Honor Code. According to the St. Lawrence University Academic Honor Policy, It is assumed that all work is done by the student unless the instructor/mentor/employer gives specific permission for collaboration. Cheating on examinations and tests consists of knowingly giving or using or attempting to use unauthorized assistance during examinations or tests. Dishonesty in work outside of examinations and tests consists of handing in or presenting as original work which is not original, where originality is required. Claims of ignorance and academic or personal pressure are unacceptable as excuses for academic dishonesty. Students must learn what constitutes one's own work and how the work of others must be acknowledged. For more information, refer to www.stlawu.edu/acadaffairs/academic_honor_policy.pdf. To avoid academic dishonesty, it is important that you follow all directions and collaboration rules and ask for clarification if you have any questions about what is acceptable for a particular assignment or exam. If I suspect academic dishonesty, a score of zero will be given for the entire assignment in which the academic dishonesty occurred for all individuals involved and Academic Honor Council will be notified. If a pattern of academic dishonesty is found to have occurred, a grade of 0.0 for the entire course can be given. It is important to work in a way that maximizes your learning. Be aware that students who rely too much on others for the homework and projects tend to do poorly on the quizzes and exams. Please note that in addition the above, any assignments in which your score is reduced due to academic dishonesty will not be dropped according to the quiz policy e.g., if you receive a zero on a quiz because of academic dishonesty, it will not be dropped from your grade. 1.5 Tentative Schedule Week Date Topics 0 8/25 Introduction to R, R Studio 1 8/30 Graphics with ggplot2 2 9/6 Data Wrangling and Transformation with dplyr 3 9/13 Data Tidying with tidyr 4 9/20 Communication with R Markdown and ggplot2 5 9/27 Basic Coding in R 6 10/4 Catch-up and Midterm 1 7 10/11 Factors with forcats and Data Ethics 8 10/18 Data Import with readr, jsonlite, rvest, and tibble 9 10/25 Data Merging with dplyr 10 11/1 Dates and Times with lubridate 11 11/8 Strings with stringr 12 11/15 Catch-up and Midterm 2 13 11/22 Thanksgiving Break 14 11/29 Predictive Modeling Final Project 14 12/6 Final Project The three mini-projects are tentatively scheduled to be due on September 27, October 25, and November 8, though these are subject to change. There will be no Final Exam, but keep your schedule open at our Final Exam time in case we decide to use it for something. "],["intro.html", " 2 Getting Started with R and R Studio 2.1 Intro to R and R Studio 2.2 What are R, R Studio, and R Markdown? 2.3 Putting Code in a .Rmd File 2.4 Alcohol Data Example 2.5 Athlete Data Example 2.6 Finishing Up: Common Errors in R 2.7 Chapter Exercises 2.8 Exercise Solutions", " 2 Getting Started with R and R Studio Goals: Use R Studio on the server Use R Markdown and code chunks Load in data to R Studio Run code and change a few things within that code Correct some common errors when running code in R 2.1 Intro to R and R Studio R is a statistical computing software used by many statisticians as well as professionals in other fields, such as biology, ecology, business, and psychology. The goal of Week 0 is to provide basic familiarity with R and R Markdown, which we will be using for the entire semester. Open R Studio on the SLU R Studio server at http://rstudio.stlawu.local:8787 and create a folder called STAT_234 or some other meaningful title to you. Note that you must be on campus to use the R Studio server, unless you use a VPN. Directions on how to set-up VPN are https://infotech.stlawu.edu/support/content/11269 &lt;&gt; for Macs and https://stlawu.teamdynamix.com/TDClient/1805/Portal/KB/ArticleDet?ID=55118 for Windows. Next, create a subfolder within your STAT_234 folder. Title it Notes (or whatever you want really). Then, create an R Project by Clicking File -&gt; New Project -&gt; Existing Directory, navigate to the Notes folder, and click Create Project. Within this folder, click the New Folder button in your bottom-left window and name a new folder data. Then, download the data.zip file from Sakai (in Resources). Upload that file in to the server by clicking “Upload” in the bottom right panel. In the dialog box that appears, you can click “Choose File” and navigate to the folder where you saved the zip file (probably Downloads by default). The zip file will automatically expand once uploaded. It includes data sets that we will use throughout the course. Finally, we want to create a new R Markdown file by clicking File -&gt; New File -&gt; R Markdown. You can give your new R Markdown file a title if you want, and then click okay. Before moving on, click the Knit button in the top-left window at the top of the menu bar (look for the knitting needle icon). Make sure that the file knits to a pretty-looking .html file. The newly knitted .html file can now be found in your folder with your R project. 2.2 What are R, R Studio, and R Markdown? The distinction between the 3 will become more clear later on. For now, * R is a statistical coding software used heavily for data analysis and statistical procedures. R Studio is a nice IDE (Integrated Development Environment) for R that has a lot of convenient features. Think of this as just a convenient User Interface. R Mardkown allows users to mix regular Microsoft-Word-style text with code. The .Rmd file ending denotes an R Mardkown file. R Markdown has many options that we will use heavily throughout the semester, but there’s no need to worry about these now. 2.2.1 R Packages and the tidyverse You can think of R packages as add-ons to R that let you do things that R on its own would not be able to do. If you’re in to video games, you can think of R packages as extra Downloadable Content (DLC). But, unlike most gaming DLC, R packages are always free and we will make very heavy use of R packages. The tidyverse is a series of R packages that are useful for data science. In the order that we will encounter them in this class, the core tidyverse packages are: ggplot2 for plotting data dplyr for data wrangling and summarizing tidyr for data tidying and reshaping readr for data import tibble for how data is stored stringr for text data forcats for factor (categorical) data purrr, for functional programming, the only one of these core 8 that we won’t get to use We will use packages outside of the core tidyverse as well, but the tidyverse is the main focus. We are going to change one option before proceeding. In the top file menu, click Tools -&gt; Global Options -&gt; R Markdown and then uncheck the box that says “Show output inline for all R Markdown documents.” Don’t worry about this for now, but changing this option just means that code results will appear in the bottom-left window and graphs will appear in the bottom-right window of R Studio. 2.3 Putting Code in a .Rmd File The first thing that we will do that involves code is to load a package into R with the library() function. A package is just an R add-on that lets you do more than you could with just R on its own. Load the tidyverse package into R by typing and running the library(tidyverse) line. To create a code chunk, click Insert -&gt; R. Within this code chunk, type in library(tidyverse) and run the code by either Clicking the “Run” button in the menu bar of the top-left window of R Studio or (Recommended) Clicking “Command + Enter” on a Mac or “Control + Enter” on a PC. Note that all code appears in grey boxes surrounded by three backticks while normal text has a different colour background with no backticks. library(tidyverse) When you run the previous line, some text will appear in the bottom-left window. We won’t worry too much about what this text means now, but we also won’t ignore it completely. You should be able to spot the 8 core tidyverse packages listed above as well as some numbers that follow each package. The numbers correspond to the package version. There’s some other things too, but as long as this text does not start with “Error:” you’re good to go! Congrats on running your first line of code for this class! This particular code isn’t particularly exciting because it doesn’t really do anything that we can see. We have run R code using an R chunk. In your R chunk, on a new line, try typing in a basic calculation, like 71 + 9 or 4 / 3, them run the line and observe the result. So, that still wasn’t super exciting. R can perform basic calculations, but you could just use a calculator or Excel for that. In order to look at things that are a bit more interesting, we need some data. 2.4 Alcohol Data Example We will be looking at two data sets just to get a little bit of a preview of things we will be working on for the rest of the semester. Important: Do not worry about understanding what the following code is doing at this point. There will be plenty of time to understand this in the weeks ahead. The purpose of this section is just to get used to using R: there will be more detailed explanations and exercises about the functions used and various options in the coming weeks. In particular, the following code uses the ggplot2, dplyr, and tidyr packages, which we will cover in detail throughout the first ~ 3-4 weeks of this course. Data for this first part was obtained from fivethirtyeight at Five Thirty Eight GitHub page. The first step is to read the data set into R. Though you have already downloaded alcohol.csv in the data zip, we still need to load it into R. Check to make sure the alcohol.csv is in the data folder in your bottom-right hand window. The following code can be copied to an R code chunk to read in the data: read_csv(&quot;data/alcohol.csv&quot;) Note that we do not need the full file extension if we have the data set in an R project. Did something show up in your console window? If so, great! If not, make sure that the data set is in the data folder and that you have an R project set up. We would like to name our data set something so that we could easily reference it later, so name your data set using the &lt;- operator, as in alcohol_data &lt;- read_csv(&quot;data/alcohol.csv&quot;) You can name your data set whatever you want to (with a few restrictions). I’ve named it alcohol_data. Now, if you run the line of code above where you name the data set, and run alcohol_data, you should see the data set appear: alcohol_data #&gt; # A tibble: 193 x 5 #&gt; country beer_servings spirit_servings wine_servings #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Afghanistan 0 0 0 #&gt; 2 Albania 89 132 54 #&gt; 3 Algeria 25 0 14 #&gt; 4 Andorra 245 138 312 #&gt; 5 Angola 217 57 45 #&gt; 6 Antigua &amp; Ba… 102 128 45 #&gt; 7 Argentina 193 25 221 #&gt; 8 Armenia 21 179 11 #&gt; 9 Australia 261 72 212 #&gt; 10 Austria 279 75 191 #&gt; # … with 183 more rows, and 1 more variable: #&gt; # total_litres_of_pure_alcohol &lt;dbl&gt; What’s in this data set? We see a few variables on the columns: country: the name of the country beer_servings: the average number of beer servings per person per year spirit_servings: the average number of spirit (hard alcohol) servings per person per year wine_servings: the average number of wine servings per person per year total_litres_of_pure_alcohol: the average total litres of pure alcohol consumed per person per year. One goal of this class is for you to be able to pose questions about a data set and then use the tools we will learn to answer those questions. For example, we might want to know what the distribution of total litres of alcohol consumed per person looks like across countries. To do this, we can make a plot with the ggplot2 package, one of the packages that automatically loads with tidyverse. We might start by constructing the following plot. Reminder: the goal of this is not for everyone to understand the code in this plot, so don’t worry too much about that. ggplot(data = alcohol_data, mapping = aes(total_litres_of_pure_alcohol)) + geom_histogram(colour = &quot;black&quot;, fill = &quot;white&quot;, bins = 15) I now want to see where the United States (USA) falls on this distribution by drawing a red vertical line for the total litres of alcohol consumed in the United States. To do so, I’ll first use the filter() function in the dplyr package (again, we will learn about that function in detail later). Copy and paste the following lines of code into a new R chunk. Then, run the lines. small_df &lt;- alcohol_data %&gt;% filter(country == &quot;USA&quot;) ggplot(data = alcohol_data, mapping = aes(total_litres_of_pure_alcohol)) + geom_histogram(colour = &quot;black&quot;, fill = &quot;white&quot;, bins = 15) + geom_vline(data = small_df, aes(xintercept = total_litres_of_pure_alcohol), colour = &quot;red&quot;) It looks like there are some countries that consume little to no alcohol. We might want to know what these countries are: alcohol_data %&gt;% filter(total_litres_of_pure_alcohol == 0) #&gt; # A tibble: 13 x 5 #&gt; country beer_servings spirit_servings wine_servings #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Afghanistan 0 0 0 #&gt; 2 Bangladesh 0 0 0 #&gt; 3 North Korea 0 0 0 #&gt; 4 Iran 0 0 0 #&gt; 5 Kuwait 0 0 0 #&gt; 6 Libya 0 0 0 #&gt; 7 Maldives 0 0 0 #&gt; 8 Marshall Isl… 0 0 0 #&gt; 9 Mauritania 0 0 0 #&gt; 10 Monaco 0 0 0 #&gt; 11 Pakistan 0 0 0 #&gt; 12 San Marino 0 0 0 #&gt; 13 Somalia 0 0 0 #&gt; # … with 1 more variable: #&gt; # total_litres_of_pure_alcohol &lt;dbl&gt; It looks like there are 13 countries in the data set that consume no alcohol. Note that, in the chunk above, we have to use in total_litres_of_pure_alcohol as the variable name because this is the name of the variable in the data set. Even something like spelling litres in the American English liters (total_liters_of_pure_alcohol) would throw an error because this isn’t the exact name of the variable in the data set. This is something that can be very aggravating when you are first learning any coding language. Now suppose that we want to know the 3 countries that consume the most beer, the 3 countries that consume the most spirits, and the 3 countries that consume the most wine per person. If you’re a trivia person, you can form some guesses. Without cheating, I am going to guess (Germany, USA, and UK) for beer, (Spain, Italy, and USA) for wine, and (Russia, Poland, and Lithuania) for spirits. Let’s do beer first! alcohol_data %&gt;% mutate(rankbeer = rank(desc(beer_servings))) %&gt;% arrange(rankbeer) %&gt;% filter(rankbeer &lt;= 3) Let’s do the same thing for Wine and Spirits: alcohol_data %&gt;% mutate(rankwine = rank(desc(wine_servings))) %&gt;% arrange(rankwine) %&gt;% filter(rankwine &lt;= 3) alcohol_data %&gt;% mutate(rankspirits = rank(desc(spirit_servings))) %&gt;% arrange(rankspirits) %&gt;% filter(rankspirits &lt;= 3) Finally, suppose that I want to know which country consumes the most wine relative to their beer consumption? Let’s first look at this question graphically. I need to tidy the data first with the pivot_longer() function from the tidyr package: onecountry_df &lt;- alcohol_data %&gt;% filter(country == &quot;Denmark&quot;) library(ggrepel) ggplot(data = alcohol_data, mapping = aes(x = beer_servings, y = wine_servings)) + geom_point(alpha = 0.5) + geom_label_repel(data = onecountry_df, aes(label = country), colour = &quot;purple&quot;) + geom_point(data = onecountry_df, colour = &quot;purple&quot;, size = 2.5, shape = 1) + geom_abline(aes(slope = 1, intercept = 0), alpha = 0.3) The x-axis corresponds to beer servings while the y-axis corresponds to wine servings. A reference line is given so with countries above the line consuming more wine than beer. We will get into how to make a plot like this later: for now, copy the code chunk and change the labeled point so that it corresponds to a country that interests you (other than Denmark). We might be able to better answer the original question numerically by computing the wine to beer ratio for each country and then ordering from the largest ratio to the smallest ratio: alcohol_data %&gt;% mutate(wbratio = wine_servings / beer_servings) %&gt;% arrange(desc(wbratio)) %&gt;% select(country, beer_servings, wine_servings, wbratio) #&gt; # A tibble: 193 x 4 #&gt; country beer_servings wine_servings wbratio #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Cook Islands 0 74 Inf #&gt; 2 Qatar 1 7 7 #&gt; 3 Montenegro 31 128 4.13 #&gt; 4 Timor-Leste 1 4 4 #&gt; 5 Syria 5 16 3.2 #&gt; 6 France 127 370 2.91 #&gt; 7 Georgia 52 149 2.87 #&gt; 8 Italy 85 237 2.79 #&gt; 9 Equatorial Guinea 92 233 2.53 #&gt; 10 Sao Tome &amp; Principe 56 140 2.5 #&gt; # … with 183 more rows Why is one of the ratios Inf? 2.4.1 Exercises What is the shape of the distribution of total alcohol consumption? Left-skewed, right-skewed, or approximately symmetric? Unimodal or multimodal? In the histogram of total alcohol consumption, pick a country other than the USA that interests you. See if you can change the code in the chunk that made the histogram so that the red vertical line is drawn for the country that interests you. Hint: Use the View() function to look at the alcohol data set by typing View(alcohol_data) in your bottom-left window to help you see which countries are in the data set. View(alcohol_data) Note: careful about capitalization: R is case sensitive so USA is different than usa. In the histogram of total alcohol consumption, change the fill colour of the bins in the histogram above: what should be changed in the code chunk? In the rankings code, what if you wanted to look at the top 5 countries instead of the top 3? See if you could change the code. In the spirit rankings, why do you think only 2 countries showed up instead of 3? Can you do any investigation as to why this is the case? Change the wine to beer ratio code example to find the countries with the highest beer to wine consumption (instead of wine to beer consumption). 2.5 Athlete Data Example Secondly, we will look at a data set on the top 100 highest paid athletes in 2014. The athletesdata was obtained from https://github.com/ali-ce/datasets data set has information on the following variables from the 100 highest paid athletes of 2014, according to Forbes (pay includes both salary and endorsements): Name (name of the athlete) Rank (where the athlete ranks, with 1 being the highest paid) Sport (the sport the athlete plays) endorsements (money from sponsorships from companies) totalpay (in millions in the year of 2014, salary + endorsements) salary (money from tournaments or contract salary) age of athlete in 2014 Gender (Male or Female) We will first read in the data set below and name it athletes. We can then use the head() function to look at the first few rows of the data set. athletes &lt;- read_csv(&quot;data/athletesdata.csv&quot;) head(athletes) #&gt; # A tibble: 6 x 9 #&gt; X1 Name Rank Sport endorsements totalpay salary age #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 Aaro… 55 Foot… 7500000 22000000 1.45e7 31 #&gt; 2 2 Adam… 95 Golf 9000000 17700000 8.7 e6 34 #&gt; 3 3 Adri… 60 Base… 400000 21500000 2.11e7 32 #&gt; 4 4 Alex… 48 Base… 300000 22900000 2.26e7 39 #&gt; 5 5 Alfo… 93 Base… 50000 18050000 1.8 e7 38 #&gt; 6 6 Amar… 27 Bask… 5000000 26700000 2.17e7 32 #&gt; # … with 1 more variable: Gender &lt;chr&gt; There are many different interesting questions to answer with this data set. First, we might be interested in the relationship between athlete age and salary for the top 100 athletes. Recall from an earlier stat course that one appropriate graphic to examine this relationship is a scatterplot: ggplot(data = athletes, mapping = aes(x = age, y = salary)) + geom_point() + geom_smooth(se = FALSE) Do you see anything strange with the scatterplot? What do you think the y-axis tick labels of 2.5e+07, 5.0e+07, etc. mean? Now let’s see if we can count the number of athletes in the Top 100 that are in my personal favourite sport, Tennis: athletes %&gt;% group_by(Sport) %&gt;% summarise(counts = n()) %&gt;% filter(Sport == &quot;Tennis&quot;) #&gt; # A tibble: 1 x 2 #&gt; Sport counts #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Tennis 6 It looks like there are 6 athletes: we can see who they are and sort them by their Rank with: athletes %&gt;% filter(Sport == &quot;Tennis&quot;) %&gt;% arrange(Rank) #&gt; # A tibble: 6 x 9 #&gt; X1 Name Rank Sport endorsements totalpay salary age #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 82 Roge… 7 Tenn… 52000000 56200000 4.2 e6 33 #&gt; 2 78 Rafa… 9 Tenn… 30000000 44500000 1.45e7 28 #&gt; 3 72 Nova… 17 Tenn… 21000000 33100000 1.21e7 27 #&gt; 4 64 Mari… 34 Tenn… 22000000 24400000 2.4 e6 27 #&gt; 5 60 Li Na 41 Tenn… 18000000 23600000 5.6 e6 32 #&gt; 6 89 Sere… 55 Tenn… 11000000 22000000 1.1 e7 33 #&gt; # … with 1 more variable: Gender &lt;chr&gt; Finally, let’s see if we can compare the ratio of endorsements (from commercials and products) to salary of professional athletes in the Top 100 in 2 sports: Football (referring to American Football) and Basketball. Recall from an earlier Stat class that we might want to use side-by-side boxplots to make this comparison since we have one categorical variable (Sport Type) and one quantitative variable (Ratio of Endorsements to Salary). athletes %&gt;% filter(Sport == &quot;Football&quot; | Sport == &quot;Basketball&quot;) %&gt;% ggplot(data = ., aes(x = Sport, y = endorsements / salary)) + geom_boxplot() + labs(y = &quot;Endorsements / Salary&quot;) In the graph an endorsements / salary ratio of 1 indicates that the person makes half of their overall pay from endorsements and half of their overall pay from salary. Which sport looks like it tends to receive a larger proportion of their overall pay from endorsements for athletes in the top 100? 2.5.1 Exercises Instead of looking at the relationship between age and salary in the top 100 athletes of 2014, change the plot to look at the relationship between age and endorsements. What would you change in the code above? Try it! Pick a Sport other than Tennis and see if you can count the number of athletes in the top 100 in that sport as well as sort them by Rank. Careful: not all sports will have athletes in the Top 100. How many athletes are in the top 100 in the sport that you chose? In the endorsements / salary example, change one of the sports to the sport of your choice and make a comparison. Which sport tends to receive a larger proportion of their overall pay from endorsements. What qualification might you want to make about your statement in the previous exercise? (Is this a random sample of athletes from each sport? Why does that matter?). In the side-by-side boxplots comparing the endorsements to salary ratio of two different sports, I’ve changed the y-axis label above to be Endorsements / Salary using the labs(y = \"Endorsements / Salary\") statement. Try changing the x-axis label to something else. What do you think you would need to add to the plot? 2.6 Finishing Up: Common Errors in R We will now talk a little bit about getting errors in R and what can be done to correct some common errors. You may have encountered some errors by this point in the document. Let’s go over a few common errors as well as discuss how to comment your code. A missing parenthesis: any open parenthesis ( needs to close ). Try running the following code chunk without fixing anything. ggplot(data = athletes, aes(x = Sport, y = salary) + geom_boxplot() Notice in your bottom-left window that the &gt; symbol that starts a line changes to a +. This is generally bad!! It means that you forgot to close a parenthesis ) or a quote (' or \"). No code will run since R thinks you are still trying to type something into a function. To fix this issue, click your cursor into the bottom-left window and press Esc. Then, try to find the error in the code chunk. Can you find the missing closing parenthesis above? Missing Comma. Try running the following code chunk without fixing anything. ggplot(data = athletes aes(x = Sport, y = salary)) + geom_boxplot() R gives you an “Error: unexpected symbol in ….” Oftentimes, this means that there is a missing comma or that you spelled a variable name incorrectly. Can you find the missed comma above? Capitalization Issues athletes %&gt;% filter(sport == &quot;Tennis&quot;) In the original data set, the variable Sport is capitalized. Not capitalizing it means that R won’t be able to find it and proclaims that “object sport not found.” Forgetting Quotes. Character strings need to have quotation marks around them. We will discuss more of this later, but graph labels and titles need to have quotes around them since they don’t directly refer to columns or rows in our data set: ggplot(data = athletes, aes(x = Sport, y = endorsements)) + geom_boxplot() + xlab(Popularity Measure) The error for forgetting quotes is typically an “Unexpected Symbol” though this error is also given for other issues. Where are the quotes missing in the code chunk above? Finally, you can add a comment to a code chunk with the # symbol (I always use double ## for some reason though). This allows you to type a comment into a code chunk that isn’t code: ## this is a comment ## this calculation might be useful later 7 * 42 #&gt; [1] 294 Comments are most useful for longer code chunks, as they allow you to remember why you did something. They also tell someone whom you’ve shared your code with why you did something. Save this file by clicking File -&gt; Save or by using the keyboard shortcut Command + s (or Control + s on a PC). Knit this file by clicking the Knit button in the top-left window (with the knitting needles). You should see a .html file pop up, if there are no errors in your code! 2.7 Chapter Exercises Note: Usually, exercises will ask you to write code on your own using the week’s chapter as a reference. However, for this initial chapter, we will do something a little different. Open a new .Rmd file (File -&gt; New File -&gt; R Markdown -&gt; OK) and delete the text explaining what R Markdown is in lines 10 and below. Then, complete the following exercises. Exercise 1. Read the very short paper at https://joss.theoj.org/papers/10.21105/joss.01686 on an Introduction to the tidyverse, and answer the questions below in your R Markdown file. I’m imagining this whole exercise should only take you ~ 20-25 minutes. Answer the following questions by typing answers in your .Rmd document. You should not need to make any new code chunks, as the questions don’t ask you to do any coding! What are the two major areas that the tidyverse doesn’t provide tools for? How do the authors define “tidy?” What does it mean for the tidyverse to be “human-centred?” In about 2 sentences, describe the data science “cycle” given in the diagram at the top of page 3. Exercise 2. You may continue to use the same .Rmd file to answer these questions. For each question, type your answer on a new line, with a line space between your answers. All of these questions should be answered outside of code chunks since your answers will all be text, not code. What is your name and what is your class year (first-year, sophomore, junior, senior)? What is/are your major(s) and minor(s), either actual or intended? Why are you taking this course? (Major requirement?, Minor requirement?, recommended by advisor or student?, exploring the field?, etc.). In what semester and year did you take STAT 113 and who was your professor? Have you taken STAT 213? Have you taken CS 140? What is your hometown: city, state, country? Do you play a sport on campus? If so, what sport? If not, what is an activity that you do on or off-campus? What is your favorite TV show or movie or band/musical artist? Tell me something about yourself. Take a look at the learning outcomes listed on the syllabus. Which are you most excited for and why? What are your expectations for this class and/or what do you hope to gain from this class? Knit your .Rmd file into an .html file and submit your knitted .html file to Sakai. If your file won’t knit, then submit the .Rmd file instead. To submit either file, you first need to get the file off of the server and onto your computer so that you can upload it to Sakai. Use the following steps to do so: Click the checkbox next to your knitted .html file. Click the Gear Icon “More” -&gt; Export If you would like, rename your file to something like Week0_YOURLASTNAME.html, but, make sure to keep the correct extension (either .html or .Rmd). After you export it, the file should appear in your downloads folder. Now, go to Sakai -&gt; Assignments -&gt; Week 0 Exercises and complete the upload process. Nice work: we will dive into ggplot() in the ggplot2 package next! 2.8 Exercise Solutions In most sections, some exercise solutions will be posted at the end of the section. However, for the introduction, we will do all of the coding exercises as a class to make sure that we all start off well. "],["ggplot2.html", " 3 Plotting with ggplot2 3.1 Introduction and Basic Terminology 3.2 Basic Plot Structure 3.3 Graphing a Single Variable 3.4 Graphing Two Quantitative Variables, Faceting, and aes() Options 3.5 Boxplots, Stacked Barplots and Others 3.6 Chapter Exercises 3.7 Exercise Solutions 3.8 Non-Exercise R Code", " 3 Plotting with ggplot2 Goals: Use the ggplot2 package to make exploratory plots from STAT 113 of a single quantitative variable, two quantitative variables, a quantitative and a categorical variable, a single categorical variable, and two categorical variables. Use the plots produced to answer questions about the Presidential election data set and the Fitness data set. Further practice running code in R. 3.1 Introduction and Basic Terminology We will begin our data science journey with plotting in the ggplot2 package. We are starting with plotting for a couple of reasons: Plotting is cool! We get to see an immediate result of our coding efforts in the form of a nice-to-look-at plot. In an exploratory data analysis, you would typically start by making plots of your data. Plotting can lead us to ask and subsequently investigate interesting questions, as we will see in our first example. We will first use a data set on the 2000 United States Presidential election between former President George Bush and Al Gore obtained from http://www.econometrics.com/intro/votes.htm. For those unfamiliar with U.S. political elections, it is enough to know that each state is allocated a certain number of “electoral votes” for the president: states award all of their electoral votes to the candidate that receives the most ballots in that state. You can read more about this strange system on Wikipedia. Florida is typically a highly-contentious “battleground” state. The data set that we have has the following variables, recorded for each of the 67 counties in Florida: Gore, the number of people who voted for Al Gore in 2000 Bush, the number of people who voted for George Bush in 2000 Buchanan, the number of people who voted for the third-party candidate Buchanan Nader, the number of people who voted for the third-party candidate Nader Other, the number of people who voted for a candidate other than the previous 4 listed County, the name of the county in Florida To get started exploring the data, complete the following steps that you learned in Week 0: Log-on to the SLU R Studio server http://rstudio.stlawu.local:8787 Create a new .Rmd file in the same folder as your Notes R Project using File -&gt; New File -&gt; R Markdown. Finally, read in and name the data set pres_df, and take a look at the data set by running the head(pres_df) line, which shows the first few observations of the data set: library(tidyverse) pres_df &lt;- read_table(&quot;data/PRES2000.txt&quot;) ## don&#39;t worry about the `read_table` function....yet head(pres_df) #&gt; # A tibble: 6 x 6 #&gt; Gore Bush Buchanan Nader Other County #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 47365 34124 263 3226 751 ALACHUA #&gt; 2 2392 5610 73 53 26 BAKER #&gt; 3 18850 38637 248 828 242 BAY #&gt; 4 3075 5414 65 84 35 BRADFORD #&gt; 5 97318 115185 570 4470 852 BREVARD #&gt; 6 386561 177323 788 7101 1623 BROWAR Pay special attention to the variable names: we’ll need to use these names when we make all of our plots. And, R is case-sensitive, meaning that we will, for example, need to use Gore, not gore. We are trying to go very light on the technical code terminology to start out with (but we will come back to some things later in the semester). The terminology will make a lot more sense once you’ve actually worked with data. But, there are three terms that will be thrown around quite a bit in the next few weeks: function, argument, and object. a function in R is always* (*always for this class) followed by an open ( and ended with a closed ). In non-technical terms, a function does something to its inputs and is often analogous to an English verb. For example, the mean() function calculates the mean, the rank() functions ranks a variable from lowest to highest, and the labs() is used to add labels to a plot. Every function has a help file that can be accessed by typing in ?name_of_function. Try typing ?mean in your lower left window. an argument is something that goes inside the parentheses in a function. Arguments could include objects, or they might not. In the bottom-left window, type ?mean to view the Help file on this R function. We see that mean() has 3 arguments: x, which is an R object, trim, and na.rm. trim = 0 is the default, which means that, by default, R will not trim any of the numbers when computing the mean. an object is something created in R, usually with &lt;-. So, looking at the code above where we read in the data, pres_df is an R object. All of this will make more sense as we go through these first couple of weeks. 3.2 Basic Plot Structure We will use the ggplot() function in the ggplot2 package to construct visualizations of data. the ggplot() function has 3 basic components: a data argument, specifying the name of your data set (pres_df above) a mapping argument, specifying that specifies the aesthetics of your plot (aes()). Common aesthetics are x position, y position, colour, size, shape, group, and fill. a geom_ () component, specifying the geometric shape used to display the data. The components are combined in the following form: ggplot(data = name_of_data, mapping = aes(x = name_of_x_var, y = name_of_y_var, colour = name_of_colour_var, etc.)) + geom_nameofgeom() + .....&lt;other stuff&gt; The structure of ggplot() plots is based on the Grammar of Graphics https://www.springer.com/gp/book/9780387245447. As with most new things, the components above will be easier to think about with some examples. 3.3 Graphing a Single Variable 3.3.1 Histograms and Frequency Plots for a Quantitative Variable Let’s go ahead and begin our exploration of the data by making a histogram of the number of people who voted for Gore in each county. Recall that a histogram is useful if we would like a graph of a single quantitative variable. Copy the following code to an R chunk and run the code: ggplot(data = pres_df, mapping = aes(x = Gore)) + geom_histogram(colour = &quot;black&quot;, fill = &quot;white&quot;) + xlab(&quot;Votes for Gore in Florida&quot;) #&gt; `stat_bin()` using `bins = 30`. Pick better value with #&gt; `binwidth`. What do the 1e+05, 2e+05, etc. labels on the x-axis mean? R gives us a message to “Pick a better value with binwidth” instead of the default bins = 30. Add , bins = 15 inside the parentheses of geom_histogram() to change the number of bins in the histogram. Change the colour of the inside of the bins to “darkred.” Do you think that the colour of the inside of the bins maps to colour or fill? Try both! There are a couple of observations with very high vote values. What could explain these large outliers? Another graph useful in visualizing a single quantitative variable is a frequency plot. The code to make a frequency plot is given below. We are simply replacing geom_histogram() with geom_freqpoly(). ggplot(data = pres_df, mapping = aes(x = Gore)) + geom_freqpoly(colour = &quot;black&quot;) + xlab(&quot;Votes for Gore in Florida&quot;) #&gt; `stat_bin()` using `bins = 30`. Pick better value with #&gt; `binwidth`. The frequency plot is just like a histogram but the counts are connected by a line instead of represented with bins. You can see how they relate by including both a geom_freqpoly() and a geom_histogram() in your plot, though it doesn’t make for the prettiest graph: ggplot(data = pres_df, mapping = aes(x = Gore)) + geom_freqpoly(colour = &quot;black&quot;) + xlab(&quot;Votes for Gore in Florida&quot;) + geom_histogram() #&gt; `stat_bin()` using `bins = 30`. Pick better value with #&gt; `binwidth`. #&gt; `stat_bin()` using `bins = 30`. Pick better value with #&gt; `binwidth`. 3.3.2 R Code Style We want our code to be as readable as possible. This not only benefits other people who may read your code (like me), but it also benefits you, particularly if you read your own code in the future. I try to follow the Style Guide in the Advanced R book: http://adv-r.had.co.nz/Style.html. Feel free to skim through that, but you don’t need to worry about it too much: you should be able to pick up on some important elements just from going through this course. You might actually end up having better code style if you haven’t had any previous coding experience. As a quick example of why code style can be important, consider the following two code chunks, both of which produce the same graph. ggplot(data=pres_df,mapping=aes(x=Gore))+geom_histogram(colour=&quot;black&quot;,fill=&quot;white&quot;)+ xlab(&quot;Votes for Gore in Florida&quot;) ggplot(data = pres_df, mapping = aes(x = Gore)) + geom_histogram(colour = &quot;black&quot;, fill = &quot;white&quot;) + xlab(&quot;Votes for Gore in Florida&quot;) Which code chunk would you want to read two years from now? Which code chunk would you want your classmate/friend/coworker to read? (assuming you like your classmate/friend/coworker….) 3.3.3 Bar Plots for a Categorical Variable Recall from STAT 113 that bar plots are useful if you want to examine the distribution of one categorical variable. Side-by-side bar plots or stacked bar plots are plots that are useful for looking at the relationship between two categorical variables. There actually aren’t any categorical variables that would be interesting to plot in this data set, so we’ll make one, called winner using code that we don’t need to understand until next week. winner will be \"Gore\" if Gore won the county and \"Bush\" if Bush won the county. We’ll name this new data set pres_cat. pres_cat &lt;- pres_df %&gt;% mutate(winner = if_else(Gore &gt; Bush, true = &quot;Gore&quot;, false = &quot;Bush&quot;)) pres_cat #&gt; # A tibble: 67 x 7 #&gt; Gore Bush Buchanan Nader Other County winner #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 47365 34124 263 3226 751 ALACHUA Gore #&gt; 2 2392 5610 73 53 26 BAKER Bush #&gt; 3 18850 38637 248 828 242 BAY Bush #&gt; 4 3075 5414 65 84 35 BRADFORD Bush #&gt; 5 97318 115185 570 4470 852 BREVARD Bush #&gt; 6 386561 177323 788 7101 1623 BROWAR Gore #&gt; 7 2155 2873 90 39 17 CALHOUN Bush #&gt; 8 29645 35426 182 1462 181 CHARLOTTE Bush #&gt; 9 25525 29765 270 1379 261 CITRUS Bush #&gt; 10 14632 41736 186 562 237 CLAY Bush #&gt; # … with 57 more rows Using this data set, we can make a bar plot with geom_bar(). The beauty of ggplot() is that the code is super-similar to what we used for histograms and frequency plots! ggplot(data = pres_cat, aes(x = winner)) + geom_bar() Note that, sometimes, data are in format such that one column contains the levels of the categorical variable while another column contains the counts directly. For example, we can create such a data set using code that we will learn next week: pres_cat2 &lt;- pres_cat %&gt;% group_by(winner) %&gt;% summarise(nwins = n()) pres_cat2 #&gt; # A tibble: 2 x 2 #&gt; winner nwins #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Bush 51 #&gt; 2 Gore 16 This data set has just two observations and contains a column for the two major presidential candidates and a column for the number of counties that each candidate won. If we wanted to make a barplot showing the number of wins for each candidate, we can’t use geom_bar(). Predict what the result will be from running the following code. ggplot(pres_cat2, aes(x = winner)) + geom_bar() Instead, we can use geom_col(), which takes an x aesthetic giving the column with names of the levels of our categorical variable, and a y aesthetic giving the column with the counts: ggplot(pres_cat2, aes(x = winner, y = nwins)) + geom_col() 3.3.4 Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 3.7. Change the frequency plot to plot the number of votes for Bush instead of the number for Gore. Are there any obvious outliers in the Bush frequency plot? Do you have a preference for histograms or a preference for frequency plots? Can you think of a situation where one would be more desirable than the other? It looks like Bush won a lot more….does that necessarily mean that Bush won more votes in total in Florida? Why or why not? We will be using survey data from STAT 113 in the 2018-2019 academic year for many exercises in this section. For those who may not have taken STAT 113 from having AP credit or another reason, the STAT 113 survey is given to all students in STAT 113 across all sections. Some analyses in Intro Stat are then carried out using the survey. library(tidyverse) stat113_df &lt;- read_csv(&quot;data/stat113.csv&quot;) head(stat113_df) #&gt; # A tibble: 6 x 12 #&gt; Year Sex Hgt Wgt Haircut GPA Exercise Sport TV #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Soph… M 66 155 0 2.9 15 Yes 8 #&gt; 2 Firs… F 69 170 17 3.87 14 Yes 12 #&gt; 3 Firs… F 64 130 40 3.3 5 No 5 #&gt; 4 Firs… M 68 157 35 3.21 10 Yes 15 #&gt; 5 Firs… M 72 175 20 3.1 2 No 5 #&gt; 6 Juni… F 62 150 50 3.3 8 Yes 5 #&gt; # … with 3 more variables: Award &lt;chr&gt;, Pulse &lt;dbl&gt;, #&gt; # SocialMedia &lt;chr&gt; The data set contains the following variables: Year, FirstYear, Sophomore, Junior, or Senior Sex, M or F (for this data set, Sex is considered binary). Hgt, height, in inches. Wgt, weight, in pounds. Haircut, how much is paid for a haircut, typically. GPA Exercise, amount of hours of exercise in a typical week. Sport, whether or not the student plays a varsity sport. TV, amount of hours spent watching TV in a typical week. Award, Award preferred: choices are Olympic Medal, Nobel Prize, or Academy Award. Pulse, pulse rate, in beats per minute. SocialMedia, most used social media platform (Instagram, SnapChat, FaceBook, Twitter, Other, or None). * Create a histogram of the Exercise variable, change the x-axis label to be “Exercise (hours per typical week),” change the number of bins to 14, and change the fill of the bins to be “lightpink2” and the outline colour of the bins to be black. * We can change the y-axis of a histogram to be “density” instead of a raw count. This means that each bar shows a proportion of cases instead of a raw count. Google something like “geom_histogram with density” to figure out how to create a y aes() to show density instead of count. Construct a histogram using a quantitative variable of your choice. Change the fill and colour using http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf to help you choose colours. Construct a bar plot for a variable of your choosing. What do you find? What format would the STAT 113 data set need to be in to construct your bar plot with geom_col() instead of geom_bar()? 3.4 Graphing Two Quantitative Variables, Faceting, and aes() Options 3.4.1 Scatterplots Moving back to the 2000 presidential election data set, thus far, we’ve figured out that there a couple of counties with very large numbers of votes for Gore and very large number of votes for Bush. We don’t know the reason for this (if some counties are very democratic, very republican, or if some counties are just more populous). Do the counties that have a large number of votes for Bush also tend to have a large number of votes for Gore? And what about the other candidates: do they have any interesting patterns? Let’s start by making a scatterplot of the number of votes for Gore and the number of votes for Bush. Note that the geom_ for making a scatterplot is called geom_point() because we are adding a layer of points to the plot. ggplot(data = pres_df, mapping = aes(x = Gore, y = Bush)) + geom_point() What patterns do you see in the scatterplot? Now, change the x variable from Gore to Buchanan. You should notice something strange in this scatterplot. Try to come up with one explanation for why the outlying point has so many votes for Buchanan. In trying to come up with an explanation, it would be nice to figure out which Florida county has that outlying point and it would be nice if we knew something about Florida counties. To remedy the first issue, recall that we can type View(pres_df) to pull up the data set. Once you have the new window open, click on the column heading Buchanan to sort the votes for Buchanan from high to low to figure out which county is the outlier. Use some Google sleuthing skills to find an explanation: try to search for “2000 united states presidential election [name of outlier county].” Write a sentence about what you find. Hint: if nothing useful pops up, try adding the term “butterfly ballot” to your search. We have used the 2000 Presidential data set to find out something really interesting! In particular, we have used exploratory data analysis to examine a data set, without having a specific question of interest that we want to answer. This type of exploring is often really useful, but does have some drawbacks, which we will discuss later in the semester. 3.4.2 Aesthetics in aes() For the remainder of this chapter, we will work with some fitness data collected from my Apple Watch since November 2018. The higham_fitness_clean.csv contains information on the following variables: Start, the month, day, and year that the fitness data was recorded on month, the month weekday, the day of the week dayofyear, the day of the year (so that 304 corresponds to the 304th day of the year) distance, distance walked in miles steps, the number of steps taken flights, the number of flights of stairs climbed active_cals, the number of calories burned from activity stepgoal, whether or not I reached 10,000 steps for the day weekend_ind, a variable for whether or not the day of the week was a weekend day (Saturday or Sunday) or a weekday (Monday - Friday). library(tidyverse) fitness_full &lt;- read_csv(&quot;data/higham_fitness_clean.csv&quot;) %&gt;% mutate(weekend_ind = case_when(weekday == &quot;Sat&quot; | weekday == &quot;Sun&quot; ~ &quot;weekend&quot;, TRUE ~ &quot;weekday&quot;)) #&gt; #&gt; ── Column specification ──────────────────────────────────── #&gt; cols( #&gt; Start = col_date(format = &quot;&quot;), #&gt; month = col_character(), #&gt; weekday = col_character(), #&gt; dayofyear = col_double(), #&gt; distance = col_double(), #&gt; steps = col_double(), #&gt; flights = col_double(), #&gt; active_cals = col_double(), #&gt; stepgoal = col_character() #&gt; ) First, let’s make a basic scatterplot to illustrate why it’s so important to plot your data. I’ll use the variable distance as the x-variable and active_cals as the y-variable. ggplot(data = fitness_full, aes(x = distance, y = active_cals)) + geom_point() One aspect of the plot that you may notice is that there are observations where I burned 0 or very few active calories, yet walked/jogged/ran/moved some distance. Is it possible to not burn any calories and move ~ 4 miles? Probably not, so let’s drop these observations from the data set and make a note of why we dropped those observations. Unfortunately, we don’t have the tools to do this yet, so just run the following chunk of code without worrying too much about the syntax. ## drop observations that have active calories &lt; 50. ## assuming that these are data errors or ## days where the Apple Watch wasn&#39;t worn. fitness &lt;- fitness_full %&gt;% filter(active_cals &gt; 50) Let’s make the plot again with the fitness data set instead of fitness_full to see if the outliers are actually gone. This time, we will put the aes() in the geom_point() function: ggplot(data = fitness) + geom_point(aes(x = distance, y = active_cals)) Putting the aes() in ggplot() and putting the aes() in geom_point() results in the same graph in this case. When you put the aes() in ggplot(), R perpetuates these aes() aesthetics in all geom_s in your plotting command. However, if you put your aes() in geom_point(), then any future geoms that you use will need you to re-specify different aes(). We’ll see an example of this in the exercises. Other aes() Options In addition to x and y, we can also use aes() to map variables to things like colour, size, and shape. For example, we might make a scatterplot with Start on the x-axis (for the date) and active_cals on the y-axis, colouring by whether or not the day of the week was a weekend. ggplot(data = fitness) + geom_point(aes(x = Start, y = active_cals, colour = weekend_ind)) Is there anything useful that you notice about the plot? Is there anything about the plot that could be improved? Instead of using colour, you can also specify the point shape. This could be useful, for example, if you are printing something in black and white. ggplot(data = fitness) + geom_point(aes(x = Start, y = active_cals, shape = weekend_ind)) Do you prefer the colour or the shape? Why? Finally, another common aes() is size. For example, we could make the size of the points in the scatterplot change depending on how many flights of stairs I climbed. ggplot(data = fitness) + geom_point(aes(x = Start, y = active_cals, size = flights)) I don’t think any of the previous three plots are necessarily the “best” and need some work, but, part of the fun of exploratory data analysis is making trying out different plots to see what “works.” Inside vs Outside aes() We’ve changed the colour of the points to correspond to weekend_ind, but what if we just wanted to change the colour of points to all be the same colour, \"purple\". Try running the following code chunk: ggplot(data = fitness) + geom_point(aes(x = Start, y = active_cals, colour = &quot;purple&quot;)) What does the graph look like? Did it do what you expected? Putting colour = ____ inside aes() or outside aes() achieves different things. In general, when we want to map something in our data set (fitness) to something in our plot (x, y, colour, size, etc.), we put that inside the aes() as in geom_point(aes(colour = weekend_ind)). When we assign fixed characteristics that don’t come from the data, we put them outside the aes(), as in geom_point(colour = \"purple\"). You can also change the overall point size and shape. The standard size is 1 so the following code chunk makes the points bigger. The standard shape is 19: you can try changing that to other integers to see what other shapes you can get. ggplot(data = fitness) + geom_point(aes(x = Start, y = active_cals), size = 1.5, shape = 19) 3.4.3 Using More Than One geom() We might also be interested in fitting a smooth curve to our scatterplot. When we want to put more than one “geom” on our plot, we can use multiple geoms. Since I want the aes() to apply to both geom_point() and geom_smooth(), I am going to move the aes() command to the overall ggplot() line of code: ggplot(data = fitness, aes(x = Start, y = active_cals)) + geom_point() + geom_smooth() #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Within geom_smooth(), you can set se = FALSE to get rid of the grey standard errors around each of the lines, and you can setmethod = \"lm\" to fit straight linear regression lines instead of smooth curves: ggplot(data = fitness, aes(x = Start, y = active_cals)) + geom_point() + geom_smooth(se = FALSE, method = &quot;lm&quot;) #&gt; `geom_smooth()` using formula &#39;y ~ x&#39; Does it look like there is an increasing overall trend? decreasing? Does it make sense to use a line to model the relationship or did you prefer the smooth curve? 3.4.4 Line Plots with geom_line() Line plots are often useful when you have a quantitative variable that you’d like to explore over time. The y-axis is the quantitative variable while the x-axis is typically time. More generally, line plots are often used when the x-axis variable has one discrete value for each y-axis variable. For example, suppose we want to explore how my step count has changed through time over the past couple of years. Compare the standard scatterplot with the following line plot: which do you prefer? ggplot(data = fitness, mapping = aes(x = Start, y = steps)) + geom_point() + geom_smooth() + xlab(&quot;Date&quot;) #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ggplot(data = fitness, mapping = aes(x = Start, y = steps)) + geom_line() + geom_smooth() + xlab(&quot;Date&quot;) #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Can you spot the start of the pandemic in the graph? What seemed to happen with the step count? 3.4.5 Faceting Using colour to colour points of different levels of a categorical variable is generally fine when there are just a couple of levels and/or there is little overlap among the levels. But, what if there are a lot more than two categories to colour by. For example, let’s move back to the STAT 113 survey data set and investigate the relationship between Pulse and Exercise for different class Year’s. We might hypothesize that students who get more exercise tend to have lower pulse rates. ggplot(data = stat113_df, aes(x = Exercise, y = Pulse, colour = Year)) + geom_point() + geom_smooth(se = TRUE) #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; #&gt; Warning: Removed 40 rows containing non-finite values #&gt; (stat_smooth). #&gt; Warning: Removed 40 rows containing missing values #&gt; (geom_point). When there are many different categories for a categorical variable (there are only 4 categories for Year, but this particular plot is still a bit difficult to read), it can sometimes be useful to facet the plot by that variable instead of trying to use different colours or shapes. ggplot(data = stat113_df, aes(x = Exercise, y = Pulse)) + geom_point() + geom_smooth(se = TRUE) + facet_wrap(~ Year) #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; #&gt; Warning: Removed 40 rows containing non-finite values #&gt; (stat_smooth). #&gt; Warning: Removed 40 rows containing missing values #&gt; (geom_point). We have eliminated the colour = argument and added facet_wrap( ~ name_of_facet_variable). Doing so creates a different scatterplot and smooth line for each level of name_of_facet_variable. What can you see from this plot that was harder to see from the plot with colour? Does the data seem to support the hypothesis that more exercise is associated with lower pulse rates in this sample of students? 3.4.6 Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 3.7. Fix the code chunk where we tried to specify the colour of all points to be purple to actually make all of the points “purple” by moving colour = \"purple\" outside the parentheses in aes() (but still inside geom_point()). In the console (bottom-left) window, type ?geom_smooth and scroll down to “Arguments.” Find span, read about it, and then, within the geom_smooth() argument of the line plot with steps vs. date, add a span argument to make the smooth line wigglier. Explain why it doesn’t make sense to construct a line plot of Exercise vs. GPA. * Make a scatterplot of Hgt on the y-axis and Wgt on the x-axis, colouring by Sport. Add a smooth fitted curve to your scatterplot. Then, move colour = Sport from an aes() in the ggplot() function to an aes() in the geom_point() function. What changes in the plot? Can you give an explanation as to why that change occurs? * Faceting can be used for other types of plots too! Make a pair of faceted histograms for a quantitative variable of your choosing that are faceted by a categorical variable of your choosing. 3.5 Boxplots, Stacked Barplots and Others There are a few other common geoms that will be useful throughout the semester. These only skim the surface: we’ll come back to plotting in a few weeks, after we’re able to do more with data wrangling and reshaping. 3.5.1 Graphing a Quant. Variable vs. a Cat. Variable Another common plot used in Intro Stat courses is a boxplot. Side-by-side boxplots are particularly useful if you want to compare a quantitative response variable across two or more levels of a categorical variable. Let’s stick with the STAT 113 survey data to examine the relationship between Exercise and Award preference. ggplot(data = stat113_df, aes(x = Award, y = Exercise)) + geom_boxplot() #&gt; Warning: Removed 7 rows containing non-finite values #&gt; (stat_boxplot). What can you conclude from the plot? An alternative to side-by-side boxplots are violin plots: ggplot(data = stat113_df, aes(x = Award, y = Exercise)) + geom_violin() #&gt; Warning: Removed 7 rows containing non-finite values #&gt; (stat_ydensity). Read about Violin plots by typing ?geom_violin into your console (bottom-left window). How are they different than boxplots? 3.5.2 Graphing Two Categorical Variables The only combination of two variables that we have yet to explore are two variables that are both categorical. Let’s look at the relationship between Year and SocialMedia first using a stacked bar plot. To make the graph, we specify position = \"fill\" so that the bars are “filled” by stepgoal. ggplot(data = stat113_df, aes(x = Year, fill = SocialMedia)) + geom_bar(position = &quot;fill&quot;) + ylab(&quot;Proportion&quot;) What patterns do you notice from the plot? Is there anything about the plot that could be improved? 3.5.3 Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 3.7. * Change the colour of the inside of the boxplots in the Exercise vs. Award graph to be \"blue\". Do you think you’ll use colour = \"blue\" or fill = \"blue\"? * Create a side-by-side boxplot that compares the GPAs of students who prefer different Awards. Then change the fill of the boxplot to be a colour of your choice. What do you notice in the plot? * When making the previous plot, R gives us a warning message that it “Removed 70 rows containing non-finite values.” This is R’s robotic way of telling us that 70 GPA values are missing in the data set. Use what you know about how the data was collected (Fall and Spring semester of the 2018-2019 school-year) to guess why these are missing. * Make a stacked bar plot for two variables of your choosing in the STAT 113 data set. Comment on something that you notice in the plot. 3.6 Chapter Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 3.7. * The default of geom_smooth() is to use LOESS (locally estimated scatterplot smoothing). Read about LOESS here: here. Write one or two sentences explaining what LOESS does. * Thus far, we have only faceted by a single variable. Use Google to figure out how to facet by two variables to make a plot that shows the relationship between GPA (y-axis) and Exercise (x-axis) with four facets: one for male students who play a sport, one for female students who play a sport, one for male students who do not play a sport, and one for female students who do not play a sport. * In Intro-Stat, boxplots are typically introduced using the * symbol to identify outliers. Using a combination of the help ?geom_boxplot and Googling “R point shapes,” figure out how to modify your side-by-side boxplots so that the outliers are shown using *, not the default dots. Then, using Google, figure out how to add the mean to each boxplot as a “darkgreen” diamond-shaped symbol with stat_summary(). A common theme that we’ll see throughout the course is that it’s advantageous to know as much background information as possible about the data set we are analyzing. Data sets will be easier to analyze and pose questions about if you’re familiar with the subject matter. Give an example of something that you know about STAT 113 and the survey data set that helped you answer or pose a question that someone from another university (and therefore unfamiliar with our intro stat course) wouldn’t know. Give an example of something that you don’t know about the fitness data set that the person who owns the fitness data would know. Why does that give an advantage to the person who is more familiar with the fitness data? 3.7 Exercise Solutions 3.7.1 Introduction etc. S 3.7.2 Basic Plot Structure S 3.7.3 Graphing a Single Variable S * Create a histogram of the Exercise variable, change the x-axis label to be “Exercise (hours per typical week),” change the number of bins to 14, and change the fill of the bins to be “lightpink2” and the outline colour of the bins to be black. ggplot(data = stat113_df, aes(x = Exercise)) + geom_histogram(bins = 14, fill = &quot;lightpink2&quot;, colour = &quot;black&quot;) + xlab(&quot;Exercise (hours per typical week)&quot;) #&gt; Warning: Removed 7 rows containing non-finite values #&gt; (stat_bin). * We can change the y-axis of a histogram to be “density” instead of a raw count. This means that each bar shows a proportion of cases instead of a raw count. Google something like “geom_histogram with density” to figure out how to create a y aes() to show density instead of count. ggplot(data = stat113_df, aes(x = Exercise, y = ..density..)) + geom_histogram(bins = 14, fill = &quot;lightpink2&quot;, colour = &quot;black&quot;) + xlab(&quot;Exercise (hours per typical week)&quot;) #&gt; Warning: Removed 7 rows containing non-finite values #&gt; (stat_bin). 3.7.4 Graphing Two Quant. etc. S * Make a scatterplot of Hgt on the y-axis and Wgt on the x-axis, colouring by Sport. Add a smooth fitted curve to your scatterplot. Then, move colour = Sport from an aes() in the ggplot() function to an aes() in the geom_point() function. What changes in the plot? Can you give an explanation as to why that change occurs? ggplot(data = stat113_df, aes(x = Wgt, y = Hgt, colour = Sport)) + geom_point() + geom_smooth() ggplot(data = stat113_df, aes(x = Wgt, y = Hgt)) + geom_point(aes(colour = Sport)) + geom_smooth() The points are now coloured by Sport but there is only one smooth fitted line. This makes sense because geom_point() now has the two global aesthetics x and y, as well as the colour aesthetic. geom_smooth() no longer has the colour aesthetic but still inherits the two global aesthetics, x and y. * Faceting can be used for other types of plots too! Make a pair of faceted histograms for a quantitative variable of your choosing that are faceted by a categorical variable of your choosing. Answers will vary: ggplot(data = stat113_df, aes(x = GPA)) + geom_histogram(bins = 15) + facet_wrap( ~ Sport) 3.7.5 Boxplots, Stacked, etc. S * Change the colour of the inside of the boxplots in the Exercise vs. Award graph to be \"blue\". Do you think you’ll use colour = \"blue\" or fill = \"blue\"? ggplot(data = stat113_df, aes(x = Award, y = Exercise)) + geom_boxplot(fill = &quot;blue&quot;) #&gt; Warning: Removed 7 rows containing non-finite values #&gt; (stat_boxplot). fill because it’s the inside of the boxplots that we want to modify. colour will modify the outline colour. * Create a side-by-side boxplot that compares the GPAs of students who prefer different Awards. Then change the fill of the boxplot to be a colour of your choice. What do you notice in the plot? ggplot(data = stat113_df, aes(x = Award, y = GPA)) + geom_boxplot(fill = &quot;lightpink1&quot;) #&gt; Warning: Removed 70 rows containing non-finite values #&gt; (stat_boxplot). There are a few outlier students, but the three groups overall seem to have similar GPAs. * When making the previous plot, R gives us a warning message that it “Removed 70 rows containing non-finite values.” This is R’s robotic way of telling us that 70 GPA values are missing in the data set. Use what you know about how the data was collected (Fall and Spring semeseter of the 2018-2019 school-year) to guess why these are missing. STAT 113 has first-year students: first-years taking the course in the fall would not have a GPA to report. Additionally, another reason might be that a student chose not to report his or her GPA. * Make a stacked bar plot for two variables of your choosing in the STAT 113 data set. Comment on something that you notice in the plot. Answers will vary. ggplot(data = stat113_df, aes(x = Sport, fill = Award)) + geom_bar(position = &quot;fill&quot;) As we might expect, it does seem like a higher proportion of students who play a sport would prefer to win an Olympic medal, compared with students who do not play a sport. 3.7.6 Chapter Exercises S * The default of geom_smooth() is to use LOESS (locally estimated scatterplot smoothing). Read about LOESS here: here. Write one or two sentences explaining what LOESS does. Loess uses a bunch of local regressions to predict the y-variable at each point, giving more weight to observations near the point of interest on the x-axis. Once this is done for every point, the predictions are connected with a smooth curve. * Thus far, we have only faceted by a single variable. Use Google to figure out how to facet by two variables to make a plot that shows the relationship between GPA (y-axis) and Exercise (x-axis) with four facets: one for male students who play a sport, one for female students who play a sport, one for male students who do not play a sport, and one for female students who do not play a sport. ggplot(data = stat113_df %&gt;% filter(!is.na(Sport) &amp; !is.na(Sex)), aes(x = Exercise, y = GPA)) + geom_point() + geom_smooth() + facet_grid(Sex ~ Sport) #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; #&gt; Warning: Removed 71 rows containing non-finite values #&gt; (stat_smooth). #&gt; Warning: Removed 71 rows containing missing values #&gt; (geom_point). * In Intro-Stat, boxplots are typically introduced using the * symbol to identify outliers. Using a combination of the help ?geom_boxplot and Googling “R point shapes,” figure out how to modify your side-by-side boxplots so that the outliers are shown using *, not the default dots. Then, using Google, figure out how to add the mean to each boxplot as a “darkgreen” diamond-shaped symbol with stat_summary(). ggplot(data = stat113_df, aes(x = Sex, y = GPA)) + geom_boxplot(fill = &quot;lightpink1&quot;, outlier.shape = 8) + stat_summary(fun = mean, shape = 18, colour = &quot;darkgreen&quot;) #&gt; Warning: Removed 70 rows containing non-finite values #&gt; (stat_boxplot). #&gt; Warning: Removed 70 rows containing non-finite values #&gt; (stat_summary). #&gt; Warning: Removed 3 rows containing missing values #&gt; (geom_segment). 3.8 Non-Exercise R Code library(tidyverse) pres_df &lt;- read_table(&quot;data/PRES2000.txt&quot;) ## don&#39;t worry about the `read_table` function....yet head(pres_df) ggplot(data = pres_df, mapping = aes(x = Gore)) + geom_histogram(colour = &quot;black&quot;, fill = &quot;white&quot;) + xlab(&quot;Votes for Gore in Florida&quot;) ggplot(data = pres_df, mapping = aes(x = Gore)) + geom_freqpoly(colour = &quot;black&quot;) + xlab(&quot;Votes for Gore in Florida&quot;) ggplot(data = pres_df, mapping = aes(x = Gore)) + geom_freqpoly(colour = &quot;black&quot;) + xlab(&quot;Votes for Gore in Florida&quot;) + geom_histogram() pres_cat &lt;- pres_df %&gt;% mutate(winner = if_else(Gore &gt; Bush, true = &quot;Gore&quot;, false = &quot;Bush&quot;)) pres_cat ggplot(data = pres_cat, aes(x = winner)) + geom_bar() pres_cat2 &lt;- pres_cat %&gt;% group_by(winner) %&gt;% summarise(nwins = n()) pres_cat2 ggplot(pres_cat2, aes(x = winner)) + geom_bar() ggplot(pres_cat2, aes(x = winner, y = nwins)) + geom_col() ggplot(data = pres_df, mapping = aes(x = Gore, y = Bush)) + geom_point() library(tidyverse) fitness_full &lt;- read_csv(&quot;data/higham_fitness_clean.csv&quot;) %&gt;% mutate(weekend_ind = case_when(weekday == &quot;Sat&quot; | weekday == &quot;Sun&quot; ~ &quot;weekend&quot;, TRUE ~ &quot;weekday&quot;)) ggplot(data = fitness_full, aes(x = distance, y = active_cals)) + geom_point() ## drop observations that have active calories &lt; 50. ## assuming that these are data errors or ## days where the Apple Watch wasn&#39;t worn. fitness &lt;- fitness_full %&gt;% filter(active_cals &gt; 50) ggplot(data = fitness) + geom_point(aes(x = distance, y = active_cals)) ggplot(data = fitness) + geom_point(aes(x = Start, y = active_cals, colour = weekend_ind)) ggplot(data = fitness) + geom_point(aes(x = Start, y = active_cals, shape = weekend_ind)) ggplot(data = fitness) + geom_point(aes(x = Start, y = active_cals, size = flights)) ggplot(data = fitness) + geom_point(aes(x = Start, y = active_cals, colour = &quot;purple&quot;)) ggplot(data = fitness) + geom_point(aes(x = Start, y = active_cals), size = 1.5, shape = 19) ggplot(data = fitness, aes(x = Start, y = active_cals)) + geom_point() + geom_smooth() ggplot(data = fitness, aes(x = Start, y = active_cals)) + geom_point() + geom_smooth(se = FALSE, method = &quot;lm&quot;) ggplot(data = fitness, mapping = aes(x = Start, y = steps)) + geom_point() + geom_smooth() + xlab(&quot;Date&quot;) ggplot(data = fitness, mapping = aes(x = Start, y = steps)) + geom_line() + geom_smooth() + xlab(&quot;Date&quot;) ggplot(data = stat113_df, aes(x = Exercise, y = Pulse, colour = Year)) + geom_point() + geom_smooth(se = TRUE) ggplot(data = stat113_df, aes(x = Exercise, y = Pulse)) + geom_point() + geom_smooth(se = TRUE) + facet_wrap(~ Year) ggplot(data = stat113_df, aes(x = Award, y = Exercise)) + geom_boxplot() ggplot(data = stat113_df, aes(x = Award, y = Exercise)) + geom_violin() ggplot(data = stat113_df, aes(x = Year, fill = SocialMedia)) + geom_bar(position = &quot;fill&quot;) + ylab(&quot;Proportion&quot;) "],["dplyr.html", " 4 Wrangling with dplyr 4.1 mutate(): Create Variables 4.2 arrange() (Ordering Rows), select() (Choosing Columns), and slice() and filter() (Choosing Rows) 4.3 summarise() and group_by(): Create Summaries 4.4 Missing Values 4.5 Chapter Exercises 4.6 Exercise Solutions 4.7 Non-Exercise R Code", " 4 Wrangling with dplyr Goals: Use the mutate(), if_else(), and case_when() functions to create new variables. Use the filter() and slice(), select(), and arrange() functions in dplyr to choose certain rows to keep or get rid of, choose certain columns to keep or get rid of, and to sort the data, respectively. Use group_by() and summarise() to create useful summaries of a data set. Combine the above goals with plotting to explore the babynames data set and a data set on SLU majors. Throughout this chapter, we will use the babynames data set in the babynames R package. To begin, read about the data set, by running library(babynames) and then typing ?babynames in your bottom-left window of R Studio. We see that this data set contains baby name data provided by the SSA in the United States dating back to 1880: head(babynames) #&gt; # A tibble: 6 x 5 #&gt; year sex name n prop #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1880 F Mary 7065 0.0724 #&gt; 2 1880 F Anna 2604 0.0267 #&gt; 3 1880 F Emma 2003 0.0205 #&gt; 4 1880 F Elizabeth 1939 0.0199 #&gt; 5 1880 F Minnie 1746 0.0179 #&gt; 6 1880 F Margaret 1578 0.0162 The second data set that we will use has 27 observations, one for each of SLU’s majors and contains 3 variables: Major, the name of the major. nfemales, the number of female graduates in that major from 2015 - 2019. nmales, the number of male graduates in that major from 2015 - 2019. The data has kindly been provided by Dr. Ramler. With your Notes R Project open, you can read in the data set with library(tidyverse) slumajors_df &lt;- read_csv(&quot;data/SLU_Majors_15_19.csv&quot;) slumajors_df #&gt; # A tibble: 27 x 3 #&gt; Major nfemales nmales #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Anthropology 34 15 #&gt; 2 Art &amp; Art History 65 11 #&gt; 3 Biochemistry 14 11 #&gt; 4 Biology 162 67 #&gt; 5 Business in the Liberal Arts 135 251 #&gt; 6 Chemistry 26 14 #&gt; 7 Computer Science 21 47 #&gt; 8 Conservation Biology 38 20 #&gt; 9 Economics 128 349 #&gt; 10 English 131 54 #&gt; # … with 17 more rows There are many interesting and informative plots that we could make with either data set, but most require some data wrangling first. This chapter will provide the foundation for such wrangling skills. 4.1 mutate(): Create Variables Sometimes, we will want to create a new variable that’s not in the data set, oftentimes using if_else(), case_when(), or basic algebraic operations on one or more of the columns already present in the data set. R understands the following symbols: + for addition, - for subtraction * for multiplication, / for division ^ for raising something to a power (3 ^ 2 is equal to 9) R also does the same order of operations as usual: parentheses, then exponents, then multiplication and division, then addition and subtraction. For example, suppose that we want to create a variable in slumajors_df that has the total number of students graduating in each major. We can do this with mutate(): slumajors_df %&gt;% mutate(ntotal = nfemales + nmales) #&gt; # A tibble: 27 x 4 #&gt; Major nfemales nmales ntotal #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Anthropology 34 15 49 #&gt; 2 Art &amp; Art History 65 11 76 #&gt; 3 Biochemistry 14 11 25 #&gt; 4 Biology 162 67 229 #&gt; 5 Business in the Liberal Arts 135 251 386 #&gt; 6 Chemistry 26 14 40 #&gt; 7 Computer Science 21 47 68 #&gt; 8 Conservation Biology 38 20 58 #&gt; 9 Economics 128 349 477 #&gt; 10 English 131 54 185 #&gt; # … with 17 more rows There’s a lot to break down in that code chunk: most importantly, we’re seeing our first of many, many, many, many, many, many, many instances of using %&gt;% to pipe! The %&gt;% operator approximately reads take slumajors_df “and then” mutate() it. Piping is a really convenient, easy-to-read way to build a sequence of commands. How you can read the above code is: Take slumajors_df and with slumajors_df, perform a mutate() step to create the new variable called ntotal, which is nfemales plus nmales. Since this is our first time using mutate(), let’s also delve into what the function is doing. In general, mutate() reads: mutate(name_of_new_variable = operations_on_old_variables). R just automatically assumes that you want to do the operation for every single row in the data set, which is often quite convenient! We might also want to create a variable that is the percentage of students identifying as female for each major: slumajors_df %&gt;% mutate(percfemale = 100 * nfemales / (nfemales + nmales)) #&gt; # A tibble: 27 x 4 #&gt; Major nfemales nmales percfemale #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Anthropology 34 15 69.4 #&gt; 2 Art &amp; Art History 65 11 85.5 #&gt; 3 Biochemistry 14 11 56 #&gt; 4 Biology 162 67 70.7 #&gt; 5 Business in the Liberal Arts 135 251 35.0 #&gt; 6 Chemistry 26 14 65 #&gt; 7 Computer Science 21 47 30.9 #&gt; 8 Conservation Biology 38 20 65.5 #&gt; 9 Economics 128 349 26.8 #&gt; 10 English 131 54 70.8 #&gt; # … with 17 more rows But what happened to ntotal? Is it still in the printout? It’s not: when we created the variable ntotal, we didn’t actually save the new data set as anything. So R makes and prints the new variable, but it doesn’t get saved to any data set. If we want to save the new data set, then we can use the &lt;- operator. Here, we’re saving the new data set with the same name as the old data set: slumajors_df. Then, we’re doing the same thing for the percfemale variable. We won’t always want to give the new data set the same name as the old one: we’ll talk about this more in the chapter exercises. slumajors_df &lt;- slumajors_df %&gt;% mutate(percfemale = 100 * nfemales / (nfemales + nmales)) slumajors_df &lt;- slumajors_df %&gt;% mutate(ntotal = nfemales + nmales) But, you can pipe as many things together as you want to, so it’s probably easier to just create both variables in one go. The following chunk says to “Take slumajors_df and create a new variable ntotal. With that new data set, create a new variable called percfemale.” Finally, the slumajors_df &lt;- at the beginning says to “save this new data set as a data set with the same name, slumajors_df.” slumajors_df &lt;- slumajors_df %&gt;% mutate(ntotal = nfemales + nmales) %&gt;% mutate(percfemale = 100 * nfemales / (nfemales + nmales)) 4.1.1 A Little More on Piping We are jumping straight into using piping, but we do want to have an appreciation on how terrible life would be without it. What piping does is make whatever is given before the %&gt;% pipe the first argument of whatever function follows the %&gt;%. So df %&gt;% mutate(x = y + 4) is equivalent to mutate(df, x = y + 4) Piping really isn’t that useful if you just have something that can be done with a single %&gt;%. But, doing our previous example without piping might look like: mutate(mutate(slumajors_df, ntotal = nfemales + nmales), percfemale = 100 * nfemales / (nfemales + nmales)) #&gt; # A tibble: 27 x 5 #&gt; Major nfemales nmales percfemale ntotal #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Anthropology 34 15 69.4 49 #&gt; 2 Art &amp; Art History 65 11 85.5 76 #&gt; 3 Biochemistry 14 11 56 25 #&gt; 4 Biology 162 67 70.7 229 #&gt; 5 Business in the Libera… 135 251 35.0 386 #&gt; 6 Chemistry 26 14 65 40 #&gt; 7 Computer Science 21 47 30.9 68 #&gt; 8 Conservation Biology 38 20 65.5 58 #&gt; 9 Economics 128 349 26.8 477 #&gt; 10 English 131 54 70.8 185 #&gt; # … with 17 more rows It’s still not that bad here because we aren’t doing that many operations to the data set, but it’s already much harder to read. But we will get to examples where you are using 5+ pipes. It might also help to use an analogy when thinking about piping. Consider the Ke$ha’s morning routine in the opening of the song Tik Tok. If we were to write her morning routine in terms of piping, kesha %&gt;% wake_up(time = &quot;morning&quot;, feels_like = &quot;P-Diddy&quot;) %&gt;% grab(glasses) %&gt;% brush(teeth, item = &quot;jack&quot;, unit = &quot;bottle&quot;) %&gt;% .... Kesha first wakes up in the morning, and then the Kesha that has woken up grabs her glasses, and then the Kesha who has woken up and has her glasses brushes her teeth, etc. 4.1.2 if_else() and case_when() Suppose that you want to make a new variable that is conditional on another variable (or more than one variable) in the data set. Then we would typically use mutate() coupled with if_else() if your new variable is created on only one condition case_when() if your new variable is created on more than one condition Suppose we want to create a new variable that tells us whether or not the Major has a majority of Women. That is, we want this new variable, morewomen to be \"Yes\" if the Major has more than 50% women and \"No\" if it has 50% or less. slumajors_df %&gt;% mutate(morewomen = if_else(percfemale &gt; 50, true = &quot;Yes&quot;, false = &quot;No&quot;)) #&gt; # A tibble: 27 x 6 #&gt; Major nfemales nmales percfemale ntotal morewomen #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 Anthropology 34 15 69.4 49 Yes #&gt; 2 Art &amp; Art Hi… 65 11 85.5 76 Yes #&gt; 3 Biochemistry 14 11 56 25 Yes #&gt; 4 Biology 162 67 70.7 229 Yes #&gt; 5 Business in … 135 251 35.0 386 No #&gt; 6 Chemistry 26 14 65 40 Yes #&gt; 7 Computer Sci… 21 47 30.9 68 No #&gt; 8 Conservation… 38 20 65.5 58 Yes #&gt; 9 Economics 128 349 26.8 477 No #&gt; 10 English 131 54 70.8 185 Yes #&gt; # … with 17 more rows The mutate() statement reads: create a new variable called morewomen that is equal to \"Yes\" if percfemale &gt; 50 is true and is equal to \"No\" if perfemale is not &gt; 0.5. The first argument is the condition, the second is what to name the new variable when the condition holds, and the third is what to name the variable if the condition does not hold. We use conditions all of the time in every day life. For example, New York had a quarantine order stating that people coming from 22 states in July 2020 would need to quarantine. In terms of a condition, this would read “if you are traveling to New York from one of the 22 states, then you need to quarantine for 2 weeks. Else, if not, then you don’t need to quarantine.” The trick in using these conditions in R is getting used to the syntax of the code. We can see from the above set up that if we had more than one condition, then we’d need to use a different function (or use nested if_else() statements, which can be a nightmare to read). If we have more than one condition for creating the new variable, we will use case_when(). For example, when looking at the output, we see that Biochemistry has 56% female graduates. That’s “about” a 50/50 split, so suppose we want a variable called large_majority that is “female” when the percent women is 70 or more, “male” when the percent women is 30 or less, and “none” when the percent female is between 30 and 70. slumajors_df %&gt;% mutate(large_majority = case_when(percfemale &gt;= 70 ~ &quot;female&quot;, percfemale &lt;= 30 ~ &quot;male&quot;, percfemale &gt; 30 &amp; percfemale &lt; 70 ~ &quot;none&quot;)) #&gt; # A tibble: 27 x 6 #&gt; Major nfemales nmales percfemale ntotal large_majority #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 Anthrop… 34 15 69.4 49 none #&gt; 2 Art &amp; A… 65 11 85.5 76 female #&gt; 3 Biochem… 14 11 56 25 none #&gt; 4 Biology 162 67 70.7 229 female #&gt; 5 Busines… 135 251 35.0 386 none #&gt; 6 Chemist… 26 14 65 40 none #&gt; 7 Compute… 21 47 30.9 68 none #&gt; 8 Conserv… 38 20 65.5 58 none #&gt; 9 Economi… 128 349 26.8 477 male #&gt; 10 English 131 54 70.8 185 female #&gt; # … with 17 more rows The case_when() function reads “When the percent female is more than or equal to 70, assign the new variable large_majority the value of”female“, when it’s less or equal to 30, assign the more than 30 and less than 70, assign the variable the value of”none\" .\" The &amp; is a boolean operator: we’ll talk more about that later so don’t worry too much about that for now. Let’s save these two new variables to the slumajors_df: slumajors_df &lt;- slumajors_df %&gt;% mutate(morewomen = if_else(percfemale &gt; 50, true = &quot;Yes&quot;, false = &quot;No&quot;)) %&gt;% mutate(large_majority = case_when(percfemale &gt;= 70 ~ &quot;female&quot;, percfemale &lt;= 30 ~ &quot;male&quot;, percfemale &gt; 30 &amp; percfemale &lt; 70 ~ &quot;none&quot;)) 4.1.3 Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 4.6. Do you think it is ethical to exclude non-binary genders from analyses and graphs in the slumajors data set? Why or why not? * Create a new variable that is called major_size and is “large” when the total number of majors is 100 or more and “small” when the total number of majors is less than 100. Create a new variable that is called major_size2 and is “large when the total number of majors is 150 or more,”medium\" when the total number of majors is between 41 and 149, and “small” when the total number of majors is 40 or fewer. About 55% of SLU students identify as female. So, in the definition of the morewomen variable, does it make more sense to use 55% as the cutoff or 50%? * Investigate what happens with case_when() when you give overlapping conditions and when you give conditions that don’t cover all observations. For overlapping conditions, create a variable testcase that is \"Yes\" when percfemale is greater than or equal to 40 and \"No\" when percfemale is greater than 60 For conditions that don’t cover all observations, create a variable testcase2 that is \"Yes\" when percefemale is greater than or equal to 55 and \"No\" when percfemale is less than 35. With one or two of the newly created variables from mutate(), create a plot that investigates a question of interest you might have about the data. 4.2 arrange() (Ordering Rows), select() (Choosing Columns), and slice() and filter() (Choosing Rows) arrange() is used to order rows in the data set according to some variable, select() is used to choose columns to keep (or get rid of) and filter() is used to keep (or get rid of) only some of the observations (rows). 4.2.1 arrange(): Ordering Rows The arrange() function allows us to order rows in the data set using one or more variables. The function is very straightforward. Suppose that we want to order the rows so that the majors with the lowest percfemale are first: slumajors_df %&gt;% arrange(percfemale) #&gt; # A tibble: 27 x 7 #&gt; Major nfemales nmales percfemale ntotal morewomen #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 Economics 128 349 26.8 477 No #&gt; 2 Physics 6 14 30 20 No #&gt; 3 Computer Sci… 21 47 30.9 68 No #&gt; 4 Business in … 135 251 35.0 386 No #&gt; 5 Music 13 21 38.2 34 No #&gt; 6 Geology 28 41 40.6 69 No #&gt; 7 History 62 82 43.1 144 No #&gt; 8 Philosophy 24 29 45.3 53 No #&gt; 9 Mathematics 74 83 47.1 157 No #&gt; 10 Government 127 116 52.3 243 Yes #&gt; # … with 17 more rows, and 1 more variable: #&gt; # large_majority &lt;chr&gt; Which major has the lowest percentage of female graduates? We see that, by default, arrange() orders the rows from low to high. To order from high to low so that the majors with the highest percfemale are first, use desc() around the variable that you are ordering by: slumajors_df %&gt;% arrange(desc(percfemale)) #&gt; # A tibble: 27 x 7 #&gt; Major nfemales nmales percfemale ntotal morewomen #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 Art &amp; Art Hi… 65 11 85.5 76 Yes #&gt; 2 Psychology 278 61 82.0 339 Yes #&gt; 3 French 27 7 79.4 34 Yes #&gt; 4 Spanish 35 10 77.8 45 Yes #&gt; 5 Statistics 28 9 75.7 37 Yes #&gt; 6 Global Studi… 69 27 71.9 96 Yes #&gt; 7 Neuroscience 61 24 71.8 85 Yes #&gt; 8 Performance … 144 57 71.6 201 Yes #&gt; 9 Religious St… 10 4 71.4 14 Yes #&gt; 10 English 131 54 70.8 185 Yes #&gt; # … with 17 more rows, and 1 more variable: #&gt; # large_majority &lt;chr&gt; What is the major with the highest percentage of women graduates? 4.2.2 select() Choose Columns We might also be interested in getting rid of some of the columns in a data set. One reason to do this is if there are an overwhelming (30+) columns in a data set, but we know that we just need a few of them. The easiest way to use select() is to just input the names of the columns that you want to keep. For example, if we were only interested in majors and their totals, we could do slumajors_df %&gt;% select(Major, ntotal) #&gt; # A tibble: 27 x 2 #&gt; Major ntotal #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Anthropology 49 #&gt; 2 Art &amp; Art History 76 #&gt; 3 Biochemistry 25 #&gt; 4 Biology 229 #&gt; 5 Business in the Liberal Arts 386 #&gt; 6 Chemistry 40 #&gt; 7 Computer Science 68 #&gt; 8 Conservation Biology 58 #&gt; 9 Economics 477 #&gt; 10 English 185 #&gt; # … with 17 more rows If I wanted to use this data set for anything else, I’d also need to name, or rename, it with &lt;-. We would probably want to name it something other than slumajors_df so as to not overwrite the original data set, in case we want to use those other variables again later! We might also want to use select() to get rid of one or two columns. If this is the case, we denote any column you want to get rid of with -. For example, we might want to get rid of the ntotal column that we made and get rid of the nmales and nfemales columns: slumajors_df %&gt;% select(-ntotal, -nfemales, -nmales) #&gt; # A tibble: 27 x 4 #&gt; Major percfemale morewomen large_majority #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Anthropology 69.4 Yes none #&gt; 2 Art &amp; Art History 85.5 Yes female #&gt; 3 Biochemistry 56 Yes none #&gt; 4 Biology 70.7 Yes female #&gt; 5 Business in the Libe… 35.0 No none #&gt; 6 Chemistry 65 Yes none #&gt; 7 Computer Science 30.9 No none #&gt; 8 Conservation Biology 65.5 Yes none #&gt; 9 Economics 26.8 No male #&gt; 10 English 70.8 Yes female #&gt; # … with 17 more rows select() comes with many useful helper functions, but these are oftentimes not needed. One of the helper functions that is actually often useful is everything(). We can, for example, use this after using mutate() to put the variable that was just created at the front of the data set to make sure there weren’t any unexpected issues: slumajors_df %&gt;% mutate(propfemale = percfemale / 100) %&gt;% select(propfemale, everything()) #&gt; # A tibble: 27 x 8 #&gt; propfemale Major nfemales nmales percfemale ntotal #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.694 Anthropology 34 15 69.4 49 #&gt; 2 0.855 Art &amp; Art H… 65 11 85.5 76 #&gt; 3 0.56 Biochemistry 14 11 56 25 #&gt; 4 0.707 Biology 162 67 70.7 229 #&gt; 5 0.350 Business in… 135 251 35.0 386 #&gt; 6 0.65 Chemistry 26 14 65 40 #&gt; 7 0.309 Computer Sc… 21 47 30.9 68 #&gt; 8 0.655 Conservatio… 38 20 65.5 58 #&gt; 9 0.268 Economics 128 349 26.8 477 #&gt; 10 0.708 English 131 54 70.8 185 #&gt; # … with 17 more rows, and 2 more variables: #&gt; # morewomen &lt;chr&gt;, large_majority &lt;chr&gt; Verify that propfemale now appears first in the data set. everything() tacks on all of the remaining variables after propfemale. So, in this case, it’s a useful way to re-order the columns so that what you might be most interested in appears first. 4.2.3 slice() and filter(): Choose Rows Instead of choosing which columns to keep, we can also choose certain rows to keep using either slice() or filter(). slice() allows you to specify the row numbers corresponding to rows that you want to keep. For example, suppose that we only want to keep the rows with the five most popular majors: slumajors_df %&gt;% arrange(desc(ntotal)) %&gt;% slice(1, 2, 3, 4, 5) #&gt; # A tibble: 5 x 7 #&gt; Major nfemales nmales percfemale ntotal morewomen #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 Economics 128 349 26.8 477 No #&gt; 2 Business in t… 135 251 35.0 386 No #&gt; 3 Psychology 278 61 82.0 339 Yes #&gt; 4 Government 127 116 52.3 243 Yes #&gt; 5 Biology 162 67 70.7 229 Yes #&gt; # … with 1 more variable: large_majority &lt;chr&gt; We can alternatively use slice(1:5), which is shorthand for slice(1, 2, 3, 4, 5). While slice() is useful, it is relatively simple. We’ll come back to it again in a few weeks as well when we discuss subsetting in base R. filter() is a way to keep rows by specifying a condition related to one or more of the variables in the data set. We’ve already seen conditions in if_else() and case_when() statements, but they’ll now be used to “filter” the rows in our data set. We can keep rows based on a categorical variable or a quantitative variable or a combination of any number of categorical and quantitative variables. R uses the following symbols to make comparisons. We’ve already been using the more intuitive symbols (like &lt; and &gt;): &lt; and &lt;= for less than and less than or equal to, respectively &gt; and &gt;= for greater than and greater than or equal to, respectively == for equal to (careful: equal to is a double equal sign ==) != for not equal to (in general, ! denotes “not”) It’s probably time for a change of data set too! We’ll be working with the babynames data set for the rest of this chapter: library(babynames) babynames #&gt; # A tibble: 1,924,665 x 5 #&gt; year sex name n prop #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1880 F Mary 7065 0.0724 #&gt; 2 1880 F Anna 2604 0.0267 #&gt; 3 1880 F Emma 2003 0.0205 #&gt; 4 1880 F Elizabeth 1939 0.0199 #&gt; 5 1880 F Minnie 1746 0.0179 #&gt; 6 1880 F Margaret 1578 0.0162 #&gt; 7 1880 F Ida 1472 0.0151 #&gt; 8 1880 F Alice 1414 0.0145 #&gt; 9 1880 F Bertha 1320 0.0135 #&gt; 10 1880 F Sarah 1288 0.0132 #&gt; # … with 1,924,655 more rows If needed, we can remind ourselves what is in the babynames data set by typing ?babynames in the console window. What do the following statements do? See if you can guess before running the code. babynames %&gt;% filter(name == &quot;Matthew&quot;) babynames %&gt;% filter(year &gt;= 2000) babynames %&gt;% filter(sex != &quot;M&quot;) babynames %&gt;% filter(prop &gt; 0.05) babynames %&gt;% filter(year == max(year)) Why are some things put in quotes, like \"Matthew\" while some things aren’t, like 2000? Can you make out a pattern? We can also combine conditions on multiple variables in filter() using Boolean operators. We’ve already seen one of these in the case_when() statement above: &amp; means “and.” Look at the Venn diagrams in R for Data Science to learn about the various Boolean operators you can use in R: https://r4ds.had.co.nz/transform.html#logical-operators. The Boolean operators can be used in other functions in R as well, as we’ve already seen with if_else() and case_when(). The following gives some examples. See if you can figure out what each line of code is doing before running it. babynames %&gt;% filter(n &gt; 20000 | prop &gt; 0.05) babynames %&gt;% filter(sex == &quot;F&quot; &amp; name == &quot;Mary&quot;) babynames %&gt;% filter(sex == &quot;F&quot; &amp; name == &quot;Mary&quot; &amp; prop &gt; 0.05) 4.2.4 Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 4.6. What happens when you arrange() by one of the categorical variables in the slumajors_df data set? * Use select() and everything() to put the large_majority variable as the first column in the slumajors_df data set. * In the babynames data set, use filter(), mutate() with rank(), and arrange() to print the 10 most popular Male babynames in 2017. In the babynames data set, use filter() to keep only the rows with your name (or, another name that interests you) and one sex (either \"M\" or \"F\"). Name the new data set something and then construct a line plot that looks at the either the n or prop of your chosen name through year. 4.3 summarise() and group_by(): Create Summaries The summarise() function is useful to get summaries from the data. For example, suppose that we want to know the average major size at SLU across the five year span or the total number of majors across those five years. Then we can use summarise() and a summary function, like mean(), sum(), median(), max(), min(), n(), etc. You’ll notice that the format of summarise() is extremely similar to the format of mutate(). Using the slumajors_df data again just for one quick example, slumajors_df %&gt;% summarise(meantotalmajor = mean(ntotal), totalgrad = sum(ntotal)) #&gt; # A tibble: 1 x 2 #&gt; meantotalmajor totalgrad #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 124. 3347 4.3.1 group_by(): Groups summarise() is often most useful when paired with a group_by() statement. Doing so allows us to get summaries across different groups. For example, suppose that you wanted the total number of registered births per year in the babynames data set: babynames %&gt;% group_by(year) %&gt;% summarise(totalbirths = sum(n)) #&gt; # A tibble: 138 x 2 #&gt; year totalbirths #&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 1880 201484 #&gt; 2 1881 192696 #&gt; 3 1882 221533 #&gt; 4 1883 216946 #&gt; 5 1884 243462 #&gt; 6 1885 240854 #&gt; 7 1886 255317 #&gt; 8 1887 247394 #&gt; 9 1888 299473 #&gt; 10 1889 288946 #&gt; # … with 128 more rows group_by() takes a grouping variable, and then, using summarise() computes the given summary function on each group. Most summary functions are intuitive if you’ve had intro stat. But, if you’re not sure whether the summary for getting the maximum is maximum() or max(), just try both! The n() function can be used within summarise() to obtain the number of observations. It will give you the total number of rows, if used without group_by() babynames %&gt;% summarise(totalobs = n()) #&gt; # A tibble: 1 x 1 #&gt; totalobs #&gt; &lt;int&gt; #&gt; 1 1924665 Note that n() typically doesn’t have any inputs. It’s typically more useful when paired with group_by(): this allows us to see the number of observations within each year, for instance: babynames %&gt;% group_by(year) %&gt;% summarise(ngroup = n()) #&gt; # A tibble: 138 x 2 #&gt; year ngroup #&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 1880 2000 #&gt; 2 1881 1935 #&gt; 3 1882 2127 #&gt; 4 1883 2084 #&gt; 5 1884 2297 #&gt; 6 1885 2294 #&gt; 7 1886 2392 #&gt; 8 1887 2373 #&gt; 9 1888 2651 #&gt; 10 1889 2590 #&gt; # … with 128 more rows 4.3.2 Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 4.6. Compare summarise() with mutate() using the following code. What’s the difference between the two functions? slumajors_df %&gt;% summarise(meantotalmajor = mean(ntotal), totalgrad = sum(ntotal)) slumajors_df %&gt;% mutate(meantotalmajor = mean(ntotal), totalgrad = sum(ntotal)) %&gt;% select(meantotalmajor, totalgrad, everything()) Using the data set from the group_by() and n() combination, babynames %&gt;% group_by(year) %&gt;% summarise(ngroup = n()) #&gt; # A tibble: 138 x 2 #&gt; year ngroup #&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 1880 2000 #&gt; 2 1881 1935 #&gt; 3 1882 2127 #&gt; 4 1883 2084 #&gt; 5 1884 2297 #&gt; 6 1885 2294 #&gt; 7 1886 2392 #&gt; 8 1887 2373 #&gt; 9 1888 2651 #&gt; 10 1889 2590 #&gt; # … with 128 more rows make a line plot with ngroup on the x-axis and year on the y-axis. How would you interpret the plot? * Create a data set that has a column for name and a column that shows the total number of births for that name across all years and both sexes. * group_by() can also be used with other functions, including mutate(). Use group_by() and mutate() to rank the names from most to least popular in each year-sex combination. * From the data set in 4, filter() the data to keep only the most popular name in each year-sex combination and then construct a summary table showing how many times each name appears as the most popular name. * Run the following code. Intuitively, a slice(1, 2, 3, 4, 5) should grab the first five rows of the data set, but, when we try to run that, we get 1380 rows. Try to figure out what the issue is by using Google to search something like “dplyr not slicing correctly after using group by.” What do you find? babynames_test &lt;- babynames %&gt;% group_by(year, sex) %&gt;% mutate(ntest = n / prop) babynames_test %&gt;% slice(1, 2, 3, 4, 5) #&gt; # A tibble: 1,380 x 6 #&gt; # Groups: year, sex [276] #&gt; year sex name n prop ntest #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1880 F Mary 7065 0.0724 97605. #&gt; 2 1880 F Anna 2604 0.0267 97605. #&gt; 3 1880 F Emma 2003 0.0205 97605. #&gt; 4 1880 F Elizabeth 1939 0.0199 97605. #&gt; 5 1880 F Minnie 1746 0.0179 97605. #&gt; 6 1880 M John 9655 0.0815 118400. #&gt; 7 1880 M William 9532 0.0805 118400. #&gt; 8 1880 M James 5927 0.0501 118400. #&gt; 9 1880 M Charles 5348 0.0452 118400. #&gt; 10 1880 M George 5126 0.0433 118400. #&gt; # … with 1,370 more rows 4.4 Missing Values Both of the data sets that we’ve worked with are nice in that they do not have any missing values. We’ll see plenty of examples of data sets with missing values later, so we should examine how the various functions that we’ve talked about so far tackle missing values. Missing values in R are denoted with NA for “Not Available.” Run the following code to create a toy data set with some missing values so that we can see how the various functions we’ve used so far deal with NA values. toy_df &lt;- tibble(x = c(NA, 3, 4, 7), y = c(1, 4, 3, 2), z = c(&quot;A&quot;, &quot;A&quot;, &quot;B&quot;, NA)) toy_df #&gt; # A tibble: 4 x 3 #&gt; x y z #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 NA 1 A #&gt; 2 3 4 A #&gt; 3 4 3 B #&gt; 4 7 2 &lt;NA&gt; 4.4.1 Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 4.6. * mutate(). Try to create a new variable with mutate() involving x. What does R do with the missing value? arrange(). Try arranging the data set by x. What does R do with the missing value? filter(). Try filtering so that only observations where x is less than 5 are kept. What does R do with the missing value? summarise(). Try using summarise() with a function involving x. What does R return? group_by() and summarise(). To your statement in 4, add a group_by(z) statement before your summarise(). What does R return now? 4.4.2 Removing Missing Values Missing values should not be removed without carefully examination and a note of what the consequences might be (e.g. why are these values missing?). We have a toy data set that is meaningless, so we aren’t asking those questions now, but we will for any data set that does have missing values! If we have investigated the missing values and are comfortable with removing them, many functions that we would use in summarise() have an na.rm argument that we can set to TRUE to tell summarise() to remove any NAs before taking the mean(), median(), max(), etc. toy_df %&gt;% summarise(meanx = mean(x, na.rm = TRUE)) #&gt; # A tibble: 1 x 1 #&gt; meanx #&gt; &lt;dbl&gt; #&gt; 1 4.67 If we want to remove the missing values more directly, we can use the is.na() function in combination with filter(). If the variable is NA (Not Available) for an observation, is.na() evaluates to TRUE; if not, is.na() evaluates to FALSE. Test this out using mutate() to create a new variable for whether Median is missing: toy_df %&gt;% mutate(missingx = is.na(x)) #&gt; # A tibble: 4 x 4 #&gt; x y z missingx #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; #&gt; 1 NA 1 A TRUE #&gt; 2 3 4 A FALSE #&gt; 3 4 3 B FALSE #&gt; 4 7 2 &lt;NA&gt; FALSE missingx is TRUE only for the the first observation. We can use this to our advantage with filter() to filter it out of the data set, without going through the extra step of actually making a new variable missingx: toy_df %&gt;% filter(is.na(x) != TRUE) #&gt; # A tibble: 3 x 3 #&gt; x y z #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 3 4 A #&gt; 2 4 3 B #&gt; 3 7 2 &lt;NA&gt; You’ll commonly see this written as short-hand in people’s code you may come across as: toy_df %&gt;% filter(!is.na(x)) #&gt; # A tibble: 3 x 3 #&gt; x y z #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 3 4 A #&gt; 2 4 3 B #&gt; 3 7 2 &lt;NA&gt; which says to “keep anything that does not have a missing x value” (recall that the ! means “not”). 4.5 Chapter Exercises We found both in the SLU majors data set and in the FiveThirtyEight majors data set that Statistics has a higher proportion of women than almost all other STEM fields. Read the first two sections of this article. Write 2-3 sentences about the article’s reasoning of why there are more women in statistics than in other STEM fields. * a. Choose 5 names that interest you and create a new data set that only has data on those 5 names. Use group_by() and summarise() to add together the number of Females and Males for each name in each year. Hint: you can group_by() more than one variable! Make a line plot showing the popularity of these 5 names over time. Choose a year and a sex that interests you and filter the data set to only contain observations from that year and sex. Create a new variable that ranks the names from most popular to least popular. Create a bar plot that shows the 10 most popular names as well as the count for each name. * In some cases throughout this chapter, we’ve renamed data sets using &lt;- with the same name like toy_df &lt;- toy_df %&gt;% mutate(newvar = x / y) In other cases, we’ve given the data set a new name, like toy_small &lt;- toy_df %&gt;% filter(!is.na(x)) For which of the functions below is a generally “safe” to name the data set using the same name after using the function. Why? mutate() arrange() filter() summarise() select() Pose a question about the babynames data set and then answer your question with either a graphic or a data summary. 4.6 Exercise Solutions 4.6.1 mutate() S * Create a new variable that is called major_size and is “large” when the total number of majors is 100 or more and “small” when the total number of majors is less than 100. slumajors_df %&gt;% mutate(major_size = if_else(ntotal &gt;= 100, true = &quot;large&quot;, false = &quot;small&quot;)) #&gt; # A tibble: 27 x 8 #&gt; Major nfemales nmales percfemale ntotal morewomen #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 Anthropology 34 15 69.4 49 Yes #&gt; 2 Art &amp; Art Hi… 65 11 85.5 76 Yes #&gt; 3 Biochemistry 14 11 56 25 Yes #&gt; 4 Biology 162 67 70.7 229 Yes #&gt; 5 Business in … 135 251 35.0 386 No #&gt; 6 Chemistry 26 14 65 40 Yes #&gt; 7 Computer Sci… 21 47 30.9 68 No #&gt; 8 Conservation… 38 20 65.5 58 Yes #&gt; 9 Economics 128 349 26.8 477 No #&gt; 10 English 131 54 70.8 185 Yes #&gt; # … with 17 more rows, and 2 more variables: #&gt; # large_majority &lt;chr&gt;, major_size &lt;chr&gt; ## OR slumajors_df %&gt;% mutate(major_size = case_when(ntotal &gt;= 100 ~ &quot;large&quot;, ntotal &lt; 100 ~ &quot;small&quot;)) #&gt; # A tibble: 27 x 8 #&gt; Major nfemales nmales percfemale ntotal morewomen #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 Anthropology 34 15 69.4 49 Yes #&gt; 2 Art &amp; Art Hi… 65 11 85.5 76 Yes #&gt; 3 Biochemistry 14 11 56 25 Yes #&gt; 4 Biology 162 67 70.7 229 Yes #&gt; 5 Business in … 135 251 35.0 386 No #&gt; 6 Chemistry 26 14 65 40 Yes #&gt; 7 Computer Sci… 21 47 30.9 68 No #&gt; 8 Conservation… 38 20 65.5 58 Yes #&gt; 9 Economics 128 349 26.8 477 No #&gt; 10 English 131 54 70.8 185 Yes #&gt; # … with 17 more rows, and 2 more variables: #&gt; # large_majority &lt;chr&gt;, major_size &lt;chr&gt; * Investigate what happens with case_when() when you give overlapping conditions and when you give conditions that don’t cover all observations. For overlapping conditions, create a variable testcase that is \"Yes\" when percfemale is greater than or equal to 40 and \"No\" when percfemale is greater than 60 For conditions that don’t cover all observations, create a variable testcase2 that is \"Yes\" when percefemale is greater than or equal to 55 and \"No\" when percfemale is less than 35. #&gt; # A tibble: 27 x 9 #&gt; Major nfemales nmales percfemale ntotal morewomen #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 Anthropology 34 15 69.4 49 Yes #&gt; 2 Art &amp; Art Hi… 65 11 85.5 76 Yes #&gt; 3 Biochemistry 14 11 56 25 Yes #&gt; 4 Biology 162 67 70.7 229 Yes #&gt; 5 Business in … 135 251 35.0 386 No #&gt; 6 Chemistry 26 14 65 40 Yes #&gt; 7 Computer Sci… 21 47 30.9 68 No #&gt; 8 Conservation… 38 20 65.5 58 Yes #&gt; 9 Economics 128 349 26.8 477 No #&gt; 10 English 131 54 70.8 185 Yes #&gt; # … with 17 more rows, and 3 more variables: #&gt; # large_majority &lt;chr&gt;, testcase &lt;chr&gt;, testcase2 &lt;chr&gt; For overlapping cases, case_when prioritizes the first case given. For non-coverage, any observation that is not covered is given an NA. 4.6.2 arrange(), select(), …. S * Use select() and everything() to put the large_majority variable as the first column in the slumajors_df data set. slumajors_df %&gt;% select(large_majority, everything()) #&gt; # A tibble: 27 x 7 #&gt; large_majority Major nfemales nmales percfemale ntotal #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 none Anthrop… 34 15 69.4 49 #&gt; 2 female Art &amp; A… 65 11 85.5 76 #&gt; 3 none Biochem… 14 11 56 25 #&gt; 4 female Biology 162 67 70.7 229 #&gt; 5 none Busines… 135 251 35.0 386 #&gt; 6 none Chemist… 26 14 65 40 #&gt; 7 none Compute… 21 47 30.9 68 #&gt; 8 none Conserv… 38 20 65.5 58 #&gt; 9 male Economi… 128 349 26.8 477 #&gt; 10 female English 131 54 70.8 185 #&gt; # … with 17 more rows, and 1 more variable: morewomen &lt;chr&gt; * In the babynames data set, use filter(), mutate() with rank(), and arrange() to print the 10 most popular Male babynames in 2017. babynames %&gt;% filter(sex == &quot;M&quot; &amp; year == 2017) %&gt;% mutate(rankname = rank(desc(n))) %&gt;% filter(rankname &lt;= 10) #&gt; # A tibble: 10 x 6 #&gt; year sex name n prop rankname #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2017 M Liam 18728 0.00954 1 #&gt; 2 2017 M Noah 18326 0.00933 2 #&gt; 3 2017 M William 14904 0.00759 3 #&gt; 4 2017 M James 14232 0.00725 4 #&gt; 5 2017 M Logan 13974 0.00712 5 #&gt; 6 2017 M Benjamin 13733 0.00699 6 #&gt; 7 2017 M Mason 13502 0.00688 7 #&gt; 8 2017 M Elijah 13268 0.00676 8 #&gt; 9 2017 M Oliver 13141 0.00669 9 #&gt; 10 2017 M Jacob 13106 0.00668 10 4.6.3 summarise() and group_by() S * Create a data set that has a column for name and a column that shows the total number of births for that name across all years and both sexes. babynames %&gt;% group_by(name) %&gt;% summarise(totalbirths = sum(n)) #&gt; # A tibble: 97,310 x 2 #&gt; name totalbirths #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Aaban 107 #&gt; 2 Aabha 35 #&gt; 3 Aabid 10 #&gt; 4 Aabir 5 #&gt; 5 Aabriella 32 #&gt; 6 Aada 5 #&gt; 7 Aadam 254 #&gt; 8 Aadan 130 #&gt; 9 Aadarsh 199 #&gt; 10 Aaden 4658 #&gt; # … with 97,300 more rows * group_by() can also be used with other functions, including mutate(). Use group_by() and mutate() to rank the names from most to least popular in each year-sex combination. ranked_babynames &lt;- babynames %&gt;% group_by(year, sex) %&gt;% mutate(rankname = rank((desc(n)))) * From the data set in 4, filter() the data to keep only the most popular name in each year-sex combination and then construct a summary table showing how many times each name appears as the most popular name. ranked_babynames %&gt;% filter(rankname == 1) %&gt;% group_by(name) %&gt;% summarise(nappear = n()) %&gt;% arrange(desc(nappear)) #&gt; # A tibble: 18 x 2 #&gt; name nappear #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Mary 76 #&gt; 2 John 44 #&gt; 3 Michael 44 #&gt; 4 Robert 17 #&gt; 5 Jennifer 15 #&gt; 6 Jacob 14 #&gt; 7 James 13 #&gt; 8 Emily 12 #&gt; 9 Jessica 9 #&gt; 10 Lisa 8 #&gt; 11 Linda 6 #&gt; 12 Emma 5 #&gt; 13 Noah 4 #&gt; 14 Sophia 3 #&gt; 15 Ashley 2 #&gt; 16 Isabella 2 #&gt; 17 David 1 #&gt; 18 Liam 1 * Run the following code. Intuitively, a slice(1, 2, 3, 4, 5) should grab the first five rows of the data set, but, when we try to run that, we get 1380 rows. Try to figure out what the issue is by using Google to search something like “dplyr not slicing correctly after using group by.” What do you find? babynames_test &lt;- babynames %&gt;% group_by(year, sex) %&gt;% mutate(ntest = n / prop) babynames_test %&gt;% slice(1, 2, 3, 4, 5) #&gt; # A tibble: 1,380 x 6 #&gt; # Groups: year, sex [276] #&gt; year sex name n prop ntest #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1880 F Mary 7065 0.0724 97605. #&gt; 2 1880 F Anna 2604 0.0267 97605. #&gt; 3 1880 F Emma 2003 0.0205 97605. #&gt; 4 1880 F Elizabeth 1939 0.0199 97605. #&gt; 5 1880 F Minnie 1746 0.0179 97605. #&gt; 6 1880 M John 9655 0.0815 118400. #&gt; 7 1880 M William 9532 0.0805 118400. #&gt; 8 1880 M James 5927 0.0501 118400. #&gt; 9 1880 M Charles 5348 0.0452 118400. #&gt; 10 1880 M George 5126 0.0433 118400. #&gt; # … with 1,370 more rows Functions like slice() and rank() operate on defined groups in the data set if using a function like group_by() first. Sometimes this feature is quite convenient. But, if we no longer want slice() or rank() or other functions to account for these groups, we need to add an ungroup() pipe, which simply drops the groups that we had formed: babynames_test %&gt;% ungroup() %&gt;% slice(1:5) #&gt; # A tibble: 5 x 6 #&gt; year sex name n prop ntest #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1880 F Mary 7065 0.0724 97605. #&gt; 2 1880 F Anna 2604 0.0267 97605. #&gt; 3 1880 F Emma 2003 0.0205 97605. #&gt; 4 1880 F Elizabeth 1939 0.0199 97605. #&gt; 5 1880 F Minnie 1746 0.0179 97605. 4.6.4 Missing Values S * mutate(). Try to create a new variable with mutate() involving x. What does R do with the missing value? toy_df %&gt;% mutate(xy = x * y) #&gt; # A tibble: 4 x 5 #&gt; x y z newvar xy #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 NA 1 A NA NA #&gt; 2 3 4 A 0.75 12 #&gt; 3 4 3 B 1.33 12 #&gt; 4 7 2 &lt;NA&gt; 3.5 14 R puts another NA in place of x times y for the observation with the missing x. 4.6.5 Chapter Exercises S * a. Choose 5 names that interest you and create a new data set that only has data on those 5 names. Use group_by() and summarise() to add together the number of Females and Males for each name in each year. Hint: you can group_by() more than one variable! Make a line plot showing the popularity of these 5 names over time. baby5 &lt;- babynames %&gt;% filter(name == &quot;Matthew&quot; | name == &quot;Ivan&quot; | name == &quot;Jessica&quot; | name == &quot;Robin&quot; | name == &quot;Michael&quot;) baby5_tot &lt;- baby5 %&gt;% group_by(year, name) %&gt;% summarise(ntot = sum(n)) #&gt; `summarise()` has grouped output by &#39;year&#39;. You can override using the `.groups` argument. ggplot(data = baby5_tot, aes(x = year, y = ntot, colour = name)) + geom_line() * In some cases throughout this chapter, we’ve renamed data sets using &lt;- with the same name like toy_df &lt;- toy_df %&gt;% mutate(newvar = x / y) In other cases, we’ve given the data set a new name, like toy_small &lt;- toy_df %&gt;% filter(!is.na(x)) For which of the functions below is a generally “safe” to name the data set using the same name after using the function. Why? mutate() Usually fine: mutating creates a new variable, which doesn’t change any of the other variables in the data set, if things get messed up with the new variable. arrange() Usually fine: ordering the rows a certain way won’t change any plots and doesn’t change any of the underlying data. filter() Usually not the best practice. Naming the data set the same name after the filter means that you permanently lose data that you filtered out, unless you re-read in the data set at the beginning. summarise() Usually not the best practice. Again, naming the summarized data set the same as the original data means that you lose the original data, unless you re-read it in at the beginning. For example, toy_df &lt;- toy_df %&gt;% summarise(meanx = mean(x)) toy_df #&gt; # A tibble: 1 x 1 #&gt; meanx #&gt; &lt;dbl&gt; #&gt; 1 NA means that we now have no way to access the original data in toy_df. select() This can sometimes be okay if you’re sure that the variables you are removing won’t ever be used. 4.7 Non-Exercise R Code library(babynames) head(babynames) library(tidyverse) slumajors_df &lt;- read_csv(&quot;data/SLU_Majors_15_19.csv&quot;) slumajors_df slumajors_df %&gt;% mutate(ntotal = nfemales + nmales) slumajors_df %&gt;% mutate(percfemale = 100 * nfemales / (nfemales + nmales)) slumajors_df &lt;- slumajors_df %&gt;% mutate(percfemale = 100 * nfemales / (nfemales + nmales)) slumajors_df &lt;- slumajors_df %&gt;% mutate(ntotal = nfemales + nmales) slumajors_df &lt;- slumajors_df %&gt;% mutate(ntotal = nfemales + nmales) %&gt;% mutate(percfemale = 100 * nfemales / (nfemales + nmales)) mutate(mutate(slumajors_df, ntotal = nfemales + nmales), percfemale = 100 * nfemales / (nfemales + nmales)) slumajors_df %&gt;% mutate(morewomen = if_else(percfemale &gt; 50, true = &quot;Yes&quot;, false = &quot;No&quot;)) slumajors_df %&gt;% mutate(large_majority = case_when(percfemale &gt;= 70 ~ &quot;female&quot;, percfemale &lt;= 30 ~ &quot;male&quot;, percfemale &gt; 30 &amp; percfemale &lt; 70 ~ &quot;none&quot;)) slumajors_df &lt;- slumajors_df %&gt;% mutate(morewomen = if_else(percfemale &gt; 50, true = &quot;Yes&quot;, false = &quot;No&quot;)) %&gt;% mutate(large_majority = case_when(percfemale &gt;= 70 ~ &quot;female&quot;, percfemale &lt;= 30 ~ &quot;male&quot;, percfemale &gt; 30 &amp; percfemale &lt; 70 ~ &quot;none&quot;)) slumajors_df %&gt;% arrange(percfemale) slumajors_df %&gt;% arrange(desc(percfemale)) slumajors_df %&gt;% select(Major, ntotal) slumajors_df %&gt;% select(-ntotal, -nfemales, -nmales) slumajors_df %&gt;% mutate(propfemale = percfemale / 100) %&gt;% select(propfemale, everything()) slumajors_df %&gt;% arrange(desc(ntotal)) %&gt;% slice(1, 2, 3, 4, 5) library(babynames) babynames babynames %&gt;% filter(name == &quot;Matthew&quot;) babynames %&gt;% filter(year &gt;= 2000) babynames %&gt;% filter(sex != &quot;M&quot;) babynames %&gt;% filter(prop &gt; 0.05) babynames %&gt;% filter(year == max(year)) babynames %&gt;% filter(n &gt; 20000 | prop &gt; 0.05) babynames %&gt;% filter(sex == &quot;F&quot; &amp; name == &quot;Mary&quot;) babynames %&gt;% filter(sex == &quot;F&quot; &amp; name == &quot;Mary&quot; &amp; prop &gt; 0.05) slumajors_df %&gt;% summarise(meantotalmajor = mean(ntotal), totalgrad = sum(ntotal)) babynames %&gt;% group_by(year) %&gt;% summarise(totalbirths = sum(n)) babynames %&gt;% summarise(totalobs = n()) babynames %&gt;% group_by(year) %&gt;% summarise(ngroup = n()) toy_df %&gt;% summarise(meanx = mean(x, na.rm = TRUE)) toy_df %&gt;% mutate(missingx = is.na(x)) toy_df %&gt;% filter(is.na(x) != TRUE) toy_df %&gt;% filter(!is.na(x)) "],["tidying-with-tidyr.html", " 5 Tidying with tidyr 5.1 What is Tidy Data? 5.2 separate() and unite() Columns 5.3 Reshaping with pivot_() 5.4 Skimming Data with skimr 5.5 Chapter Exercises 5.6 Exercise Solutions 5.7 Non-Exercise R Code", " 5 Tidying with tidyr Goals: describe what it means for a data set to be tidy. use separate() and unite() to transform a data set into tidy form. use pivot_longer() and pivot_wider() to transform a data set into tidy form. combine tidyr functions with dplyr and ggplot2 functions to form a more complete workflow. The Data: We will first use a polling data set that contains variables collected from a few different polls in July 2016 for the U.S. presidential election. The data set was scraped from RealClear politics https://www.realclearpolitics.com/epolls/latest_polls/president/ by Dr. Ramler. The variables are: Poll, the name of the poll Date, the date range that the poll was conducted Sample, contains the sample size of the poll and whether the poll was of Likely Voters or Registered Voters MoE, the margin of error of the poll (recall this term from IntroStat) Clinton (D), the percentage of people in the poll voting for Clinton Trump (R), the percentage of people in the poll voting for Trump Johnson (L), the percentage of people in the poll voting for Johnson Steing (G), the percentage of people in the poll voting for Stein 5.1 What is Tidy Data? R usually (but not always) works best when your data is in tidy form. A tidy data set has a few characteristics. Note that you should already be quite familiar with tidy data because, up to this point, all of the data sets we have used in this class (and probably most of the data sets that you see in STAT 113 an all of the data sets that you may have seen in STAT 213) are tidy. This definition of tidy data is taken from R for Data Science: every variable in the data set is stored in its own column every case in the data set is stored in its own row each value of a variable is stored in one cell values in the data set should not contain units there should not be any table headers or footnotes We will begin by focusing on the first characteristic: every variable in a the data set should be stored in its own column (and correspondingly, number 3: each value of a variable should be stored in one cell). 5.2 separate() and unite() Columns In a fresh .Rmd file (File -&gt; New File -&gt; R Markdown) that is in your Notes project, copy and paste the following code into an R chunk: library(tidyverse) polls &lt;- read_csv(&quot;data/rcp-polls.csv&quot;, na = &quot;--&quot;) polls #&gt; # A tibble: 7 x 8 #&gt; Poll Date Sample MoE `Clinton (D)` `Trump (R)` #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Monmouth 7/14 -… 688 LV 3.7 45 43 #&gt; 2 CNN/ORC 7/13 -… 872 RV 3.5 42 37 #&gt; 3 ABC News/W… 7/11 -… 816 RV 4 42 38 #&gt; 4 NBC News/W… 7/9 - … 1000 … 3.1 41 35 #&gt; 5 Economist/… 7/9 - … 932 RV 4.5 40 37 #&gt; 6 Associated… 7/7 - … 837 RV NA 40 36 #&gt; 7 McClatchy/… 7/5 - … 1053 … 3 40 35 #&gt; # … with 2 more variables: Johnson (L) &lt;dbl&gt;, #&gt; # Stein (G) &lt;dbl&gt; Suppose that you wanted to know what the average sample size of the polls was. Using dplyr functions, polls %&gt;% summarise(meansample = mean(Sample)) What warning do you get? Why? You would get a similar warning (or sometimes an error) any time that you want to try to use Sample size in plotting or summaries. The issue is that the Sample column actually contains two variables so the data set is not tidy. 5.2.1 separate() a Column Let’s separate() the two variables into Sample_size and Sample_type: polls %&gt;% separate(col = Sample, into = c(&quot;Sample_size&quot;, &quot;Sample_type&quot;), sep = &quot; &quot;) #&gt; # A tibble: 7 x 9 #&gt; Poll Date Sample_size Sample_type MoE `Clinton (D)` #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Monmouth 7/14… 688 LV 3.7 45 #&gt; 2 CNN/ORC 7/13… 872 RV 3.5 42 #&gt; 3 ABC New… 7/11… 816 RV 4 42 #&gt; 4 NBC New… 7/9 … 1000 RV 3.1 41 #&gt; 5 Economi… 7/9 … 932 RV 4.5 40 #&gt; 6 Associa… 7/7 … 837 RV NA 40 #&gt; 7 McClatc… 7/5 … 1053 RV 3 40 #&gt; # … with 3 more variables: Trump (R) &lt;dbl&gt;, #&gt; # Johnson (L) &lt;dbl&gt;, Stein (G) &lt;dbl&gt; The arguments to separate() are fairly easy to learn: col is the name of the column in the data set you want to separate. into is the name of the new columns. These could be anything you want, and are entered in as a vector (with c() to separate the names) sep is the character that you want to separate the column by. In this case, the sample size and sample type were separated by whitespace, so our sep = \" \", white space. The sep argument is the newest piece of information here. Note that even using sep = \"\" will produce an error (there is not a space now, so R doesn’t know what to separate by). polls %&gt;% separate(col = Sample, into = c(&quot;Sample_size&quot;, &quot;Sample_type&quot;), sep = &quot;&quot;) Similarly, we would like the Date column to be separated into a poll start date and a poll end date: polls_sep &lt;- polls %&gt;% separate(col = Date, into = c(&quot;Start&quot;, &quot;End&quot;), sep = &quot; - &quot;) Why should you use \" - \" as the separator instead of \"-\"? Try using \"-\" if you aren’t sure: you shouldn’t get an error but something should look off. What happened to Sample? Why is it back to its un-separated form? 5.2.2 unite() Columns unite() is the “opposite” of separate(): use it when one variable is stored across multiple columns, but each row still represents a single case. The need to use unite() is less common than separate(). In our current data set, there is no need to use it at all. But, for the sake of seeing an example, let’s separate the Start date into month and day and then use unite() to re-unite those columns: polls_sillytest &lt;- polls_sep %&gt;% separate(col = Start, into = c(&quot;Start_month&quot;, &quot;Start_day&quot;), sep = &quot;/&quot;) polls_sillytest #&gt; # A tibble: 7 x 10 #&gt; Poll Start_month Start_day End Sample MoE #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Monmouth 7 14 7/16 688 LV 3.7 #&gt; 2 CNN/ORC 7 13 7/16 872 RV 3.5 #&gt; 3 ABC News/Wash Po… 7 11 7/14 816 RV 4 #&gt; 4 NBC News/Wall St… 7 9 7/13 1000 … 3.1 #&gt; 5 Economist/YouGov 7 9 7/11 932 RV 4.5 #&gt; 6 Associated Press… 7 7 7/11 837 RV NA #&gt; 7 McClatchy/Marist 7 5 7/9 1053 … 3 #&gt; # … with 4 more variables: Clinton (D) &lt;dbl&gt;, #&gt; # Trump (R) &lt;dbl&gt;, Johnson (L) &lt;dbl&gt;, Stein (G) &lt;dbl&gt; This situation could occur in practice: the date variable is in multiple columns: one for month and one for day (and if there are multiple years, there could be a third for year). We would use unite() to combine these two columns into a single Date, called New_start_date: polls_sillytest %&gt;% unite(&quot;New_start_date&quot;, c(Start_month, Start_day), sep = &quot;/&quot;) #&gt; # A tibble: 7 x 9 #&gt; Poll New_start_date End Sample MoE `Clinton (D)` #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Monmouth 7/14 7/16 688 LV 3.7 45 #&gt; 2 CNN/ORC 7/13 7/16 872 RV 3.5 42 #&gt; 3 ABC News/… 7/11 7/14 816 RV 4 42 #&gt; 4 NBC News/… 7/9 7/13 1000 … 3.1 41 #&gt; 5 Economist… 7/9 7/11 932 RV 4.5 40 #&gt; 6 Associate… 7/7 7/11 837 RV NA 40 #&gt; 7 McClatchy… 7/5 7/9 1053 … 3 40 #&gt; # … with 3 more variables: Trump (R) &lt;dbl&gt;, #&gt; # Johnson (L) &lt;dbl&gt;, Stein (G) &lt;dbl&gt; Note how unite() just switches around the first two arguments of separate(). Argument 1 is now the name of the new column and Argument 2 is the names of columns in the data set that you want to combine. We have also used the c() function in separate() and unite(). While c() is a very general R function and isn’t specific to tidy data, this is the first time that we’re seeing it in this course. c() officially stands for concatenate, but, in simpler terms, c() combines two or more “things,” separated by a comma. c(1, 4, 2) #&gt; [1] 1 4 2 c(&quot;A&quot;, &quot;A&quot;, &quot;D&quot;) #&gt; [1] &quot;A&quot; &quot;A&quot; &quot;D&quot; This is useful if a function argument expects two or more “things”: for example, in separate(), the into argument requires two column names for this example. Those column names must be specified by combining the names together with c(). 5.2.3 Column Names and rename() You might have noticed that the columns with percentage of votes for Clinton, Trump, etc. are surrounded by backticks ` ` when you print polls or polls_sep: polls_sep #&gt; # A tibble: 7 x 9 #&gt; Poll Start End Sample MoE `Clinton (D)` `Trump (R)` #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Monmou… 7/14 7/16 688 LV 3.7 45 43 #&gt; 2 CNN/ORC 7/13 7/16 872 RV 3.5 42 37 #&gt; 3 ABC Ne… 7/11 7/14 816 RV 4 42 38 #&gt; 4 NBC Ne… 7/9 7/13 1000 … 3.1 41 35 #&gt; 5 Econom… 7/9 7/11 932 RV 4.5 40 37 #&gt; 6 Associ… 7/7 7/11 837 RV NA 40 36 #&gt; 7 McClat… 7/5 7/9 1053 … 3 40 35 #&gt; # … with 2 more variables: Johnson (L) &lt;dbl&gt;, #&gt; # Stein (G) &lt;dbl&gt; This happens because the column names have a space in them (this also would occur if the columns started with a number or had odd special characters in them). Then, any time you want to reference a variable, you need the include the backticks: polls_sep %&gt;% summarise(meanclinton = mean(Clinton (D))) ## throws an error polls_sep %&gt;% summarise(meanclinton = mean(`Clinton (D)`)) ## backticks save the day! Having variable names with spaces doesn’t technically violate any principle of tidy data, but it can be quite annoying. Always using backticks can be a huge pain. We can rename variables easily with rename(), which just takes a series of new_name = old_name arguments. polls_new &lt;- polls_sep %&gt;% rename(Clinton_D = `Clinton (D)`, Trump_R = `Trump (R)`, Johnson_L = `Johnson (L)`, Stein_G = `Stein (G)`) polls_new #&gt; # A tibble: 7 x 9 #&gt; Poll Start End Sample MoE Clinton_D Trump_R Johnson_L #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Monm… 7/14 7/16 688 LV 3.7 45 43 5 #&gt; 2 CNN/… 7/13 7/16 872 RV 3.5 42 37 13 #&gt; 3 ABC … 7/11 7/14 816 RV 4 42 38 8 #&gt; 4 NBC … 7/9 7/13 1000 … 3.1 41 35 11 #&gt; 5 Econ… 7/9 7/11 932 RV 4.5 40 37 5 #&gt; 6 Asso… 7/7 7/11 837 RV NA 40 36 6 #&gt; 7 McCl… 7/5 7/9 1053 … 3 40 35 10 #&gt; # … with 1 more variable: Stein_G &lt;dbl&gt; rename() can also be very useful if you have variable names that are very long to type out. rename() is actually from dplyr, not tidyr, but we didn’t have a need for it with any of the dplyr data sets. 5.2.4 Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 5.6. The MLB salary data set contains salaries on all 862 players in Major League Baseball in 2016. The data set was obtained from http://www.usatoday.com/sports/mlb/salaries/2016/player/all/ Read in the data using the following code chunk and write a sentence or two that explains why the data set is not tidy. library(tidyverse) baseball_df &lt;- read_csv(&quot;data/mlb2016.csv&quot;) head(baseball_df) #&gt; # A tibble: 6 x 7 #&gt; Name Team POS Salary Years Total.Value Avg.Annual #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Clayto… LAD SP $ 33,00… 7 (20… $ 215,000,… $ 30,714,… #&gt; 2 Zack G… ARI SP $ 31,79… 6 (20… $ 206,500,… $ 34,416,… #&gt; 3 David … BOS SP $ 30,00… 7 (20… $ 217,000,… $ 31,000,… #&gt; 4 Miguel… DET 1B $ 28,00… 10 (2… $ 292,000,… $ 29,200,… #&gt; 5 Justin… DET SP $ 28,00… 7 (20… $ 180,000,… $ 25,714,… #&gt; 6 Yoenis… NYM CF $ 27,32… 3 (20… $ 75,000,0… $ 25,000,… * Tidy the data set just so that Duration of the salary contract (currently given in the Year column) is in its own column the year range (also currently given in the Year column) is split into a variable called Start and a variable called End year that give the start and end years of the contract. You can still have special characters for now (like ( and )) in the start and end year. You should have received a warning message. What does this message mean? See if you can figure it out by typing View(baseball_df) in your console window and scrolling down to some of the rows that the warning mentions: 48, 59, 60, etc. We won’t learn about parse_number() until readr, but the function is straightforward enough to mention here. It’s useful when you have extra characters in the values of a numeric variable (like a $ or a (), but you just want to grab the actual number: baseball_df &lt;- baseball_df %&gt;% mutate(Salary = parse_number(Salary), Total.Value = parse_number(Total.Value), Avg.Annual = parse_number(Avg.Annual), Start = parse_number(Start), End = parse_number(End)) Run the code above so that the parsing is saved to baseball_df. * Using a function from dplyr. fix the End variable that you created so that, for example, the first observation is 2020 instead of just 20. * tidyr is extremely useful, but it’s not glamorous. What you end up with is a data set that ggplot2 and dplyr can use to do cool things. So, let’s do something with our tidy data set to make all that tidying a little worth it before moving on. Make a graphic that investigates how player Salary compares for different POS. * State the reason why making that plot would not have worked before we tidied the data set. 5.3 Reshaping with pivot_() We will continue to use the polling data set to introduce the pivoting functions and data reshaping. To make sure that we are all working with the same data set, run the following line of code: polls_clean &lt;- polls %&gt;% separate(col = Sample, into = c(&quot;Sample_size&quot;, &quot;Sample_type&quot;), sep = &quot; &quot;) %&gt;% separate(col = Date, into = c(&quot;Start&quot;, &quot;End&quot;), sep = &quot; - &quot;) %&gt;% rename(Clinton_D = `Clinton (D)`, Trump_R = `Trump (R)`, Johnson_L = `Johnson (L)`, Stein_G = `Stein (G)`) polls_clean #&gt; # A tibble: 7 x 10 #&gt; Poll Start End Sample_size Sample_type MoE Clinton_D #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Monmo… 7/14 7/16 688 LV 3.7 45 #&gt; 2 CNN/O… 7/13 7/16 872 RV 3.5 42 #&gt; 3 ABC N… 7/11 7/14 816 RV 4 42 #&gt; 4 NBC N… 7/9 7/13 1000 RV 3.1 41 #&gt; 5 Econo… 7/9 7/11 932 RV 4.5 40 #&gt; 6 Assoc… 7/7 7/11 837 RV NA 40 #&gt; 7 McCla… 7/5 7/9 1053 RV 3 40 #&gt; # … with 3 more variables: Trump_R &lt;dbl&gt;, Johnson_L &lt;dbl&gt;, #&gt; # Stein_G &lt;dbl&gt; The data set polls_clean still isn’t tidy!! The candidate variable is spread out over 4 different columns and the values in each of these 4 columns actually represent 1 variable: poll percentage. Thinking about data “tidyness” using the definitions above can sometimes be a little bit confusing. In practice, oftentimes we will usually realize that a data set is untidy when we go to do something that should be super simple but that something turns out to not be super simple at all when the data is in its current form. For example, one thing we might want to do is to make a plot that has poll Start time on the x-axis, polling numbers on the y-axis, and has candidates represented by different colours. For this small data set, we might not see any trends through time, but you could imagine this graph would be quite useful if we had polling numbers through June, July, August, September, etc. Take a moment to think about how you would make this graph in ggplot2: what is your x-axis variable? What variable are you specifying for the y-axis? For the colours? A first attempt in making a graph would be: ggplot(data = polls_clean, aes(x = Start, y = Clinton_D)) + geom_point(aes(colour = ....??????????)) And we’re stuck. It’s certainly not impossible to make the graph with the data in its current form (keep adding geom_point() and re-specifying the aesthetics, then manually specify colours, then manually specify a legend), but it’s definitely a huge pain. This is where pivot_longer() can help! https://www.youtube.com/watch?v=8w3wmQAMoxQ 5.3.1 pivot_longer() to Gather Columns pivot_longer() “pivots” the data set so that is has more rows (hence the “longer”) by collapsing multiple columns into two columns. One new column is a “key” column, which is the new variable containing the old data set’s column names. The second new column is a “value” column, which is the new variable containing the old data set’s values for each of the old data set’s column names. It’s easier to see this with an example. We know from our plotting exercise above that we’d really like a candidate variable to colour by and a poll_percent variable for the y-axis of our plot. So, we can use pivot_longer() to make these two columns: polls_clean %&gt;% pivot_longer(cols = c(Clinton_D, Trump_R, Johnson_L, Stein_G), names_to = &quot;candidate&quot;, values_to = &quot;poll_percent&quot;) #&gt; # A tibble: 28 x 8 #&gt; Poll Start End Sample_size Sample_type MoE candidate #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 Monm… 7/14 7/16 688 LV 3.7 Clinton_D #&gt; 2 Monm… 7/14 7/16 688 LV 3.7 Trump_R #&gt; 3 Monm… 7/14 7/16 688 LV 3.7 Johnson_L #&gt; 4 Monm… 7/14 7/16 688 LV 3.7 Stein_G #&gt; 5 CNN/… 7/13 7/16 872 RV 3.5 Clinton_D #&gt; 6 CNN/… 7/13 7/16 872 RV 3.5 Trump_R #&gt; 7 CNN/… 7/13 7/16 872 RV 3.5 Johnson_L #&gt; 8 CNN/… 7/13 7/16 872 RV 3.5 Stein_G #&gt; 9 ABC … 7/11 7/14 816 RV 4 Clinton_D #&gt; 10 ABC … 7/11 7/14 816 RV 4 Trump_R #&gt; # … with 18 more rows, and 1 more variable: #&gt; # poll_percent &lt;dbl&gt; pivot_longer() has three important arguments: cols, the names of the columns that you want to PIVOT! names_to, the name of the new variable that will have the old column names (anything you want it to be!) values_to, the name of the new variable that will have the old column values (anything you want it to be!) What happens when you omit names_to and values_to arguments? Give it a try! Now we can make our plot using Week 1 ggplot functions. But don’t forget to give a name to our new “long” data set first! polls_long &lt;- polls_clean %&gt;% pivot_longer(cols = c(Clinton_D, Trump_R, Johnson_L, Stein_G), names_to = &quot;candidate&quot;, values_to = &quot;poll_percent&quot;) ## ignore as.Date for now....we will get to dates later! ggplot(data = polls_long, aes(x = as.Date(Start, &quot;%m/%d&quot;), y = poll_percent, colour = candidate)) + geom_point() + xlab(&quot;Poll Start Date&quot;) 5.3.2 pivot_wider() to Spread to Multiple Columns The “opposite” of pivot_longer() is pivot_wider(). We need to use pivot_wider() when one case is actually spread across multiple rows. Again, I typically will realize there is an issue with untidy data when I go to do something that should be simple and it’s not. Let’s examine some airline safety data that fivethirtyeight used in their Should Travelers Avoid Flying Airlines that Have Had Crashes in the Past? story: https://fivethirtyeight.com/features/should-travelers-avoid-flying-airlines-that-have-had-crashes-in-the-past/ . The raw data can be found here. airlines &lt;- read_csv(&quot;data/airline-safety.csv&quot;) head(airlines) #&gt; # A tibble: 6 x 8 #&gt; airline avail_seat_km_p… `incidents 1985… `fatal_accident… #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Aer Li… 320906734 2 0 #&gt; 2 Aerofl… 1197672318 76 14 #&gt; 3 Aeroli… 385803648 6 0 #&gt; 4 Aerome… 596871813 3 1 #&gt; 5 Air Ca… 1865253802 2 0 #&gt; 6 Air Fr… 3004002661 14 4 #&gt; # … with 4 more variables: fatalities 1985_1999 &lt;dbl&gt;, #&gt; # incidents 2000_2014 &lt;dbl&gt;, #&gt; # fatal_accidents 2000_2014 &lt;dbl&gt;, #&gt; # fatalities 2000_2014 &lt;dbl&gt; The data set contains the following columns: airline, the name of the airline avail_seat_km_per_week, the available seat kilometers flown each week incidents 1985_1999, the number of incidents between 1985 and 1999 fatal_accidents 1985_1999, the number of fatal accidents between 1985 and 1999 fatalities 1985_1999, the number of fatalities between 1985 and 1999 incidents 2000_2014 fatal_accidents 2000_2014 fatalities 2000_2014 There’s a whole lot of mess in this data set: we really want a variable for year that has two values (1985-1999 and 2000-2014). Sometimes it’s tough to know where to even start, but one strategy is to draw a sketch of a data frame that you’d like to end with. For example, we think that we want a data set with the following columns: airline, available seat km, years, incidents, fatal accidents, and fatalities. So, our sketch might look something like: airline avail years incidents fatalacc fatalities airline1 1009391 1985-1999 0 1 2 airline1 1009391 2000-2014 9 1 1 airline2 2141 1985-1999 2 0 0 etc. Let’s start with pivot_longer() to see if we can get year to be its own variable (We know that a year variable, which is what we want, will make more rows so pivot_longer() seems like a good place to start): airlines %&gt;% pivot_longer(c(3, 4, 5, 6, 7, 8), names_to = &quot;type_year&quot;, values_to = &quot;total_num&quot;) #&gt; # A tibble: 336 x 4 #&gt; airline avail_seat_km_per_… type_year total_num #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Aer Ling… 320906734 incidents 1985_1… 2 #&gt; 2 Aer Ling… 320906734 fatal_accidents … 0 #&gt; 3 Aer Ling… 320906734 fatalities 1985_… 0 #&gt; 4 Aer Ling… 320906734 incidents 2000_2… 0 #&gt; 5 Aer Ling… 320906734 fatal_accidents … 0 #&gt; 6 Aer Ling… 320906734 fatalities 2000_… 0 #&gt; 7 Aeroflot* 1197672318 incidents 1985_1… 76 #&gt; 8 Aeroflot* 1197672318 fatal_accidents … 14 #&gt; 9 Aeroflot* 1197672318 fatalities 1985_… 128 #&gt; 10 Aeroflot* 1197672318 incidents 2000_2… 6 #&gt; # … with 326 more rows Instead of giving pivot_longer() names of variables, we gave it the column numbers instead. So c(3, 4, 5, 6, 7, 8) corresponds to the 3rd, 4th, …., 8th columns in the data set. That didn’t quite give us a year variable, but we should be excited to see an opportunity to take advantage of separate(): airlines %&gt;% pivot_longer(c(3, 4, 5, 6, 7, 8), names_to = &quot;type_year&quot;, values_to = &quot;total_num&quot;) %&gt;% separate(type_year, into = c(&quot;type&quot;, &quot;year&quot;), sep = &quot; &quot;) #&gt; # A tibble: 336 x 5 #&gt; airline avail_seat_km_per_… type year total_num #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Aer Ling… 320906734 incidents 1985_… 2 #&gt; 2 Aer Ling… 320906734 fatal_acc… 1985_… 0 #&gt; 3 Aer Ling… 320906734 fatalities 1985_… 0 #&gt; 4 Aer Ling… 320906734 incidents 2000_… 0 #&gt; 5 Aer Ling… 320906734 fatal_acc… 2000_… 0 #&gt; 6 Aer Ling… 320906734 fatalities 2000_… 0 #&gt; 7 Aeroflot* 1197672318 incidents 1985_… 76 #&gt; 8 Aeroflot* 1197672318 fatal_acc… 1985_… 14 #&gt; 9 Aeroflot* 1197672318 fatalities 1985_… 128 #&gt; 10 Aeroflot* 1197672318 incidents 2000_… 6 #&gt; # … with 326 more rows Is this the format that we want the data set to be in? Depending on the task, it could be. But, we also might want each of the accident types to be its own variable. That is, we might want to collapse the data set to have a variable for incidents, a variable for fatal_accidents, and a variable for fatalities. If so, we want to add more columns to the data set, so we need to use pivot_wider(). ## name the long data set airlines_long &lt;- airlines %&gt;% pivot_longer(c(3, 4, 5, 6, 7, 8), names_to = &quot;type_year&quot;, values_to = &quot;total_num&quot;) %&gt;% separate(type_year, into = c(&quot;type&quot;, &quot;year&quot;), sep = &quot; &quot;) ## use pivot_wider() to create variables for incidents, fatalities, and ## fatal_accidents: airlines_long %&gt;% pivot_wider(names_from = type, values_from = total_num) #&gt; # A tibble: 112 x 6 #&gt; airline avail_seat_km_p… year incidents fatal_accidents #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Aer Lin… 320906734 1985… 2 0 #&gt; 2 Aer Lin… 320906734 2000… 0 0 #&gt; 3 Aeroflo… 1197672318 1985… 76 14 #&gt; 4 Aeroflo… 1197672318 2000… 6 1 #&gt; 5 Aerolin… 385803648 1985… 6 0 #&gt; 6 Aerolin… 385803648 2000… 1 0 #&gt; 7 Aeromex… 596871813 1985… 3 1 #&gt; 8 Aeromex… 596871813 2000… 5 0 #&gt; 9 Air Can… 1865253802 1985… 2 0 #&gt; 10 Air Can… 1865253802 2000… 2 0 #&gt; # … with 102 more rows, and 1 more variable: #&gt; # fatalities &lt;dbl&gt; pivot_wider() has two main arguments: names_from, the column in the old data set that will provide the names of the new columns and values_from, the column in the old data set that will provide the values that fill in the new columns We will see more examples of pivot_wider() and pivot_longer() in the Exercises. Note that tidy data isn’t necessarily always better: you might find cases where you need to “untidy” the data by using pivot_longer() or pivot_wider(). However, most functions in R (and in other languages) work best with tidy data. 5.3.3 Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 5.6. Once you have a handle on data science terminology, it’s not too difficult to transfer what you’ve learned to a different language. For example, students in computer science might be more familiar with Python. Google something like “pivot from wide to long in python” to find help on achieving the equivalent of pivot_longer() in Python. The UBSprices2 data set contains information on prices of common commodities in cities throughout the world in the years 2003 and 2009. The three commodities in the data set are Rice (1 kg worth), Bread (1 kg worth), and a Big Mac https://media1.giphy.com/media/Fw5LicDKem6nC/source.gif prices_df &lt;- read_csv(&quot;data/UBSprices2.csv&quot;) * Convert the data set to a tidier form so that there is a year variable and a commodity variable that has 3 values: \"bigmac\", \"bread\", and \"rice\" Hint: At some point, you will need to separate the commodity from the year in, for example, bread2009. But, you’ll notice this is different than our other uses of separate() because there is no “-” or \" \" or “/” to use as a separating character. Look at the help for separate() and scroll down to the sep argument to see if you can figure out this issue. The first code chunk below shows the solution for this particular issue in case you only get stuck on this part while the second code chunk shows the entire solution. separate(name_of_variable, into = c(&quot;newname1&quot;, &quot;newname2&quot;), sep = -4) * Convert your data set from the previous exercise so that commodity is split up into 3 variables: bigmac price, rice price and bread price. In which data set would it be easiest to make a line plot with year on the x-axis and price of rice on the y-axis with lines for each city? In which data set would it be easiest to make a line chart with 3 lines, one for each type of commodity, for the city of Amsterdam? If you have time, make these plots! 5.4 Skimming Data with skimr We’ve now talked about plotting, wrangling, and tidying data. When you first load a data set, it can be helpful to obtain a quick summary of all of the variables in that data set. Doing so gives you an idea about how many observations are missing, which variables are numeric and which are character, and the basic distribution of each variable. A helpful function for this is the skim() function in the skimr package. Load the package with library(skimr) and obtain summaries of the data sets we’ve used so far: library(skimr) skim(airlines) Table 5.1: Data summary Name airlines Number of rows 56 Number of columns 8 _______________________ Column type frequency: character 1 numeric 7 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace airline 0 1 3 26 0 56 0 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist avail_seat_km_per_week 0 1 1.384621e+09 1.465317e+09 259373346 474036223 802908893.0 1.847239e+09 7139291291 ▇▂▁▁▁ incidents 1985_1999 0 1 7.180000e+00 1.104000e+01 0 2 4.0 8.000000e+00 76 ▇▁▁▁▁ fatal_accidents 1985_1999 0 1 2.180000e+00 2.860000e+00 0 0 1.0 3.000000e+00 14 ▇▃▁▁▁ fatalities 1985_1999 0 1 1.124100e+02 1.466900e+02 0 0 48.5 1.842500e+02 535 ▇▁▂▁▁ incidents 2000_2014 0 1 4.120000e+00 4.540000e+00 0 1 3.0 5.250000e+00 24 ▇▃▁▁▁ fatal_accidents 2000_2014 0 1 6.600000e-01 8.600000e-01 0 0 0.0 1.000000e+00 3 ▇▃▁▃▁ fatalities 2000_2014 0 1 5.552000e+01 1.113300e+02 0 0 0.0 8.325000e+01 537 ▇▁▁▁▁ Find in the output the following: the number of rows in the data set and the number of columns the number of missing values for each variable the number of unique values for each character variable the completion rate (the proportion of values that are non-missing). In the future, we will use skim() to get a brief summary of data sets before we begin working with them. There are a few more topics to discuss in tidying data. We have not yet discussed the 4th or 5th characteristics of tidy data (cells should not contain units and there should be no headers or footers), but these are usually dealt with when we read in the data. Therefore, these issues will be covered when we discuss readr. 5.4.1 Exercises {exercise-4-3} Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 5.6. The under5mortality.csv file contains data on mortality for people under the age of 5 in countries around the world (mortality in deaths per 1000 people). The data come from https://www.gapminder.org/data/. The data set is extremely wide in its current form, having a column for each year in the data set. Read in the data set with mortality_df &lt;- read_csv(&quot;data/under5mortality.csv&quot;) head(mortality_df) #&gt; # A tibble: 6 x 217 #&gt; `Under five mor… `1800` `1801` `1802` `1803` `1804` `1805` #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Abkhazia NA NA NA NA NA NA #&gt; 2 Afghanistan 469. 469. 469. 469. 469. 469. #&gt; 3 Akrotiri and Dh… NA NA NA NA NA NA #&gt; 4 Albania 375. 375. 375. 375. 375. 375. #&gt; 5 Algeria 460. 460. 460. 460. 460. 460. #&gt; 6 American Samoa NA NA NA NA NA NA #&gt; # … with 210 more variables: 1806 &lt;dbl&gt;, 1807 &lt;dbl&gt;, #&gt; # 1808 &lt;dbl&gt;, 1809 &lt;dbl&gt;, 1810 &lt;dbl&gt;, 1811 &lt;dbl&gt;, #&gt; # 1812 &lt;dbl&gt;, 1813 &lt;dbl&gt;, 1814 &lt;dbl&gt;, 1815 &lt;dbl&gt;, #&gt; # 1816 &lt;dbl&gt;, 1817 &lt;dbl&gt;, 1818 &lt;dbl&gt;, 1819 &lt;dbl&gt;, #&gt; # 1820 &lt;dbl&gt;, 1821 &lt;dbl&gt;, 1822 &lt;dbl&gt;, 1823 &lt;dbl&gt;, #&gt; # 1824 &lt;dbl&gt;, 1825 &lt;dbl&gt;, 1826 &lt;dbl&gt;, 1827 &lt;dbl&gt;, #&gt; # 1828 &lt;dbl&gt;, 1829 &lt;dbl&gt;, 1830 &lt;dbl&gt;, 1831 &lt;dbl&gt;, #&gt; # 1832 &lt;dbl&gt;, 1833 &lt;dbl&gt;, 1834 &lt;dbl&gt;, 1835 &lt;dbl&gt;, #&gt; # 1836 &lt;dbl&gt;, 1837 &lt;dbl&gt;, 1838 &lt;dbl&gt;, 1839 &lt;dbl&gt;, #&gt; # 1840 &lt;dbl&gt;, 1841 &lt;dbl&gt;, 1842 &lt;dbl&gt;, 1843 &lt;dbl&gt;, #&gt; # 1844 &lt;dbl&gt;, 1845 &lt;dbl&gt;, 1846 &lt;dbl&gt;, 1847 &lt;dbl&gt;, #&gt; # 1848 &lt;dbl&gt;, 1849 &lt;dbl&gt;, 1850 &lt;dbl&gt;, 1851 &lt;dbl&gt;, #&gt; # 1852 &lt;dbl&gt;, 1853 &lt;dbl&gt;, 1854 &lt;dbl&gt;, 1855 &lt;dbl&gt;, #&gt; # 1856 &lt;dbl&gt;, 1857 &lt;dbl&gt;, 1858 &lt;dbl&gt;, 1859 &lt;dbl&gt;, #&gt; # 1860 &lt;dbl&gt;, 1861 &lt;dbl&gt;, 1862 &lt;dbl&gt;, 1863 &lt;dbl&gt;, #&gt; # 1864 &lt;dbl&gt;, 1865 &lt;dbl&gt;, 1866 &lt;dbl&gt;, 1867 &lt;dbl&gt;, #&gt; # 1868 &lt;dbl&gt;, 1869 &lt;dbl&gt;, 1870 &lt;dbl&gt;, 1871 &lt;dbl&gt;, #&gt; # 1872 &lt;dbl&gt;, 1873 &lt;dbl&gt;, 1874 &lt;dbl&gt;, 1875 &lt;dbl&gt;, #&gt; # 1876 &lt;dbl&gt;, 1877 &lt;dbl&gt;, 1878 &lt;dbl&gt;, 1879 &lt;dbl&gt;, #&gt; # 1880 &lt;dbl&gt;, 1881 &lt;dbl&gt;, 1882 &lt;dbl&gt;, 1883 &lt;dbl&gt;, #&gt; # 1884 &lt;dbl&gt;, 1885 &lt;dbl&gt;, 1886 &lt;dbl&gt;, 1887 &lt;dbl&gt;, #&gt; # 1888 &lt;dbl&gt;, 1889 &lt;dbl&gt;, 1890 &lt;dbl&gt;, 1891 &lt;dbl&gt;, #&gt; # 1892 &lt;dbl&gt;, 1893 &lt;dbl&gt;, 1894 &lt;dbl&gt;, 1895 &lt;dbl&gt;, #&gt; # 1896 &lt;dbl&gt;, 1897 &lt;dbl&gt;, 1898 &lt;dbl&gt;, 1899 &lt;dbl&gt;, #&gt; # 1900 &lt;dbl&gt;, 1901 &lt;dbl&gt;, 1902 &lt;dbl&gt;, 1903 &lt;dbl&gt;, #&gt; # 1904 &lt;dbl&gt;, 1905 &lt;dbl&gt;, … * Use the skim() function in skimr to obtain some preliminary information about the data set. * Notice that there are 217 columns (at the top of the print out of the header, 217 is the second number). When we use tidyr, we aren’t going to want to type out c(2, 3, 4, 5, .....) all the way up to 217! R has short-hand notation that we can use with :. For example, type in 4:9 in your console window. Use this notation to tidy the mortality_df data set. Note: You’ll need to add something to your pivot_longer() function to convert the variable Year to numeric. We haven’t talked too much about variable types yet so, after your values_to = \"Mortality\" statement, add , names_transform = list(Year = as.numeric), making sure you have a second ) to close the pivot_longer() function. Make a line plot to look at the overall under 5 mortality trends for each country. What is the overall trend in under 5 mortality? Does every single country follow this trend? What looks strange about the plot, specifically about the data collected before 1900? Write two short paragraphs about an article found on https://www.r-bloggers.com/. The most important thing for this exercise is to pick an article that interests you. There are many to choose from, with multiple posts being put up each day. For the purposes of this assignment though, find an article where the author actually provides code (most have this but there are a few that are more “big-picture” views of certain topics). In the first paragraph, answer the following: (a) What is the main purpose of the blog post? (b) What data is/are the author(s) using? (c) What are the main findings? (d) Why was it important for the author to have data in their main findings? In the second paragraph, discuss (a) Any code that you see that we have explicitly seen in class, (b) Any code that you see that we have not explicitly seen in class, and (c) anything else you find interesting about your article. Then, copy and paste the URL. 5.5 Chapter Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 5.6. We will use nfl salary data obtained from FiveThirtyEight that were originally obtained from Spotrac.com. The data set has the top 100 paid players for each year for each position from 2011 through 2018, broken down by player position. For those unfamiliar with American football, the positions in the data set are Quarterback, Running Back, Wide Receiver, Tight End, and Offensive Lineman for offense, Cornerback, Defensive Lineman, Linebacker, and Safety for Defense, and a separate category for Special Teams players that includes punters and kickers. You can review a summary of player positions here. We are interested in how salaries compare for the top 100 players in each position and on how salaries have changed through time for each position. Read in the data set with nfl_df &lt;- read_csv(&quot;data/nfl_salary.csv&quot;) Use the skim() and head() functions to look at the data, and then explain why this data set is not in tidy form. * Use a function in tidyr to make the data tidy, and give your tidy data set a new name. * To your data set in the previous exercise, add a ranking variable that ranks the salaries within each player position so that the highest paid players in each position all receive a 1, the second highest paid players receive a 2, etc. Compare your results for the default way that R uses to break ties between two salaries that are the same and using ties.method = \"first\". Hint: See Exercise 4 in 4.3.2 for another example on how to do this. * Find the maximum salary for each player position in each year. Then, create two different line graphs that shows how the maximum salary has changed from 2011 to 2018 for each position. For one line graph, make the colours of the lines different for each position. For the second line graph, facet by position. Which graph do you like better? * The maximum salary is very dependent on one specific player. Make the same graph, but plot the average salary of the top 20 players in each position of each year. What do you notice? Any interesting patterns for any of the positions? If you’re a fan of football, provide a guess as to why one of the positions has had their salary plateau in recent years. * Sometimes for graphs involving cost or salary, we want to take into account the inflation rate. Google what the inflation rate was between 2011 and 2018 (Google something like “inflation rate from 2011 to 2018” and you should be able to find something). Adjust all of the 2011 salaries for inflation so that they are comparable to the 2018 salaries. Then, make a similar line plot as above but ignore all of the years between 2012 and 2017 (so your line plot will just have 2 points per position). After adjusting for inflation, how many positions have average higher salaries for the top 20 players in that position? Construct a graph that shows how much salary decreases moving from higher ranked players to lower ranked players for each position in the year 2018. Why do you think the depreciation is so large for Quarterbacks? 5.6 Exercise Solutions 5.6.1 What is Tidy Data? S 5.6.2 separate() and unite() S * Tidy the data set just so that Duration of the salary contract (currently given in the Year column) is in its own column the year range (also currently given in the Year column) is split into a variable called Start and a variable called End year that give the start and end years of the contract. You can still have special characters for now (like ( and )) in the start and end year. baseball_df &lt;- baseball_df %&gt;% separate(Years, into = c(&quot;Duration&quot;, &quot;Range&quot;), sep = &quot; &quot;) %&gt;% separate(Range, into = c(&quot;Start&quot;, &quot;End&quot;), sep = &quot;-&quot;) * Using a function from dplyr. fix the End variable that you created so that, for example, the first observation is 2020 instead of just 20. baseball_df &lt;- baseball_df %&gt;% mutate(End = End + 2000) This is a somewhat lazy way to do this and could get you into trouble (what if one of the years was in the 1990s?) But, it’s safe for this particular data set. * tidyr is extremely useful, but it’s not glamorous. What you end up with is a data set that ggplot2 and dplyr can use to do cool things. So, let’s do something with our tidy data set to make all that tidying a little worth it before moving on. Make a graphic that investigates how player Salary compares for different POS. ggplot(data = baseball_df, aes(x = POS, y = Salary)) + geom_boxplot() ggplot(data = baseball_df, aes(x = Salary, colour = POS)) + geom_freqpoly() ## boxplots look better in this case * State the reason why making that plot would not have worked before we tidied the data set. The Salary variable had a dollar sign in it so ggplot would not have known how to plot it. 5.6.3 pivot_() S * Convert the data set to a tidier form so that there is a year variable and a commodity variable that has 3 values: \"bigmac\", \"bread\", and \"rice\" Hint: At some point, you will need to separate the commodity from the year in, for example, bread2009. But, you’ll notice this is different than our other uses of separate() because there is no “-” or \" \" or “/” to use as a separating character. Look at the help for separate() and scroll down to the sep argument to see if you can figure out this issue. The first code chunk below shows the solution for this particular issue in case you only get stuck on this part while the second code chunk shows the entire solution. separate(name_of_variable, into = c(&quot;newname1&quot;, &quot;newname2&quot;), sep = -4) prices_long &lt;- prices_df %&gt;% pivot_longer(cols = c(2, 3, 4, 5, 6, 7), names_to = &quot;commod_year&quot;, values_to = &quot;price&quot;) %&gt;% separate(col = &quot;commod_year&quot;, into = c(&quot;commodity&quot;, &quot;year&quot;), sep = -4) head(prices_long) #&gt; # A tibble: 6 x 4 #&gt; city commodity year price #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Amsterdam bigmac 2009 19 #&gt; 2 Amsterdam bread 2009 10 #&gt; 3 Amsterdam rice 2009 11 #&gt; 4 Amsterdam bigmac 2003 16 #&gt; 5 Amsterdam bread 2003 9 #&gt; 6 Amsterdam rice 2003 9 * Convert your data set from the previous exercise so that commodity is split up into 3 variables: bigmac price, rice price and bread price. prices_wide &lt;- prices_long %&gt;% pivot_wider(names_from = commodity, values_from = price) head(prices_wide) #&gt; # A tibble: 6 x 5 #&gt; city year bigmac bread rice #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Amsterdam 2009 19 10 11 #&gt; 2 Amsterdam 2003 16 9 9 #&gt; 3 Athens 2009 30 13 27 #&gt; 4 Athens 2003 21 12 19 #&gt; 5 Auckland 2009 19 19 13 #&gt; 6 Auckland 2003 19 19 9 5.6.4 Skimming Data with skimr() S * Use the skim() function in skimr to obtain some preliminary information about the data set. library(skimr) skim(mortality_df) * Notice that there are 217 columns (at the top of the print out of the header, 217 is the second number). When we use tidyr, we aren’t going to want to type out c(2, 3, 4, 5, .....) all the way up to 217! R has short-hand notation that we can use with :. For example, type in 4:9 in your console window. Use this notation to tidy the mortality_df data set. mortality_long &lt;- mortality_df %&gt;% pivot_longer(cols = 2:217, names_to = &quot;Year&quot;, values_to = &quot;Mortality&quot;, names_transform = list(Year = as.numeric)) You’ll need to add something to your pivot_longer() function to convert the variable Year to numeric. We haven’t talked too much about variable types yet so, after your values_to = \"Mortality\" statement, add , names_transform = list(Year = as.numeric), making sure you have a second ) to close the pivot_longer() function. 5.6.5 Chapter Exercises S * Use a function in tidyr to make the data tidy, and give your tidy data set a new name. nfl_long &lt;- nfl_df %&gt;% pivot_longer(c(2, 3, 4, 5, 6, 7, 8, 9, 10, 11), names_to = &quot;position&quot;, values_to = &quot;salary&quot;) nfl_long #&gt; # A tibble: 8,000 x 3 #&gt; year position salary #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 2011 Cornerback 11265916 #&gt; 2 2011 Defensive Lineman 17818000 #&gt; 3 2011 Linebacker 16420000 #&gt; 4 2011 Offensive Lineman 15960000 #&gt; 5 2011 Quarterback 17228125 #&gt; 6 2011 Running Back 12955000 #&gt; 7 2011 Safety 8871428 #&gt; 8 2011 Special Teamer 4300000 #&gt; 9 2011 Tight End 8734375 #&gt; 10 2011 Wide Receiver 16250000 #&gt; # … with 7,990 more rows * To your data set in the previous exercise, add a ranking variable that ranks the salaries within each player position so that the highest paid players in each position all receive a 1, the second highest paid players receive a 2, etc. Compare your results for the default way that R uses to break ties between two salaries that are the same and using ties.method = \"first\". Hint: See Exercise 4 in 4.3.2 for another example on how to do this. nfl_long_default &lt;- nfl_long %&gt;% group_by(position, year) %&gt;% mutate(rank = rank(desc(salary))) nfl_long &lt;- nfl_long %&gt;% group_by(position, year) %&gt;% mutate(rank = rank(desc(salary), ties.method = &quot;first&quot;)) The first ranking code allows observations that have the same ranking get their rankings averaged together (e.g. two observations tied for 5th would get a ranking of (5 + 6) / 2 = 5.5). In the second ranking method, the first observation in the data set gets the “higher” rank. * Find the maximum salary for each player position in each year. Then, create two different line graphs that shows how the maximum salary has changed from 2011 to 2018 for each position. For one line graph, make the colours of the lines different for each position. For the second line graph, facet by position. Which graph do you like better? nfl_max &lt;- nfl_long %&gt;% group_by(position, year) %&gt;% summarise(maxsal = max(salary, na.rm = TRUE)) #&gt; `summarise()` has grouped output by &#39;position&#39;. You can override using the `.groups` argument. ggplot(data = nfl_max, aes(x = year, y = maxsal, group = position, colour = position)) + geom_line() ggplot(data = nfl_max, aes(x = year, y = maxsal)) + geom_line() + facet_wrap( ~ position) With this number of levels, I personally prefer the faceted graph for its cleaner look. * The maximum salary is very dependent on one specific player. Make the same graph, but plot the average salary of the top 20 players in each position of each year. What do you notice? Any interesting patterns for any of the positions? If you’re a fan of football, provide a guess as to why one of the positions has had their salary plateau in recent years. nfl_rank &lt;- nfl_long %&gt;% filter(rank &lt;= 20) %&gt;% group_by(position, year) %&gt;% summarise(mean20 = mean(salary, na.rm = TRUE)) #&gt; `summarise()` has grouped output by &#39;position&#39;. You can override using the `.groups` argument. ggplot(data = nfl_rank, aes(x = year, y = mean20)) + geom_line() + facet_wrap( ~ position) Running backs haven’t had much of a salary increase whereas all other offensive positions have had a large salary increase. There are many plausible explanations for why this is the case. One is that the NFL is much more of a “passing league” now than it was decades ago. * Sometimes for graphs involving cost or salary, we want to take into account the inflation rate. Google what the inflation rate was between 2011 and 2018 (Google something like “inflation rate from 2011 to 2018” and you should be able to find something). Adjust all of the 2011 salaries for inflation so that they are comparable to the 2018 salaries. Then, make a similar line plot as above but ignore all of the years between 2012 and 2017 (so your line plot will just have 2 points per position). After adjusting for inflation, how many positions have average higher salaries for the top 20 players in that position? ## 11.6% from 2011 to 2018. nfl_inf &lt;- nfl_long %&gt;% mutate(salary_inf = if_else(year == 2011, true = salary * 1.116, false = salary)) %&gt;% filter(year == 2011 | year == 2018) %&gt;% filter(rank &lt;= 20) %&gt;% group_by(position, year) %&gt;% summarise(mean20 = mean(salary, na.rm = TRUE)) #&gt; `summarise()` has grouped output by &#39;position&#39;. You can override using the `.groups` argument. ggplot(data = nfl_inf, aes(x = year, y = mean20)) + geom_line() + geom_point() + facet_wrap( ~ position) All positions have higher salaries, even after adjusting for inflation, except perhaps running backs (it’s too hard to tell from the graph). 5.7 Non-Exercise R Code library(tidyverse) polls &lt;- read_csv(&quot;data/rcp-polls.csv&quot;, na = &quot;--&quot;) polls polls %&gt;% summarise(meansample = mean(Sample)) polls %&gt;% separate(col = Sample, into = c(&quot;Sample_size&quot;, &quot;Sample_type&quot;), sep = &quot; &quot;) polls_sep &lt;- polls %&gt;% separate(col = Date, into = c(&quot;Start&quot;, &quot;End&quot;), sep = &quot; - &quot;) polls_sillytest &lt;- polls_sep %&gt;% separate(col = Start, into = c(&quot;Start_month&quot;, &quot;Start_day&quot;), sep = &quot;/&quot;) polls_sillytest polls_sillytest %&gt;% unite(&quot;New_start_date&quot;, c(Start_month, Start_day), sep = &quot;/&quot;) c(1, 4, 2) c(&quot;A&quot;, &quot;A&quot;, &quot;D&quot;) polls_sep polls_new &lt;- polls_sep %&gt;% rename(Clinton_D = `Clinton (D)`, Trump_R = `Trump (R)`, Johnson_L = `Johnson (L)`, Stein_G = `Stein (G)`) polls_new polls_clean &lt;- polls %&gt;% separate(col = Sample, into = c(&quot;Sample_size&quot;, &quot;Sample_type&quot;), sep = &quot; &quot;) %&gt;% separate(col = Date, into = c(&quot;Start&quot;, &quot;End&quot;), sep = &quot; - &quot;) %&gt;% rename(Clinton_D = `Clinton (D)`, Trump_R = `Trump (R)`, Johnson_L = `Johnson (L)`, Stein_G = `Stein (G)`) polls_clean polls_clean %&gt;% pivot_longer(cols = c(Clinton_D, Trump_R, Johnson_L, Stein_G), names_to = &quot;candidate&quot;, values_to = &quot;poll_percent&quot;) polls_long &lt;- polls_clean %&gt;% pivot_longer(cols = c(Clinton_D, Trump_R, Johnson_L, Stein_G), names_to = &quot;candidate&quot;, values_to = &quot;poll_percent&quot;) ## ignore as.Date for now....we will get to dates later! ggplot(data = polls_long, aes(x = as.Date(Start, &quot;%m/%d&quot;), y = poll_percent, colour = candidate)) + geom_point() + xlab(&quot;Poll Start Date&quot;) airlines &lt;- read_csv(&quot;data/airline-safety.csv&quot;) head(airlines) airlines %&gt;% pivot_longer(c(3, 4, 5, 6, 7, 8), names_to = &quot;type_year&quot;, values_to = &quot;total_num&quot;) airlines %&gt;% pivot_longer(c(3, 4, 5, 6, 7, 8), names_to = &quot;type_year&quot;, values_to = &quot;total_num&quot;) %&gt;% separate(type_year, into = c(&quot;type&quot;, &quot;year&quot;), sep = &quot; &quot;) ## name the long data set airlines_long &lt;- airlines %&gt;% pivot_longer(c(3, 4, 5, 6, 7, 8), names_to = &quot;type_year&quot;, values_to = &quot;total_num&quot;) %&gt;% separate(type_year, into = c(&quot;type&quot;, &quot;year&quot;), sep = &quot; &quot;) ## use pivot_wider() to create variables for incidents, fatalities, and ## fatal_accidents: airlines_long %&gt;% pivot_wider(names_from = type, values_from = total_num) library(skimr) skim(airlines) "],["communication-with-rmarkdown.html", " 6 Communication with RMarkdown 6.1 Reproducbility 6.2 R Markdown Files 6.3 ggplot2 Communication 6.4 Chapter Exercises 6.5 Exercise Solutions 6.6 Non-Exercise R Code", " 6 Communication with RMarkdown Goals: Explain what reproducibility means and explain why it’s important for analyses to be reproducible. Explain why R Markdown provides more tools for making analyses reproducible than base R with Microsoft Word and than Microsoft Excel. Use the Code Options and the Markdown Text Options to modify an R Markdown file so that it knits to a readable, professional .html file. Use titles, labels, colour scales, annotations, and themes to make your plots easy to read, including for people with Colour Vision Deficiency. Overall: If you’re making some quick plots just for you, some of the things on communication won’t apply. But, if you’re planning on sharing results (usually you are, eventually), then communication tools become much more important. 6.1 Reproducbility We’ve been using R Markdown for a while now, but have not yet talked about any of its features or how to do anything except insert a new code chunk. By the end of this section, we want to be able to use some of the R Markdown options to make a nice-looking document (so that you can implement some of these options in your first mini-project). Reproducibility is a concept that has recently gained popularity in the sciences for describing analyses that another researcher is able to repeat. That is, an analysis is reproducible if you provide enough information that the person sitting next to you can obtain identical results as long as they follow your procedures. An analysis is not reproducible if this isn’t the case. R Markdown makes it easy for you to make your analysis reproducible for a couple of reasons: an R Markdown file will not knit unless all of your code runs, meaning that you won’t accidentally give someone code that doesn’t work. R Markdown combines the “coding” steps with the “write-up” steps into one coherent document that contains the code, all figures and tables, and any explanations. 6.1.1 R Scripts vs. R Markdown We’ve been using R Markdown for the entirety of this course. But, you may have noticed that when you go to File -&gt; New File to open a new R Markdown file, there are a ton of other options. The first option is R Script. Go ahead and open a new R Script file now. The file you open should be completely blank. An R Script is a file that reads only R code. It cannot have any text in it at all, unless that text is commented out with a #. For example, you could copy and paste all of the code that is inside a code chunk in a .Rmd file to the .R file and run it line by line. So, what are the advantages and disadvantages of using an R Script file compared to using an R Markdown file? Let’s start with the advantages of R Markdown. R Markdown allows you to fully integrate text explanations of the code and results, the actual tables and figures themselves, and the code to make those tables and figures in one cohesive document. As we will see, if using R Scripts to write-up an analysis in Word, there is a lot of copy-pasting involved of results. For this reason, using R Markdown often results in more reproducible analyses. The advantage of an R Script would be in a situation where you really aren’t presenting results to anyone and you also don’t need any text explanations. This often occurs in two situations. (1) There are a lot of data preparation steps. In this case, you would typically complete all of these data prep steps in an R script and then write the resulting clean data to a .csv that you’d import in an R Markdown file. (2) What you’re doing is complicated statistically. If this is the case, then the code is much more of a focus than the text or creating figures so you’d use an R Script. We will “demo” a reproducible analysis in class on political data. 6.1.2 Spell-Checking If using R Markdown for communication, you probably want to utilize its spell-check feature. Go to Edit -&gt; Check Spelling, and you’ll be presented with a spell-checker that lets you change the spelling of any words you may have misspelled. 6.1.3 Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 6.5. What’s the difference between R and R Markdown? Why is an R Markdown analysis more reproducible than the base R script analysis? Why is an R Markdown analysis easier to make more reproducible than an analysis with Excel? Your friend Chaz is doing a data analysis project in Excel to compare the average GPA of student athletes with the average GPA of non-student athletes. He has two variables: whether or not a student is a student athlete and GPA. He decides that a two-sample t-test is an appropriate procedure for this data (recall from Intro Stat that this procedure is appropriate for comparing a quantitative response (GPA) across two groups). Here are the steps of his analysis. He writes the null and alternative hypotheses in words and in statistical notation. He uses Excel to make a set of side-by-side boxplots. He changes the labels and the limits on the y-axis using Point-and-Click Excel operations. From his boxplots, he see that there are 3 outliers in the non-athlete group. These three students have GPAs of 0 because they were suspended for repeatedly refusing to wear masks indoors. Chaz decides that these 3 students should be removed from the analysis because, if they had stayed enrolled, their GPAs would have been different than 0. He deletes these 3 rows in Excel. Chaz uses the t.test function in Excel to run the test. He writes down the degrees of freedom, the T-stat, and the p-value. Chaz copies his graph to Word and writes a conclusion in context of the problem. State 2 aspects of Chaz’s analysis that are not reproducible. 6.2 R Markdown Files Let’s talk a bit more about the components of the R Markdown file used to make the reproducible analysis shown in class. First, open a new R Markdown file by clicking File -&gt; New File -&gt; R Markdown and keep the new file so that it knits to HTML for now. The first six lines at the top of the file make up the YAML (Yet Another Markup Language) header. We’ll come back to this at the end, as it’s the more frustrating part to learn. Lines 8-10 are the set-up chunk. Again, we’ll come back to this in a bit. For now, just delete lines 12-30 and copy and paste the following code chunks to your clean .Rmd file: library(tidyverse) head(cars) ggplot(data = cars, aes(x = speed, y = dist)) + geom_point() summary(cars) The cars data set is built into R so there’s no need to do anything to read it in (it already exists in R itself). 6.2.1 Code Chunk Options First, knit, your new file (and give it a name, when prompted). You should see some code, a couple of results tables, and a scatterplot. Chunk options allow you to have some control over what gets printed to the file that you knit. For example, you may or may not want: the code to be printed, the figure to be printed, the results to be printed, the tidyverse message to be printed, etc. See page 2 as a reference for R chunk options: https://rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf. There’s a ton of them! We are going to just focus on a few that are more commonly used. echo. This is set to either TRUE to print the code or FALSE to not print the code. After the r in the first line of your first code chunk, add a , echo = FALSE inside the curly braces and reknit to see what happens! You can keep adding other options, each separated by a comma. Some other options include: message. This is set to either TRUE to print messages or FALSE to not print messages. When you load in the tidyverse, a message automatically prints out. In that same code chunk, add a , message = FALSE to get rid of the message. Re-knit to make sure the message is actually gone. warning. This is set to either TRUE to print warnings or FALSE to not print warnings. We don’t have any warnings so changing this in our current code chunks won’t do anything. results. By default, this is set to ‘markup’ and shows results of tables. Change this to ‘hide’ to not print the results. Practice adding a , results = 'hide' to the code chunk in your R Markdown file with summary(cars) and re-knit to make sure the results from summary(cars) are gone. fig.keep. Add fig.keep = 'none' to not print a figure. Practice adding a , fig.keep = 'none' to the code chunk options with the scatterplot and re-knit to make sure the figure is gone. fig.keep can also be set to 'last', in which case R will only keep the last figure created in a code chunk. fig.height and fig.width control the height and width of figures. By default, these are both 7, but I often change the fig.height to make figures shorter (fig.height = 5, for example). fig.cap adds a figure caption to your figure. Try inserting , fig.cap = \"Figure 1: caption text blah blah blah\" to your chunk options. include, eval, and collapse are also sometimes useful: check these out in the reference guide! Finally, you’ll notice that each time you make a new document, there is a code chunk at the beginning called “setup.” By default, setup has echo = TRUE as a global option. A global option is something that gets applied to all code chunks in the entire document. So, having echo = TRUE means that all code chunks will have their code printed, unless specifically overridden in that particular chunk. So, having echo = TRUE means that all code will print, except in chunks where you have set echo = FALSE. You can add other options to the global chunk like fig.height = 5 to make all figures in all chunks have a height of 5 instead of adding this option to each and every chunk. 6.2.2 Figures and Tables We’ve already seen that Figures will pop up automatically (unless we set fig.keep = 'none'), which is quite convenient. Making tables that look nice requires one extra step. Delete the results = 'hide' option that you added earlier. When you knit your .Rmd file now, results tables from head(cars) and summary(cars) look kind of ugly. We will focus on using the kable() function from the knitr package to make these tables much more aesthetically pleasing. Another option is to use the pander() function in the pander package. Both pander() and kable() are very simple functions to generate tables but will be more than sufficient for our purposes. To generate more complicated tables, see the xtable package. To use these functions, simply replace add a %&gt;% pipe with the name of the table function you want to use. head(cars) %&gt;% kable() will make a nice-looking table with kable and head(cars) %&gt;% pander() will use pander(). Before using kable(), you’ll need to load its library by adding the line library(knitr) above head(cars) %&gt;% kable(). Before using pander(), you’ll need to load its library by adding the line library(pander) above head(cars) %&gt;% pander(). Try these out in your R Markdown file. Which table do you like better in this case? There are plenty of options for making tables look presentable, which we will discuss in the Exercises. Keep in mind that you probably wouldn’t use these when making tables for yourself. They’re much more useful when you’re writing a report that you want to share with others. 6.2.3 Non-Code Options R Markdown combines R (in the code chunks, which we’ve already discussed) with the Markdown syntax, which comprises the stuff outside the code chunks, like what you’re reading right now! There are so many Markdown options, but most of the time, if you want to do something specific, you can just Google it. The purpose of what follows is just to get us familiar with the very basics and things you will probably use most often. Bullet Points and Sub-bullet Points: Denoted with a * and -, respectively. The sub bullets should be indented by 4 spaces. Note that bullet points are not code and should not appear in a code chunk. * Bullet 1 * Bullet 2 - Sub bullet 1 - Sub bullet 2 - Sub bullet 3 Note: Everything in Markdown is very particular with spacing. Things often have to be very precise. I personally just love it, but it can be frustrating sometimes. For example, indenting a sub-bullet by 3 spaces instead of 4 spaces will not make a sub-bullet. * Bullet 1 - Sub bullet 1 Numbered Lists are the same as bulleted ones, except * is replaced with numbers 1., 2., etc. Bold, Italics, Code. Surround text with __bold text__ to make text bold, _italic text_ to make text Italics, and backticks to make text look like Code. Links: The simplest way to create a link to something on the web is to surround it with &lt; &gt; as in &lt;https://www.youtube.com/watch?v=gJf_DDAfDXs&gt; If you want to name you link something other than the web address, use [name of link](https://www.youtube.com/watch?v=gJf_DDAfDXs), which should show up in your knitted document as “name of link” and, when clicked on, take you to the youtube video. Headers: Headers are created with ## with fewer hashtags resulting in a bigger Header. Typing in #Big Header at the beginning of a line would make a big header, ### Medium Header would make a medium header, and ##### Small Header would make a small header. Headers are important because they get mapped to a table of contents. There’s a lot of other stuff to explore: &lt;a href=“https://rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf” target=\"blank&gt; https://rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf . But, if you want to do something other than the basics, Google will definitely help. 6.2.4 YAML Finally, we can return to what’s given at the top of every .Rmd file: a YAML header. The YAML header is the most frustrating part to change because it’s the most particular with spacing. The biggest thing that you can take advantage of with YAML is themes that other people have written for R Markdown. By default, we’re just using R Markdown’s default theme, which looks okay. One package that you can use to make a “pretty” themed document easily is the rmdformats package using the readthedown theme. This does a lot of the heavy lifting in making the resulting .html file super nice to look at. For example, paste the following lines over your YAML header in your current .Rmd file. --- title: &quot;Week 4: Communication with `R Markdown` and `ggplot2`&quot; author: &quot;Matt Higham&quot; output: rmdformats::readthedown: toc_depth: 5 --- The toc_depth: 5 controls the required number of header ##### for something to appear in the Table of Contents (toc). The document you just knitted won’t have anything in the table of contents because you haven’t included any headers with 5 or fewer hashtags. As mentioned before, the spacing with these YAML headers is extremely important. For example, delete one of the spaces at the beginning of the line for toc_depth: 5 in the R Markdown file you created. The file should no longer knit, all because we deleted a single space. Another package that you could use to create a pretty .html file is the prettydoc package (you might recall using this package for reports if you took STAT 213 with Professor Higham). Try copying and pasting the following over your .Rmd YAML header in the R Markdown file you created: --- title: &quot;Title&quot; author: &quot;Name&quot; date: &quot;Put Today&#39;s Date&quot; output: prettydoc::html_pretty: theme: hpstr toc: true --- prettydoc has 5 themes for you to choose from. The YAML header above uses hpstr. Other choices are cayman, tactile, architect, and leonids. 6.2.5 Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 5.6. For the rest of this section, we will use the built-in R data set mtcars, which has observations on makes and models of cars. The variables we will be using are: cyl, the number of cylinders a car has mpg, the mileage of the car, in miles per gallon Because the data set is loaded every time R is started up, there is no need to have a line that reads in the data set. We can examine the first few observations with head(mtcars) #&gt; mpg cyl disp hp drat wt qsec vs am #&gt; Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 #&gt; Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 #&gt; Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 #&gt; Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 #&gt; Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 #&gt; Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 #&gt; gear carb #&gt; Mazda RX4 4 4 #&gt; Mazda RX4 Wag 4 4 #&gt; Datsun 710 4 1 #&gt; Hornet 4 Drive 3 1 #&gt; Hornet Sportabout 3 2 #&gt; Valiant 3 1 * Create a table showing the mean mpg for each cyl group (cyl stands for cylinder and can be 4-cylinder, 6-cylinder, or 8-cylinder) with both kable() and pander(). Hint: remember to call the knitr library and the pander library. * Type ?kable into your console window and scroll through the Help file. Change the rounding of the mean so that it only displays one number after the decimal. Then, add a caption to the table that says “My First Table Caption!!” * Google “How to Change Column Names in kable” and replace the column names with “Cylinder Numb.” and “Mean Mileage.” Find a table that you plan to use in your first mini-project. Use the column names, caption, and digits options to make this table look nicer with the kable() function. Create a new R chunk and copy and paste the following into your new R chunk. Don’t worry about what factor() is doing: we will cover that next week! library(tidyverse) head(mtcars) ggplot(data = mtcars, aes(x = factor(cyl), y = mpg)) + geom_boxplot() Modify the R chunk so that: (a) the figure height is 3, (b) the code from the R chunk shows in the .html file, (c) the results from running head(cars) are hidden in the .html file. Make (b) and (c) a local chunk option, but set (a) as a global option that applies to all of your R chunks. Change your global options in your mini-project to (a) hide messages from loading in the tidyverse and (b) show all code. Use bullet points in the Introduction in your first mini-project that explains what a few of the important variables are. Then, add a header to your mini-project that marks the Introduction. Change the YAML header in your mini-project so that you are the Author and so that the file uses either one of the prettydoc themes or the readthedown theme. 6.3 ggplot2 Communication When we first introduced plotting, we used histograms, boxplots, frequency plots, bar plots, scatterplots, line plots, and more to help us explore our data set. You will probably make many different plots in a single analysis, and, when exploring, it’s fine to keep these plots unlabeled and untitled with the default colour scheme and theme. They’re just for you, and you typically understand the data and what each variable means. However, when you’ve finished exploring and you’d like to communicate your results, both graphically and numerically, you’ll likely want to tweak your plots to look more aesthetically pleasing. You certainly wouldn’t be presenting every exploratory plot you made so this tweaking needs to be done on only a few plots. You might consider: changing the x-axis and y-axis labels, changing the legend title, adding a title, adding a subtitle, and adding a caption with + labs() changing the limits of the x-axis and y-axis with + xlim() and + ylim() changing the colour scheme to be more visually appealing and easy to see for people with colour-vision-deficiency (CVD) labeling certain points or lines with + geom_label() or + geom_text() changing from the default theme with + theme_&lt;name_of_theme&gt;() The bullet about labeling only certain points is the data set is one reason why we are doing this second ggplot2 section now, as opposed to immediately after the first ggplot2 section. As we will see, we’ll make use of combining what we’ve learned in dplyr to help us label interesting observations in our plots. The Data The Happy Planet Index (HPI) is a measure of how efficiently a country uses its ecological resources to give its citizens long “happy” lives. You can read more about this data here: here. But, the basic idea is that the HPI is a metric that computes how happy and healthy a country’s citizens are, but adjusts that by that country’s ecological footprint (how much “damage” the country does to planet Earth). The data set was obtained from https://github.com/aepoetry/happy_planet_index_2016. Variables in the data set are: HPIRank, the rank of the country’s Happy Planet Index (lower is better) Country, the name of the country LifeExpectancy, the average life expectancy of a citizen (in years) Wellbeing, the average well being score (on a scale from 1 - 10). See the ladder question in the documentation for how this was calculated. HappyLifeYears, a combination of LifeExpectancy and Wellbeing Footprint, the ecological footprint per person (higher footprint means the average person in the country is less ecologically friendly) Read in the data set with library(tidyverse) hpi_df &lt;- read_csv(&quot;data/hpi-tidy.csv&quot;) head(hpi_df) #&gt; # A tibble: 6 x 11 #&gt; HPIRank Country LifeExpectancy Wellbeing HappyLifeYears #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 109 Afghanist… 48.7 4.76 29.0 #&gt; 2 18 Albania 76.9 5.27 48.8 #&gt; 3 26 Algeria 73.1 5.24 46.2 #&gt; 4 127 Angola 51.1 4.21 28.2 #&gt; 5 17 Argentina 75.9 6.44 55.0 #&gt; 6 53 Armenia 74.2 4.37 41.9 #&gt; # … with 6 more variables: Footprint &lt;dbl&gt;, #&gt; # HappyPlanetIndex &lt;dbl&gt;, Population &lt;dbl&gt;, #&gt; # GDPcapita &lt;dbl&gt;, GovernanceRank &lt;chr&gt;, Region &lt;chr&gt; Let’s look at the relationship between HappyLifeYears and Footprint for countries of different Regions of the world. ggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears, colour = Region)) + geom_point() Which region seems to have the most variability in their Ecological Footprint? 6.3.1 Change Labels and Titles We can add + labs() to change various labels and titles throughout the plot: ggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears, colour = Region)) + geom_point() + labs(title = &quot;Countries with a Higher Ecological Footprint Tend to Have Citizens with Longer, Happier Lives&quot;, ## add title subtitle = &quot;HappyLifeYears is a Combination of Life Expectancy and Citizen Well-Being&quot;, ## add subtitle (smaller text size than the title) caption = &quot;Data Source: http://happyplanetindex.org/countries&quot;, ## add caption to the bottom of the figure x = &quot;Ecological Footprint&quot;, ## change x axis label y = &quot;Happy Life Years&quot;, ## change y axis label colour = &quot;World Region&quot;) ## change label of colour legend Any aes() that you use in your plot gets its own label and can be changed by name_of_aethetic = \"Your Label\". In the example above, we changed all three aes() labels: x, y, and colour. What is the only text on the plot that we aren’t able to change with labs()? 6.3.2 Changing x and y axis Limits We can also change the x-axis limits and the y-axis limits to, for example, start at 0 for the y-axis: ggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears, colour = Region)) + geom_point() + ylim(c(0, 70)) In this case, it makes the points on the plot a bit harder to see. You can also change where and how often tick marks appear on the x and y-axes. For special things like this, I think it’s best to just resort to Google (“ggplot how to change x-axis breaks tick marks” should help). 6.3.3 Changing A Colour Scale We want to use our graphics to communicate with others as clearly as possible. We also want to be as inclusive as possible in our communications. This means that, if we choose to use colour, our graphics should be made so that a colour-vision-deficient (CVD) person can read our graphs. About 4.5% of people are colour vision deficient, so it’s actually quite likely that a CVD person will view the graphics that you make (depending on how many people you share it with) More Information on CVD. The colour scales from R Colour Brewer are readable for common types of CVD. A list of scales can be found here. You would typically use the top scales if the variable you are colouring by is ordered sequentially (called seq for sequential, like grades in a course: A, B, C, D, F), the bottom scales if the variable is diverging (called div for diverging, like Republican / Democrat lean so that the middle is colourless), and the middle set of scales if the variable is not unordered and is categorical (called qual for qualitative like the names of different treatment drugs for a medical experiment). In which of those 3 situations are we in for the World Region graph? If we want to use one of these colour scales, we just need to add scale_colour_brewer() with the name of the scale we want to use. ggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears, colour = Region)) + geom_point() + scale_colour_brewer(palette = &quot;Accent&quot;) Try changing the palette to something else besides \"Accent\". Do you like the new palette better or worse? One more option to easily change the colour scale is to use the viridis package. The base viridis functions automatically load with ggplot2 so there’s no need to call the package with library(viridis). The viridis colour scales were made to be both aesthetically pleasing and CVD-friendly. ggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears, colour = Region)) + geom_point() + scale_colour_viridis_d(option = &quot;plasma&quot;) A drawback of the viridis package is that the yellow can be really hard to see (at least for me). Read the examples section of the Help file for ?scale_colour_viridis_d. What’s the difference between scale_colour_viridis_d(), ?scale_colour_viridis_c(), and scale_colour_viridis_b()? Which do you like better: the Colour Brewer scale or the Viridis scale? 6.3.4 Labeling Points or Lines of Interest One goal we might have with communication is highlighting particular points in a data set that show something interesting. For example, we might want to label the points on the graph corresponding to the countries with the highest HPI in each region: these countries are doing the best in terms of using resources efficiently to maximize citizen happiness. Or, we might want to highlight some “bad” example of countries that are the least efficient in each region. Or, we might want to label the country that we are from on the graph. All of this can be done with geom_label(). Let’s start by labeling all of the points. geom_label() needs one aesthetic called label which is the name of the column in the data set with the labels you want to use. ggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears, colour = Region)) + geom_point() + scale_colour_brewer(palette = &quot;Dark2&quot;) + geom_label(aes(label = Country)) Yikes! It’s quite uncommon to want to label all of the points. Let’s see if we can instead label each country with the best HPI in that country’s region. To do so, we first need to use our dplyr skills to create a new data set that has these 7 “best” countries. When we used group_by(), we typically used summarise() afterward. But, group_by() works with filter() as well! plot_df &lt;- hpi_df %&gt;% group_by(Region) %&gt;% filter(HPIRank == min(HPIRank)) What is the code in the previous chunk doing? Now that we have this new data set, we can use it within geom_label(). Recall that the data = argument in ggplot() carries on through all geoms unless we specify otherwise. Now is our chance to “specify otherwise” by including another data = argument within geom_label(): ggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears, colour = Region)) + geom_point() + scale_colour_brewer(palette = &quot;Dark2&quot;) + geom_label(data = plot_df, aes(label = Country)) Why do you think the colour legend changed to showing the letter “a” for each region? ggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears, colour = Region)) + geom_point(aes(colour = Region)) + scale_colour_brewer(palette = &quot;Dark2&quot;) + geom_label(data = plot_df, aes(label = Country), show.legend = FALSE) Why does the code chunk above change all of the “a”’s back to points? A common issue, even with few labels, is that some of the labels could overlap. The ggrepel package solves this problem by including a geom_label_repel() geom that automatically repels any overlapping labels: library(ggrepel) ggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears, colour = Region)) + geom_point() + scale_colour_brewer(palette = &quot;Dark2&quot;) + geom_label_repel(data = plot_df, aes(label = Country), show.legend = FALSE) And a final issue with the plot is that it’s not always very clear which point on the plot is being labeled. A trick used in the R for Data Science book is to surround the points that are being labeled with an open circle using an extra geom_point() function: ggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears, colour = Region)) + geom_point() + scale_colour_brewer(palette = &quot;Dark2&quot;) + geom_label_repel(data = plot_df, aes(label = Country), show.legend = FALSE) + geom_point(data = plot_df, size = 3, shape = 1, show.legend = FALSE) In the code above, shape = 1 says that the new point should be an open circle and size = 3 makes the point bigger, ensuring that it goes around the original point. show.legend = FALSE ensures that the larger open circles don’t become part of the legend. You can use this same strategy to label specific countries. I’m interested in where the United States of America falls on this graph because I’m from the U.S. I’m also interested in where Denmark falls because that’s the country I’m most interested in visiting. Feel free to replace those countries with any that you’re interested in! plot_df_us &lt;- hpi_df %&gt;% filter(Country == &quot;United States of America&quot; | Country == &quot;Denmark&quot;) ggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears, colour = Region)) + geom_point() + scale_colour_brewer(palette = &quot;Dark2&quot;) + geom_point(data = plot_df_us, size = 3, shape = 1, show.legend = FALSE) + geom_label_repel(data = plot_df_us, aes(label = Country), show.legend = FALSE) 6.3.5 Plot Themes Plot themes are an easy way to change many aspects of your plot with an overall theme that someone developed. The default theme for ggplot2 graphs is theme_grey(), which is the graph with the grey background that we’ve been using in the entirety of this class. The 7 other themes are given in R for Data Science in Figure 28.3. However, there are many more choices in the ggthemes package. Load the package with library(ggthemes) and check out https://yutannihilation.github.io/allYourFigureAreBelongToUs/ggthemes/ for a few of the themes in the package. My personal favorites, all given below, are theme_solarized(), theme_fivethirtyeight(), and theme_economist(), but choosing a theme is mostly a matter of personal taste. library(ggthemes) ggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears, colour = Region)) + geom_point() + scale_colour_brewer(palette = &quot;Dark2&quot;) + geom_point(data = plot_df_us, size = 3, shape = 1, show.legend = FALSE) + geom_label_repel(data = plot_df_us, aes(label = Country), show.legend = FALSE) + theme_solarized() ggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears, colour = Region)) + geom_point() + scale_colour_brewer(palette = &quot;Dark2&quot;) + geom_point(data = plot_df_us, size = 3, shape = 1, show.legend = FALSE) + geom_label_repel(data = plot_df_us, aes(label = Country), show.legend = FALSE) + theme_fivethirtyeight() ggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears, colour = Region)) + geom_point() + scale_colour_brewer(palette = &quot;Dark2&quot;) + geom_point(data = plot_df_us, size = 3, shape = 1, show.legend = FALSE) + geom_label_repel(data = plot_df_us, aes(label = Country), show.legend = FALSE) + theme_economist() There’s still much more you can do with ggplot2. In fact, there are entire books on it. But, for most other specializations, you can usually use Google to help you! 6.3.6 Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 6.5. The theme() function is a way to really specialise your plot. We will explore some of these in the exercise below. Using only options in theme() or options to change colours, shapes, sizes, etc., create the ugliest possible ggplot2 graph that you can make. You may not change the underlying data for this graph, but your goal is to investigate some of the options given in theme(). ggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears, colour = Region)) + geom_point() We will practice more with communicating with plots in the chapter exercises. 6.4 Chapter Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 6.5. Some data sets exist within specific R packages. For example, Jenny Bryan, who is quite famous in the stats/data science community, has put together the gapminder package so that users in R have access to a specific data set on countries throughout the world. https://github.com/jennybc/gapminder. To load a data set within a specific R package, you first need to load the package itself: library(gapminder) Then, name the data set something. In this case, the name of the data set is gapminder, but it’s not always the same name as the package itself. We will name the data set country_df. country_df &lt;- gapminder Explore the data set with head(), skim(), and ?gapminder before proceeding to the following exercises. * Make a line graph that shows the relationship between lifeExp and year for each of the countries in the data set, faceting the graph by continent and also colouring by continent (though this is redundant). Add an x-axis label, a y-axis label, a legend label, and a title to the graph. * Change the colour palette to be CVD-friendly using either scale_colour_brewer() or scale_colour_viridis_d(). * We can see a couple of interesting trends in life expectancy. There is one country in Africa and one country in Asia that sees a sharp decline in life expectancy at one point. In Europe, there is one country that has a substantially lower life expectancy than the rest in the 1950s but catches up to other European countries by the 2000s. Use filter() to create a data set that only has these 3 countries. Then, use geom_label() to label all three countries on your plot. Google the history of the countries in Africa and Asia that you just labeled. Add a very short description of why each country experienced a dip in life expectancy as a caption in your graph. Read the help file for ?annotate. How is this different than geom_label(). Which one allows finer tuning? Which one takes more code to use? One of the functions (annotate() or geom_label()) becomes more of a pain to use when there are many labels. Which one becomes harder to use and why? Suppose that we want the legend to appear on the bottom of the graph. Without using an entirely different theme, use Google to figure out how to move the legend from the right-hand side to the bottom. If there are a lot of overlapping points or overlapping lines, we can use alpha to control the transparency of the lines. Google “change transparency of lines in ggplot” and change the alpha so that the lines are more transparent. Change the theme of your plot to a theme in the ggthemes package. Then, change the order of your two commands to change the legend position and to change the overall theme. What happens? Modify your .Rmd file so that: only the figure that you made in Exercise 8 prints on your .html file. (Hint: use global options to help with this). none of the code gets printed. the messages that R prints by default are hidden in all code chunks. the figure height is 5 instead of the default 7. Create a new .Rmd file and knit this file to PDF. Then, knit this file to Microsoft Word. Which output format do you like the best? Note that PDF only supports some R Markdown theme options while Word supports very few theme options. Read the following up to “How can software tools make our research more reproducible?” https://ropensci.github.io/reproducibility-guide/sections/introduction/. How does what is discussed in the article related to R Markdown? 6.5 Exercise Solutions 6.5.1 Reproducibility S 6.5.2 R Markdown Files S * Create a table showing the mean mpg for each cyl group (cyl stands for cylinder and can be 4-cylinder, 6-cylinder, or 8-cylinder) with both kable() and pander(). Hint: remember to call the knitr library and the pander library. library(knitr) library(pander) library(tidyverse) mpg_df &lt;- mtcars %&gt;% group_by(cyl) %&gt;% summarise(meanmpg = mean(mpg)) mpg_df %&gt;% kable() cyl meanmpg 4 26.66364 6 19.74286 8 15.10000 mpg_df %&gt;% pander() cyl meanmpg 4 26.66 6 19.74 8 15.1 * Type ?kable into your console window and scroll through the Help file. Change the rounding of the mean so that it only displays one number after the decimal. Then, add a caption to the table that says “My First Table Caption!!” kable(mpg_df, digits = 1, caption = &quot;My First Table Caption!!&quot;) Table 6.1: My First Table Caption!! cyl meanmpg 4 26.7 6 19.7 8 15.1 * Google “How to Change Column Names in kable” and replace the column names with “Cylinder Numb.” and “Mean Mileage.” kable(mpg_df, digits = 1, caption = &quot;My First Table Caption!!&quot;, col.names = c(&quot;Cylinder Numb.&quot;, &quot;Mean Mileage&quot;)) Table 6.2: My First Table Caption!! Cylinder Numb. Mean Mileage 4 26.7 6 19.7 8 15.1 6.5.3 ggplot2 Communication S 6.5.4 Chapter Exercises S * Make a line graph that shows the relationship between lifeExp and year for each of the countries in the data set, faceting the graph by continent and also colouring by continent (though this is redundant). Add an x-axis label, a y-axis label, a legend label, and a title to the graph. * Change the colour palette to be CVD-friendly using either scale_colour_brewer() or scale_colour_viridis_d(). * We can see a couple of interesting trends in life expectancy. There is one country in Africa and one country in Asia that sees a sharp decline in life expxectancy at one point. In Europe, there is one country that has a substantially lower life expectancy than the rest in the 1950s but catches up to other European countries by the 2000s. Use filter() to create a data set that only has these 3 countries. Then, use geom_label() to label all three countries on your plot. interest_countries &lt;- country_df %&gt;% filter((year == 1952 &amp; continent == &quot;Europe&quot; &amp; lifeExp &lt; 50) | (year == 1992 &amp; continent == &quot;Africa&quot; &amp; lifeExp &lt; 30) | (year == 1977 &amp; continent == &quot;Asia&quot; &amp; lifeExp &lt; 35)) ggplot(data = country_df, aes(x = year, y = lifeExp, group = country, colour = continent)) + geom_line() + facet_wrap( ~ continent) + scale_colour_brewer(palette = &quot;Set2&quot;) + labs(x = &quot;Year&quot;, y = &quot;Life Expectancy (Years)&quot;, colour = &quot;Continent&quot;, title = &quot;Life Expectancy Increases Across time for nearly every country&quot;) + geom_label(data = interest_countries, aes(label = country), nudge_x = 7) 6.6 Non-Exercise R Code library(tidyverse) hpi_df &lt;- read_csv(&quot;data/hpi-tidy.csv&quot;) head(hpi_df) ggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears, colour = Region)) + geom_point() ggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears, colour = Region)) + geom_point() + labs(title = &quot;Countries with a Higher Ecological Footprint Tend to Have Citizens with Longer, Happier Lives&quot;, ## add title subtitle = &quot;HappyLifeYears is a Combination of Life Expectancy and Citizen Well-Being&quot;, ## add subtitle (smaller text size than the title) caption = &quot;Data Source: http://happyplanetindex.org/countries&quot;, ## add caption to the bottom of the figure x = &quot;Ecological Footprint&quot;, ## change x axis label y = &quot;Happy Life Years&quot;, ## change y axis label colour = &quot;World Region&quot;) ## change label of colour legend ggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears, colour = Region)) + geom_point() + ylim(c(0, 70)) ggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears, colour = Region)) + geom_point() + scale_colour_brewer(palette = &quot;Accent&quot;) ggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears, colour = Region)) + geom_point() + scale_colour_viridis_d(option = &quot;plasma&quot;) ggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears, colour = Region)) + geom_point() + scale_colour_brewer(palette = &quot;Dark2&quot;) + geom_label(aes(label = Country)) plot_df &lt;- hpi_df %&gt;% group_by(Region) %&gt;% filter(HPIRank == min(HPIRank)) ggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears, colour = Region)) + geom_point() + scale_colour_brewer(palette = &quot;Dark2&quot;) + geom_label(data = plot_df, aes(label = Country)) ggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears, colour = Region)) + geom_point(aes(colour = Region)) + scale_colour_brewer(palette = &quot;Dark2&quot;) + geom_label(data = plot_df, aes(label = Country), show.legend = FALSE) library(ggrepel) ggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears, colour = Region)) + geom_point() + scale_colour_brewer(palette = &quot;Dark2&quot;) + geom_label_repel(data = plot_df, aes(label = Country), show.legend = FALSE) ggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears, colour = Region)) + geom_point() + scale_colour_brewer(palette = &quot;Dark2&quot;) + geom_label_repel(data = plot_df, aes(label = Country), show.legend = FALSE) + geom_point(data = plot_df, size = 3, shape = 1, show.legend = FALSE) plot_df_us &lt;- hpi_df %&gt;% filter(Country == &quot;United States of America&quot; | Country == &quot;Denmark&quot;) ggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears, colour = Region)) + geom_point() + scale_colour_brewer(palette = &quot;Dark2&quot;) + geom_point(data = plot_df_us, size = 3, shape = 1, show.legend = FALSE) + geom_label_repel(data = plot_df_us, aes(label = Country), show.legend = FALSE) library(ggthemes) ggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears, colour = Region)) + geom_point() + scale_colour_brewer(palette = &quot;Dark2&quot;) + geom_point(data = plot_df_us, size = 3, shape = 1, show.legend = FALSE) + geom_label_repel(data = plot_df_us, aes(label = Country), show.legend = FALSE) + theme_solarized() ggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears, colour = Region)) + geom_point() + scale_colour_brewer(palette = &quot;Dark2&quot;) + geom_point(data = plot_df_us, size = 3, shape = 1, show.legend = FALSE) + geom_label_repel(data = plot_df_us, aes(label = Country), show.legend = FALSE) + theme_fivethirtyeight() ggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears, colour = Region)) + geom_point() + scale_colour_brewer(palette = &quot;Dark2&quot;) + geom_point(data = plot_df_us, size = 3, shape = 1, show.legend = FALSE) + geom_label_repel(data = plot_df_us, aes(label = Country), show.legend = FALSE) + theme_economist() ggplot(data = hpi_df, aes(x = Footprint, y = HappyLifeYears, colour = Region)) + geom_point() "],["r-basics.html", " 7 R Basics 7.1 Variable Classes in R 7.2 Classes in Detail 7.3 Object Types and Subsetting 7.4 Piping with magrittr 7.5 R Cleanup: Other Topics 7.6 Chapter Exercises 7.7 Exercise Solutions 7.8 Non-Exercise R Code", " 7 R Basics Goals: describe common classes for variables in a data set. explain why some R errors come about from class misspecifications. use indexing to reference rows, columns, or specific observations in a tibble or data set. explain when you can and cannot use piping. describe what a server is, when you have to install packages, and what the benefit is of using R Projects. Motivation Why is the chapter on R basics not the first chapter that we discuss? There certainly are advantages of doing things that way, but there are also advantages of not starting out with something like “classes of variables in R.” First, it’s not the most inherently interesting thing to look at. It’s a lot more fun to make plots and wrangle data. As long as someone makes sure that the variables are already of the “correct” class, then there’s no need to talk about this. Second, much of what we discuss here will make more sense, having the previous four chapters under our belt. We’ll be able to see how misspecified variable classes cause issues in certain summaries and plots and we already know how to make those plots and get those summaries. 7.1 Variable Classes in R R has a few different classes that variables take, including numeric, factor, character Date, and logical. Before we delve into the specifics of what these classes mean, let’s try to make some plots to illustrate why we should care about what these classes mean. The videogame_clean.csv file contains variables on video games from 2004 - 2019, including game, the name of the game release_date, the release date of the game release_date2, a second coding of release date price, the price in dollars, owners, the number of owners (given in a range) median_playtime, the median playtime of the game metascore, the score from the website Metacritic price_cat, 1 for Low (less than 10.00 dollars), 2 for Moderate (between 10 and 29.99 dollars), and 3 for High (30.00 or more dollars) meta_cat, Metacritic’s review system, with the following categories: “Overwhelming Dislike,” “Generally Unfavorable,” “Mixed Reviews,” “Generally Favorable,” “Universal Acclaim.” playtime_miss, whether median play time is missing (TRUE) or not (FALSE) The data set was modified from https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-07-30. Run the code in the following R chunk to read in the data. library(tidyverse) videogame_df &lt;- read_csv(&quot;data/videogame_clean.csv&quot;) head(videogame_df) #&gt; # A tibble: 6 x 15 #&gt; game release_date release_date2 price owners #&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 Half-Life 2 Nov 16, 2004 2004-11-16 9.99 10,000,000… #&gt; 2 Counter-Stri… Nov 1, 2004 2004-11-01 9.99 10,000,000… #&gt; 3 Counter-Stri… Mar 1, 2004 2004-03-01 9.99 10,000,000… #&gt; 4 Half-Life 2:… Nov 1, 2004 2004-11-01 4.99 5,000,000 … #&gt; 5 Half-Life: S… Jun 1, 2004 2004-06-01 9.99 2,000,000 … #&gt; 6 CS2D Dec 24, 2004 2004-12-24 NA 1,000,000 … #&gt; # … with 10 more variables: median_playtime &lt;dbl&gt;, #&gt; # metascore &lt;dbl&gt;, price_cat &lt;dbl&gt;, meta_cat &lt;chr&gt;, #&gt; # playtime_miss &lt;lgl&gt;, number &lt;dbl&gt;, developer &lt;chr&gt;, #&gt; # publisher &lt;chr&gt;, average_playtime &lt;dbl&gt;, #&gt; # meta_cat_factor &lt;chr&gt; A data frame or tibble holds variables that are allowed to be different classes. If a variable is a different class than you would expect, you’ll get some strange errors or results when trying to wrangle the data or make graphics. Run the following lines of code. In some cases, we are only using the first 100 observations in videogame_small. Otherwise, the code would take a very long time to run. videogame_small &lt;- videogame_df %&gt;% slice(1:100) ggplot(data = videogame_small, aes(x = release_date, y = price)) + geom_point() #&gt; Warning: Removed 5 rows containing missing values #&gt; (geom_point). ggplot(data = videogame_small, aes(x = release_date2, y = metascore)) + geom_point(aes(colour = price_cat)) #&gt; Warning: Removed 43 rows containing missing values #&gt; (geom_point). In the first plot, release_date isn’t ordered according to how you would expect (by date). Instead, R orders it alphabetically. In the second plot, we would expect to get a plot with 3 different colours, one for each level of price_cat. Instead, we get a continuous colour scale, which doesn’t make sense, given that price_cat can only be 1, 2, or 3. Both plots are not rendered correctly because the variable classes are not correct in the underlying data set. Up until this point, the data that has been provided has almost always had the correct variable classes, by default, but that won’t always be the case! We’ve actually seen both of these issues before as well (the Date issue in the exercise data and the continuous colour scale in the cars data), but, in both of these instances, code was provided to “fix” the problem. After this section, you’ll have the tools to fix many class issues on your own! If you examine the output of the following line of code head(videogame_df) #&gt; # A tibble: 6 x 15 #&gt; game release_date release_date2 price owners #&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 Half-Life 2 Nov 16, 2004 2004-11-16 9.99 10,000,000… #&gt; 2 Counter-Stri… Nov 1, 2004 2004-11-01 9.99 10,000,000… #&gt; 3 Counter-Stri… Mar 1, 2004 2004-03-01 9.99 10,000,000… #&gt; 4 Half-Life 2:… Nov 1, 2004 2004-11-01 4.99 5,000,000 … #&gt; 5 Half-Life: S… Jun 1, 2004 2004-06-01 9.99 2,000,000 … #&gt; 6 CS2D Dec 24, 2004 2004-12-24 NA 1,000,000 … #&gt; # … with 10 more variables: median_playtime &lt;dbl&gt;, #&gt; # metascore &lt;dbl&gt;, price_cat &lt;dbl&gt;, meta_cat &lt;chr&gt;, #&gt; # playtime_miss &lt;lgl&gt;, number &lt;dbl&gt;, developer &lt;chr&gt;, #&gt; # publisher &lt;chr&gt;, average_playtime &lt;dbl&gt;, #&gt; # meta_cat_factor &lt;chr&gt; you’ll see that, at the very top of the output, right below the variable names, R provides you with the classes of variables in the tibble. &lt;chr&gt; is character, used for strings or text. &lt;fct&gt; is used for variables that are factors, typically used for character variables with a finite number of possible values the variable can take on. &lt;date&gt; is used for dates. &lt;dbl&gt; stands for double and is used for the numeric class. &lt;int&gt; is for numbers that are all integers. In practice, there is not much difference between this class and class dbl. &lt;lgl&gt; is for logical, variables that are either TRUE or FALSE. 7.1.1 Referencing Variables and Using str() We can use name_of_dataset$name_of_variable to look at a specific variable in a data set: videogame_df$game prints the first thousand entries of the variable game. There are a few ways to get the class of this variable: the way that we will use most often is with str(), which stands for “structure,” and gives the class of the variable, the number of observations (26688), as well as the first couple of observations: str(videogame_df$game) #&gt; chr [1:26688] &quot;Half-Life 2&quot; &quot;Counter-Strike: Source&quot; ... You can also get a variable’s class more directly with class() class(videogame_df$game) #&gt; [1] &quot;character&quot; 7.2 Classes in Detail The following gives summary information about each class of variables in R: 7.2.1 &lt;chr&gt; and &lt;fct&gt; Class With the character class, R will give you a warning and/or a missing value if you try to perform any numerical operations: mean(videogame_df$game) #&gt; Warning in mean.default(videogame_df$game): argument is not #&gt; numeric or logical: returning NA #&gt; [1] NA videogame_df %&gt;% summarise(maxgame = max(game)) #&gt; # A tibble: 1 x 1 #&gt; maxgame #&gt; &lt;chr&gt; #&gt; 1 &lt;NA&gt; You also can’t convert a character class to numeric. You can, however, convert a character class to a &lt;fct&gt; class, using as.factor(). The &lt;fct&gt; class will be useful when we discuss the forcats package, but isn’t particularly useful now. class(videogame_df$meta_cat) #&gt; [1] &quot;character&quot; class(as.factor(videogame_df$meta_cat)) #&gt; [1] &quot;factor&quot; In general, as._____ will lets you convert between classes. Note, however, that we aren’t saving our converted variable anywhere. If we wanted the conversion to the factor to be saved in the data set, we can use mutate(): videogame_df &lt;- videogame_df %&gt;% mutate(meta_cat_factor = as.factor(meta_cat)) str(videogame_df$meta_cat_factor) #&gt; Factor w/ 4 levels &quot;Generally Favorable&quot;,..: 4 1 3 NA NA NA 4 1 3 NA ... For most R functions, it won’t matter whether your variable is in class character or class factor. In general, though, character classes are for variables that have a ton of different levels, like the name of the videogame, whereas factors are reserved for categorical variables with a finite number of levels. 7.2.2 &lt;date&gt; Class The &lt;date&gt; class is used for dates, and the &lt;datetime&gt; class is used for Dates with times. R requires a very specific format for dates and times. Note that, while to the human eye, both of the following variables contain dates, only one is of class &lt;date&gt;: str(videogame_df$release_date) #&gt; chr [1:26688] &quot;Nov 16, 2004&quot; &quot;Nov 1, 2004&quot; ... str(videogame_df$release_date2) #&gt; Date[1:26688], format: &quot;2004-11-16&quot; &quot;2004-11-01&quot; &quot;2004-03-01&quot; ... release_date is class character, which is why we had the issue with the odd ordering of the dates earlier. You can try converting it using as.Date, but this function doesn’t always work: as.Date(videogame_df$release_date) #&gt; Error in charToDate(x): character string is not in a standard unambiguous format Dates and times can be pretty complicated. In fact, we will spend an entire week covering them using the lubridate package. On variables that are in Date format, like release_date2, we can use numerical operations: median(videogame_df$release_date2, na.rm = TRUE) #&gt; [1] &quot;2017-06-09&quot; mean(videogame_df$release_date2, na.rm = TRUE) #&gt; [1] &quot;2016-09-15&quot; What do you think taking the median or taking the mean of a date class means? 7.2.3 &lt;dbl&gt; and &lt;int&gt; Class Class &lt;dbl&gt; and &lt;int&gt; are probably the most self-explanatory classes. &lt;dbl&gt;, the numeric class, are just variables that have only numbers in them while &lt;int&gt; only have integers (…, -2, -1, 0, 1, 2, ….). You can do numerical operations on either of these classes (and we’ve been doing them throughout the semester). For our purposes, these two classes are interchangeable. str(videogame_df$price) #&gt; num [1:26688] 9.99 9.99 9.99 4.99 9.99 ... Problems arise when numeric variables are coded as something non-numeric, or when non-numeric variables are coded as numeric. For example, examine: str(videogame_df$price_cat) #&gt; num [1:26688] 1 1 1 1 1 NA 2 1 1 1 ... price_cat is categorical but is coded as 1 for cheap games, 2 for moderately priced games, and 3 for expensive games. Therefore, R thinks that the variable is numeric, when, it’s actually a factor. str(as.factor(videogame_df$price_cat)) #&gt; Factor w/ 3 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;: 1 1 1 1 1 NA 2 1 1 1 ... This is the cause of the odd colour scale that we encountered earlier and can be fixed by converting price_cat to a factor: videogame_df &lt;- videogame_df %&gt;% mutate(price_factor = as.factor(price_cat)) ggplot(data = videogame_df, aes(x = release_date2, y = metascore)) + geom_point(aes(colour = price_factor)) #&gt; Warning: Removed 23838 rows containing missing values #&gt; (geom_point). 7.2.4 &lt;lgl&gt; Class Finally, there is a class of variables called logical. These variables can only take 2 values: TRUE or FALSE. For example, playtime_miss, a variable for whether or not the median_playtime variable is missing or not, is logical: str(videogame_df$playtime_miss) #&gt; logi [1:26688] FALSE FALSE FALSE TRUE TRUE FALSE ... It’s a little strange at first, but R can perform numeric operations on logical classes. All R does is treat every TRUE as a 1 and every FALSE as a 0. Therefore, sum() gives the total number of TRUEs and mean() gives the proportion of TRUEs. So, we can find the number and proportion of games that are missing their median_playtime as: sum(videogame_df$playtime_miss) #&gt; [1] 25837 mean(videogame_df$playtime_miss) #&gt; [1] 0.968113 There’s a lot of games that are missing this information! We’ve actually used the ideas of logical variables for quite some time now, particularly in statements involving if_else(), case_when(), filter(), and mutate(). The primary purpose of this section is to be able to identify variable classes and be able to convert between the different variable types with mutate() to “fix” variables with the incorrect class. 7.2.5 Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 7.7. We will use the fitness data set again for this set of exercises, as the data set has some of the issues with variable class that we have discussed. However, in week 1, some of the work of the work to fix those issues was already done before you saw the data. Now, you’ll get to fix a couple of those issues! Read in the data set with: library(tidyverse) fitness_df &lt;- read_csv(&quot;data/higham_fitness_notclean.csv&quot;) What is the issue with the following plot? There’s no need to fix the plot (we’ll see how to fix it when we introduce the lubridate package). * What is the issue with the following plot? After you figure out the issue, use mutate() to create a new variable that fixes the issue and then reconstruct the graph. * What is a third variable in the data set that has an incorrect class? Create a new variable, called step_goal that is 1 or TRUE if at least 10000 steps were walked and 0 or FALSE if fewer than 10000 steps were walked. Using this variable, find the total number of days where the goal was met and the proportion of the days where the goal was met. 7.3 Object Types and Subsetting Variables of these different classes can be stored in a variety of different objects in R. We have almost exclusively used the tibble object type. The tidy tibble is “rectangular” and has a specific number of rows and columns. has columns that are variables each column must have elements that are of the same class, but different columns can be of different classes. This allows us to have character and numeric variables in the same tibble. 7.3.1 tibble and data.frame The tibble object is very similar to the data.frame object. You can also check what type of object you’re working with using the str() command: str(videogame_df) ## look at the beginning to see &quot;tibble&quot; We will have a small section on tibbles in the coming weeks so we won’t focus on them here. But, we should take note that, to reference a specific element in a tibble, called indexing, you can use [# , #]. So, for example, videogame_df[5, 3] grabs the value in the fifth row and the third column: videogame_df[5, 3] #&gt; # A tibble: 1 x 1 #&gt; release_date2 #&gt; &lt;date&gt; #&gt; 1 2004-06-01 More often, we’d want to grab an entire row (or range of rows) or an entire column. We can do this by leaving the row number blank (to grab the entire column) or by leaving the column number blank (to grab the entire row): videogame_df[ ,3] ## grab the third column videogame_df[5, ] ## grab the fifth row We can also grab a range of columns or rows using the : operator: 3:7 videogame_df[ ,3:7] ## grab columns 3 through 7 videogame_df[3:7, ] ## grab rows 3 through 7 or we can grab different columns or rows using c(): videogame_df[ ,c(1, 3, 4)] ## grab columns 1, 3, and 4 videogame_df[c(1, 3, 4), ] ## grab rows 1, 3, and 4 To get rid of an entire row or column, use -: videogame_df[ ,-c(1, 2)] drops the first and second columns while videogame_df[-c(1, 2), ] drops the first and second rows. 7.3.2 Vectors A vector is an object that holds “things,” or elements, of the same class. You can create a vector in R using the c() function, which stands for “concatenate.” We’ve used the c() function before to bind things together; we just hadn’t yet discussed it in the context of creating a vector. vec1 &lt;- c(1, 3, 2) vec2 &lt;- c(&quot;b&quot;, 1, 2) vec3 &lt;- c(FALSE, FALSE, TRUE) str(vec1); str(vec2); str(vec3) #&gt; num [1:3] 1 3 2 #&gt; chr [1:3] &quot;b&quot; &quot;1&quot; &quot;2&quot; #&gt; logi [1:3] FALSE FALSE TRUE Notice that vec2 is a character class. R requires all elements in a vector to be of one class; since R knows b can’t be numeric, it makes all of the numbers characters as well. Using the dataset$variable draws out a vector from a tibble or data.frame: videogame_df$metascore If you wanted to make the above vector “by hand,” you’d need to have a lot of patience: c(96, 88, 65, NA, NA, NA, 93, .........) Just like tibbles, you can save vectors as something for later use: metavec &lt;- videogame_df$metascore mean(metavec, na.rm = TRUE) #&gt; [1] 71.89544 How would you get the mean metascore using dplyr functions? Vectors are one-dimensional: if we want to grab the 100th element of a vector we just use name_of_vector[100]: metavec[100] ## 100th element is missing #&gt; [1] NA Be aware that, if you’re coming from a math perspective, a “vector” in R doesn’t correspond to a “vector” in mathematics or physics. 7.3.3 Lists Lists are one of the more flexible objects in R: you can put objects of different classes in the same list and lists aren’t required to be rectangular (like tibbles are). Lists are extremely useful because of this flexibility, but, we won’t use them much in this class. Therefore, we will just see an example of a list before moving on: testlist &lt;- list(&quot;a&quot;, 4, c(1, 4, 2, 6), tibble(x = c(1, 2), y = c(3, 2))) testlist #&gt; [[1]] #&gt; [1] &quot;a&quot; #&gt; #&gt; [[2]] #&gt; [1] 4 #&gt; #&gt; [[3]] #&gt; [1] 1 4 2 6 #&gt; #&gt; [[4]] #&gt; # A tibble: 2 x 2 #&gt; x y #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 3 #&gt; 2 2 2 testlist has four elements: a single character \"a\", a single number 4, a vector of 1, 4, 2, 6, and a tibble with a couple of variables. Lists can therefore be used to store complex information that wouldn’t be as easily stored in a vector or tibble. 7.3.4 Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 7.7. * Look at the subsetting commands with [ , ]. What dplyr functions can you use to do the same thing? Create a tibble called last100 that only has the last 100 days in the data set using both (1) indexing with [ , ] and (2) a dplyr function. Create a tibble that doesn’t have the flights variable using both (1) indexing with [ , ] and (2) a dplyr function. * Use the following steps to create a new variable weekend_ind, which will be “weekend” if the day of the week is Saturday or Sunday and “weekday” if the day of the week is any other day. The current weekday variable is coded so that 1 represents Sunday, 2 represents Monday, …., and 7 represents Saturday. Create a vector that has the numbers corresponding to the two weekend days. Name the vector and then create a second vector that has the numbers corresponding to the five weekday days. Use dplyr functions and the %in% operator to create the new weekend_ind variable. You can use the following code chunk to help with what %in% does: 1 %in% c(1, 2, 3, 4) 2 %in% c(1, 2, 3, 4) 2 %in% c(3, 4, 5, 6) 7.4 Piping with magrittr The pipe %&gt;% is loaded automatically when the tidyverse library is loaded, but the operator comes from the magrittr package. We’ve been using the pipe quite a bit, but let’s delve a little deeper into what it’s actually doing. Let’s say you want to filter() the data set to include only observations with a metascore that isn’t missing. We’ve been using: videogame_df %&gt;% filter(!is.na(metascore)) What the pipe is doing is putting videogame_df as the first argument in the filter() function so that the piping statement in the chunk above is equivalent to: filter(videogame_df, !is.na(metascore)) If we want to first filter out games with a non-missing metascore, get rid of all observations with a median play time of 0, and then obtain the “median” median_playtime for each of the 3 price categories, we would typically use videogame_df %&gt;% filter(!is.na(metascore)) %&gt;% filter(median_playtime &gt; 0) %&gt;% group_by(price_cat) %&gt;% summarise(avg_med_time = median(median_playtime, na.rm = TRUE)) We see from the summary that, in general, games do tend to give you more “bang for the buck”: more expensive games tend to have a larger median play time. Consecutive pipes build on each other: we can slowly build out what the pipe is doing step-by-step. Starting from the top, videogame_df is the first argument in the filter(!is.na(metascore)) function: filter(videogame_df, !is.na(metascore)) filter(videogame_df, !is.na(metascore)) is the first argument in filter(median_playtime &gt; 0): filter(filter(videogame_df, !is.na(metascore)), median_playtime &gt; 0) filter(filter(videogame_df, !is.na(metascore)), median_playtime &gt; 0) is the first argument in group_by(price_cat): group_by(filter(filter(videogame_df, !is.na(metascore)), median_playtime &gt; 0), price_cat) and group_by(filter(filter(videogame_df, !is.na(metascore)), median_playtime &gt; 0), price_cat) is the first argument of summarise(avg_med_time = median(median_playtime, na.rm = TRUE)): summarise(group_by(filter(filter(videogame_df, !is.na(metascore)), median_playtime &gt; 0), price_cat), avg_med_time = median(median_playtime, na.rm = TRUE)) and we obtain the same result without the %&gt;% pipe. The example shows that, for our purposes, the pipe is most useful in aiding the readability of our code. It’s a lot easier to see what’s happening in the code chunk with the pipes than it is in the previous code chunk without the pipe. 7.4.1 When You Can’t Use the Pipe So, the pipe is a convenient way to put what precedes the pipe as the first argument to the function that follows the pipe. It’s important to understand this as you learn more about R because, while the functions in tidyverse are purposefully made to make good use of the pipe, not all functions in R utilize the pipe. Most of the functions in the tidyverse have a first argument that is a data set (which is why we can use pipes consecutively), but this isn’t the case with all R functions. For example, if you have taken STAT 213, you’ve used lm() to fit many different types of linear models. If you haven’t taken STAT 213, lm(response ~ explanatory) stands for “linear model” and can be used to fit the simple linear regression model that you learned about in STAT 113. You might expect something like this to work: videogame_df %&gt;% lm(metascore ~ price) #&gt; Error in as.data.frame.default(data): cannot coerce class &#39;&quot;formula&quot;&#39; to a data.frame But it throws us an error. Typing in ?lm reveals that its first argument is a formula to fit the model, not a data set. So the function is trying to run lm(videogame_df, metascore ~ price) #&gt; Error in as.data.frame.default(data): cannot coerce class &#39;&quot;formula&quot;&#39; to a data.frame which doesn’t work because the arguments to the function are mixed up (the formula should appear first and the data set should appear second). 7.4.2 Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 7.7. * Recode the following to look cleaner by using the pipe %&gt;%: summarise(group_by(filter(fitness_df, weekday == 1 | weekday == 7), month), meanweekend = mean(distance, na.rm = TRUE)) #&gt; # A tibble: 12 x 2 #&gt; month meanweekend #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 3.96 #&gt; 2 2 4.94 #&gt; 3 3 5.15 #&gt; 4 4 5.48 #&gt; 5 5 4.96 #&gt; 6 6 3.78 #&gt; 7 7 5.81 #&gt; 8 8 5.59 #&gt; 9 9 4.62 #&gt; 10 10 3.98 #&gt; 11 11 2.86 #&gt; 12 12 3.55 Explain why the following code gives a warning message and returns NA. Use the list of Arguments in ?mean in your explanation. fitness_df %&gt;% mean(distance, na.rm = TRUE) #&gt; Warning in mean.default(., distance, na.rm = TRUE): argument #&gt; is not numeric or logical: returning NA #&gt; [1] NA 7.5 R Cleanup: Other Topics 7.5.1 R Projects and Working Directories R Projects are a convenient way to keep related code, data sets, and analyses together. Read this very short introduction in R for Data Science here: https://r4ds.had.co.nz/workflow-projects.html#paths-and-directories and https://r4ds.had.co.nz/workflow-projects.html#rstudio-projects. Why should you rarely use an absolute directory? Look at the top of your bottom-left terminal window. If you’ve made an R project (which you should have!), you should see that a file path that is the current folder you’re working in. This is where R Studio will look for files by default. So, fitness_df &lt;- read_csv(&quot;data/higham_fitness_notclean.csv&quot;) #&gt; #&gt; ── Column specification ──────────────────────────────────── #&gt; cols( #&gt; Start = col_character(), #&gt; month = col_double(), #&gt; weekday = col_double(), #&gt; dayofyear = col_double(), #&gt; distance = col_double(), #&gt; steps = col_double(), #&gt; flights = col_double(), #&gt; active_cals = col_double() #&gt; ) starts at the path given in the console, looks for a folder called data in that path and looks for a file called higham_fitness_notclean.csv in the data folder. If you zipped up your project and sent it to someone else, they’d be able to open it and read that data file without needing to change any directory code! 7.5.2 R Studio Server The R Studio server is a computer that is set-up to carry out any R-based analyses for students with remote access to the computer (in our case, through SLU Login credentials). It might be helpful to think about the server as a large machine with no keyboard and no screen: it’s only purpose is to execute the code. R and R Studio are completely free, so you can install both on most computers (not Chromebooks though). The R Studio server has a couple of advantages, particularly for a classroom or workshop setting: using the server ensures that we are all using the same version of R. In theory, if one person gets an error, then everyone should get that same error. installing R and R Studio on your personal device is much easier after you’ve had some experience using it through the server. you don’t need a computer that is capable of running R to use the server (you can use a tablet or a Chromebook since the server does all of the actual computation). This week, however, we will move away from the server and, if you have a device that can install R (pretty much any computer that isn’t a Chromebook), you will install R on your own device. Though the server does have some advantages, there are also some disadvantages: you won’t have your SLU login forever, so, if you wanted to use R post graduation, you’d need to know how to install it. you haven’t had experience installing R packages. This is quite easy to do, but I’ve installed all necessary R packages on the server for us so you haven’t had to worry about this step. the server requires good Internet access and also has the potential to crash. 7.5.3 Installing R Packages So far, either myself or one of the other statistics faculty members have installed all packages that we’ve needed to use on the server globally. These packages include tidyverse, ggthemes, and ggrepel. However, if you want to use a package that isn’t installed on the server, or, you want to use a package using R Studio on your own personal computer, you need to install it first. Installation only needs to happen once (or until you upgrade R, which usually doesn’t happen too often), whereas the package needs to be loaded with library() every time you open R. The analogy of a lightbulb might be helpful. You only need to screw in the lightbulb into a socket once, but, every time you want the lightbulb to provide light, you need to flip the light switch. In the lightbulb analogy, what does putting the lightbulb into the socket correspond to? What does flipping the light switch correspond to? When you have R on your own computer, you’ll need to install all packages that you want to use (but, remember that you just need to install each package once). 7.5.4 Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 7.7. * Click the “Packages” button in the lower-right hand window to bring up the packages menu. Instead of using library(name_of_package), you can click the checkbox by the package name to load it into R. Try it out by un-checking and then re-checking tidyverse. Explain, from a reproducibility perspective, why loading packages this way is not good practice. Occasionally, something odd will happen in your R session where a “turn it off and turn it back on” strategy is the best fix. Save your current file, and restart R by clicking Session -&gt; Restart R. We’ve seen how R Projects are useful for keeping a data analysis organized. For large-scale data analysis projects, it’s usually nice to have a a specific folder for “data” within the R Project folder. Thus far, we have used a data folder, but you could also put a data set in the same directory as an R Project. Move the fitness data set out of the data folder and into the folder of your current R Project. To do so, click the checkbox next to the video games data set and the fitness data set, click “More” -&gt; Move and select the main folder. Now that your data set is in a different spot, you’ll need to let R know as well in your read_csv() statements! To do so, simply remove the data/ before the name of the file in your read_csv() statement in this current .Rmd file for any statement involving the fitness data. 7.6 Chapter Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 7.7. In class, we will work on installing R and R Studio to your personal laptop. If you already have R and R Studio installed and you have all of the basic packages working (from having STAT 213 in the Covid Spring 2020 or from some other class), then you can come to class ~ 30 minutes late (though you’re also welcome to come on time and work on other things for the first 45 minutes). If you’re unsure about whether your device can install R or R Studio, shoot me an email. It probably can, unless it’s a Chromebook. Step 1. Installing R: https://cran.rstudio.com/ and choose the appropriate version. Go through all of the installation steps. Step 2. This isn’t necessary, but it’s interesting to see what the basic R interface looks like without R Studio. Open up R and you should pretty much just see something that resembles the lower-left window of R Studio. Step 3. Installing R Studio: https://rstudio.com/products/rstudio/download/. Choose the free version and then choose either the Mac version or Windows version (assuming you don’t have Linux). Go through all of the installation steps. Step 4a. Open R Studio. Recall that one thing that we did on the server was change an R Markdown option so that results and plots wouldn’t appear in-line. To change this, click R Studio -&gt; Preferences -&gt; R Markdown and then uncheck “Show output inline for all R Markdown documents.” Step 4b. Change your colour theme! This is fun, at least for me….Go to R Studio -&gt; Preferences -&gt; Appearance and choose a theme that you like. I’ve had mine on Cobalt for a really long time, but the Vibrant Ink theme is always there, tempting me. Step 5. If you want your STAT 234 materials off of the server (which you should!), then you’ll need to download them to your personal device. Click the checkbox next to the STAT 234 folder, and then click More -&gt; Export -&gt; Download. There should now be a zipped folder of your materials in your Downloads folder. Double click to unzip the folder and then move the STAT 234 folder to a place that’s convenient for you (since you’ll be using it a lot this semester, one place you could put it is your Desktop). Verify that the contents downloaded by opening up the Week5_R_Basics R Project by double clicking the R Project icon in the Week5 folder. Step 6. There’s a lot of packages that we’ve used that were already installed on the server. By far, the largest package (and the one that’s most likely to give you an error in installation) is the tidyverse package. Install this package using install.packages(&quot;tidyverse&quot;) Step 7. Open your Week4 Project in a new window and verify that your .Rmd files can knit (you’ll at least need to install ggthemes and ggrepel and possibly a couple of other packages that I’m not thinking of). Once R is installed and ready to go, work through the following exercises pertaining to the video game data set. * Read in the data set and use filter() to remove any rows with missing metascores, missing median playtime, or have a median playtime of 0 hours. Note: We usually don’t want to remove missing values without a valid reason. In this case, a missing metascore means that the game wasn’t “major” enough to get enough critic reviews, and a missing or 0 hour median playtime means that there weren’t enough users who uploaded their playtime to the database. Therefore, any further analyses are constructed to games that are popular enough to both get enough reviews on metacritic and have enough users upload their median playtimes. videogame_df &lt;- read_csv(&quot;data/videogame_clean.csv&quot;) * Make a scatterplot with median_playtime on the y-axis and metascore on the x-axis with the filtered data set. * Something you may notice is that many of the points directly overlap one another. This is common when at least one of the variables on a scatterplot is discrete: metascore can only take on integer values in this case. Change geom_point() in your previous plot to geom_jitter(). Then, use the help to write a sentence about what geom_jitter() does. * Another option is to control point transparency with alpha. In your geom_jitter() statement, change alpha so that you can still see all of the points, but so that you can tell in the plot where a lot of points are overlapping. * Label the points that have median playtimes above 1500 hours. You may want to use the ggrepel package so that the labels don’t overlap. Choose one of the games that got labeled and Google that game’s median, or possibly average, play time. Is it in the vicinity as the median_playtime recorded in our data set? What should be done about the outliers? We will discuss and investigate this issue as a class. 7.7 Exercise Solutions 7.7.1 Variable Classes S 7.7.2 Classes in Detail S * What is the issue with the following plot? After you figure out the issue, use mutate() to create a new variable that fixes the issue and then reconstruct the graph. The issue is that weekday should be a factor, not numeric. fitness_df &lt;- fitness_df %&gt;% mutate(weekday_cat = as.factor(weekday)) ggplot(data = fitness_df, aes(x = active_cals)) + geom_freqpoly(aes(group = weekday_cat, colour = weekday_cat)) + scale_colour_viridis_d() #&gt; `stat_bin()` using `bins = 30`. Pick better value with #&gt; `binwidth`. * What is a third variable in the data set that has an incorrect class? Month should be an ordered factor, not numeric. 7.7.3 Object Types S * Look at the subsetting commands with [ , ]. What dplyr functions can you use to do the same thing? slice() can be used for the row indexing while select() can be used for the column indexing. * Use the following steps to create a new variable weekend_ind, which will be “weekend” if the day of the week is Saturday or Sunday and “weekday” if the day of the week is any other day. The current weekday variable is coded so that 1 represents Sunday, 2 represents Monday, …., and 7 represents Saturday. Create a vector that has the numbers corresponding to the two weekend days. Name the vector and then create a second vector that has the numbers corresponding to the five weekday days. vecweekend &lt;- c(1, 7) vecweekday &lt;- 2:6 ## or c(2, 3, 4, 5, 6) Use dplyr functions and the %in% operator to create the new weekend_ind variable. You can use the following code chunk to help with what %in% does: 1 %in% c(1, 2, 3, 4) #&gt; [1] TRUE 2 %in% c(1, 2, 3, 4) #&gt; [1] TRUE 2 %in% c(3, 4, 5, 6) #&gt; [1] FALSE fitness_df %&gt;% mutate(weekend_ind = case_when(weekday %in% vecweekend ~ &quot;weekend&quot;, weekday %in% vecweekday ~ &quot;weekday&quot;)) %&gt;% select(weekend_ind, everything()) #&gt; # A tibble: 584 x 10 #&gt; weekend_ind Start month weekday dayofyear distance steps #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 weekday 11/2… 11 4 332 0.930 1885. #&gt; 2 weekday 11/2… 11 5 333 4.64 8953. #&gt; 3 weekday 11/3… 11 6 334 6.05 11665 #&gt; 4 weekend 12/1… 12 7 335 6.80 12117 #&gt; 5 weekend 12/2… 12 1 336 4.61 8925. #&gt; 6 weekday 12/3… 12 2 337 3.96 7205 #&gt; 7 weekday 12/4… 12 3 338 6.60 12483. #&gt; 8 weekday 12/5… 12 4 339 4.91 9258. #&gt; 9 weekday 12/6… 12 5 340 7.50 14208 #&gt; 10 weekday 12/7… 12 6 341 4.27 8269. #&gt; # … with 574 more rows, and 3 more variables: #&gt; # flights &lt;dbl&gt;, active_cals &lt;dbl&gt;, weekday_cat &lt;fct&gt; ## can also use if_else, which is actually a little simpler in this case: fitness_df %&gt;% mutate(weekend_ind = if_else(weekday %in% vecweekend, true = &quot;weekend&quot;, false = &quot;weekday&quot;)) %&gt;% select(weekend_ind, everything()) #&gt; # A tibble: 584 x 10 #&gt; weekend_ind Start month weekday dayofyear distance steps #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 weekday 11/2… 11 4 332 0.930 1885. #&gt; 2 weekday 11/2… 11 5 333 4.64 8953. #&gt; 3 weekday 11/3… 11 6 334 6.05 11665 #&gt; 4 weekend 12/1… 12 7 335 6.80 12117 #&gt; 5 weekend 12/2… 12 1 336 4.61 8925. #&gt; 6 weekday 12/3… 12 2 337 3.96 7205 #&gt; 7 weekday 12/4… 12 3 338 6.60 12483. #&gt; 8 weekday 12/5… 12 4 339 4.91 9258. #&gt; 9 weekday 12/6… 12 5 340 7.50 14208 #&gt; 10 weekday 12/7… 12 6 341 4.27 8269. #&gt; # … with 574 more rows, and 3 more variables: #&gt; # flights &lt;dbl&gt;, active_cals &lt;dbl&gt;, weekday_cat &lt;fct&gt; 7.7.4 Piping S * Recode the following to look cleaner by using the pipe %&gt;%: summarise(group_by(filter(fitness_df, weekday == 1 | weekday == 7), month), meanweekend = mean(distance, na.rm = TRUE)) #&gt; # A tibble: 12 x 2 #&gt; month meanweekend #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 3.96 #&gt; 2 2 4.94 #&gt; 3 3 5.15 #&gt; 4 4 5.48 #&gt; 5 5 4.96 #&gt; 6 6 3.78 #&gt; 7 7 5.81 #&gt; 8 8 5.59 #&gt; 9 9 4.62 #&gt; 10 10 3.98 #&gt; 11 11 2.86 #&gt; 12 12 3.55 fitness_df %&gt;% filter(weekday == 1 | weekday == 7) %&gt;% group_by(month) %&gt;% summarise(meanweekend = mean(distance, na.rm = TRUE)) #&gt; # A tibble: 12 x 2 #&gt; month meanweekend #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 3.96 #&gt; 2 2 4.94 #&gt; 3 3 5.15 #&gt; 4 4 5.48 #&gt; 5 5 4.96 #&gt; 6 6 3.78 #&gt; 7 7 5.81 #&gt; 8 8 5.59 #&gt; 9 9 4.62 #&gt; 10 10 3.98 #&gt; 11 11 2.86 #&gt; 12 12 3.55 7.7.5 Other R Topics S * Click the “Packages” button in the lower-right hand windown to bring up the packages menu. Instead of using library(name_of_package), you can click the checkbox by the package name to load it into R. Try it out by un-checking and then re-checking tidyverse. Explain, from a reproducibility perspective, why loading packages this way is not good practice. If I loaded a package by checking the box, did some analysis, and gave my code to someone else, that person couldn’t easily see which packages I loaded in. Therefore, they wouldn’t necessarily be able to reproduce my results exactly. 7.7.6 Chapter Exercises S * Read in the data set and use filter() to remove any rows with missing metascores, missing median playtime, or have a median playtime of 0 hours. Note: We usually don’t want to remove missing values without a valid reason. In this case, a missing metascore means that the game wasn’t “major” enough to get enough critic reviews, and a missing or 0 hour median playtime means that there weren’t enough users who uploaded their playtime to the database. Therefore, any further analyses are constructed to games that are popular enough to both get enough reviews on metacritic and have enough users upload their median playtimes. videogame_df &lt;- read_csv(&quot;data/videogame_clean.csv&quot;) videogame_nomiss &lt;- videogame_df %&gt;% filter(!is.na(median_playtime) &amp; !is.na(metascore) &amp; median_playtime != 0) * Make a scatterplot with median_playtime on the y-axis and metascore on the x-axis with the filtered data set. ggplot(data = videogame_nomiss, aes(x = metascore, y = median_playtime)) + geom_point() * Something you may notice is that many of the points directly overlap one another. This is common when at least one of the variables on a scatterplot is discrete: metascore can only take on integer values in this case. Change geom_point() in your previous plot to geom_jitter(). Then, use the help to write a sentence about what geom_jitter() does. ggplot(data = videogame_nomiss, aes(x = metascore, y = median_playtime)) + geom_jitter() geom_jitter() adds a small amount of “noise” to each data point so that points don’t overlap quite as much. * Another option is to control point transparency with alpha. In your geom_jitter() statement, change alpha so that you can still see all of the points, but so that you can tell in the plot where a lot of points are overlapping. ggplot(data = videogame_nomiss, aes(x = metascore, y = median_playtime)) + geom_jitter(alpha = 0.4) ## can see a lot of ponits have median playtimes close to 0 * Label the points that have median playtimes above 1500 hours. You may want to use the ggrepel package so that the labels don’t overlap. library(ggrepel) videogame_long &lt;- videogame_nomiss %&gt;% filter(median_playtime &gt; 1500) ggplot(data = videogame_nomiss, aes(x = metascore, y = median_playtime)) + geom_jitter(alpha = 0.4) + geom_label_repel(data = videogame_long, aes(label = game)) 7.8 Non-Exercise R Code library(tidyverse) videogame_df &lt;- read_csv(&quot;data/videogame_clean.csv&quot;) head(videogame_df) videogame_small &lt;- videogame_df %&gt;% slice(1:100) ggplot(data = videogame_small, aes(x = release_date, y = price)) + geom_point() ggplot(data = videogame_small, aes(x = release_date2, y = metascore)) + geom_point(aes(colour = price_cat)) head(videogame_df) videogame_df$game str(videogame_df$game) class(videogame_df$game) mean(videogame_df$game) videogame_df %&gt;% summarise(maxgame = max(game)) class(videogame_df$meta_cat) class(as.factor(videogame_df$meta_cat)) videogame_df &lt;- videogame_df %&gt;% mutate(meta_cat_factor = as.factor(meta_cat)) str(videogame_df$meta_cat_factor) str(videogame_df$release_date) str(videogame_df$release_date2) median(videogame_df$release_date2, na.rm = TRUE) mean(videogame_df$release_date2, na.rm = TRUE) str(videogame_df$price) str(videogame_df$price_cat) str(as.factor(videogame_df$price_cat)) videogame_df &lt;- videogame_df %&gt;% mutate(price_factor = as.factor(price_cat)) ggplot(data = videogame_df, aes(x = release_date2, y = metascore)) + geom_point(aes(colour = price_factor)) str(videogame_df$playtime_miss) sum(videogame_df$playtime_miss) mean(videogame_df$playtime_miss) str(videogame_df) ## look at the beginning to see &quot;tibble&quot; videogame_df[5, 3] videogame_df[ ,3] ## grab the third column videogame_df[5, ] ## grab the fifth row 3:7 videogame_df[ ,3:7] ## grab columns 3 through 7 videogame_df[3:7, ] ## grab rows 3 through 7 videogame_df[ ,c(1, 3, 4)] ## grab columns 1, 3, and 4 videogame_df[c(1, 3, 4), ] ## grab rows 1, 3, and 4 vec1 &lt;- c(1, 3, 2) vec2 &lt;- c(&quot;b&quot;, 1, 2) vec3 &lt;- c(FALSE, FALSE, TRUE) str(vec1); str(vec2); str(vec3) videogame_df$metascore metavec &lt;- videogame_df$metascore mean(metavec, na.rm = TRUE) metavec[100] ## 100th element is missing testlist &lt;- list(&quot;a&quot;, 4, c(1, 4, 2, 6), tibble(x = c(1, 2), y = c(3, 2))) testlist videogame_df %&gt;% filter(!is.na(metascore)) filter(videogame_df, !is.na(metascore)) videogame_df %&gt;% filter(!is.na(metascore)) %&gt;% filter(median_playtime &gt; 0) %&gt;% group_by(price_cat) %&gt;% summarise(avg_med_time = median(median_playtime, na.rm = TRUE)) filter(videogame_df, !is.na(metascore)) filter(filter(videogame_df, !is.na(metascore)), median_playtime &gt; 0) group_by(filter(filter(videogame_df, !is.na(metascore)), median_playtime &gt; 0), price_cat) summarise(group_by(filter(filter(videogame_df, !is.na(metascore)), median_playtime &gt; 0), price_cat), avg_med_time = median(median_playtime, na.rm = TRUE)) fitness_df &lt;- read_csv(&quot;data/higham_fitness_notclean.csv&quot;) "],["factors-with-forcats.html", " 8 Factors with forcats 8.1 Change Factor Levels 8.2 Reorder Factor Levels 8.3 Chapter Exercises 8.4 Exercise Solutions 8.5 Non-Exercise R Code", " 8 Factors with forcats Goals: Use the forcats package to change the levels of factors, or to re-order levels of factors in a way that makes tables and graphs easier to read. 8.1 Change Factor Levels The Data: The pokemon_allgen.csv data set contains observations on Pokemon from the first 6 Generations (the first 6 games). There are 20 variable in this data set, but, of particular interest for this chapter are Type 1, the first Type characteristic of the Pokemon (a factor with 13 levels) Type 2, the second Type characteristic of the Pokemon (a factor with 13 levels, NA if the Pokemon only has one type) Generation, the generation the Pokemon first appeared in (a factor with 6 levels) Read in the data set with read_csv(). Then, use a mutate() statement to make a Generation_cat variable that is a factor. library(tidyverse) pokemon_df &lt;- read_csv(&quot;data/pokemon_allgen.csv&quot;) %&gt;% mutate(Generation_cat = factor(Generation)) One easy way to get a quick summary of a factor variable is to use group_by() and n() within a summarise() statement: pokemon_df %&gt;% group_by(`Type 1`) %&gt;% summarise(counttype = n()) #&gt; # A tibble: 18 x 2 #&gt; `Type 1` counttype #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Bug 75 #&gt; 2 Dark 31 #&gt; 3 Dragon 41 #&gt; 4 Electric 90 #&gt; 5 Fairy 18 #&gt; 6 Fighting 27 #&gt; 7 Fire 56 #&gt; 8 Flying 6 #&gt; 9 Ghost 58 #&gt; 10 Grass 73 #&gt; 11 Ground 42 #&gt; 12 Ice 24 #&gt; 13 Normal 108 #&gt; 14 Poison 30 #&gt; 15 Psychic 73 #&gt; 16 Rock 47 #&gt; 17 Steel 29 #&gt; 18 Water 119 8.1.1 fct_recode() to Rename Levels Now, let’s make a bar plot that examines how many Legendary Pokemon first appear in each generation, using dplyr commands that we’ve used and a simple geom_col(): pokemon_legend &lt;- pokemon_df %&gt;% filter(Legendary == TRUE) %&gt;% group_by(Generation_cat) %&gt;% summarise(nlegend = n()) ggplot(data = pokemon_legend, aes(x = Generation_cat, y = nlegend)) + geom_col() We’ve discussed how to change many aspects of ggplot2 graphs, but we haven’t discussed how to rename the labels of levels of a categorical variable, whether those appear in the x-axis or in a separate legend. The easiest way to do this is to rename the levels in the factor itself using fct_recode(). Suppose, for example, that we want to relabel the Generation number with the actual region corresponding to each game (Kanto, Johto, Hoenn, Sinnoh, Unova, and Kalos). The function fct_recode() takes the name of a factor already present in the data set as its first argument and then a series of renaming schemes (new_name = “old_name”) as its remaining arguments. pokemon_legend &lt;- pokemon_legend %&gt;% mutate(Generation_cat2 = fct_recode(Generation_cat, Kanto = &quot;1&quot;, Johto = &quot;2&quot;, Hoenn = &quot;3&quot;, Sinnoh = &quot;4&quot;, Unova = &quot;5&quot;, Kalos = &quot;6&quot;)) %&gt;% select(Generation_cat2, everything()) head(pokemon_legend) #&gt; # A tibble: 6 x 3 #&gt; Generation_cat2 Generation_cat nlegend #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Kanto 1 6 #&gt; 2 Johto 2 5 #&gt; 3 Hoenn 3 34 #&gt; 4 Sinnoh 4 17 #&gt; 5 Unova 5 27 #&gt; 6 Kalos 6 13 ggplot(data = pokemon_legend, aes(x = Generation_cat2, y = nlegend)) + geom_col() 8.1.2 Collapsing Many Levels Into Fewer Levels with fct_collapse() Sometimes, you might want to collapse the levels of two or more factors into a single level. With the Pokemon data set, there isn’t an example where this really makes sense, but, in the exercises, you’ll see a good use for this function with the social survey data set. For practice, we can collapse the Ice and Dark type Pokemon into a new level called Coolest and we can collapse the Poison, Fighting, and Fire type Pokemon into a new level called Least_Cool. pokemon_long &lt;- pokemon_df %&gt;% pivot_longer(c(`Type 1`, `Type 2`), names_to = &quot;Number&quot;, values_to = &quot;Type&quot;) pokemon_long %&gt;% mutate(new_type = fct_collapse(Type, Coolest = c(&quot;Ice&quot;, &quot;Dark&quot;), Least_Cool = c(&quot;Fire&quot;, &quot;Fighting&quot;, &quot;Poison&quot;))) %&gt;% select(new_type, Type, everything()) #&gt; # A tibble: 1,894 x 22 #&gt; new_type Type `#` Name Total HP Attack Defense #&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Grass Grass 1 Bulbas… 318 45 49 49 #&gt; 2 Least_Co… Poison 1 Bulbas… 318 45 49 49 #&gt; 3 Grass Grass 2 Ivysaur 405 60 62 63 #&gt; 4 Least_Co… Poison 2 Ivysaur 405 60 62 63 #&gt; 5 Grass Grass 3 Venusa… 525 80 82 83 #&gt; 6 Least_Co… Poison 3 Venusa… 525 80 82 83 #&gt; 7 Grass Grass 3 Venusa… 525 80 82 83 #&gt; 8 Least_Co… Poison 3 Venusa… 525 80 82 83 #&gt; 9 Least_Co… Fire 4 Charma… 309 39 52 43 #&gt; 10 &lt;NA&gt; &lt;NA&gt; 4 Charma… 309 39 52 43 #&gt; # … with 1,884 more rows, and 14 more variables: #&gt; # Sp. Atk &lt;dbl&gt;, Sp. Def &lt;dbl&gt;, Speed &lt;dbl&gt;, #&gt; # Generation &lt;dbl&gt;, Legendary &lt;lgl&gt;, id &lt;chr&gt;, #&gt; # identifier &lt;chr&gt;, height &lt;dbl&gt;, weight &lt;dbl&gt;, #&gt; # base_experience &lt;dbl&gt;, order &lt;dbl&gt;, is_default &lt;dbl&gt;, #&gt; # Generation_cat &lt;fct&gt;, Number &lt;chr&gt; What happens to the levels that aren’t being re-specified? 8.1.3 Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 8.4. What dplyr function(s) could you also use to create the new levels that were created with fct_collapse()? Why might it be a little easier to use fct_collapse()? * We did not properly explore the data set before making the graphs above, and, in fact, there is some double counting of Pokemon in this data set (this is another example where being familiar with the data set you’re working with is advantageous: people familiar with Pokemon know that there are fewer than 947 Pokemon in Generations 1 through 6). Figure out why some Pokemon are double counted. Then, create a new data set that only keeps one observation per Pokemon #. Create the bar plot with your non-duplicated data set. Are your results significantly changed? 8.2 Reorder Factor Levels 8.2.1 Change the Order of Levels by a Quantitative Variable with fct_reorder() You might also be interested in re-ordering the x or y-axis of a particular graph so that the order of the factors correspond to, for example, the median of a quantitative variable for each level. The reason you would want to do this is easiest to see with an example. For example, suppose you want to look at the most common Pokemon types across the first 6 generations. We use the non-duplicated data set from the previous section’s exercises, we pivot the data so that type is in one column, and we remove observations with missing Type, which correspond to the second Type of Pokemon that only have a single Type: pokemon_nodup &lt;- pokemon_df %&gt;% group_by(`#`) %&gt;% slice(1) %&gt;% ungroup() pokemon_long &lt;- pokemon_nodup %&gt;% pivot_longer(c(`Type 1`, `Type 2`), names_to = &quot;Number&quot;, values_to = &quot;Type&quot;) pokemon_sum &lt;- pokemon_long %&gt;% group_by(Type) %&gt;% summarise(count_type = n()) %&gt;% filter(!is.na(Type)) ggplot(data = pokemon_sum, aes(x = Type, y = count_type)) + geom_col() + coord_flip() ## flips the x and y axes How does R order the levels of the Type factor, by default? How might you like them to be ordered to make the graph more readable? The following code creates a new factor variable called Type_ordered that orders type by the count_type variable. fct_reorder() takes a factor as its first argument and a numeric variable to re-order that factor by as its second argument. The bar plot is then reconstructed with this new variable. pokemon_sum &lt;- pokemon_sum %&gt;% mutate(Type_ordered = fct_reorder(.f = Type, .x = count_type)) ggplot(data = pokemon_sum, aes(x = Type_ordered, y = count_type)) + geom_col() + coord_flip() fct_reorder() also works with boxplots or simple point plots that show, for example, the median response for each level of a factor. The following set of plots investigate how the Defense stat changes for different Pokemon types pokemon_long &lt;- pokemon_long %&gt;% filter(!is.na(Type)) %&gt;% mutate(Type_Deford = fct_reorder(.f = Type, .x = Defense, .fun = median)) ggplot(data = pokemon_long, aes(x = Type_Deford, y = Defense)) + geom_boxplot() + coord_flip() The following code makes a point plot that shows the median defense for each type instead of boxplots. pokemon_med &lt;- pokemon_long %&gt;% group_by(Type_Deford) %&gt;% summarise(med_def = median(Defense)) %&gt;% mutate(Type_Deford = fct_reorder(.f = Type_Deford, .x = med_def, .fun = median)) ggplot(data = pokemon_med, aes(x = med_def, y = Type_Deford)) + geom_point() Do you have a preference between the boxplot graph and the point plot? New Data. The gun_violence_us.csv data set was obtained from https://www.openintro.org/book/statdata/index.php?data=gun_violence_us and contains the following variables on gun violence in 2014: state, the name of the U.S. state mortality_rate, number of deaths from gun violence per 100,000 people ownership_rate, the proportion of adults who own a gun region, region of the U.S. (South, West, NE, and MW) mortality_df &lt;- read_csv(&quot;data/gun_violence_us.csv&quot;) %&gt;% mutate(region = factor(region)) 8.2.2 Re-Leveling By Two Quantitative Variables with fct_reorder2() Suppose that we want to investigate the relationship between mortality_rate and ownership_rate using this data set. Run the following code to create a scatterplot of mortality_rate vs. ownership_rate with fitted linear regression lines for each region of the United States: ggplot(data = mortality_df, aes(x = ownership_rate, y = mortality_rate, colour = region)) + geom_point() + geom_smooth(method = &quot;lm&quot;) Notice the order of the levels in the legend. Most people would prefer the order to actually match up with where the lines in the plot end, not for the order to be alphabetical. To achieve this, we can use fct_reorder2() to change the order of the factor levels: mortality_df &lt;- mortality_df %&gt;% mutate(region_2 = fct_reorder2(region, .x = ownership_rate, .y = mortality_rate)) ggplot(data = mortality_df, aes(x = ownership_rate, y = mortality_rate, colour = region_2)) + geom_point() + geom_smooth(method = &quot;lm&quot;) Did it change the order of the levels how you would expect? fct_reorder2() actually looks at points, not lines, when determining the ordering. If you want the levels to match up exactly, then we’ll have to reorder the levels manually with fct_relevel(): 8.2.3 Reordering Levels Manually with fct_relevel() Factors are ordered alphabetically by default. If we want precise control over the order of the levels of a factor, we can use fct_relevel(), which takes a factor and a vector of the new levels as inputs: mortality_df &lt;- mortality_df %&gt;% mutate(region_3 = fct_relevel(region, c(&quot;South&quot;, &quot;West&quot;, &quot;MW&quot;, &quot;NE&quot;))) ggplot(data = mortality_df, aes(x = ownership_rate, y = mortality_rate, colour = region_3)) + geom_point() + geom_smooth(method = &quot;lm&quot;) Reordering the levels of a factor manually might also be useful in fitting linear models. Recall that, by default, R makes the reference group in a linear model the first level alphabetically. If you’d like a different reference group, you can reorder the levels of the factor: mod &lt;- lm(mortality_rate ~ ownership_rate + region, data = mortality_df) mod2 &lt;- lm(mortality_rate ~ ownership_rate + region_2, data = mortality_df) mod3 &lt;- lm(mortality_rate ~ ownership_rate + region_3, data = mortality_df) summary(mod) summary(mod2) summary(mod3) 8.2.4 Exercises Make the side-by-side boxplots again with the pokemon data but do not use ungroup() by running the following code. pokemon_nodup &lt;- pokemon_df %&gt;% group_by(`#`) %&gt;% slice(1) ## %&gt;% ## ungroup() pokemon_long &lt;- pokemon_nodup %&gt;% pivot_longer(c(`Type 1`, `Type 2`), names_to = &quot;Number&quot;, values_to = &quot;Type&quot;) pokemon_long &lt;- pokemon_long %&gt;% filter(!is.na(Type)) %&gt;% mutate(Type_Deford = fct_reorder(.f = Type, .x = Defense, .fun = median)) ggplot(data = pokemon_long, aes(x = Type_Deford, y = Defense)) + geom_boxplot() + coord_flip() Why aren’t the types ordered by median defense anymore? The .fun argument in fct_reorder() controls how the Type factor is ordered. Change this to specify ordering by the mean, max, and min. What ordering makes the most sense? Why? 8.3 Chapter Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 8.4. We will use the general social survey data set, which is in the forcats library in R. You should some of this Wikipedia page to better understand where this data comes from Wikipedia. Most variables are self-explanatory, but a couple that aren’t are: partyid, political leaning and denom, religious denomination (if unfamiliar with this, you can think of it as a “more specific” subset of a particular religion). Note that some of these exercises are from the R for Data Science textbook. Load in the data set with library(tidyverse) gss_cat * Using a forcats function, change the name of the level Not str republican to be Weak republican and change the name of the level Not str democrat to be Weak democrat. These names more closely match the levels Strong republican and Strong democrat. Then, create a table of counts that shows the number of respondents in each political party partyid. Note: Levels that aren’t specified in your forcats function do not change. Note 2: In naming something Weak republican, you’ll need to use backticks since there is a space in the level name. * Use a forcats function so that partyid just has 4 categories: Other (corresponding to No answer, Don’t know, Other party), Ind (corresponding to Ind,near rep, Independent, Ind, near dem), Rep (corresponding to Strong republican and Not str republican), and Dem (corresponding to Not str democrat and Strong democrat). * Run the code to create the following plot that shows the average number of hours of television people watch from various religions. relig_summary &lt;- gss_cat %&gt;% group_by(relig) %&gt;% summarise( age = mean(age, na.rm = TRUE), tvhours = mean(tvhours, na.rm = TRUE), n = n() ) ggplot(data = relig_summary, aes(tvhours, relig)) + geom_point() Then, use a forcats function create a new variable in the data set that reorders the religion factor levels and remake the barplot so that the religion watches the most television, on average, is on the top, and the religion that watches the least television, on average, is on the bottom. * Run the code to make the following line plot that shows age on the x-axis, the proportion on the y-axis, and is coloured by various marital statuses (married, divorced, widowed, etc.): by_age &lt;- gss_cat %&gt;% filter(!is.na(age)) %&gt;% count(age, marital) %&gt;% group_by(age) %&gt;% mutate(prop = n / sum(n)) ggplot(by_age, aes(age, prop, colour = marital)) + geom_line(na.rm = TRUE) + labs(colour = &quot;marital&quot;) Then, use a forcats function to make the plot so that the legend labels line up better with the different coloured marital status lines (e.g. so that the label for widowed is the first that appears in the legend, the label for married is second, etc.). We haven’t talked much about creating two-way tables (or contingency tables). These are generally quite difficult to make with the tidyverse functions, but you can use the base R table() and prop.table() functions to make these. Using data only from the year 2014, run the following code to make 4 two-way tables with the party_small variable that was constructed earlier and race: gss_cat &lt;- gss_cat %&gt;% mutate(party_small = fct_collapse(partyid, Other = c(&quot;No answer&quot;, &quot;Don&#39;t know&quot;, &quot;Other party&quot;), Ind = c(&quot;Ind,near rep&quot;, &quot;Independent&quot;, &quot;Ind,near dem&quot;), Rep = c(&quot;Strong republican&quot;, &quot;Not str republican&quot;), Dem = c(&quot;Not str democrat&quot;, &quot;Strong democrat&quot;))) gss_recent &lt;- gss_cat %&gt;% filter(year != 2014) tab1 &lt;- table(gss_recent$party_small, gss_recent$race) tab1 #&gt; #&gt; Other Black White Not applicable #&gt; Other 39 46 375 0 #&gt; Rep 215 127 4467 0 #&gt; Ind 863 827 5631 0 #&gt; Dem 580 1743 4032 0 prop.table(tab1) #&gt; #&gt; Other Black White Not applicable #&gt; Other 0.002058591 0.002428081 0.019794141 0.000000000 #&gt; Rep 0.011348641 0.006703616 0.235787807 0.000000000 #&gt; Ind 0.045552916 0.043652679 0.297228820 0.000000000 #&gt; Dem 0.030614938 0.092003167 0.212826603 0.000000000 prop.table(tab1, margin = 1) #&gt; #&gt; Other Black White Not applicable #&gt; Other 0.08478261 0.10000000 0.81521739 0.00000000 #&gt; Rep 0.04470784 0.02640882 0.92888334 0.00000000 #&gt; Ind 0.11788007 0.11296271 0.76915722 0.00000000 #&gt; Dem 0.09126672 0.27427223 0.63446105 0.00000000 prop.table(tab1, margin = 2) #&gt; #&gt; Other Black White Not applicable #&gt; Other 0.02298173 0.01676996 0.02585315 #&gt; Rep 0.12669417 0.04629967 0.30796277 #&gt; Ind 0.50854449 0.30149471 0.38821096 #&gt; Dem 0.34177961 0.63543565 0.27797311 Use the help on ?prop.table to figure out how each of these three tables are constructed. Which table do you think is most informative? What conclusions does it help you to draw? 8.4 Exercise Solutions 8.4.1 Change Factor Levels S * We did not properly explore the data set before making the graphs above, and, in fact, there is some double counting of Pokemon in this data set (this is another example where being familiar with the data set you’re working with is advantageous: people familiar with Pokemon know that there are fewer than 947 Pokemon in Generations 1 through 6). Figure out why some Pokemon are double counted. Then, create a new data set that only keeps one observation per Pokemon #. pokemon_nodup &lt;- pokemon_df %&gt;% group_by(`#`) %&gt;% slice(1) %&gt;% ungroup() 8.4.2 Reorder Factor Levels S 8.4.3 Chapter Exercises S * Using a forcats function, change the name of the level Not str republican to be Weak republican and change the name of the level Not str democrat to be Weak democrat. These names more closely match the levels Strong republican and Strong democrat. Then, create a table of counts that shows the number of respondents in each political party partyid. Note: Levels that aren’t specified in your forcats function do not change. Note 2: In naming something Weak republican, you’ll need to use backticks since there is a space in the level name. gss_cat %&gt;% mutate(partyid_new = fct_recode(partyid, `Weak republican` = &quot;Not str republican&quot;, `Weak democrat` = &quot;Not str democrat&quot;)) %&gt;% group_by(partyid_new) %&gt;% summarise(ncount = n()) * Use a forcats function so that partyid just has 4 categories: Other (corresponding to No answer, Don’t know, Other party), Ind (corresponding to Ind,near rep, Independent, Ind, near dem), Rep (corresponding to Strong republican and Not str republican), and Dem (corresponding to Not str democrat and Strong democrat). gss_cat &lt;- gss_cat %&gt;% mutate(party_small = fct_collapse(partyid, Other = c(&quot;No answer&quot;, &quot;Don&#39;t know&quot;, &quot;Other party&quot;), Ind = c(&quot;Ind,near rep&quot;, &quot;Independent&quot;, &quot;Ind,near dem&quot;), Rep = c(&quot;Strong republican&quot;, &quot;Not str republican&quot;), Dem = c(&quot;Not str democrat&quot;, &quot;Strong democrat&quot;))) * Run the code to create the following plot that shows the average number of hours of television people watch from various religions. relig_summary &lt;- gss_cat %&gt;% group_by(relig) %&gt;% summarise( age = mean(age, na.rm = TRUE), tvhours = mean(tvhours, na.rm = TRUE), n = n() ) ggplot(data = relig_summary, aes(tvhours, relig)) + geom_point() Then, use a forcats function create a new variable in the data set that reorders the religion factor levels and remake the barplot so that the religion watches the most television, on average, is on the top, and the religion that watches the least television, on average, is on the bottom. relig_summary &lt;- relig_summary %&gt;% mutate(relig = fct_reorder(relig, tvhours)) ggplot(data = relig_summary, aes(tvhours, relig)) + geom_point() * Run the code to make the following line plot that shows age on the x-axis, the proportion on the y-axis, and is coloured by various marital statuses (married, divorced, widowed, etc.): by_age &lt;- gss_cat %&gt;% filter(!is.na(age)) %&gt;% count(age, marital) %&gt;% group_by(age) %&gt;% mutate(prop = n / sum(n)) ggplot(by_age, aes(age, prop, colour = marital)) + geom_line(na.rm = TRUE) + labs(colour = &quot;marital&quot;) Then, use a forcats function to make the plot so that the legend labels line up better with the different coloured marital status lines (e.g. so that the label for widowed is the first that appears in the legend, the label for married is second, etc.). by_age2 &lt;- by_age %&gt;% ungroup() %&gt;% mutate(marital2 = fct_reorder2(marital, .x = age, .y = prop)) ggplot(by_age2, aes(age, prop, colour = marital2)) + geom_line(na.rm = TRUE) + labs(colour = &quot;marital&quot;) + scale_colour_viridis_d() 8.5 Non-Exercise R Code library(tidyverse) pokemon_df &lt;- read_csv(&quot;data/pokemon_allgen.csv&quot;) %&gt;% mutate(Generation_cat = factor(Generation)) pokemon_df %&gt;% group_by(`Type 1`) %&gt;% summarise(counttype = n()) pokemon_legend &lt;- pokemon_df %&gt;% filter(Legendary == TRUE) %&gt;% group_by(Generation_cat) %&gt;% summarise(nlegend = n()) ggplot(data = pokemon_legend, aes(x = Generation_cat, y = nlegend)) + geom_col() pokemon_legend &lt;- pokemon_legend %&gt;% mutate(Generation_cat2 = fct_recode(Generation_cat, Kanto = &quot;1&quot;, Johto = &quot;2&quot;, Hoenn = &quot;3&quot;, Sinnoh = &quot;4&quot;, Unova = &quot;5&quot;, Kalos = &quot;6&quot;)) %&gt;% select(Generation_cat2, everything()) head(pokemon_legend) ggplot(data = pokemon_legend, aes(x = Generation_cat2, y = nlegend)) + geom_col() pokemon_long &lt;- pokemon_df %&gt;% pivot_longer(c(`Type 1`, `Type 2`), names_to = &quot;Number&quot;, values_to = &quot;Type&quot;) pokemon_long %&gt;% mutate(new_type = fct_collapse(Type, Coolest = c(&quot;Ice&quot;, &quot;Dark&quot;), Least_Cool = c(&quot;Fire&quot;, &quot;Fighting&quot;, &quot;Poison&quot;))) %&gt;% select(new_type, Type, everything()) pokemon_nodup &lt;- pokemon_df %&gt;% group_by(`#`) %&gt;% slice(1) %&gt;% ungroup() pokemon_long &lt;- pokemon_nodup %&gt;% pivot_longer(c(`Type 1`, `Type 2`), names_to = &quot;Number&quot;, values_to = &quot;Type&quot;) pokemon_sum &lt;- pokemon_long %&gt;% group_by(Type) %&gt;% summarise(count_type = n()) %&gt;% filter(!is.na(Type)) ggplot(data = pokemon_sum, aes(x = Type, y = count_type)) + geom_col() + coord_flip() ## flips the x and y axes pokemon_sum &lt;- pokemon_sum %&gt;% mutate(Type_ordered = fct_reorder(.f = Type, .x = count_type)) ggplot(data = pokemon_sum, aes(x = Type_ordered, y = count_type)) + geom_col() + coord_flip() pokemon_long &lt;- pokemon_long %&gt;% filter(!is.na(Type)) %&gt;% mutate(Type_Deford = fct_reorder(.f = Type, .x = Defense, .fun = median)) ggplot(data = pokemon_long, aes(x = Type_Deford, y = Defense)) + geom_boxplot() + coord_flip() pokemon_med &lt;- pokemon_long %&gt;% group_by(Type_Deford) %&gt;% summarise(med_def = median(Defense)) %&gt;% mutate(Type_Deford = fct_reorder(.f = Type_Deford, .x = med_def, .fun = median)) ggplot(data = pokemon_med, aes(x = med_def, y = Type_Deford)) + geom_point() mortality_df &lt;- read_csv(&quot;data/gun_violence_us.csv&quot;) %&gt;% mutate(region = factor(region)) ggplot(data = mortality_df, aes(x = ownership_rate, y = mortality_rate, colour = region)) + geom_point() + geom_smooth(method = &quot;lm&quot;) mortality_df &lt;- mortality_df %&gt;% mutate(region_2 = fct_reorder2(region, .x = ownership_rate, .y = mortality_rate)) ggplot(data = mortality_df, aes(x = ownership_rate, y = mortality_rate, colour = region_2)) + geom_point() + geom_smooth(method = &quot;lm&quot;) mortality_df &lt;- mortality_df %&gt;% mutate(region_3 = fct_relevel(region, c(&quot;South&quot;, &quot;West&quot;, &quot;MW&quot;, &quot;NE&quot;))) ggplot(data = mortality_df, aes(x = ownership_rate, y = mortality_rate, colour = region_3)) + geom_point() + geom_smooth(method = &quot;lm&quot;) mod &lt;- lm(mortality_rate ~ ownership_rate + region, data = mortality_df) mod2 &lt;- lm(mortality_rate ~ ownership_rate + region_2, data = mortality_df) mod3 &lt;- lm(mortality_rate ~ ownership_rate + region_3, data = mortality_df) summary(mod) summary(mod2) summary(mod3) "],["reprexes-and-import.html", " 9 Reprexes and Import 9.1 Reprexes and tibble 9.2 readr to Read in Data 9.3 Data Scraping with rvest 9.4 JSON Files with jsonlite 9.5 Chapter Exercises 9.6 Exercise Solutions 9.7 Non-Exercise R Code", " 9 Reprexes and Import Goals: Use tibble to create data sets in R, and describe the benefits of reprexes. Use readr to read in data to R from .csv, .txt, and .tsv files. Use rvest to scrape data from public websites. Use jsonlite to read in data in JSON (Java Script Object Notation) files. 9.1 Reprexes and tibble We can also create a data set directly within R with the tibble() function in the tibble package. This is most useful when we want to make a small reproducible example so that someone else may help with our code. A reproducible example, or reprex, is a chunk of code that we can give to someone else that runs without any outside data. These are used often on StackExchange. The following code chunk is not a reprex because people would not necessarily have the data set parsedf.csv. ## Hello! How do I get rid of the units from the values in ## my variable `x`? Thanks! library(tidyverse) test_df &lt;- read_csv(&quot;data/parsedf.csv&quot;) head(test_df) #&gt; # A tibble: 3 x 2 #&gt; x y #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 20,000 dollars 1 #&gt; 2 40 dollars 2 #&gt; 3 only 13 dollars 3 We want to post on StackExchange for someone to help us convert a variable from a character vector with units to a numeric vector without units. We want to be able to give any possible helpers a small example data set to work with. For this, we can create our own tiny data set with tibble(): ## Hello! How do I get rid of the units from the values in ## my variable `xvar`? Thanks! library(tidyverse) test_df2 &lt;- tibble(xvar = c(&quot;20,000 dollars&quot;, &quot;40 dollars&quot;), yvar = c(1, 2)) test_df2 #&gt; # A tibble: 2 x 2 #&gt; xvar yvar #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 20,000 dollars 1 #&gt; 2 40 dollars 2 Why is library(tidyverse) necessary in the code chunk above for my reprex? We can copy and paste the code chunk above to our question: it’s code that anyone can run as long as they have the tidyverse package installed, and really encourages more people to help. 9.1.1 Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 9.6. For Project 2, we will work with some course evaluation data for a professor at SLU. Overall, you’ll answer some questions about how the professor can improve their courses at SLU by looking at course evaluation data. The variables and data set will be described in more detail in the project description. * Suppose that you can’t figure out how to create a semester variable and a year variable from Term in evals_prof_S21.csv. (You want to split the Term variable into two variables: Semester with levels F and S and Year with levels 19, 20, and 21). library(tidyverse) evals_df &lt;- read_csv(&quot;data/evals_prof_S21.csv&quot;) head(evals_df) #&gt; # A tibble: 6 x 10 #&gt; Term Course Question `Agree strongly` Agree #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 F19 113-02 1. Course has been a … 9 9 #&gt; 2 F19 113-02 2. Effectively Organi… 12 8 #&gt; 3 F19 113-02 3. Environment Conduc… 11 8 #&gt; 4 F19 113-02 5a. Fair Assessment o… 5 13 #&gt; 5 F19 113-02 5b. Timely Assessment… 8 12 #&gt; 6 F19 113-02 5c. Constructive Asse… 5 8 #&gt; # … with 5 more variables: Agree Somewhat &lt;dbl&gt;, #&gt; # Neutral &lt;dbl&gt;, Disagree Somewhat &lt;dbl&gt;, Disagree &lt;dbl&gt;, #&gt; # Disagree Strongly &lt;dbl&gt; Put together a reprex using tibble() that someone would be able to run to help you figure out your question. * You should actually be able to answer your own question using a function that we learned a couple of weeks ago. Do so, creating a Semester and Year variable. 9.2 readr to Read in Data Up to now, we have mostly worked with data that was “R Ready”: meaning that it was in a nice .csv file that could be read into R easily with read_csv() from the readr package. We will begin by looking at some options in the read_csv() function and then move into formats other than .csv that data are commonly stored in. 9.2.1 read_csv() Options The mtcarsex.csv has observations on different car models with variables that include things like gas mileage, number of cylinders, etc. Read in the mtcarsex.csv data set with the following code. Then, examine the data set with head(). library(tidyverse) cars_df &lt;- read_csv(&quot;data/mtcarsex.csv&quot;) head(cars_df) #&gt; # A tibble: 6 x 11 #&gt; `This is a data… X2 X3 X4 X5 X6 X7 X8 #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 &quot;I&#39;m a na\\x95ve… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 2 &quot;mpg&quot; cyl disp hp drat wt qsec vs #&gt; 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 5 &quot;-999&quot; 6 160 110 3.9 2.62 16.46 0 #&gt; 6 &quot;21&quot; 6 160 110 3.9 2.875 17.02 0 #&gt; # … with 3 more variables: X9 &lt;chr&gt;, X10 &lt;chr&gt;, X11 &lt;chr&gt; What do you notice about the data set that seems odd? Open the .csv file with Excel or some other program to examine the data set outside of R. Type in ?read_csv in the bottom-left window and look at some of the options in read_csv(). In particular, we will use the na and the skip arguments to fix up our reading. Let’s start with skip so that we aren’t reading in the first two rows of the data set: cars_df &lt;- read_csv(&quot;data/mtcarsex.csv&quot;, skip = 2) ## first two lines will be skipped head(cars_df) #&gt; # A tibble: 6 x 11 #&gt; mpg cyl disp hp drat wt qsec vs am #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 NA NA NA NA NA NA NA NA NA #&gt; 2 NA NA NA NA NA NA NA NA NA #&gt; 3 -999 6 160 110 3.9 2.62 16.5 0 1 #&gt; 4 21 6 160 110 3.9 2.88 17.0 0 1 #&gt; 5 22.8 4 108 93 3.85 2.32 18.6 1 1 #&gt; 6 21.4 6 258 110 3.08 3.22 19.4 1 0 #&gt; # … with 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt; That looks better, but there are still a couple of problems. What do you notice? Go the help and read about the na argument. Let’s add that as an option to fix the missing value issue. cars_df &lt;- read_csv(&quot;data/mtcarsex.csv&quot;, na = c(NA, &quot;-999&quot;), skip = 2) head(cars_df) #&gt; # A tibble: 6 x 11 #&gt; mpg cyl disp hp drat wt qsec vs am #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 NA NA NA NA NA NA NA NA NA #&gt; 2 NA NA NA NA NA NA NA NA NA #&gt; 3 NA 6 160 110 3.9 2.62 16.5 0 1 #&gt; 4 21 6 160 110 3.9 2.88 17.0 0 1 #&gt; 5 22.8 4 108 93 3.85 2.32 18.6 1 1 #&gt; 6 21.4 6 258 110 3.08 3.22 19.4 1 0 #&gt; # … with 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt; Now look at the classes of each variable. Which classes look like they are incorrect? We’ve talked about how to re-specify classes of variables using mutate() and the as.factor() or as.Date() or as.numeric() functions, but sometimes it’s easier just to respecify the class when we are reading in the data. Notice how, when we use read_csv(), R gives us a message about each of the column types. This is actually an argument in read_csv() called col_types. R prints it so that it’s easy for us to copy and paste it into read_csv() and change any classes. For example, notice how cyl = col_double() is changed to cyl = col_factor() in the code chunk below: cars_df &lt;- read_csv(&quot;data/mtcarsex.csv&quot;, na = c(NA, &quot;-999&quot;), skip = 2, col_types = cols( mpg = col_double(), cyl = col_factor(), disp = col_double(), hp = col_double(), drat = col_double(), wt = col_double(), qsec = col_double(), vs = col_factor(), am = col_double(), gear = col_double(), carb = col_double() )) Finally, there are two rows with all missing values. These aren’t providing anything useful so we can slice() them out: cars_df &lt;- read_csv(&quot;data/mtcarsex.csv&quot;, na = c(NA, &quot;-999&quot;), skip = 2, col_types = cols( mpg = col_double(), cyl = col_factor(), disp = col_double(), hp = col_double(), drat = col_double(), wt = col_double(), qsec = col_double(), vs = col_factor(), am = col_double(), gear = col_double(), carb = col_double() )) %&gt;% slice(-(1:2)) head(cars_df) #&gt; # A tibble: 6 x 11 #&gt; mpg cyl disp hp drat wt qsec vs am #&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 NA 6 160 110 3.9 2.62 16.5 0 1 #&gt; 2 21 6 160 110 3.9 2.88 17.0 0 1 #&gt; 3 22.8 4 108 93 3.85 2.32 18.6 1 1 #&gt; 4 21.4 6 258 110 3.08 3.22 19.4 1 0 #&gt; 5 NA 8 360 175 3.15 3.44 17.0 0 0 #&gt; 6 18.1 6 225 105 2.76 3.46 20.2 1 0 #&gt; # … with 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt; There are many other possible file formats for data storage. For example, there is a data set called oscars.tsv, which is a tab-separated file. You can read it in with read_tsv() instead of read_csv(). oscars_df &lt;- read_tsv(&quot;data/oscars.tsv&quot;) head(oscars_df) #&gt; # A tibble: 6 x 51 #&gt; FilmName OscarYear Duration Rating DirectorName #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 Crash 2006 113 4 Haggis #&gt; 2 Brokeback Mountain 2006 134 4 Lee #&gt; 3 Capote 2006 114 4 Miller #&gt; 4 Good Night, and Go… 2006 93 2 Clooney #&gt; 5 Munich 2006 164 4 Spielberg #&gt; 6 The Departed 2007 151 4 Scorsese #&gt; # … with 46 more variables: DirectorGender &lt;dbl&gt;, #&gt; # OscarWinner &lt;dbl&gt;, GenreName &lt;chr&gt;, Genre_Drama &lt;dbl&gt;, #&gt; # Genre_Bio &lt;dbl&gt;, CountryName &lt;chr&gt;, #&gt; # ForeignandUSA &lt;dbl&gt;, ProductionName &lt;chr&gt;, #&gt; # ProductionCompany &lt;dbl&gt;, BudgetRevised &lt;chr&gt;, #&gt; # Budget &lt;chr&gt;, DomesticBoxOffice &lt;dbl&gt;, #&gt; # WorldwideRevised &lt;dbl&gt;, WorldwideBoxOffice &lt;dbl&gt;, #&gt; # DomesticPercent &lt;dbl&gt;, LimitedOpeningWnd &lt;dbl&gt;, #&gt; # LimitedTheaters &lt;dbl&gt;, LimitedAveragePThtr &lt;dbl&gt;, #&gt; # WideOpeningWkd &lt;dbl&gt;, WideTheaters &lt;dbl&gt;, #&gt; # WideTheaterAverage &lt;dbl&gt;, WidestTheaters &lt;dbl&gt;, #&gt; # Days &lt;chr&gt;, Rotten &lt;dbl&gt;, Metacritic &lt;dbl&gt;, IMDb &lt;dbl&gt;, #&gt; # CriticAverage &lt;dbl&gt;, MPrinicpalCast &lt;dbl&gt;, #&gt; # FPrincipalCast &lt;dbl&gt;, FPercentPrincipalCast &lt;dbl&gt;, #&gt; # MLeadTime &lt;dbl&gt;, FLeadTime &lt;dbl&gt;, GuestMLeadTIme &lt;dbl&gt;, #&gt; # GuestFLEadTime &lt;dbl&gt;, MLeadPercentinFilm &lt;dbl&gt;, #&gt; # FLeadPercentinFilm &lt;dbl&gt;, GMLeadPerinFilm &lt;chr&gt;, #&gt; # GFLeadPerinFilm &lt;chr&gt;, M/FDifference &lt;dbl&gt;, #&gt; # F/MRatio &lt;dbl&gt;, M/FPercentMore &lt;dbl&gt;, FHighLow &lt;dbl&gt;, #&gt; # LeadTime &lt;dbl&gt;, MorF &lt;dbl&gt;, MaleLAA &lt;dbl&gt;, #&gt; # FemaleLAA &lt;dbl&gt; You’ll be able to work with .txt files and Excel files in the Exercises. Check out https://rawgit.com/rstudio/cheatsheets/master/data-import.pdf for a data import cheatsheet. The final issue that we will discuss in this section occurs when a data set has units within its cells. Consider the earlier example that we used in the reprex section: test_df &lt;- read_csv(&quot;data/parsedf.csv&quot;) head(test_df) #&gt; # A tibble: 3 x 2 #&gt; x y #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 20,000 dollars 1 #&gt; 2 40 dollars 2 #&gt; 3 only 13 dollars 3 The parse_number() function is really useful if you just want the number (no commas, no units, etc.). The function is often paired with mutate() since we are creating a new variable: test_df %&gt;% mutate(x2 = parse_number(x)) #&gt; # A tibble: 3 x 3 #&gt; x y x2 #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 20,000 dollars 1 20000 #&gt; 2 40 dollars 2 40 #&gt; 3 only 13 dollars 3 13 9.2.2 Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 9.6. * The birthdays.txt file has information on the birthdays of various animals on my Animal Crossing island. There are also columns for the Animal’s Name, Animal Type, and how long the animal has lived on the island (in weeks). Click on the file to open it to look at the format of the data. Start with the following code chunk and use the options of read_delim() to read in the data (?read_delim). The delim argument that’s already provided specifies that the delimiter (separator) that you’ll use is a -, as opposed to, for example, a , in a .csv file. Arguments that you may need to change include skip col_names na trim_ws col_types library(tidyverse) df &lt;- read_delim(&quot;data/birthdays.txt&quot;, delim = &quot; - &quot;) head(df) * Another common format for data to be stored in is an Excel file. Often, it’s easiest just to save the Excel file as a .csv file and read it in using read_csv(). But, sometimes this route can be difficult (for example, if your Excel file has thousands of sheets). To read in directly from Excel, you’ll need to install the readxl with install.packages(\"readxl\"). Once installed, load the package with library(readxl), and read in the first sheet evals_prof.xlsx data set, the data set used for Project 2, with the read_excel() function. * Now, read in the second sheet in the Excel file, using the help file for ?read_excel to change one of the arguments. 9.3 Data Scraping with rvest Sometimes, you might want data from a public website that isn’t provided in a file format. To obtain this data, you’ll need to use web scraping, a term which just means “getting data from a website.” The easiest way to do this in R is with the rvest package. Note that we could spend an entire semester talking about web scraping, but we will focus only on websites where the scraping of data is “easy” and won’t give us any major errors. Go to the following website and suppose that you wanted the table of gun violence statistics in R: https://en.wikipedia.org/wiki/Gun_violence_in_the_United_States_by_state. You could try copy-pasting the table into Excel and reading the data set in with read_excel(). Depending on the format of the table, that strategy may work but it may not. Another way is to scrape it directly with rvest. Additionally, if the website continually updates (standings for a sports league, enrollment data for a school, best-selling products for a company, etc.), then scraping is much more convenient, as you don’t need to continually copy-paste for updated data. In the following code chunk, read_html() reads in the entire html file from the url provided while html_nodes() extracts only the tables on the website. library(tidyverse) library(rvest) ## provide the URL and name it something (in this case, url). url &lt;- &quot;https://en.wikipedia.org/wiki/Gun_violence_in_the_United_States_by_state&quot; ## convert the html code into something R can read h &lt;- read_html(url) ## grabs the tables tab &lt;- h %&gt;% html_nodes(&quot;table&quot;) You’ll see that, for this example, there are 3 tables provided. The tables are stored in a list and we can reference the first table using [[1]], the second table using [[2]], etc. For the purposes of this class, we will figure out which of the 3 tables is the one we actually want using trial and error. The html_table() function converts the table into a data.frame object. test1 &lt;- tab[[1]] %&gt;% html_table() test2 &lt;- tab[[2]] %&gt;% html_table() test3 &lt;- tab[[3]] %&gt;% html_table() head(test1) head(test2) head(test3) Which of the 3 tables is the one that we would want to use for an analysis on gun violence in the United States? As another example, consider scraping data from SLU’s athletics page. In particular, suppose we want to do an analysis on SLU’s baseball team. Go to the following website to look at the table of data that we want to scrape: https://saintsathletics.com/sports/baseball/stats/2021. After looking at the website, use the following code to scrape the data set. url &lt;- &quot;https://saintsathletics.com/sports/baseball/stats/2021&quot; h &lt;- read_html(url) tab &lt;- h %&gt;% html_nodes(&quot;table&quot;) tab obj &lt;- tab[[1]] %&gt;% html_table(fill = TRUE) head(obj) tail(obj) obj2 &lt;- tab[[2]] %&gt;% html_table(fill = TRUE) head(obj2) tail(obj2) There’s now 72 different tables! See if you can figure out where the first few tables are coming from on the website. 9.3.1 Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 9.6. Choose a topic/person/place/etc. that interests you that has tables on Wikipedia and scrape the table that is related to that topic. * SLU keeps track of diversity of faculty through time and makes this data public on the following website: https://www.stlawu.edu/ir/diversity/faculty. Use rvest to scrape the data tables into R. Hint: You may need to use an extra argument in html_table() like fill. 9.4 JSON Files with jsonlite A final common data format that we will discuss is JSON (JavaScript Object Notation). We will only cover the very basics of JSON data and use the jsonlite package in R to read in some .json files. JSON files are read in to R as a list object. 9.4.1 Everything Working Well First, consider data from the mobile game Clash Royale. Install the jsonlite package and then use it to read in the json file with the function fromJSON(): ## install.packages(&quot;jsonlite&quot;) library(jsonlite) cr_cards &lt;- fromJSON(&quot;data/clash_royale_card_info.json&quot;) You should get a warning message, which we will investigate in class. Next, type View(cr_cards) in your console (bottom-left) window to look at the data. See if you can pull out the data set by clicking on some things in the View() window. The following give a couple of ways to grab the data using code. The as_tibble() function converts a rectangular object into our familiar tibble. The first option specifies the name of the table that’s in the JSON file (in this case, the name is \"cards\"): library(tidyverse) cr_cards_flat &lt;- cr_cards[[&quot;cards&quot;]] cr_cards_df &lt;- as_tibble(cr_cards_flat) head(cr_cards_df) #&gt; # A tibble: 6 x 8 #&gt; key name elixir type rarity arena description id #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 knight Knig… 3 Troop Common 0 A tough mel… 2.6 e7 #&gt; 2 arche… Arch… 3 Troop Common 0 A pair of l… 2.60e7 #&gt; 3 gobli… Gobl… 2 Troop Common 1 Three fast,… 2.60e7 #&gt; 4 giant Giant 5 Troop Rare 0 Slow but du… 2.60e7 #&gt; 5 pekka P.E.… 7 Troop Epic 4 A heavily a… 2.60e7 #&gt; 6 minio… Mini… 3 Troop Common 0 Three fast,… 2.60e7 The second method uses the flatten() function from the purrr package, the only package in the core tidyverse that we do not talk about in detail in this class. There is also a different flatten() function in the jsonlite package. In the code below, we specify that we want to use flatten() from purrr with purrr::flatten(). If we wanted to use flatten() from jsonlite, we’d use jsonlite::flatten() cr_cards_flat2 &lt;- purrr::flatten(cr_cards) cr_cards_df2 &lt;- as_tibble(cr_cards_flat2) head(cr_cards_df2) #&gt; # A tibble: 6 x 8 #&gt; key name elixir type rarity arena description id #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 knight Knig… 3 Troop Common 0 A tough mel… 2.6 e7 #&gt; 2 arche… Arch… 3 Troop Common 0 A pair of l… 2.60e7 #&gt; 3 gobli… Gobl… 2 Troop Common 1 Three fast,… 2.60e7 #&gt; 4 giant Giant 5 Troop Rare 0 Slow but du… 2.60e7 #&gt; 5 pekka P.E.… 7 Troop Epic 4 A heavily a… 2.60e7 #&gt; 6 minio… Mini… 3 Troop Common 0 Three fast,… 2.60e7 Both methods give a tibble that we can then use our usual tidyverse tools ggplot2, dplyr, tidyr, etc. on. 9.4.2 Things Aren’t Always So Easy Now let’s try to look at some animal crossing data that were obtained from https://github.com/jefflomacy/villagerdb. We first just want to look at the data from one individual villager (ace) in the file ace.json. acedata &lt;- fromJSON(&quot;data/ace.json&quot;) aceflat &lt;- purrr::flatten(acedata) head(aceflat) #&gt; $gender #&gt; [1] &quot;male&quot; #&gt; #&gt; $species #&gt; [1] &quot;bird&quot; #&gt; #&gt; $birthday #&gt; [1] &quot;3-13&quot; #&gt; #&gt; $ac #&gt; $ac$personality #&gt; [1] &quot;jock&quot; #&gt; #&gt; $ac$clothes #&gt; [1] &quot;spade-shirt&quot; #&gt; #&gt; $ac$song #&gt; [1] &quot;K.K. Parade&quot; #&gt; #&gt; $ac$phrase #&gt; [1] &quot;ace&quot; #&gt; #&gt; #&gt; $`afe+` #&gt; $`afe+`$personality #&gt; [1] &quot;jock&quot; #&gt; #&gt; $`afe+`$clothes #&gt; [1] &quot;spade-shirt&quot; #&gt; #&gt; $`afe+`$song #&gt; [1] &quot;K.K. Parade&quot; #&gt; #&gt; #&gt; $name #&gt; [1] &quot;Ace&quot; Things are now….more complicated. This example is just to show that it’s not always easy working with JSON data. Lists can be nested and that creates problems when trying to convert a deeply nested list into our “rectangular” format that’s easy to work with. There’s also the added problem of reading in the .json files from all villagers at the same time We could do this with a for loop or a mapping function from purrr to download and read in the JSON files for all villagers. We won’t delve any more deeply into this, but there’s a lot more to all of the file formats that we’ve discussed this week, particularly web scraping and .json files. 9.4.3 Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 9.6. * Read in the pokedex.json file, a data set that has information on the 151 original Pokemon. Then, use the flatten() function from purrr to flatten the list. * Use as_tibble() to convert your flattened list to a tibble. * Use parse_number() with mutate() to tidy two of the variables in the data set. * Look at the type variable. What looks odd about it? What happens when you try to use it, either in a plot, or using a dplyr function? You can unnest() the Type variable with the unnest() function from tidyr. We didn’t discuss this function but feel free to read about it with ?unnest pokemon_unnest &lt;- unnest(pokemon_df, cols = c(type)) There are 6 pokemon with a spawn_chance of 0. Figure out what these 6 pokemon are. Figure out what the 5 most common Pokemon types are in the first generation (you’ll need to use the unnest()-ed data set for this: why?). 9.5 Chapter Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 9.6. Choose a sports team at SLU, and go to that team’s website (by simply googling SLU name_of_sport). Scrape the data tables from the “Results” or “Statistics” section of this sport. After you scrape the data, tidy the data set. Then, choose one of the following options (different options might make more/less sense for different sports) (a). Summarise different team statistics, either numerically or graphically. Perhaps make some graphs showing different statistics through time. (b). Summarise different individual statistics, either numerically or graphically. (c). Ask and answer any other questions that make sense for the particular sport that you are looking at! Note: A few sports (men’s and women’s golf, for example), give results in PDF format. PDF format is generally a horrible way to record and share data, as it’s very difficult to read in to almost any program. Therefore, avoid sports with PDF results for the purposes of this exercise. 9.6 Exercise Solutions 9.6.1 Reprexes and tibble S * Suppose that you can’t figure out how to create a semester variable and a year variable from Term in evals_prof_S21.csv. (You want to split the Term variable into two variables: Semester with levels F and S and Year with levels 19, 20, and 21). library(tidyverse) evals_df &lt;- read_csv(&quot;data/evals_prof_S21.csv&quot;) head(evals_df) #&gt; # A tibble: 6 x 10 #&gt; Term Course Question `Agree strongly` Agree #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 F19 113-02 1. Course has been a … 9 9 #&gt; 2 F19 113-02 2. Effectively Organi… 12 8 #&gt; 3 F19 113-02 3. Environment Conduc… 11 8 #&gt; 4 F19 113-02 5a. Fair Assessment o… 5 13 #&gt; 5 F19 113-02 5b. Timely Assessment… 8 12 #&gt; 6 F19 113-02 5c. Constructive Asse… 5 8 #&gt; # … with 5 more variables: Agree Somewhat &lt;dbl&gt;, #&gt; # Neutral &lt;dbl&gt;, Disagree Somewhat &lt;dbl&gt;, Disagree &lt;dbl&gt;, #&gt; # Disagree Strongly &lt;dbl&gt; Put together a reprex using tibble() that someone would be able to run to help you figure out your question. library(tidyverse) df &lt;- tibble(Term = c(&quot;F19&quot;, &quot;S20&quot;), x = c(1, 2)) ## Hello! I need help creating a variable that has F/S and ## a separate year variable that has 19 and 20 from the data set above. ## Thanks! * You should actually be able to answer your own question using a function that we learned a couple of weeks ago. Do so, creating a Semester and Year variable. new_df &lt;- df %&gt;% separate(Term, sep = 1, into = c(&quot;Semester&quot;, &quot;Year&quot;)) 9.6.2 readr S * The birthdays.txt file has information on the birthdays of various animals on my Animal Crossing island. There are also columns for the Animal’s Name, Animal Type, and how long the animal has lived on the island (in weeks). Click on the file to open it to look at the format of the data. Start with the following code chunk and use the options of read_delim() to read in the data (?read_delim). The delim argument that’s already provided specifies that the delimiter (separator) that you’ll use is a -, as opposed to, for example, a , in a .csv file. Arguments that you may to change include skip col_names na trim_ws col_types library(tidyverse) df &lt;- read_delim(&quot;data/birthdays.txt&quot;, delim = &quot; - &quot;) head(df) read_delim(&quot;data/birthdays.txt&quot;, delim = &quot;-&quot;, skip = 4, col_names = c(&quot;Birthday&quot;, &quot;Name&quot;, &quot;Animal&quot;, &quot;Island&quot;), na = c(&quot;N/A&quot;, &quot;?&quot;), trim_ws = TRUE, col_types = list( col_character(), col_character(), col_character(), col_number() )) * Another common format for data to be stored in is an Excel file. Often, it’s easiest just to save the Excel file as a .csv file and read it in using read_csv(). But, sometimes this route can be difficult (for example, if your Excel file has thousands of sheets). To read in directly from Excel, you’ll need to install the readxl with install.packages(\"readxl\"). Once installed, load the package with library(readxl), and read in the first sheet evals_prof.xlsx data set, the data set used for Project 2, with the read_excel() function. ## install.packages(&quot;readxl&quot;) library(readxl) read_excel(&quot;data/evals_prof.xlsx&quot;) * Now, read in the second sheet, using the help file for ?read_excel to change one of the arguments. read_excel(&quot;data/evals_prof.xlsx&quot;, sheet = 2) 9.6.3 rvest and Data Scraping S * SLU keeps track of diversity of faculty through time and makes this data public on the following website: https://www.stlawu.edu/ir/diversity/faculty. Use rvest to scrape the data tables into R. url &lt;- &quot;https://www.stlawu.edu/ir/diversity/faculty&quot; h &lt;- read_html(url) tab &lt;- h %&gt;% html_nodes(&quot;table&quot;) obj &lt;- tab[[1]] %&gt;% html_table(fill = TRUE) obj 9.6.4 JSON with jsonlite S * Read in the pokedex.json file, a data set that has information on the 151 original Pokemon. Then, use the flatten() function from purrr to flatten the list. library(jsonlite) pokedex &lt;- fromJSON(&quot;data/pokedex.json&quot;) df &lt;- purrr::flatten(pokedex) * Use as_tibble() to convert your flattened list to a tibble. pokemon_df &lt;- as_tibble(df) * Use parse_number() with mutate() to tidy two of the variables in the data set. pokemon_df &lt;- pokemon_df %&gt;% mutate(height = parse_number(height), weight = parse_number(weight)) * Look at the type variable. What looks odd about it? What happens when you try to use it, either in a plot, or using a dplyr function? ## it&#39;s a variable of lists....this is happening because some ## pokemon have more than one type. ## most ggplot() and dplyr() functions won&#39;t work, or ## won&#39;t work as you&#39;d expect You can unnest() the Type variable with the unnest() function from tidyr. We didn’t discuss this function but feel free to read about it with ?unnest pokemon_unnest &lt;- unnest(pokemon_df, cols = c(type)) 9.6.5 Chapter Exercises S 9.7 Non-Exercise R Code ## Hello! How do I get rid of the units from the values in ## my variable `x`? Thanks! library(tidyverse) test_df &lt;- read_csv(&quot;data/parsedf.csv&quot;) head(test_df) ## Hello! How do I get rid of the units from the values in ## my variable `xvar`? Thanks! library(tidyverse) test_df2 &lt;- tibble(xvar = c(&quot;20,000 dollars&quot;, &quot;40 dollars&quot;), yvar = c(1, 2)) test_df2 library(tidyverse) cars_df &lt;- read_csv(&quot;data/mtcarsex.csv&quot;) head(cars_df) cars_df &lt;- read_csv(&quot;data/mtcarsex.csv&quot;, skip = 2) ## first two lines will be skipped head(cars_df) cars_df &lt;- read_csv(&quot;data/mtcarsex.csv&quot;, na = c(NA, &quot;-999&quot;), skip = 2) head(cars_df) cars_df &lt;- read_csv(&quot;data/mtcarsex.csv&quot;, na = c(NA, &quot;-999&quot;), skip = 2, col_types = cols( mpg = col_double(), cyl = col_factor(), disp = col_double(), hp = col_double(), drat = col_double(), wt = col_double(), qsec = col_double(), vs = col_factor(), am = col_double(), gear = col_double(), carb = col_double() )) cars_df &lt;- read_csv(&quot;data/mtcarsex.csv&quot;, na = c(NA, &quot;-999&quot;), skip = 2, col_types = cols( mpg = col_double(), cyl = col_factor(), disp = col_double(), hp = col_double(), drat = col_double(), wt = col_double(), qsec = col_double(), vs = col_factor(), am = col_double(), gear = col_double(), carb = col_double() )) %&gt;% slice(-(1:2)) head(cars_df) oscars_df &lt;- read_tsv(&quot;data/oscars.tsv&quot;) head(oscars_df) test_df &lt;- read_csv(&quot;data/parsedf.csv&quot;) head(test_df) test_df %&gt;% mutate(x2 = parse_number(x)) library(tidyverse) library(rvest) ## provide the URL and name it something (in this case, url). url &lt;- &quot;https://en.wikipedia.org/wiki/Gun_violence_in_the_United_States_by_state&quot; ## convert the html code into something R can read h &lt;- read_html(url) ## grabs the tables tab &lt;- h %&gt;% html_nodes(&quot;table&quot;) test1 &lt;- tab[[1]] %&gt;% html_table() test2 &lt;- tab[[2]] %&gt;% html_table() test3 &lt;- tab[[3]] %&gt;% html_table() head(test1) head(test2) head(test3) url &lt;- &quot;https://saintsathletics.com/sports/baseball/stats/2021&quot; h &lt;- read_html(url) tab &lt;- h %&gt;% html_nodes(&quot;table&quot;) tab obj &lt;- tab[[1]] %&gt;% html_table(fill = TRUE) head(obj) tail(obj) obj2 &lt;- tab[[2]] %&gt;% html_table(fill = TRUE) head(obj2) tail(obj2) ## install.packages(&quot;jsonlite&quot;) library(jsonlite) cr_cards &lt;- fromJSON(&quot;data/clash_royale_card_info.json&quot;) library(tidyverse) cr_cards_flat &lt;- cr_cards[[&quot;cards&quot;]] cr_cards_df &lt;- as_tibble(cr_cards_flat) head(cr_cards_df) cr_cards_flat2 &lt;- purrr::flatten(cr_cards) cr_cards_df2 &lt;- as_tibble(cr_cards_flat2) head(cr_cards_df2) acedata &lt;- fromJSON(&quot;data/ace.json&quot;) aceflat &lt;- purrr::flatten(acedata) head(aceflat) "],["data-ethics.html", " 10 Data Ethics 10.1 Ethical Examples 10.2 Data Privacy 10.3 Hypothesis Generation vs. Confirmation 10.4 Chapter Exercises 10.5 Exercise Solutions", " 10 Data Ethics Goals: explain why data ethics is an important issue in data science using a couple of examples. describe a few issues with data privacy and explain why, just because data doesn’t have an individual’s name doesn’t necessarily make the data truly anonymous. explain the difference between hypothesis confirmation and hypothesis exploration and why the distinction matters. 10.1 Ethical Examples We’ve tried to interweave issues of ethics throughout many examples used already in this course, but the purpose of this section is to put data ethics in direct focus. Some questions to consider for any data collected, especially data collected on human subjects: who gets to use data and for what purposes? who collected the data and does that organization have any conflicts of interest? is presentation of an analysis harmful to a particular person or group of people? Are there benefits of an analysis? have the subjects of a data collection procedure been treated respectfully and have they given consent to their information being collected? When is consent needed and when is it not? For example, we have looked at data on professional athletes. Do they need to provide consent or is consent inherent in being in the spotlight? We’ve also scraped data from SLU’s athletics website to look at data pertaining to some of you! Is that ethical? Is there a line you wouldn’t cross pertaining to data collected on named, individual people? 10.1.1 Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 10.5. Read Sections 8.1 - 8.3 in Modern Data Science with R. Then, write a one paragraph summary of the reading and how it might pertain to the way you use or interpret data. Data Feminism is related to data ethics, though the two terms are certainly not synonymous. Recently, Catherine D’Ignazio and Lauren F. Klein published a book called Data Feminism https://datafeminism.io/ Read the following blog post on Data Feminism, focusing on the section on Missing Data. https://teachdatascience.com/datafem/ . Pick one example from the bulleted list and write a 2 sentence explanation that explains why it might be important to acknowledge the missing data in an analysis. Choose 1 of the following two articles to read https://www.theguardian.com/world/2017/sep/08/ai-gay-gaydar-algorithm-facial-recognition-criticism-stanford on the use of data in the LGBTQIA+ community https://towardsdatascience.com/5-steps-to-take-as-an-antiracist-data-scientist-89712877c214 on anti-racist data practices. For the LGBTQIA+ article, write a two sentence summary for the side of the argument that research in facial recognition software to identify members of the LGBTQ+ community should not occur, even if this viewpoint isn’t your own. Then, write a two sentence summary for the side of the argument that research in facial recognition software to identify members of the LGBTQ+ community is okay as long as the results are used responsibly, even if this viewpoint isn’t your own. For the anti-racist data science article, under Step 2, pick a News Article and read the first few paragraphs. Describe, in 2-3 sentences, what your article’s example of bias is and why the incidence of bias matters. 10.2 Data Privacy Related to data ethics is the idea of data privacy. What data is private and what data is public? For some examples, this may seem obvious, but for others (e.g. data on a government agency that collects data on people), the answer might not be as clear cut. Is anonymous data truly anonymous? What type of consent should be provided before collecting data on someone? We will explore some of these issues in the following exercises. 10.2.1 Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 10.5. Recall the course evaluations data set, which you used for Mini-Project 2. This might have been obvious, but the course evaluations were my course evaluations from last year, so I felt that it was ethically okay for me to share them. But, data privacy is not always a cut-and-dry issue. Consider the following course evaluation formats, and think about whether or not you would consider it ethically okay for me to share the evaluation information with you: I not only gave you the course averages, but I also give you PDFs of each student’s written responses. The PDFs are anonymous, but they do have the student’s sex, year, and whether they took the course for a Major, Minor, Distribution Requirement, etc. Assume that you also can obtain the class roster for each class. I not only gave you the course averages and the PDFs in (a), but I also give you the grade each student received in the course on their PDF list of responses (but they are still anonymous and you can still obtain the class roster). Another professor at SLU posts her evaluation averages in a table on a personal website. I scrape the data table and give it to you all, along with the professor’s name and courses. I don’t ask for permission, but the website that the tables are on is public. Suppose that I collect data on students in this Data Science class. In each setting (a) through (d), suppose that I give you a data set with the following variables collected on each student in the class. Which option, if any, would it be ethically okay for me to share the data with all students in the class. current grade and time spent on the R Studio server current grade, class year, and whether or not the student is a stat major favorite R package, whether or not the student took STAT 213, whether or not the student took CS 140, and Major favorite R package, whether or not the student took STAT 213, whether or not the student took CS 140, and current grade in the course How anonymous are SLU’s course evaluations? We will do an in-class activity to investigate this. 10.3 Hypothesis Generation vs. Confirmation We have focused on hypothesis generation for all data sets in this particular course. Read the following two articles that explain the difference between hypothesis generation and hypothesis confirmation: Read the following two very short articles, one from our textbook and one from another source: https://r4ds.had.co.nz/model-intro.html#hypothesis-generation-vs.-hypothesis-confirmation https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6718169/ 10.3.1 Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 10.5. Explain the difference between hypothesis generation and hypothesis confirmation. How many times can you use a single observation for hypothesis generation? for hypothesis confirmation? Which of the following questions, pertaining to someone’s fitness, sound more suitable to be answered with Hypothesis Exploration? Which with Hypothesis Confirmation? You want to know if, on average, this person exercises more on weekends or more on weekdays, with no other questions of interest. You want to look at general trends in the person’s step count and try to determine if various events influenced the step count. You want to know if the person exercises more in winter or more in summer, and you would also like to investigate other seasonal trends. Note: Prediction is different from hypothesis confirmation, because you typically don’t really care which variables are associated with your response. You only want a model that gives the “best” predictions. Because of this, if your goal is prediction, you typically have a lot more freedom with how many times you can “use” a single observation. We will talk a little more about prediction later in the semester. 10.4 Chapter Exercises There are no chapter exercises for this chapter. 10.5 Exercise Solutions There are no exercise solutions for this chapter. "],["merging-with-dplyr.html", " 11 Merging with dplyr 11.1 Stacking Rows and Appending Columns 11.2 Mutating Joins 11.3 Filtering Joins 11.4 A Note on SQL 11.5 Chapter Exercises 11.6 Exercise Solutions 11.7 Non-Exercise R Code", " 11 Merging with dplyr Goals: use bind_rows() to stack two data sets and bind_cols() to merge two data sets. identify keys in two related data sets. use the mutating join functions in dplyr to merge two data sets by a key. use the filtering join functions in dplyr to filter one data set by values in another data set. apply the appropriate join() function for a given problem and context. 11.1 Stacking Rows and Appending Columns 11.1.1 Stacking with bind_rows() First, we will talk about combining two data sets by “stacking” them on top of each other to form one new data set. The bind_rows() function can be used for this purpose if the two data sets have identical column names. A common instance where this is useful is if two data sets come from the same source and have different locations or years, but the same exact column names. For example, examine the following website and notice how there are .csv files given for each year of matches in the ATP (Association of (men’s) Tennis Professionals). https://github.com/JeffSackmann/tennis_atp. Then, read in the data sets, and look at how many columns each has. library(tidyverse) atp_2019 &lt;- read_csv(&quot;data/atp_matches_2019.csv&quot;) atp_2018 &lt;- read_csv(&quot;data/atp_matches_2018.csv&quot;) head(atp_2019) head(atp_2018) To combine results from both data sets, atp_df &lt;- bind_rows(atp_2018, atp_2019) #&gt; Error: Can&#39;t combine `winner_seed` &lt;double&gt; and `winner_seed` &lt;character&gt;. What happens? Can you fix the error? Hint: run spec(atp_2018) to get the full column specifications and use your readr knowledge to change a couple of the column types. We also did not discuss this, but, when using the col_type argument in read_csv(), you don’t need to specify all of the column types. Just specifying the ones that you want to change works too. The following code forces the seed variables in the 2018 data set to be characters. atp_2018 &lt;- read_csv(&quot;data/atp_matches_2018.csv&quot;, col_types = cols(winner_seed = col_character(), loser_seed = col_character())) We can try combining the data sets now. atp_df &lt;- bind_rows(atp_2018, atp_2019) atp_df Do a quick check to make sure the number of rows in atp_2018 plus the number of rows in atp_2019 equals the number of rows in atp_df. It might seem a little annoying, but, by default bind_rows() will only combine two data sets by stacking rows if the data sets have identical column names and identical column classes, as we saw in the previous example. Now run the following and look at the output. df_test2a &lt;- tibble(xvar = c(1, 2)) df_test2b &lt;- tibble(xvar = c(1, 2), y = c(5, 1)) bind_rows(df_test2a, df_test2b) #&gt; # A tibble: 4 x 2 #&gt; xvar y #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 NA #&gt; 2 2 NA #&gt; 3 1 5 #&gt; 4 2 1 Is this the behavior you would expect? 11.1.2 Binding Columns with bind_cols() We won’t spend much time talking about how to bind together columns because it’s generally a little dangerous. We will use a couple of test data sets, df_test1a and df_test1b, to see it in action: df_test1a &lt;- tibble(xvar = c(1, 2), yvar = c(5, 1)) df_test1b &lt;- tibble(x = c(1, 2), y = c(5, 1)) bind_cols(df_test1a, df_test1b) #&gt; # A tibble: 2 x 4 #&gt; xvar yvar x y #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 5 1 5 #&gt; 2 2 1 2 1 For a larger data set, why might this be a dangerous way to combine data? What must you be sure of about the way the data was collected in order to combine data in this way? 11.1.3 Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 11.6. * Run the following and explain why R does not simply stack the rows. Then, fix the issue with the rename() function. df_test1a &lt;- tibble(xvar = c(1, 2), yvar = c(5, 1)) df_test1b &lt;- tibble(x = c(1, 2), y = c(5, 1)) bind_rows(df_test1a, df_test1b) #&gt; # A tibble: 4 x 4 #&gt; xvar yvar x y #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 5 NA NA #&gt; 2 2 1 NA NA #&gt; 3 NA NA 1 5 #&gt; 4 NA NA 2 1 11.2 Mutating Joins If the goal is to combine two data sets using some common variable(s) that both data sets have, we need different tools than simply stacking rows or appending columns. When merging together two or more data sets, we need to have a matching identification variable in each data set. This variable is commonly called a key. A key can be an identification number, a name, a date, etc, but must be present in both data sets. As a simple first example, consider library(tidyverse) df1 &lt;- tibble(name = c(&quot;Emily&quot;, &quot;Miguel&quot;, &quot;Tonya&quot;), fav_sport = c(&quot;Swimming&quot;, &quot;Football&quot;, &quot;Tennis&quot;)) df2 &lt;- tibble(name = c(&quot;Tonya&quot;, &quot;Miguel&quot;, &quot;Emily&quot;), fav_colour = c(&quot;Robin&#39;s Egg Blue&quot;, &quot;Tickle Me Pink&quot;, &quot;Goldenrod&quot;)) Our goal is to combine the two data sets so that the people’s favorite sports and favorite colours are in one data set. Identify the key in the example above. Why can we no longer use bind_cols() here? 11.2.1 Keep All Rows of Data Set 1 with left_join() Consider the babynames R package, which has the following data sets: lifetables: cohort life tables for different sex and different year variables, starting at the year 1900. births: the number of births in the United States in each year, since 1909 babynames: popularity of different baby names per year and sex since the year 1880. ##install.packages(&quot;babynames&quot;) library(babynames) life_df &lt;- babynames::lifetables birth_df &lt;- babynames::births babynames_df &lt;- babynames::babynames head(babynames) head(births) head(lifetables) Read about each data set with ?babynames, ?births and ?lifetables. Suppose that you want to combine the births data set with the babynames data set, so that each row of babynames now has the total number of births for that year. We first need to identify the key in each data set that we will use for the joining. In this case, each data set has a year variable, and we can use left_join() to keep all observations in babynames_df, even for years that are not in the births_df data set. combined_left &lt;- left_join(babynames_df, birth_df, by = c(&quot;year&quot; = &quot;year&quot;)) head(combined_left) #&gt; # A tibble: 6 x 6 #&gt; year sex name n prop births #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 1880 F Mary 7065 0.0724 NA #&gt; 2 1880 F Anna 2604 0.0267 NA #&gt; 3 1880 F Emma 2003 0.0205 NA #&gt; 4 1880 F Elizabeth 1939 0.0199 NA #&gt; 5 1880 F Minnie 1746 0.0179 NA #&gt; 6 1880 F Margaret 1578 0.0162 NA tail(combined_left) #&gt; # A tibble: 6 x 6 #&gt; year sex name n prop births #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 2017 M Zyhier 5 0.00000255 3855500 #&gt; 2 2017 M Zykai 5 0.00000255 3855500 #&gt; 3 2017 M Zykeem 5 0.00000255 3855500 #&gt; 4 2017 M Zylin 5 0.00000255 3855500 #&gt; 5 2017 M Zylis 5 0.00000255 3855500 #&gt; 6 2017 M Zyrie 5 0.00000255 3855500 Why are births missing in head(combined_left) but not in tail(combined_left)? 11.2.2 Keep All Rows of Data Set 2 with right_join() Recall from the accompanying handout that there is no need to ever use right_join() because it is the same as using a left_join() with the first two data set arguments switched: ## these will always do the same exact thing right_join(babynames_df, birth_df, by = c(&quot;year&quot; = &quot;year&quot;)) #&gt; # A tibble: 1,839,952 x 6 #&gt; year sex name n prop births #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 1909 F Mary 19259 0.0523 2718000 #&gt; 2 1909 F Helen 9250 0.0251 2718000 #&gt; 3 1909 F Margaret 7359 0.0200 2718000 #&gt; 4 1909 F Ruth 6509 0.0177 2718000 #&gt; 5 1909 F Dorothy 6253 0.0170 2718000 #&gt; 6 1909 F Anna 5804 0.0158 2718000 #&gt; 7 1909 F Elizabeth 5176 0.0141 2718000 #&gt; 8 1909 F Mildred 5054 0.0137 2718000 #&gt; 9 1909 F Marie 4301 0.0117 2718000 #&gt; 10 1909 F Alice 4170 0.0113 2718000 #&gt; # … with 1,839,942 more rows left_join(birth_df, babynames_df, by = c(&quot;year&quot; = &quot;year&quot;)) #&gt; # A tibble: 1,839,952 x 6 #&gt; year births sex name n prop #&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1909 2718000 F Mary 19259 0.0523 #&gt; 2 1909 2718000 F Helen 9250 0.0251 #&gt; 3 1909 2718000 F Margaret 7359 0.0200 #&gt; 4 1909 2718000 F Ruth 6509 0.0177 #&gt; 5 1909 2718000 F Dorothy 6253 0.0170 #&gt; 6 1909 2718000 F Anna 5804 0.0158 #&gt; 7 1909 2718000 F Elizabeth 5176 0.0141 #&gt; 8 1909 2718000 F Mildred 5054 0.0137 #&gt; 9 1909 2718000 F Marie 4301 0.0117 #&gt; 10 1909 2718000 F Alice 4170 0.0113 #&gt; # … with 1,839,942 more rows Therefore, it’s usually easier to just always use left_join() and ignore right_join() completely. 11.2.3 Keep All Rows of Both Data Sets with full_join() A full_join() will keep all rows in data set 1 that don’t have a matching key in data set 2, and will also keep all rows in data set 2 that don’t have a matching key in data set 1, filling in NA for missing values when necessary. For our example of merging babynames_df with birth_df, full_join(babynames_df, birth_df, by = c(&quot;year&quot; = &quot;year&quot;)) 11.2.4 Keep Only Rows with Matching Keys with inner_join() We can also keep only rows with matching keys with inner_join(). For this join, any row in data set 1 without a matching key in data set 2 is dropped, and any row in data set 2 without a matching key in data set 1 is also dropped. inner_join(babynames_df, birth_df, by = c(&quot;year&quot; = &quot;year&quot;)) #&gt; # A tibble: 1,839,952 x 6 #&gt; year sex name n prop births #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 1909 F Mary 19259 0.0523 2718000 #&gt; 2 1909 F Helen 9250 0.0251 2718000 #&gt; 3 1909 F Margaret 7359 0.0200 2718000 #&gt; 4 1909 F Ruth 6509 0.0177 2718000 #&gt; 5 1909 F Dorothy 6253 0.0170 2718000 #&gt; 6 1909 F Anna 5804 0.0158 2718000 #&gt; 7 1909 F Elizabeth 5176 0.0141 2718000 #&gt; 8 1909 F Mildred 5054 0.0137 2718000 #&gt; 9 1909 F Marie 4301 0.0117 2718000 #&gt; 10 1909 F Alice 4170 0.0113 2718000 #&gt; # … with 1,839,942 more rows 11.2.5 Which xxxx_join()? Which join function we use will depend on the context of the data and what questions you will be answering in your analysis. Most importantly, if you’re using a left_join(), right_join() or inner_join(), you’re potentially cutting out some data. It’s important to be aware of what data you’re omitting. For example, with the babynames and births data, we would want to keep a note that a left_join() removed all observations before 1909 from joined data set. 11.2.6 The Importance of a Good Key The key variable is very important for joining and is not always available in a “perfect” form. Recall the college majors data sets we have, called slumajors_df, which information on majors at SLU. Another data set, collegemajors_df, has different statistics on college majors nationwide. There’s lots of interesting variables in these data sets, but we’ll focus on the Major variable here. Read in and examine the two data sets with: slumajors_df &lt;- read_csv(&quot;data/SLU_Majors_15_19.csv&quot;) collegemajors_df &lt;- read_csv(&quot;data/college-majors.csv&quot;) head(slumajors_df) #&gt; # A tibble: 6 x 3 #&gt; Major nfemales nmales #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Anthropology 34 15 #&gt; 2 Art &amp; Art History 65 11 #&gt; 3 Biochemistry 14 11 #&gt; 4 Biology 162 67 #&gt; 5 Business in the Liberal Arts 135 251 #&gt; 6 Chemistry 26 14 head(collegemajors_df) #&gt; # A tibble: 6 x 12 #&gt; Major Total Men Women Major_category Employed Full_time #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 PETRO… 2339 2057 282 Engineering 1976 1849 #&gt; 2 MININ… 756 679 77 Engineering 640 556 #&gt; 3 METAL… 856 725 131 Engineering 648 558 #&gt; 4 NAVAL… 1258 1123 135 Engineering 758 1069 #&gt; 5 CHEMI… 32260 21239 11021 Engineering 25694 23170 #&gt; 6 NUCLE… 2573 2200 373 Engineering 1857 2038 #&gt; # … with 5 more variables: Part_time &lt;dbl&gt;, #&gt; # Unemployed &lt;dbl&gt;, Median &lt;dbl&gt;, P25th &lt;dbl&gt;, #&gt; # P75th &lt;dbl&gt; The most logical key for joining these two data sets is Major, but joining the data sets won’t actually work. The following is an attempt at using Major as the key. left_join(slumajors_df, collegemajors_df, by = c(&quot;Major&quot; = &quot;Major&quot;)) #&gt; # A tibble: 27 x 14 #&gt; Major nfemales nmales Total Men Women Major_category #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 Anthrop… 34 15 NA NA NA &lt;NA&gt; #&gt; 2 Art &amp; A… 65 11 NA NA NA &lt;NA&gt; #&gt; 3 Biochem… 14 11 NA NA NA &lt;NA&gt; #&gt; 4 Biology 162 67 NA NA NA &lt;NA&gt; #&gt; 5 Busines… 135 251 NA NA NA &lt;NA&gt; #&gt; 6 Chemist… 26 14 NA NA NA &lt;NA&gt; #&gt; 7 Compute… 21 47 NA NA NA &lt;NA&gt; #&gt; 8 Conserv… 38 20 NA NA NA &lt;NA&gt; #&gt; 9 Economi… 128 349 NA NA NA &lt;NA&gt; #&gt; 10 English 131 54 NA NA NA &lt;NA&gt; #&gt; # … with 17 more rows, and 7 more variables: #&gt; # Employed &lt;dbl&gt;, Full_time &lt;dbl&gt;, Part_time &lt;dbl&gt;, #&gt; # Unemployed &lt;dbl&gt;, Median &lt;dbl&gt;, P25th &lt;dbl&gt;, #&gt; # P75th &lt;dbl&gt; Why did the collegemajors_df give only NA values when we tried to merge by major? This example underscores the importance of having a key that matches exactly. Some, but not all, of the issues involved in joining these two data sets can be solved with functions in the stringr package (discussed in a few weeks). For example, the capitalization issue can be solved with the str_to_title() function, which converts that all-caps majors in collegemajors_df to majors where only the first letter of each word is capitalized: collegemajors_df &lt;- collegemajors_df %&gt;% mutate(Major = str_to_title(Major)) left_join(slumajors_df, collegemajors_df) #&gt; Joining, by = &quot;Major&quot; #&gt; # A tibble: 27 x 14 #&gt; Major nfemales nmales Total Men Women Major_category #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 Anth… 34 15 NA NA NA &lt;NA&gt; #&gt; 2 Art … 65 11 NA NA NA &lt;NA&gt; #&gt; 3 Bioc… 14 11 NA NA NA &lt;NA&gt; #&gt; 4 Biol… 162 67 280709 111762 168947 Biology &amp; Lif… #&gt; 5 Busi… 135 251 NA NA NA &lt;NA&gt; #&gt; 6 Chem… 26 14 66530 32923 33607 Physical Scie… #&gt; 7 Comp… 21 47 128319 99743 28576 Computers &amp; M… #&gt; 8 Cons… 38 20 NA NA NA &lt;NA&gt; #&gt; 9 Econ… 128 349 139247 89749 49498 Social Science #&gt; 10 Engl… 131 54 NA NA NA &lt;NA&gt; #&gt; # … with 17 more rows, and 7 more variables: #&gt; # Employed &lt;dbl&gt;, Full_time &lt;dbl&gt;, Part_time &lt;dbl&gt;, #&gt; # Unemployed &lt;dbl&gt;, Median &lt;dbl&gt;, P25th &lt;dbl&gt;, #&gt; # P75th &lt;dbl&gt; As we can see, this solves the issue for some majors but others still have different naming conventions in the two data sets. 11.2.7 Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 11.6. Examine the following two joins that we’ve done, and explain why one resulting data set has fewer observations (rows) than the other. left_join(babynames_df, birth_df, by = c(&quot;year&quot; = &quot;year&quot;)) #&gt; # A tibble: 1,924,665 x 6 #&gt; year sex name n prop births #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 1880 F Mary 7065 0.0724 NA #&gt; 2 1880 F Anna 2604 0.0267 NA #&gt; 3 1880 F Emma 2003 0.0205 NA #&gt; 4 1880 F Elizabeth 1939 0.0199 NA #&gt; 5 1880 F Minnie 1746 0.0179 NA #&gt; 6 1880 F Margaret 1578 0.0162 NA #&gt; 7 1880 F Ida 1472 0.0151 NA #&gt; 8 1880 F Alice 1414 0.0145 NA #&gt; 9 1880 F Bertha 1320 0.0135 NA #&gt; 10 1880 F Sarah 1288 0.0132 NA #&gt; # … with 1,924,655 more rows left_join(birth_df, babynames_df, by = c(&quot;year&quot; = &quot;year&quot;)) #&gt; # A tibble: 1,839,952 x 6 #&gt; year births sex name n prop #&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1909 2718000 F Mary 19259 0.0523 #&gt; 2 1909 2718000 F Helen 9250 0.0251 #&gt; 3 1909 2718000 F Margaret 7359 0.0200 #&gt; 4 1909 2718000 F Ruth 6509 0.0177 #&gt; 5 1909 2718000 F Dorothy 6253 0.0170 #&gt; 6 1909 2718000 F Anna 5804 0.0158 #&gt; 7 1909 2718000 F Elizabeth 5176 0.0141 #&gt; 8 1909 2718000 F Mildred 5054 0.0137 #&gt; 9 1909 2718000 F Marie 4301 0.0117 #&gt; 10 1909 2718000 F Alice 4170 0.0113 #&gt; # … with 1,839,942 more rows Evaluate whether the following statement is true or false: an inner_join() will always result in a data set with the same or fewer rows than a full_join(). Evaluate whether the following statement is true or false: an inner_join() will always result in a data set with the same or fewer rows than a left_join(). 11.3 Filtering Joins Filtering joins (semi_join() and anti_join()) are useful if you would only like to keep the variables in one data set, but you want to filter out observations by a variable in the second data set. Consider again the two data sets on men’s tennis matches in 2018 and in 2019. atp_2019 &lt;- read_csv(&quot;data/atp_matches_2019.csv&quot;) atp_2018 &lt;- read_csv(&quot;data/atp_matches_2018.csv&quot;) atp_2019 #&gt; # A tibble: 2,781 x 49 #&gt; tourney_id tourney_name surface draw_size tourney_level #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 2019-M020 Brisbane Hard 32 A #&gt; 2 2019-M020 Brisbane Hard 32 A #&gt; 3 2019-M020 Brisbane Hard 32 A #&gt; 4 2019-M020 Brisbane Hard 32 A #&gt; 5 2019-M020 Brisbane Hard 32 A #&gt; 6 2019-M020 Brisbane Hard 32 A #&gt; 7 2019-M020 Brisbane Hard 32 A #&gt; 8 2019-M020 Brisbane Hard 32 A #&gt; 9 2019-M020 Brisbane Hard 32 A #&gt; 10 2019-M020 Brisbane Hard 32 A #&gt; # … with 2,771 more rows, and 44 more variables: #&gt; # tourney_date &lt;dbl&gt;, match_num &lt;dbl&gt;, winner_id &lt;dbl&gt;, #&gt; # winner_seed &lt;chr&gt;, winner_entry &lt;chr&gt;, #&gt; # winner_name &lt;chr&gt;, winner_hand &lt;chr&gt;, winner_ht &lt;dbl&gt;, #&gt; # winner_ioc &lt;chr&gt;, winner_age &lt;dbl&gt;, loser_id &lt;dbl&gt;, #&gt; # loser_seed &lt;chr&gt;, loser_entry &lt;chr&gt;, loser_name &lt;chr&gt;, #&gt; # loser_hand &lt;chr&gt;, loser_ht &lt;dbl&gt;, loser_ioc &lt;chr&gt;, #&gt; # loser_age &lt;dbl&gt;, score &lt;chr&gt;, best_of &lt;dbl&gt;, #&gt; # round &lt;chr&gt;, minutes &lt;dbl&gt;, w_ace &lt;dbl&gt;, w_df &lt;dbl&gt;, #&gt; # w_svpt &lt;dbl&gt;, w_1stIn &lt;dbl&gt;, w_1stWon &lt;dbl&gt;, #&gt; # w_2ndWon &lt;dbl&gt;, w_SvGms &lt;dbl&gt;, w_bpSaved &lt;dbl&gt;, #&gt; # w_bpFaced &lt;dbl&gt;, l_ace &lt;dbl&gt;, l_df &lt;dbl&gt;, l_svpt &lt;dbl&gt;, #&gt; # l_1stIn &lt;dbl&gt;, l_1stWon &lt;dbl&gt;, l_2ndWon &lt;dbl&gt;, #&gt; # l_SvGms &lt;dbl&gt;, l_bpSaved &lt;dbl&gt;, l_bpFaced &lt;dbl&gt;, #&gt; # winner_rank &lt;dbl&gt;, winner_rank_points &lt;dbl&gt;, #&gt; # loser_rank &lt;dbl&gt;, loser_rank_points &lt;dbl&gt; atp_2018 #&gt; # A tibble: 2,889 x 49 #&gt; tourney_id tourney_name surface draw_size tourney_level #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 2018-M020 Brisbane Hard 32 A #&gt; 2 2018-M020 Brisbane Hard 32 A #&gt; 3 2018-M020 Brisbane Hard 32 A #&gt; 4 2018-M020 Brisbane Hard 32 A #&gt; 5 2018-M020 Brisbane Hard 32 A #&gt; 6 2018-M020 Brisbane Hard 32 A #&gt; 7 2018-M020 Brisbane Hard 32 A #&gt; 8 2018-M020 Brisbane Hard 32 A #&gt; 9 2018-M020 Brisbane Hard 32 A #&gt; 10 2018-M020 Brisbane Hard 32 A #&gt; # … with 2,879 more rows, and 44 more variables: #&gt; # tourney_date &lt;dbl&gt;, match_num &lt;dbl&gt;, winner_id &lt;dbl&gt;, #&gt; # winner_seed &lt;dbl&gt;, winner_entry &lt;chr&gt;, #&gt; # winner_name &lt;chr&gt;, winner_hand &lt;chr&gt;, winner_ht &lt;dbl&gt;, #&gt; # winner_ioc &lt;chr&gt;, winner_age &lt;dbl&gt;, loser_id &lt;dbl&gt;, #&gt; # loser_seed &lt;dbl&gt;, loser_entry &lt;chr&gt;, loser_name &lt;chr&gt;, #&gt; # loser_hand &lt;chr&gt;, loser_ht &lt;dbl&gt;, loser_ioc &lt;chr&gt;, #&gt; # loser_age &lt;dbl&gt;, score &lt;chr&gt;, best_of &lt;dbl&gt;, #&gt; # round &lt;chr&gt;, minutes &lt;dbl&gt;, w_ace &lt;dbl&gt;, w_df &lt;dbl&gt;, #&gt; # w_svpt &lt;dbl&gt;, w_1stIn &lt;dbl&gt;, w_1stWon &lt;dbl&gt;, #&gt; # w_2ndWon &lt;dbl&gt;, w_SvGms &lt;dbl&gt;, w_bpSaved &lt;dbl&gt;, #&gt; # w_bpFaced &lt;dbl&gt;, l_ace &lt;dbl&gt;, l_df &lt;dbl&gt;, l_svpt &lt;dbl&gt;, #&gt; # l_1stIn &lt;dbl&gt;, l_1stWon &lt;dbl&gt;, l_2ndWon &lt;dbl&gt;, #&gt; # l_SvGms &lt;dbl&gt;, l_bpSaved &lt;dbl&gt;, l_bpFaced &lt;dbl&gt;, #&gt; # winner_rank &lt;dbl&gt;, winner_rank_points &lt;dbl&gt;, #&gt; # loser_rank &lt;dbl&gt;, loser_rank_points &lt;dbl&gt; 11.3.0.0.1 Filtering with semi_join() Suppose that we only want to keep matches in 2019 where the winning player had 10 or more wins in 2018. This might be useful if we want to not consider players in 2018 that only played in a couple of matches, perhaps because they got injured or perhaps because they received a special wildcard into the draw of only one event. To accomplish this, we can first create a data set that has the names of all of the players that won 10 or more matches in 2018, using functions that we learned from dplyr earlier in the semester: win10 &lt;- atp_2018 %&gt;% group_by(winner_name) %&gt;% summarise(nwin = n()) %&gt;% filter(nwin &gt;= 10) win10 #&gt; # A tibble: 93 x 2 #&gt; winner_name nwin #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Adrian Mannarino 26 #&gt; 2 Albert Ramos 21 #&gt; 3 Alex De Minaur 29 #&gt; 4 Alexander Zverev 58 #&gt; 5 Aljaz Bedene 19 #&gt; 6 Andreas Seppi 24 #&gt; 7 Andrey Rublev 20 #&gt; 8 Benoit Paire 27 #&gt; 9 Borna Coric 40 #&gt; 10 Cameron Norrie 19 #&gt; # … with 83 more rows Next, we apply semi_join(), which takes the names of two data sets (the second is the one that contains information about how the first should be “filtered”). The third argument gives the name of the key (winner_name) in this case. tennis_2019_10 &lt;- semi_join(atp_2019, win10, by = c(&quot;winner_name&quot; = &quot;winner_name&quot;)) tennis_2019_10$winner_name Note that this only keeps the matches in 2019 where the winner had 10 or more match wins in 2018. It drops any matches where the loser lost against someone who did not have 10 or more match wins in 2018. So this isn’t yet perfect and would take a little more thought into which matches we actually want to keep for a particular analysis. 11.3.1 Filtering with anti_join() Now suppose that we want to only keep the matches in 2019 where the winning player did not have any wins in 2018. We might think of these players as “emerging players” in 2019, players who are coming back from an injury, etc.. To do this, we can use anti_join(), which only keeps the rows in the first data set that do not have a match in the second data set. new_winners &lt;- anti_join(atp_2019, atp_2018, by = c(&quot;winner_name&quot; = &quot;winner_name&quot;)) new_winners$winner_name We can then examine how many wins each of these “new” (or perhaps previously injured) players had in 2019: new_winners %&gt;% group_by(winner_name) %&gt;% summarise(nwin = n()) %&gt;% arrange(desc(nwin)) #&gt; # A tibble: 59 x 2 #&gt; winner_name nwin #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Christian Garin 32 #&gt; 2 Juan Ignacio Londero 22 #&gt; 3 Miomir Kecmanovic 22 #&gt; 4 Hugo Dellien 12 #&gt; 5 Attila Balazs 7 #&gt; 6 Cedrik Marcel Stebe 7 #&gt; 7 Janko Tipsarevic 7 #&gt; 8 Jannik Sinner 7 #&gt; 9 Soon Woo Kwon 7 #&gt; 10 Gregoire Barrere 6 #&gt; # … with 49 more rows The filtering join functions are useful if you want to filter out observations by some criterion in a different data set. 11.3.2 Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 11.6. Take the semi_join() tennis example, but now suppose that we want to keep only the matches in 2019 where either the winning player or the losing player had 10 or more match wins in 2018. How can you modify the code to achieve that goal? tennis_2019_10 &lt;- semi_join(atp_2019, win10, by = c(&quot;winner_name&quot; = &quot;winner_name&quot;)) tennis_2019_10$winner_name * Take the semi_join() tennis example, but now suppose that we want to keep only the matches in 2019 where both the winning player and the losing player had 10 or more match wins in 2018. How can you modify the code to achieve that goal? tennis_2019_10 &lt;- semi_join(atp_2019, win10, by = c(&quot;winner_name&quot; = &quot;winner_name&quot;)) tennis_2019_10$winner_name 11.4 A Note on SQL All of the dplyr functions we’ve used (both the ones from Week 2 and the Joins from this week) have corresponding components in SQL. The dbplyr package is useful if you’re interested in learning about SQL more. Given a dplyr pipe or base R function, dbplyr can show you what the equivalent command would be in SQL. In general, SQL code is much harder to read, as SQL isn’t designed specifically for data analysis like dplyr is. Some examples of dbplyr’s translate_sql() function and its translation of R code to SQL is: library(dbplyr) translate_sql(semi_join(atp_2019, win10, c(&quot;winner_name&quot; = &quot;winner_name&quot;))) #&gt; &lt;SQL&gt; semi_join(`atp_2019`, `win10`, &#39;winner_name&#39; AS `winner_name`) translate_sql(atp_2018 %&gt;% group_by(winner_name) %&gt;% summarise(n()) %&gt;% filter(nwin &gt;= 10)) #&gt; &lt;SQL&gt; filter(summarise(group_by(`atp_2018`, `winner_name`), COUNT(*) OVER ()), `nwin` &gt;= 10.0) and a resource for learning more is https://dbplyr.tidyverse.org/articles/sql-translation.html. 11.5 Chapter Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 11.6. * Read in the gun violence data set, and suppose that you want to add a row to this data set that has the statistics on gun ownership and mortality rate in the District of Columbia (Washington D.C., which is in the NE region, has 16.7 deaths per 100,000 people, and a gun ownership rate of 8.7%). To do so, create a tibble() that has a single row representing D.C. and then combine your new tibble with the overall gun violence data set. Name this new data set all_df. library(tidyverse) mortality_df &lt;- read_csv(&quot;data/gun_violence_us.csv&quot;) Explain why each attempt at combining the D.C. data with the overall data doesn’t work or is incorrect. test1 &lt;- tibble(state = &quot;Washington D.C.&quot;, mortality_rate = 16.7, ownership_rate = 8.7, region = &quot;NE&quot;) bind_rows(mortality_df, test1) test2 &lt;- tibble(state = &quot;Washington D.C.&quot;, mortality_rate = 16.7, ownership_rate = 0.087, region = NE) #&gt; Error in eval_tidy(xs[[j]], mask): object &#39;NE&#39; not found bind_rows(mortality_df, test2) #&gt; Error in list2(...): object &#39;test2&#39; not found test3 &lt;- tibble(state = &quot;Washington D.C.&quot;, mortality_rate = &quot;16.7&quot;, ownership_rate = &quot;0.087&quot;, region = &quot;NE&quot;) bind_rows(mortality_df, test3) #&gt; Error: Can&#39;t combine `mortality_rate` &lt;double&gt; and `mortality_rate` &lt;character&gt;. Examine the following data sets that are in R’s base library on demographic statistics about the U.S. states and state abbreviations: df1 &lt;- as_tibble(state.x77) df2 &lt;- as_tibble(state.abb) df1 #&gt; # A tibble: 50 x 8 #&gt; Population Income Illiteracy `Life Exp` Murder `HS Grad` #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 3615 3624 2.1 69.0 15.1 41.3 #&gt; 2 365 6315 1.5 69.3 11.3 66.7 #&gt; 3 2212 4530 1.8 70.6 7.8 58.1 #&gt; 4 2110 3378 1.9 70.7 10.1 39.9 #&gt; 5 21198 5114 1.1 71.7 10.3 62.6 #&gt; 6 2541 4884 0.7 72.1 6.8 63.9 #&gt; 7 3100 5348 1.1 72.5 3.1 56 #&gt; 8 579 4809 0.9 70.1 6.2 54.6 #&gt; 9 8277 4815 1.3 70.7 10.7 52.6 #&gt; 10 4931 4091 2 68.5 13.9 40.6 #&gt; # … with 40 more rows, and 2 more variables: Frost &lt;dbl&gt;, #&gt; # Area &lt;dbl&gt; df2 #&gt; # A tibble: 50 x 1 #&gt; value #&gt; &lt;chr&gt; #&gt; 1 AL #&gt; 2 AK #&gt; 3 AZ #&gt; 4 AR #&gt; 5 CA #&gt; 6 CO #&gt; 7 CT #&gt; 8 DE #&gt; 9 FL #&gt; 10 GA #&gt; # … with 40 more rows Combine the two data sets with bind_cols(). What are you assuming about the data sets in order to use this function? * Combine the columns of the states data set you made in Exercise 3 with the mortality data set without Washington D.C. * Use a join function to combine the mortality data set (all_df) with D.C. with the states data set from Exercise 3 (states_df). For this exercise, keep the row with Washington D.C., having it take on NA values for any variable not observed in the states data. * Repeat Exercise 5, but now drop Washington D.C. in your merging process. Practice doing this with a join function (as opposed to slice()-ing it out explicitly). * Use semi_join() to create a subset of states_df that are in the NE region. Hint: You will need to filter all_df first to contain only states in the NE region. * Do the same thing as Exercise 7, but this time, use anti_join(). Hint: You’ll need to filter all_df in a different way to achieve this. Examine the following data sets (the first is df1 and the second is df2) and then, without running any code, answer the following questions. id xvar A 1 B 2 C 3 E 1 F 2 id yvar A 2 C 1 D 2 E 1 G 1 H 4 How many rows would be in the data set from left_join(df1, df2, by = c(\"id\" = \"id\"))? How many rows would be in the data set from left_join(df2, df1, by = c(\"id\" = \"id\"))? How many rows would be in the data set from full_join(df1, df2, by = c(\"id\" = \"id\"))? How many rows would be in the data set from inner_join(df1, df2, by = c(\"id\" = \"id\"))? How many rows would be in the data set from semi_join(df1, df2, by = c(\"id\" = \"id\"))? How many rows would be in the data set from anti_join(df1, df2, by = c(\"id\" = \"id\"))? 11.6 Exercise Solutions 11.6.1 bind_rows() and bind_cols() S * Run the following and explain why R does not simply stack the rows. Then, fix the issue with the rename() function. df_test1a &lt;- tibble(xvar = c(1, 2), yvar = c(5, 1)) df_test1b &lt;- tibble(x = c(1, 2), y = c(5, 1)) bind_rows(df_test1a, df_test1b) #&gt; # A tibble: 4 x 4 #&gt; xvar yvar x y #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 5 NA NA #&gt; 2 2 1 NA NA #&gt; 3 NA NA 1 5 #&gt; 4 NA NA 2 1 ## This doesn&#39;t stack rows because the columns are named differently ## in the two data sets. If xvar is the same variable as x and ## yvar is the same variable as y, then we can rename the columns in ## one of the data sets: df_test1a &lt;- df_test1a %&gt;% rename(x = &quot;xvar&quot;, y = &quot;yvar&quot;) bind_rows(df_test1a, df_test1b) #&gt; # A tibble: 4 x 2 #&gt; x y #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 5 #&gt; 2 2 1 #&gt; 3 1 5 #&gt; 4 2 1 11.6.2 Mutating Joins S 11.6.3 Filtering Joins S * Take the semi_join() tennis example, but now suppose that we want to keep only the matches in 2019 where both the winning player and the losing player had 10 or more match wins in 2018. How can you modify the code to achieve that goal? tennis_2019_10 &lt;- semi_join(atp_2019, win10, by = c(&quot;winner_name&quot; = &quot;winner_name&quot;)) tennis_2019_10$winner_name ## There are many ways to do this, and this solution gives just one way ## A first step would be to create a data set that keeps only ## the katches with losing players with 10 or more wins in 2018 tennis_2019_10_lose &lt;- semi_join(atp_2019, win10, by = c(&quot;loser_name&quot; = &quot;winner_name&quot;)) ## Using `bind_rows()` would result in many duplicate matches. A way ## to avoid duplicates with joining functions is tennis_temp &lt;- anti_join(tennis_2019_10_lose, tennis_2019_10) #&gt; Joining, by = c(&quot;tourney_id&quot;, &quot;tourney_name&quot;, &quot;surface&quot;, &quot;draw_size&quot;, &quot;tourney_level&quot;, &quot;tourney_date&quot;, &quot;match_num&quot;, &quot;winner_id&quot;, &quot;winner_seed&quot;, &quot;winner_entry&quot;, &quot;winner_name&quot;, &quot;winner_hand&quot;, &quot;winner_ht&quot;, &quot;winner_ioc&quot;, &quot;winner_age&quot;, &quot;loser_id&quot;, &quot;loser_seed&quot;, &quot;loser_entry&quot;, &quot;loser_name&quot;, &quot;loser_hand&quot;, &quot;loser_ht&quot;, &quot;loser_ioc&quot;, &quot;loser_age&quot;, &quot;score&quot;, &quot;best_of&quot;, &quot;round&quot;, &quot;minutes&quot;, &quot;w_ace&quot;, &quot;w_df&quot;, &quot;w_svpt&quot;, &quot;w_1stIn&quot;, &quot;w_1stWon&quot;, &quot;w_2ndWon&quot;, &quot;w_SvGms&quot;, &quot;w_bpSaved&quot;, &quot;w_bpFaced&quot;, &quot;l_ace&quot;, &quot;l_df&quot;, &quot;l_svpt&quot;, &quot;l_1stIn&quot;, &quot;l_1stWon&quot;, &quot;l_2ndWon&quot;, &quot;l_SvGms&quot;, &quot;l_bpSaved&quot;, &quot;l_bpFaced&quot;, &quot;winner_rank&quot;, &quot;winner_rank_points&quot;, &quot;loser_rank&quot;, &quot;loser_rank_points&quot;) tennis_temp ## there are 383 matches in the lose data set that aren&#39;t in the ## win data set. Now, we can bind_rows(): final_df &lt;- bind_rows(tennis_2019_10, tennis_temp) 11.6.4 Chapter Exercises S * Read in the gun violence data set, and suppose that you want to add a row to this data set that has the statistics on gun ownership and mortality rate in the District of Columbia (Washington D.C., which is in the NE region, has 16.7 deaths per 100,000 people, and a gun ownership rate of 8.7%). To do so, create a tibble() that has a single row representing D.C. and then combine your new tibble with the overall gun violence data set. Name this new data set all_df. library(tidyverse) mortality_df &lt;- read_csv(&quot;data/gun_violence_us.csv&quot;) #&gt; #&gt; ── Column specification ──────────────────────────────────── #&gt; cols( #&gt; state = col_character(), #&gt; mortality_rate = col_double(), #&gt; ownership_rate = col_double(), #&gt; region = col_character() #&gt; ) dc_df &lt;- tibble(state = &quot;Washington D.C.&quot;, mortality_rate = 16.7, ownership_rate = 0.087, region = &quot;NE&quot;) all_df &lt;- bind_rows(mortality_df, dc_df) * Combine the columns of the states data set you made in Section A Exercise 3 with the mortality data set without Washington D.C. bind_cols(mortality_df, states_df) * Use a join function to combine the mortality data set with D.C. with the states data set from Exercise 3. For this exercise, keep the row with Washington D.C., having it take on NA values for any variable not observed in the states data. left_join(all_df, states_df, by = c(&quot;state&quot; = &quot;value&quot;)) #&gt; # A tibble: 51 x 12 #&gt; state mortality_rate ownership_rate region Population #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 AL 16.7 0.489 South 3615 #&gt; 2 AK 18.8 0.617 West 365 #&gt; 3 AZ 13.4 0.323 West 2212 #&gt; 4 AR 16.4 0.579 South 2110 #&gt; 5 CA 7.4 0.201 West 21198 #&gt; 6 CO 12.1 0.343 West 2541 #&gt; 7 CT 4.9 0.166 NE 3100 #&gt; 8 DE 11.1 0.052 NE 579 #&gt; 9 FL 11.5 0.325 South 8277 #&gt; 10 GA 13.7 0.316 South 4931 #&gt; # … with 41 more rows, and 7 more variables: Income &lt;dbl&gt;, #&gt; # Illiteracy &lt;dbl&gt;, Life Exp &lt;dbl&gt;, Murder &lt;dbl&gt;, #&gt; # HS Grad &lt;dbl&gt;, Frost &lt;dbl&gt;, Area &lt;dbl&gt; ## or full_join(all_df, states_df, by = c(&quot;state&quot; = &quot;value&quot;)) #&gt; # A tibble: 51 x 12 #&gt; state mortality_rate ownership_rate region Population #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 AL 16.7 0.489 South 3615 #&gt; 2 AK 18.8 0.617 West 365 #&gt; 3 AZ 13.4 0.323 West 2212 #&gt; 4 AR 16.4 0.579 South 2110 #&gt; 5 CA 7.4 0.201 West 21198 #&gt; 6 CO 12.1 0.343 West 2541 #&gt; 7 CT 4.9 0.166 NE 3100 #&gt; 8 DE 11.1 0.052 NE 579 #&gt; 9 FL 11.5 0.325 South 8277 #&gt; 10 GA 13.7 0.316 South 4931 #&gt; # … with 41 more rows, and 7 more variables: Income &lt;dbl&gt;, #&gt; # Illiteracy &lt;dbl&gt;, Life Exp &lt;dbl&gt;, Murder &lt;dbl&gt;, #&gt; # HS Grad &lt;dbl&gt;, Frost &lt;dbl&gt;, Area &lt;dbl&gt; * Repeat Exercise 5, but now drop Washington D.C. in your merging process. Practice doing this with a join function (as opposed to slice() ing it out explictly). inner_join(all_df, states_df, by = c(&quot;state&quot; = &quot;value&quot;)) #&gt; # A tibble: 50 x 12 #&gt; state mortality_rate ownership_rate region Population #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 AL 16.7 0.489 South 3615 #&gt; 2 AK 18.8 0.617 West 365 #&gt; 3 AZ 13.4 0.323 West 2212 #&gt; 4 AR 16.4 0.579 South 2110 #&gt; 5 CA 7.4 0.201 West 21198 #&gt; 6 CO 12.1 0.343 West 2541 #&gt; 7 CT 4.9 0.166 NE 3100 #&gt; 8 DE 11.1 0.052 NE 579 #&gt; 9 FL 11.5 0.325 South 8277 #&gt; 10 GA 13.7 0.316 South 4931 #&gt; # … with 40 more rows, and 7 more variables: Income &lt;dbl&gt;, #&gt; # Illiteracy &lt;dbl&gt;, Life Exp &lt;dbl&gt;, Murder &lt;dbl&gt;, #&gt; # HS Grad &lt;dbl&gt;, Frost &lt;dbl&gt;, Area &lt;dbl&gt; ## or left_join(states_df, all_df, by = c(&quot;value&quot; = &quot;state&quot;)) #&gt; # A tibble: 50 x 12 #&gt; Population Income Illiteracy `Life Exp` Murder `HS Grad` #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 3615 3624 2.1 69.0 15.1 41.3 #&gt; 2 365 6315 1.5 69.3 11.3 66.7 #&gt; 3 2212 4530 1.8 70.6 7.8 58.1 #&gt; 4 2110 3378 1.9 70.7 10.1 39.9 #&gt; 5 21198 5114 1.1 71.7 10.3 62.6 #&gt; 6 2541 4884 0.7 72.1 6.8 63.9 #&gt; 7 3100 5348 1.1 72.5 3.1 56 #&gt; 8 579 4809 0.9 70.1 6.2 54.6 #&gt; 9 8277 4815 1.3 70.7 10.7 52.6 #&gt; 10 4931 4091 2 68.5 13.9 40.6 #&gt; # … with 40 more rows, and 6 more variables: Frost &lt;dbl&gt;, #&gt; # Area &lt;dbl&gt;, value &lt;chr&gt;, mortality_rate &lt;dbl&gt;, #&gt; # ownership_rate &lt;dbl&gt;, region &lt;chr&gt; * Use semi_join() to create a subset of states_df that are in the NE region. Hint: You will need to filter all_df first to contain only states in the NE region. ne_df &lt;- all_df %&gt;% filter(region == &quot;NE&quot;) semi_join(states_df, ne_df, by = c(&quot;value&quot; = &quot;state&quot;)) #&gt; # A tibble: 10 x 9 #&gt; Population Income Illiteracy `Life Exp` Murder `HS Grad` #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 3100 5348 1.1 72.5 3.1 56 #&gt; 2 579 4809 0.9 70.1 6.2 54.6 #&gt; 3 1058 3694 0.7 70.4 2.7 54.7 #&gt; 4 4122 5299 0.9 70.2 8.5 52.3 #&gt; 5 5814 4755 1.1 71.8 3.3 58.5 #&gt; 6 812 4281 0.7 71.2 3.3 57.6 #&gt; 7 7333 5237 1.1 70.9 5.2 52.5 #&gt; 8 18076 4903 1.4 70.6 10.9 52.7 #&gt; 9 931 4558 1.3 71.9 2.4 46.4 #&gt; 10 472 3907 0.6 71.6 5.5 57.1 #&gt; # … with 3 more variables: Frost &lt;dbl&gt;, Area &lt;dbl&gt;, #&gt; # value &lt;chr&gt; * Do the same thing as Exercise 7, but this time, use anti_join(). Hint: You’ll need to filter all_df in a different way to achieve this. notne_df &lt;- all_df %&gt;% filter(region != &quot;NE&quot;) anti_join(states_df, notne_df, by = c(&quot;value&quot; = &quot;state&quot;)) #&gt; # A tibble: 10 x 9 #&gt; Population Income Illiteracy `Life Exp` Murder `HS Grad` #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 3100 5348 1.1 72.5 3.1 56 #&gt; 2 579 4809 0.9 70.1 6.2 54.6 #&gt; 3 1058 3694 0.7 70.4 2.7 54.7 #&gt; 4 4122 5299 0.9 70.2 8.5 52.3 #&gt; 5 5814 4755 1.1 71.8 3.3 58.5 #&gt; 6 812 4281 0.7 71.2 3.3 57.6 #&gt; 7 7333 5237 1.1 70.9 5.2 52.5 #&gt; 8 18076 4903 1.4 70.6 10.9 52.7 #&gt; 9 931 4558 1.3 71.9 2.4 46.4 #&gt; 10 472 3907 0.6 71.6 5.5 57.1 #&gt; # … with 3 more variables: Frost &lt;dbl&gt;, Area &lt;dbl&gt;, #&gt; # value &lt;chr&gt; 11.7 Non-Exercise R Code library(tidyverse) atp_2019 &lt;- read_csv(&quot;data/atp_matches_2019.csv&quot;) atp_2018 &lt;- read_csv(&quot;data/atp_matches_2018.csv&quot;) head(atp_2019) head(atp_2018) spec(atp_2018) atp_2018 &lt;- read_csv(&quot;data/atp_matches_2018.csv&quot;, col_types = cols(winner_seed = col_character(), loser_seed = col_character())) atp_df &lt;- bind_rows(atp_2018, atp_2019) atp_df df_test2a &lt;- tibble(xvar = c(1, 2)) df_test2b &lt;- tibble(xvar = c(1, 2), y = c(5, 1)) bind_rows(df_test2a, df_test2b) df_test1a &lt;- tibble(xvar = c(1, 2), yvar = c(5, 1)) df_test1b &lt;- tibble(x = c(1, 2), y = c(5, 1)) bind_cols(df_test1a, df_test1b) library(tidyverse) df1 &lt;- tibble(name = c(&quot;Emily&quot;, &quot;Miguel&quot;, &quot;Tonya&quot;), fav_sport = c(&quot;Swimming&quot;, &quot;Football&quot;, &quot;Tennis&quot;)) df2 &lt;- tibble(name = c(&quot;Tonya&quot;, &quot;Miguel&quot;, &quot;Emily&quot;), fav_colour = c(&quot;Robin&#39;s Egg Blue&quot;, &quot;Tickle Me Pink&quot;, &quot;Goldenrod&quot;)) ##install.packages(&quot;babynames&quot;) library(babynames) life_df &lt;- babynames::lifetables birth_df &lt;- babynames::births babynames_df &lt;- babynames::babynames head(babynames) head(births) head(lifetables) combined_left &lt;- left_join(babynames_df, birth_df, by = c(&quot;year&quot; = &quot;year&quot;)) head(combined_left) tail(combined_left) ## these will always do the same exact thing right_join(babynames_df, birth_df, by = c(&quot;year&quot; = &quot;year&quot;)) left_join(birth_df, babynames_df, by = c(&quot;year&quot; = &quot;year&quot;)) full_join(babynames_df, birth_df, by = c(&quot;year&quot; = &quot;year&quot;)) inner_join(babynames_df, birth_df, by = c(&quot;year&quot; = &quot;year&quot;)) slumajors_df &lt;- read_csv(&quot;data/SLU_Majors_15_19.csv&quot;) collegemajors_df &lt;- read_csv(&quot;data/college-majors.csv&quot;) head(slumajors_df) head(collegemajors_df) left_join(slumajors_df, collegemajors_df, by = c(&quot;Major&quot; = &quot;Major&quot;)) collegemajors_df &lt;- collegemajors_df %&gt;% mutate(Major = str_to_title(Major)) left_join(slumajors_df, collegemajors_df) atp_2019 &lt;- read_csv(&quot;data/atp_matches_2019.csv&quot;) atp_2018 &lt;- read_csv(&quot;data/atp_matches_2018.csv&quot;) atp_2019 atp_2018 win10 &lt;- atp_2018 %&gt;% group_by(winner_name) %&gt;% summarise(nwin = n()) %&gt;% filter(nwin &gt;= 10) win10 tennis_2019_10 &lt;- semi_join(atp_2019, win10, by = c(&quot;winner_name&quot; = &quot;winner_name&quot;)) tennis_2019_10$winner_name new_winners &lt;- anti_join(atp_2019, atp_2018, by = c(&quot;winner_name&quot; = &quot;winner_name&quot;)) new_winners$winner_name new_winners %&gt;% group_by(winner_name) %&gt;% summarise(nwin = n()) %&gt;% arrange(desc(nwin)) library(dbplyr) translate_sql(semi_join(atp_2019, win10, c(&quot;winner_name&quot; = &quot;winner_name&quot;))) translate_sql(atp_2018 %&gt;% group_by(winner_name) %&gt;% summarise(n()) %&gt;% filter(nwin &gt;= 10)) "],["dates-with-lubridate.html", " 12 Dates with lubridate 12.1 Converting Variables to &lt;date&gt; 12.2 Functions for &lt;date&gt; Variables 12.3 Chapter Exercises 12.4 Exercise Solutions 12.5 Non-Exercise R Code", " 12 Dates with lubridate Goals: use lubridate functions to convert a character variable to a &lt;date&gt; variable. use lubridate functions to extract useful information from a &lt;date&gt; variable, including the year, month, day of the week, and day of the year. 12.1 Converting Variables to &lt;date&gt; The lubridate package is built to easily work with Date objects and DateTime objects. R does not actually have a class that stores Time objects (unless you install a separate package). Dates tend to be much more common than Times, so, we will primarily focus on Dates, but most functions we will see have easy extensions to Times. To begin, install the lubridate package, and load the package with library(). The today() function prints today’s date while now() prints today’s date and time. These can sometimes be useful in other contexts, but we will just run the code to see how R stores dates and date-times. library(tidyverse) library(lubridate) today() #&gt; [1] &quot;2021-08-09&quot; now() #&gt; [1] &quot;2021-08-09 10:32:44 EDT&quot; This first section will deal with how to convert a variable in R to be a Date. We will use a data set that has the holidays of Animal Crossing from January to April. The columns in this data set are: Holiday, the name of the holiday and various other columns with different date formats Read in the data set with holiday_df &lt;- read_csv(&quot;data/animal_crossing_holidays.csv&quot;) holiday_df #&gt; # A tibble: 6 x 10 #&gt; Holiday Date1 Date2 Date3 Date4 Date5 Month Year Day #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 New Yea… 1-Jan… Jan-1… 1/1/… 1/1/… 2020… 1 2020 1 #&gt; 2 Groundh… 2-Feb… Feb-2… 2/2/… 2/2/… 2020… 2 2020 2 #&gt; 3 Valenti… 14-Fe… Feb-1… 2/14… 2020… 2020… 2 2020 14 #&gt; 4 Shamroc… 17-Ma… Mar-1… 3/17… 2020… 2020… 3 2020 17 #&gt; 5 Bunny D… 12-Ap… Apr-1… 4/12… 12/4… 2020… 4 2020 12 #&gt; 6 Earth D… 22-Ap… Apr-2… 4/22… 2020… 2020… 4 2020 22 #&gt; # … with 1 more variable: Month2 &lt;chr&gt; Which columns were specified as Dates? In this example, none of the columns have the &lt;date&gt; specification: all of the date columns are read in as character variables. 12.1.1 From &lt;chr&gt; to &lt;date&gt; We will use the dmy() series of functions in lubridate to convert character variables to dates. We will typically pair this new function with a mutate() statement: much like the forcats functions, we are almost always creating a new variable. There are a series of dmy()-type variables, each corresponding to a different Day-Month-Year order. dmy() is used to parse a date from a character vector that has the day first, month second, and year last. ymd() is used to parse a date that has year first, month second, and date last ydm() is used to parse a date that has year first, day second, and month last,…. and dym(), mdy(), and myd() work similarly. lubridate is usually “smart” and picks up dates in all kinds of different formats (e.g. it can pick up specifying October as the month and Oct as the month and 10 as the month). Let’s try it out on Date1 and Date2: holiday_df %&gt;% mutate(Date_test = dmy(Date1)) %&gt;% select(Date_test, everything()) #&gt; # A tibble: 6 x 11 #&gt; Date_test Holiday Date1 Date2 Date3 Date4 Date5 Month #&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 2020-01-01 New Year… 1-Jan… Jan-1… 1/1/… 1/1/… 2020… 1 #&gt; 2 2020-02-02 Groundho… 2-Feb… Feb-2… 2/2/… 2/2/… 2020… 2 #&gt; 3 2020-02-14 Valentin… 14-Fe… Feb-1… 2/14… 2020… 2020… 2 #&gt; 4 2020-03-17 Shamrock… 17-Ma… Mar-1… 3/17… 2020… 2020… 3 #&gt; 5 2020-04-12 Bunny Day 12-Ap… Apr-1… 4/12… 12/4… 2020… 4 #&gt; 6 2020-04-22 Earth Day 22-Ap… Apr-2… 4/22… 2020… 2020… 4 #&gt; # … with 3 more variables: Year &lt;dbl&gt;, Day &lt;dbl&gt;, #&gt; # Month2 &lt;chr&gt; holiday_df %&gt;% mutate(Date_test = mdy(Date2)) %&gt;% select(Date_test, everything()) #&gt; # A tibble: 6 x 11 #&gt; Date_test Holiday Date1 Date2 Date3 Date4 Date5 Month #&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 2020-01-01 New Year… 1-Jan… Jan-1… 1/1/… 1/1/… 2020… 1 #&gt; 2 2020-02-02 Groundho… 2-Feb… Feb-2… 2/2/… 2/2/… 2020… 2 #&gt; 3 2020-02-14 Valentin… 14-Fe… Feb-1… 2/14… 2020… 2020… 2 #&gt; 4 2020-03-17 Shamrock… 17-Ma… Mar-1… 3/17… 2020… 2020… 3 #&gt; 5 2020-04-12 Bunny Day 12-Ap… Apr-1… 4/12… 12/4… 2020… 4 #&gt; 6 2020-04-22 Earth Day 22-Ap… Apr-2… 4/22… 2020… 2020… 4 #&gt; # … with 3 more variables: Year &lt;dbl&gt;, Day &lt;dbl&gt;, #&gt; # Month2 &lt;chr&gt; A Reminder: Why do &lt;date&gt; objects even matter? Compare the following two plots: one made where the date is in &lt;chr&gt; form and the other where date is in its appropriate &lt;date&gt; form. ggplot(data = holiday_df, aes(x = Date1, y = Holiday)) + geom_point() holiday_df &lt;- holiday_df %&gt;% mutate(Date_test_plot = dmy(Date1)) %&gt;% select(Date_test_plot, everything()) ggplot(data = holiday_df, aes(x = Date_test_plot, y = Holiday)) + geom_point() In which plot does the ordering on the x-axis make more sense? 12.1.2 Making a &lt;date&gt; variable from Date Components Another way to create a Date object is to assemble it with make_date() from a month, day, and year components, each stored in a separate column: holiday_df %&gt;% mutate(Date_test2 = make_date(year = Year, month = Month, day = Day)) %&gt;% select(Date_test2, everything()) #&gt; # A tibble: 6 x 12 #&gt; Date_test2 Date_test_plot Holiday Date1 Date2 Date3 Date4 #&gt; &lt;date&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 2020-01-01 2020-01-01 New Yea… 1-Ja… Jan-… 1/1/… 1/1/… #&gt; 2 2020-02-02 2020-02-02 Groundh… 2-Fe… Feb-… 2/2/… 2/2/… #&gt; 3 2020-02-14 2020-02-14 Valenti… 14-F… Feb-… 2/14… 2020… #&gt; 4 2020-03-17 2020-03-17 Shamroc… 17-M… Mar-… 3/17… 2020… #&gt; 5 2020-04-12 2020-04-12 Bunny D… 12-A… Apr-… 4/12… 12/4… #&gt; 6 2020-04-22 2020-04-22 Earth D… 22-A… Apr-… 4/22… 2020… #&gt; # … with 5 more variables: Date5 &lt;chr&gt;, Month &lt;dbl&gt;, #&gt; # Year &lt;dbl&gt;, Day &lt;dbl&gt;, Month2 &lt;chr&gt; But, when Month is stored as a character (e.g. February) instead of a number (e.g. 2), problems arise with the make_date() function: holiday_df %&gt;% mutate(Date_test2 = make_date(year = Year, month = Month2, day = Day)) %&gt;% select(Date_test2, everything()) #&gt; Warning in make_date(year = Year, month = Month2, day = #&gt; Day): NAs introduced by coercion #&gt; # A tibble: 6 x 12 #&gt; Date_test2 Date_test_plot Holiday Date1 Date2 Date3 Date4 #&gt; &lt;date&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 NA 2020-01-01 New Yea… 1-Ja… Jan-… 1/1/… 1/1/… #&gt; 2 NA 2020-02-02 Groundh… 2-Fe… Feb-… 2/2/… 2/2/… #&gt; 3 NA 2020-02-14 Valenti… 14-F… Feb-… 2/14… 2020… #&gt; 4 NA 2020-03-17 Shamroc… 17-M… Mar-… 3/17… 2020… #&gt; 5 NA 2020-04-12 Bunny D… 12-A… Apr-… 4/12… 12/4… #&gt; 6 NA 2020-04-22 Earth D… 22-A… Apr-… 4/22… 2020… #&gt; # … with 5 more variables: Date5 &lt;chr&gt;, Month &lt;dbl&gt;, #&gt; # Year &lt;dbl&gt;, Day &lt;dbl&gt;, Month2 &lt;chr&gt; So the make_date() function requires a specific format for the year, month, and day columns. It may take a little pre-processing to put a particular data set in that format. 12.1.3 Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 12.4. * What’s the issue with trying to convert Date4 to a &lt;date&gt; form? You may want to investigate Date4 further to answer this question. holiday_df %&gt;% mutate(Date_test = ymd(Date4)) %&gt;% select(Date_test, everything()) #&gt; Warning: 3 failed to parse. #&gt; # A tibble: 6 x 12 #&gt; Date_test Date_test_plot Holiday Date1 Date2 Date3 Date4 #&gt; &lt;date&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 2001-01-20 2020-01-01 New Yea… 1-Ja… Jan-… 1/1/… 1/1/… #&gt; 2 2002-02-20 2020-02-02 Groundh… 2-Fe… Feb-… 2/2/… 2/2/… #&gt; 3 NA 2020-02-14 Valenti… 14-F… Feb-… 2/14… 2020… #&gt; 4 NA 2020-03-17 Shamroc… 17-M… Mar-… 3/17… 2020… #&gt; 5 2012-04-20 2020-04-12 Bunny D… 12-A… Apr-… 4/12… 12/4… #&gt; 6 NA 2020-04-22 Earth D… 22-A… Apr-… 4/22… 2020… #&gt; # … with 5 more variables: Date5 &lt;chr&gt;, Month &lt;dbl&gt;, #&gt; # Year &lt;dbl&gt;, Day &lt;dbl&gt;, Month2 &lt;chr&gt; * Practice converting Date3 and Date5 to &lt;date&gt; variables with lubridate functions. 12.2 Functions for &lt;date&gt; Variables Once an object is in the &lt;date&gt; format, there are some special functions in lubridate that can be used on that date variable. To investigate some of these functions, we will pull stock market data from Yahoo using the quantmod package. Install the package, and run the following code, which gets stock market price data on Apple, Nintendo, Chipotle, and the S &amp; P 500 Index from 2011 to now. Note that you have the ability to understand all of the code below, but we will skip over this code for now to focus more on the new information in this section (information about date functions). ## install.packages(&quot;quantmod&quot;) library(quantmod) start &lt;- ymd(&quot;2011-01-01&quot;) end &lt;- ymd(&quot;2021-5-19&quot;) getSymbols(c(&quot;AAPL&quot;, &quot;NTDOY&quot;, &quot;CMG&quot;, &quot;SPY&quot;), src = &quot;yahoo&quot;, from = start, to = end) #&gt; [1] &quot;AAPL&quot; &quot;NTDOY&quot; &quot;CMG&quot; &quot;SPY&quot; date_tib &lt;- as_tibble(index(AAPL)) %&gt;% rename(start_date = value) app_tib &lt;- as_tibble(AAPL) nint_tib &lt;- as_tibble(NTDOY) chip_tib &lt;- as_tibble(CMG) spy_tib &lt;- as_tibble(SPY) all_stocks &lt;- bind_cols(date_tib, app_tib, nint_tib, chip_tib, spy_tib) stocks_long &lt;- all_stocks %&gt;% select(start_date, AAPL.Adjusted, NTDOY.Adjusted, CMG.Adjusted, SPY.Adjusted) %&gt;% pivot_longer(2:5, names_to = &quot;Stock_Type&quot;, values_to = &quot;Price&quot;) %&gt;% mutate(Stock_Type = fct_recode(Stock_Type, Apple = &quot;AAPL.Adjusted&quot;, Nintendo = &quot;NTDOY.Adjusted&quot;, Chipotle = &quot;CMG.Adjusted&quot;, `S &amp; P 500` = &quot;SPY.Adjusted&quot; )) tail(stocks_long) #&gt; # A tibble: 6 x 3 #&gt; start_date Stock_Type Price #&gt; &lt;date&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 2021-05-17 Chipotle 1332. #&gt; 2 2021-05-17 S &amp; P 500 414. #&gt; 3 2021-05-18 Apple 125. #&gt; 4 2021-05-18 Nintendo 70.4 #&gt; 5 2021-05-18 Chipotle 1325. #&gt; 6 2021-05-18 S &amp; P 500 411. You’ll have a chance in the Exercises to choose your own stocks to investigate. For now, I’ve made a data set with three variables: start_date, the opening date for the stock market Stock_Type, a factor with 4 levels: Apple, Nintendo, Chipotle, and S &amp; P 500 Price, the price of the stock? First, let’s make a line plot that shows how the S &amp; P 500 has changed over time: stocks_sp &lt;- stocks_long %&gt;% filter(Stock_Type == &quot;S &amp; P 500&quot;) ggplot(data = stocks_sp, aes(x = start_date, y = Price)) + geom_line() But, there’s other information that we can get from the start_date variable. We might be interested in things like day of the week, monthly trends, or yearly trends. To extract variables like “weekday” and “month” from a &lt;date&gt; variable, there are a series of functions that are fairly straightforward to use. We will discuss the year() month(), mday(), yday(), and wday() functions. 12.2.1 year(), month(), and mday() The functions year(), month(), and mday() can grab the year, month, and day of the month, respectively, from a &lt;date&gt; variable. Like the forcats functions, these will almost always be paired with a mutate() statement because they will create a new variable: stocks_long %&gt;% mutate(year_stock = year(start_date)) stocks_long %&gt;% mutate(month_stock = month(start_date)) stocks_long %&gt;% mutate(day_stock = mday(start_date)) 12.2.2 yday() and wday() The yday() function grabs the day of the year from a &lt;date&gt; object. For example, test &lt;- mdy(&quot;November 4, 2020&quot;) yday(test) #&gt; [1] 309 returns 309, indicating that November 4th is the 309th day of the year 2020. Using this function in a mutate() statement creates a new variable that has yday for each observation: stocks_long %&gt;% mutate(day_in_year = yday(start_date)) #&gt; # A tibble: 10,444 x 4 #&gt; start_date Stock_Type Price day_in_year #&gt; &lt;date&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2011-01-03 Apple 10.1 3 #&gt; 2 2011-01-03 Nintendo 36.7 3 #&gt; 3 2011-01-03 Chipotle 224. 3 #&gt; 4 2011-01-03 S &amp; P 500 103. 3 #&gt; 5 2011-01-04 Apple 10.2 4 #&gt; 6 2011-01-04 Nintendo 35.5 4 #&gt; 7 2011-01-04 Chipotle 222. 4 #&gt; 8 2011-01-04 S &amp; P 500 103. 4 #&gt; 9 2011-01-05 Apple 10.2 5 #&gt; 10 2011-01-05 Nintendo 34.6 5 #&gt; # … with 10,434 more rows Finally, the function wday() grabs the day of the week from a &lt;date&gt;. By default, wday() puts the day of the week as a numeric, but I find this confusing, as I can’t ever remember whether a 1 means Sunday or a 1 means Monday. Adding, label = TRUE creates the weekday variable as Sunday, Monday, Tuesday, etc.: stocks_long %&gt;% mutate(day_of_week = wday(start_date)) stocks_long %&gt;% mutate(day_of_week = wday(start_date, label = TRUE, abbr = FALSE)) Possible uses for these functions are: you want to look at differences between years (with year()) you want to look at differences between months (with month()) you want to look at differences between days of the week (with wday()) you want to see whether there are yearly trends within years (with yday()) Note: Working with times is extremely similar to working with dates. Instead of ymd(), mdy(), etc., you tack on a few extra letters to specify the order that the hour, minute, and seconds appear in the variable: ymd_hms() converts a character vector that has the order year, month, day, hour, minute, second to a &lt;datetime&gt;. Additionally, the functions hour(), minute(), and second() grab the hour, minute, and second from a &lt;datetime&gt; variable. Note on Complications: Things can get complicated, especially if you start to consider things like time duration. The reason is that our time system is inherently confusing. Consider how the following might affect an analysis involving time duration: time zones leap years (not all years have the same number of days) differing number of days in a given month daylight saving time (not all days have the same number of hours) 12.2.3 Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 12.4. The month() function gives the numbers corresponding to each month by default. Type ?month and figure out which argument you would need to change to get the names (January, February, etc.) instead of the month numbers. What about the abbreviations (Jan, Feb, etc.) of each month instead of the month numbers? Try making the changes in the mutate() statement below. stocks_long %&gt;% mutate(month_stock = month(start_date)) 12.3 Chapter Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 12.4. The truncated argument to ymd(), dmy(), mdy(), etc. will allow R to parse dates that aren’t actually complete. For example, library(lubridate) ymd(&quot;2019&quot;, truncated = 2) #&gt; [1] &quot;2019-01-01&quot; parses 2019 to be January 1, 2019 when the month and day are missing. The 2 means that the last two parts of the date (in this case, month and day) are allowed to be missing. Similarly, dmy(&quot;19-10&quot;, truncated = 1) #&gt; [1] &quot;0000-10-19&quot; truncates the year (which is given as 0000). The truncate function is usually most useful in the context of the first example with a truncated month and/or day. Examine the ds_google.csv, which contains Month, the year and month from 2004 to now Data_Science, the relative popularity of data science (Google keeps how it calculates “popularity” as somewhat of a mystery but it is likely based off of the number of times people search for the term “Data Science”) library(tidyverse) library(lubridate) ds_df &lt;- read_csv(&quot;data/ds_google.csv&quot;) ds_df #&gt; # A tibble: 202 x 2 #&gt; Month Data_Science #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 2004-01 14 #&gt; 2 2004-02 8 #&gt; 3 2004-03 16 #&gt; 4 2004-04 11 #&gt; 5 2004-05 5 #&gt; 6 2004-06 8 #&gt; 7 2004-07 7 #&gt; 8 2004-08 9 #&gt; 9 2004-09 13 #&gt; 10 2004-10 11 #&gt; # … with 192 more rows * Use a lubridate function with the truncated option to convert the Month variable to be in the &lt;date&gt; format. * Make a plot of the popularity of Data Science through Time. Add a smoother to your plot. What patterns do you notice? The data was obtained from Google Trends: Google Trends. Google Trends is incredibly cool to explore, even without R. * On Google Trends, Enter in a search term, and change the Time dropdown menu to be 2004-present. Then, enter in a second search term that you want to compare. You can also change the country if you want to (or, you can keep the country as United States). My search terms will be “super smash” and “animal crossing,” but yours should be something that interests you! In the top-right window of the graph, you should click the down arrow to download the data set. Delete the first two rows of your data set (either in Excel or R), read in the data set, and change the date variable so that it’s in a Date format. * Make a plot of your Popularity variables through time. Hint: Does the data set need to be tidied at all first? * Using your data set that explored a variable or two from 2004 through now, make a table of the average popularity for each year. Hint: You’ll need a lubridate function to extract the year variable from the date object. * Clear your search and now enter a search term that you’d like to investigate for the past 90 days. Mine will be “Pittsburgh Steelers” but, again, yours should be something that interests you. Again, click the download button again and read in the data to R. Convert the date variable to be in &lt;date&gt; format. * Make a plot of your popularity variable through time, adding a smoother. Using your data set that explored a variable from the past 90 days, construct a table that compares the average popularity on each day of the week (Monday through Saturday). Examine the ds_df data set again, the data set on data science in Google Trends, and suppose that you have an observation for each day of every year (not just one observation per month). You want to look at whether data science is more popular on certain days of the week. Explain why the following strategy wouldn’t really work that well. create a weekday variable with wday() use summarise() and group_by() to find the average popularity for each day of the week Use the code in the tutorial section on the Stocks data to get a data frame on stock prices for a couple of different stocks that interest you. The start and end date that you use are completely up to you. Explore the stock data that you chose, constructing a line plot of the price through time, as well as any graphs or summaries that show interesting patterns across years, months, days, days of the week, etc. Use the lag() function to create a new variable that is the previous day’s stock price. Can you predict the current stock price based on the previous day’s stock price accurately? Why or why not? Use either graphical or numerical evidence. 12.4 Exercise Solutions 12.4.1 Converting Variables to &lt;date&gt; S * What’s the issue with trying to convert Date4 to a &lt;date&gt; form? holiday_df %&gt;% mutate(Date_test = ymd(Date4)) %&gt;% select(Date_test, everything()) #&gt; Warning: 3 failed to parse. #&gt; # A tibble: 6 x 12 #&gt; Date_test Date_test_plot Holiday Date1 Date2 Date3 Date4 #&gt; &lt;date&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 2001-01-20 2020-01-01 New Yea… 1-Ja… Jan-… 1/1/… 1/1/… #&gt; 2 2002-02-20 2020-02-02 Groundh… 2-Fe… Feb-… 2/2/… 2/2/… #&gt; 3 NA 2020-02-14 Valenti… 14-F… Feb-… 2/14… 2020… #&gt; 4 NA 2020-03-17 Shamroc… 17-M… Mar-… 3/17… 2020… #&gt; 5 2012-04-20 2020-04-12 Bunny D… 12-A… Apr-… 4/12… 12/4… #&gt; 6 NA 2020-04-22 Earth D… 22-A… Apr-… 4/22… 2020… #&gt; # … with 5 more variables: Date5 &lt;chr&gt;, Month &lt;dbl&gt;, #&gt; # Year &lt;dbl&gt;, Day &lt;dbl&gt;, Month2 &lt;chr&gt; ## Date4 has two __different__ formats, ## which creates problems for `lubridate` functions * Practice converting Date3 and Date5 to date objects with lubridate functions. holiday_df %&gt;% mutate(Date_test = mdy(Date3)) %&gt;% select(Date_test, everything()) #&gt; # A tibble: 6 x 12 #&gt; Date_test Date_test_plot Holiday Date1 Date2 Date3 Date4 #&gt; &lt;date&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 2020-01-01 2020-01-01 New Yea… 1-Ja… Jan-… 1/1/… 1/1/… #&gt; 2 2020-02-02 2020-02-02 Groundh… 2-Fe… Feb-… 2/2/… 2/2/… #&gt; 3 2020-02-14 2020-02-14 Valenti… 14-F… Feb-… 2/14… 2020… #&gt; 4 2020-03-17 2020-03-17 Shamroc… 17-M… Mar-… 3/17… 2020… #&gt; 5 2020-04-12 2020-04-12 Bunny D… 12-A… Apr-… 4/12… 12/4… #&gt; 6 2020-04-22 2020-04-22 Earth D… 22-A… Apr-… 4/22… 2020… #&gt; # … with 5 more variables: Date5 &lt;chr&gt;, Month &lt;dbl&gt;, #&gt; # Year &lt;dbl&gt;, Day &lt;dbl&gt;, Month2 &lt;chr&gt; holiday_df %&gt;% mutate(Date_test = ymd(Date5)) %&gt;% select(Date_test, everything()) #&gt; # A tibble: 6 x 12 #&gt; Date_test Date_test_plot Holiday Date1 Date2 Date3 Date4 #&gt; &lt;date&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 2020-01-01 2020-01-01 New Yea… 1-Ja… Jan-… 1/1/… 1/1/… #&gt; 2 2020-02-02 2020-02-02 Groundh… 2-Fe… Feb-… 2/2/… 2/2/… #&gt; 3 2020-02-14 2020-02-14 Valenti… 14-F… Feb-… 2/14… 2020… #&gt; 4 2020-03-17 2020-03-17 Shamroc… 17-M… Mar-… 3/17… 2020… #&gt; 5 2020-04-12 2020-04-12 Bunny D… 12-A… Apr-… 4/12… 12/4… #&gt; 6 2020-04-22 2020-04-22 Earth D… 22-A… Apr-… 4/22… 2020… #&gt; # … with 5 more variables: Date5 &lt;chr&gt;, Month &lt;dbl&gt;, #&gt; # Year &lt;dbl&gt;, Day &lt;dbl&gt;, Month2 &lt;chr&gt; 12.4.2 Functions for &lt;date&gt; Variables S 12.4.3 Chapter Exercises S * Use a lubridate function with the truncated option to convert the Month variable to be in the &lt;date&gt; format. ds_df &lt;- ds_df %&gt;% mutate(Month = ymd(Month, truncated = 1)) ds_df * Make a plot of the popularity of Data Science through Time. Add a smoother to your plot. What patterns do you notice? ggplot(data = ds_df, aes(x = Month, y = Data_Science)) + geom_line() + geom_smooth(se = FALSE) #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ## it&#39;s like super popular!!!! * On Google Trends, Enter in a search term, and change the Time dropdown menu to be 2004-present. Then, enter in a second search term that you want to compare. You can also change the country if you want to (or, you can keep the country as United States). My search terms will be “super smash” and “animal crossing,” but yours should be something that interests you! In the top-right window of the graph, you should click the down arrow to download the data set. Delete the first two rows of your data set (either in Excel or R), read in the data set, and change the date variable so that it’s in a Date format. videogame_df &lt;- read_csv(&quot;data/smash_animal_crossing.csv&quot;) #&gt; #&gt; ── Column specification ──────────────────────────────────── #&gt; cols( #&gt; Month = col_character(), #&gt; super_smash = col_double(), #&gt; animal_crossing = col_double() #&gt; ) videogame_df &lt;- videogame_df %&gt;% mutate(date = ymd(Month, truncated = 1)) * Make a plot of your Popularity variables through time. Hint: Does the data set need to be tidied at all first? videogame_long &lt;- videogame_df %&gt;% pivot_longer(cols = c(&quot;super_smash&quot;, &quot;animal_crossing&quot;), names_to = &quot;game&quot;, values_to = &quot;popularity&quot;) ggplot(data = videogame_long, aes(x = date, y = popularity, colour = game)) + geom_line() + scale_colour_viridis_d(begin = 0, end = 0.9) * Using your data set that explored a variable or two from 2004 through now, make a table of the average popularity for each year. Hint: You’ll need a lubridate function to extract the year variable from the date object. * Clear your search and now enter a search term that you’d like to investigate for the past 90 days. Mine will be “pittsburgh steelers” but, again, yours should be something that interests you. Again, click the download button again and read in the data to R. Convert the date variable to be in &lt;date&gt; format. steelers_df &lt;- read_csv(&quot;data/steelers.csv&quot;) #&gt; #&gt; ── Column specification ──────────────────────────────────── #&gt; cols( #&gt; Day = col_character(), #&gt; Steelers = col_double() #&gt; ) steelers_df &lt;- steelers_df %&gt;% mutate(day_var = mdy(Day)) * Make a plot of your popularity variable through time, adding a smoother. ggplot(data = steelers_df, aes(x = day_var, y = Steelers)) + geom_smooth() + geom_line() + labs(y = &quot;Popularity&quot;) #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; 12.5 Non-Exercise R Code library(tidyverse) library(lubridate) today() now() holiday_df &lt;- read_csv(&quot;data/animal_crossing_holidays.csv&quot;) holiday_df holiday_df %&gt;% mutate(Date_test = dmy(Date1)) %&gt;% select(Date_test, everything()) holiday_df %&gt;% mutate(Date_test = mdy(Date2)) %&gt;% select(Date_test, everything()) ggplot(data = holiday_df, aes(x = Date1, y = Holiday)) + geom_point() holiday_df &lt;- holiday_df %&gt;% mutate(Date_test_plot = dmy(Date1)) %&gt;% select(Date_test_plot, everything()) ggplot(data = holiday_df, aes(x = Date_test_plot, y = Holiday)) + geom_point() holiday_df %&gt;% mutate(Date_test2 = make_date(year = Year, month = Month, day = Day)) %&gt;% select(Date_test2, everything()) holiday_df %&gt;% mutate(Date_test2 = make_date(year = Year, month = Month2, day = Day)) %&gt;% select(Date_test2, everything()) ## install.packages(&quot;quantmod&quot;) library(quantmod) start &lt;- ymd(&quot;2011-01-01&quot;) end &lt;- ymd(&quot;2021-5-19&quot;) getSymbols(c(&quot;AAPL&quot;, &quot;NTDOY&quot;, &quot;CMG&quot;, &quot;SPY&quot;), src = &quot;yahoo&quot;, from = start, to = end) date_tib &lt;- as_tibble(index(AAPL)) %&gt;% rename(start_date = value) app_tib &lt;- as_tibble(AAPL) nint_tib &lt;- as_tibble(NTDOY) chip_tib &lt;- as_tibble(CMG) spy_tib &lt;- as_tibble(SPY) all_stocks &lt;- bind_cols(date_tib, app_tib, nint_tib, chip_tib, spy_tib) stocks_long &lt;- all_stocks %&gt;% select(start_date, AAPL.Adjusted, NTDOY.Adjusted, CMG.Adjusted, SPY.Adjusted) %&gt;% pivot_longer(2:5, names_to = &quot;Stock_Type&quot;, values_to = &quot;Price&quot;) %&gt;% mutate(Stock_Type = fct_recode(Stock_Type, Apple = &quot;AAPL.Adjusted&quot;, Nintendo = &quot;NTDOY.Adjusted&quot;, Chipotle = &quot;CMG.Adjusted&quot;, `S &amp; P 500` = &quot;SPY.Adjusted&quot; )) tail(stocks_long) stocks_sp &lt;- stocks_long %&gt;% filter(Stock_Type == &quot;S &amp; P 500&quot;) ggplot(data = stocks_sp, aes(x = start_date, y = Price)) + geom_line() stocks_long %&gt;% mutate(year_stock = year(start_date)) stocks_long %&gt;% mutate(month_stock = month(start_date)) stocks_long %&gt;% mutate(day_stock = mday(start_date)) test &lt;- mdy(&quot;November 4, 2020&quot;) yday(test) stocks_long %&gt;% mutate(day_in_year = yday(start_date)) stocks_long %&gt;% mutate(day_of_week = wday(start_date)) stocks_long %&gt;% mutate(day_of_week = wday(start_date, label = TRUE, abbr = FALSE)) "],["strings-with-stringr.html", " 13 Strings with stringr 13.1 Friday, Friday 13.2 Add Second Example Here 13.3 Chapter Exercises 13.4 Exercise Solutions 13.5 Non-Exercise R Code", " 13 Strings with stringr Incomplete Chapter (Section 13.2). Goals: use functions in the stringr package to analyze text data. introduce some of the issues with manipulating strings that don’t pertain to numeric or factor data. 13.1 Friday, Friday The string below has lyrics to Rebecca Black’s critically reviewed song Friday: https://www.youtube.com/watch?v=kfVsfOSbJY0. In particular, we will answer the following questions: how many times does Rebecca Black say the word “Friday” in her song? how many times does she say any word involving “day” in her song? what are the most popular words in the song? These questions may seem simple, but they can actually still be somewhat challenging to answer. Issues like punctuation, filler words, and parsing the long string will give us some challenges to work through. library(tidyverse) library(stringr) rblack &lt;- c(&quot;Oo-ooh-ooh, hoo yeah, yeah (Yeah, ah-ah-ah-ah-ah-ark) Yeah, yeah Yeah-ah-ah, yeah-ah-ah Yeah-ah-ah Yeah, yeah, yeah Seven a.m., waking up in the morning Gotta be fresh, gotta go downstairs Gotta have my bowl, gotta have cereal Seein&#39; everything, the time is goin&#39; Tickin&#39; on and on, everybody&#39;s rushin&#39; Gotta get down to the bus stop Gotta catch my bus, I see my friends (my friends) Kickin&#39; in the front seat Sittin&#39; in the back seat Gotta make my mind up Which seat can I take? It&#39;s Friday, Friday Gotta get down on Friday Everybody&#39;s lookin&#39; forward to the weekend, weekend Friday, Friday Gettin&#39; down on Friday Everybody&#39;s lookin&#39; forward to the weekend Partyin&#39;, partyin&#39; (yeah) Partyin&#39;, partyin&#39; (yeah) Fun, fun, fun, fun Lookin&#39; forward to the weekend Seven, forty five, we&#39;re drivin&#39; on the highway Cruisin&#39; so fast, I want time to fly Fun, fun, think about fun You know what it is I got this, you got this My friend is by my right, aye I got this, you got this Now you know it Kickin&#39; in the front seat Sittin&#39; in the back seat Gotta make my mind up Which seat can I take? It&#39;s Friday, Friday Gotta get down on Friday Everybody&#39;s lookin&#39; forward to the weekend, weekend Friday, Friday Gettin&#39; down on Friday Everybody&#39;s lookin&#39; forward to the weekend Partyin&#39;, partyin&#39; (yeah) Partyin&#39;, partyin&#39; (yeah) Fun, fun, fun, fun Lookin&#39; forward to the weekend Yesterday was Thursday, Thursday Today it is Friday, Friday (partyin&#39;) We-we-we so excited We so excited We gonna have a ball today Tomorrow is Saturday And Sunday comes afterwards I don&#39;t want this weekend to end It&#39;s Friday, Friday Gotta get down on Friday Everybody&#39;s lookin&#39; forward to the weekend, weekend (we gotta get down) Friday, Friday Gettin&#39; down on Friday Everybody&#39;s lookin&#39; forward to the weekend Partyin&#39;, partyin&#39; (yeah) Partyin&#39;, partyin&#39; (yeah) Fun, fun, fun, fun Lookin&#39; forward to the weekend It&#39;s Friday, Friday Gotta get down on Friday Everybody&#39;s lookin&#39; forward to the weekend, weekend Friday, Friday Gettin&#39; down on Friday Everybody&#39;s lookin&#39; forward to the weekend Partyin&#39;, partyin&#39; (yeah) Partyin&#39;, partyin&#39; (yeah) Fun, fun, fun, fun Lookin&#39; forward to the weekend&quot;) Our first goal is to parse out every word so that we have an object. Run the following to examine the rblack object. The \\n denotes a line break in the string. rblack Our first step will be to use the \\n as a separator and split the lyric strings by that character using the str_split() function. str_split() takes a string as its first argument and what we want to split the string by (\\n in this case) as its second argument. simplify = TRUE puts the results into a matrix instead of a list. rblack2 &lt;- str_split(rblack, &quot;\\n&quot;, simplify = TRUE) rblack2 That’s much better to look at! Look at the structure of rblack2 with: str(rblack2) We see that rblack2 is a matrix with 1 row and 76 columns. But, we now want each word to be its own separate string. We can do this by using str_split() again, but, this time getting rid of the spaces. rblack3 &lt;- str_split(rblack2, c(&quot; &quot;)) rblack3 Look carefully between rblack2 and rblack3: what’s the difference? In rblack2, there’s quotes surrounding each line while in rblack3, there’s quotes surrounding each word. We can use the unlist() function to convert the list into a regular vector: rblack4 &lt;- unlist(rblack3) rblack4 What would happen if we tried to count the number of times Rebecca Black says Friday from the words below? Let’s try and see! rblack_df &lt;- tibble(words = rblack4) rblack_df %&gt;% group_by(words) %&gt;% summarise(word_count = n()) %&gt;% arrange(desc(word_count)) #&gt; # A tibble: 126 x 2 #&gt; words word_count #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 the 20 #&gt; 2 Friday 17 #&gt; 3 to 15 #&gt; 4 weekend 13 #&gt; 5 forward 12 #&gt; 6 Gotta 10 #&gt; 7 on 10 #&gt; 8 down 9 #&gt; 9 Friday, 9 #&gt; 10 fun, 9 #&gt; # … with 116 more rows The issue with this approach is that there are some instances of Friday with a , and some without. Based on this issue, let’s go back to our vector of words rblack4 and see if we can fix it using str_remove(), which removes given patterns in a string. For example, rblack5 &lt;- str_remove(rblack4, pattern = c(&quot;,&quot;)) rblack5 removes all of the commas. Another issue is that, if a word starts a sentence, it’s capitalized so R thinks Partyin' is a completely different word than partyin'. The str_to_lower() function converts all characters to lowercase: rblack6 &lt;- str_to_lower(rblack5) rblack6 Finally, we might want to get rid of “non-interesting” words, commonly known as “stop words.” These are words like “the,” “on,” “a,” “I,” etc. that don’t really provide meaning about the song. The tidytext package has a data set that has a list of stop words prepackaged. Install the package, load the tidytext library, and examine the stop words with: ##install.packages(&quot;tidytext&quot;) library(tidytext) stop_words Next, let’s make the Friday words into a tibble with the tibble() function: rblack_df &lt;- tibble(words = rblack6) rblack_df #&gt; # A tibble: 360 x 1 #&gt; words #&gt; &lt;chr&gt; #&gt; 1 oo-ooh-ooh #&gt; 2 hoo #&gt; 3 yeah #&gt; 4 yeah #&gt; 5 (yeah #&gt; 6 ah-ah-ah-ah-ah-ark) #&gt; 7 yeah #&gt; 8 yeah #&gt; 9 yeah-ah-ah #&gt; 10 yeah-ah-ah #&gt; # … with 350 more rows And we can now use the anti_join() function to get rid of the stop words in the Rebecca Black data set. rblack_small &lt;- anti_join(rblack_df, stop_words, by = c(&quot;words&quot; = &quot;word&quot;)) rblack_small #&gt; # A tibble: 208 x 1 #&gt; words #&gt; &lt;chr&gt; #&gt; 1 oo-ooh-ooh #&gt; 2 hoo #&gt; 3 yeah #&gt; 4 yeah #&gt; 5 (yeah #&gt; 6 ah-ah-ah-ah-ah-ark) #&gt; 7 yeah #&gt; 8 yeah #&gt; 9 yeah-ah-ah #&gt; 10 yeah-ah-ah #&gt; # … with 198 more rows We see that 98 words were dropped from the data set, but that some words weren’t picked up (like yeah-ah-ah). We will leave these for now, but, we could remove them with filter(words != \"yeah-ah-ah\") Finally, we can get some counts of words in the song Friday that aren’t “stop words.” rblack_small %&gt;% group_by(words) %&gt;% summarise(word_count = n()) %&gt;% arrange(desc(word_count)) #&gt; # A tibble: 60 x 2 #&gt; words word_count #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 friday 26 #&gt; 2 fun 19 #&gt; 3 weekend 17 #&gt; 4 partyin&#39; 16 #&gt; 5 gotta 13 #&gt; 6 forward 12 #&gt; 7 lookin&#39; 12 #&gt; 8 everybody&#39;s 9 #&gt; 9 (yeah) 8 #&gt; 10 yeah 7 #&gt; # … with 50 more rows 13.1.1 Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 13.4. * Look at the remaining words. Do any of them look like stop words that were missed with the stop words from the tidytext package? Create a tibble with a couple of stop words not picked up by the tidytext package, and use a join function to drop these words from rblack_small. * Ignore the parentheses for now. Make a point plot that shows the 10 most common words in Rebecca Black’s Friday, where the words are ordered from the most common to the least common on your graph. We can also make a wordcloud (though word clouds have some issues). R has a wordcloud package called wordcloud. Run the following code to make a word cloud of the words in the song Friday. rblack_sum &lt;- rblack_small %&gt;% group_by(words) %&gt;% summarise(word_count = n()) %&gt;% arrange(desc(word_count)) ## install.packages(&quot;wordcloud&quot;) library(wordcloud) wordcloud(rblack_sum$words, rblack_sum$word_count, colors = brewer.pal(8, &quot;Dark2&quot;), scale = c(5, .2), random.order = FALSE, random.color = FALSE) Then, type ?wordcloud to explore some of the options. In particular, try to increase the number of words plotted in the cloud. Finally, what about those pesky parentheses? Certain characters in R are treated as special: $, (, ., and ) are all examples. Dealing with these are fairly annoying: we need to “escape” them with a backslash and then “escape” that backslash with another backslash. If that’s confusing, no worries: we won’t focus too much on these special characters in this class. To remove the front parenthesis, use rblack7 &lt;- str_remove(rblack6, pattern = c(&quot;\\\\(&quot;)) rblack7 and then to remove the back parenthesis, use rblack8 &lt;- str_remove(rblack7, pattern = c(&quot;\\\\)&quot;)) rblack8 These are “regular expressions:” if interested, you can read more in the R4DS textbook. We analyzed a short text data set, but, you can imagine extending this type of analysis to things like: song lyrics, if you have the lyrics to all of the songs from an artist https://rpubs.com/RosieB/taylorswiftlyricanalysis book analysis, if you have the text of an entire book or series of books tv analysis, if you have the scripts to all episodes of a tv show If you were doing one of these analyses, there are lots of cool functions in tidytext to help you out! 13.2 Add Second Example Here maybe the office package? maybe the friends package? 13.3 Chapter Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 13.4. * Read in the beyonce_lyrics.csv file, which has the lyrics to all of beyonce’s songs and construct either a word cloud or a point plot that shows the most common non-stop words in either all of Beyonce’s songs. Do the best you can here (you might have some minor errors, like the parentheses in Friday’s “yeah” that we don’t have the tools to fix). Hint: You can actually skip a couple of the steps from the Friday analysis, as, for example, there is no “\\n” character to remove in Beyonce’s lyrics. beyonce &lt;- read_csv(&quot;data/beyonce_lyrics.csv&quot;) beyonce_lyrics &lt;- beyonce$line 13.4 Exercise Solutions 13.4.1 Friday, Friday S * Look at the remaining words. Do any of them look like stop words that were missed with the stop words from the tidytext package? Create a tibble with a couple of stop words not picked up by the tidytext package, and use a join function to drop these words from rblack_small. extra_stops &lt;- tibble(other_stop_words = c(&quot;gonna&quot;, &quot;yeah&quot;)) rblack_small2 &lt;- anti_join(rblack_small, extra_stops, by = c(&quot;words&quot; = &quot;other_stop_words&quot;)) rblack_wordcounts &lt;- rblack_small2 %&gt;% group_by(words) %&gt;% summarise(word_count = n()) %&gt;% arrange(desc(word_count)) rblack10 &lt;- rblack_wordcounts %&gt;% slice(1:10) * Ignore the parentheses for now. Make a point plot that shows the 10 most common words in Rebecca Black’s Friday, where the words are ordered from the most common to the least common on your graph. rblack10 &lt;- rblack10 %&gt;% mutate(words_ord = fct_reorder(words, .x = word_count)) ggplot(data = rblack10, aes(x = words_ord, y = word_count)) + geom_col() + coord_flip() 13.4.2 Second Example S 13.4.3 Chapter Exercises S * Read in the beyonce_lyrics.csv file, which has the lyrics to all of beyonce’s songs and construct either a word cloud or a point plot that shows the most common non-stop words in either all of Beyonce’s songs. Do the best you can here (you might have some minor errors, like the parentheses in Friday’s “yeah” that we don’t have the tools to fix). Hint: You can actually skip a couple of the steps from the Friday analysis, as, for example, there is no “\\n” character to remove in Beyonce’s lyrics. beyonce &lt;- read_csv(&quot;data/beyonce_lyrics.csv&quot;) beyonce_lyrics &lt;- beyonce$line beyonce_lines &lt;- str_split(beyonce$line, c(&quot; &quot;)) words_only &lt;- unlist(beyonce_lines) beyonce_lines2 &lt;- str_remove(words_only, pattern = c(&quot;,&quot;)) beyonce_lines2 beyonce_lines3 &lt;- str_remove(beyonce_lines2, pattern = c(&quot;;&quot;)) beyonce_lines3 beyonce_lines4 &lt;- str_to_lower(beyonce_lines3) beyonce_lines4 beyonce_df &lt;- tibble(b_words = beyonce_lines4) beyonce_small &lt;- anti_join(beyonce_df, stop_words, by = c(&quot;b_words&quot; = &quot;word&quot;)) beyonce_small beyonce_sum &lt;- beyonce_small %&gt;% group_by(b_words) %&gt;% summarise(word_count = n()) %&gt;% arrange(desc(word_count)) %&gt;% filter(word_count &gt; 100) library(wordcloud) wordcloud(beyonce_sum$b_words, beyonce_sum$word_count, colors = brewer.pal(8, &quot;Dark2&quot;), scale = c(5, .2), random.order = FALSE, random.color = FALSE) beyonce_bigwords &lt;- beyonce_sum %&gt;% filter(word_count &gt; 250) %&gt;% mutate(b_words = fct_reorder(b_words, .x = word_count)) ggplot(data = beyonce_bigwords, aes(x = word_count, y = b_words)) + geom_col() ## there&#39;s still some stopwords in there (yeah, ya, wanna, &#39;cause) ## but these can be removed with dplyr. 13.5 Non-Exercise R Code library(tidyverse) library(stringr) rblack &lt;- c(&quot;Oo-ooh-ooh, hoo yeah, yeah (Yeah, ah-ah-ah-ah-ah-ark) Yeah, yeah Yeah-ah-ah, yeah-ah-ah Yeah-ah-ah Yeah, yeah, yeah Seven a.m., waking up in the morning Gotta be fresh, gotta go downstairs Gotta have my bowl, gotta have cereal Seein&#39; everything, the time is goin&#39; Tickin&#39; on and on, everybody&#39;s rushin&#39; Gotta get down to the bus stop Gotta catch my bus, I see my friends (my friends) Kickin&#39; in the front seat Sittin&#39; in the back seat Gotta make my mind up Which seat can I take? It&#39;s Friday, Friday Gotta get down on Friday Everybody&#39;s lookin&#39; forward to the weekend, weekend Friday, Friday Gettin&#39; down on Friday Everybody&#39;s lookin&#39; forward to the weekend Partyin&#39;, partyin&#39; (yeah) Partyin&#39;, partyin&#39; (yeah) Fun, fun, fun, fun Lookin&#39; forward to the weekend Seven, forty five, we&#39;re drivin&#39; on the highway Cruisin&#39; so fast, I want time to fly Fun, fun, think about fun You know what it is I got this, you got this My friend is by my right, aye I got this, you got this Now you know it Kickin&#39; in the front seat Sittin&#39; in the back seat Gotta make my mind up Which seat can I take? It&#39;s Friday, Friday Gotta get down on Friday Everybody&#39;s lookin&#39; forward to the weekend, weekend Friday, Friday Gettin&#39; down on Friday Everybody&#39;s lookin&#39; forward to the weekend Partyin&#39;, partyin&#39; (yeah) Partyin&#39;, partyin&#39; (yeah) Fun, fun, fun, fun Lookin&#39; forward to the weekend Yesterday was Thursday, Thursday Today it is Friday, Friday (partyin&#39;) We-we-we so excited We so excited We gonna have a ball today Tomorrow is Saturday And Sunday comes afterwards I don&#39;t want this weekend to end It&#39;s Friday, Friday Gotta get down on Friday Everybody&#39;s lookin&#39; forward to the weekend, weekend (we gotta get down) Friday, Friday Gettin&#39; down on Friday Everybody&#39;s lookin&#39; forward to the weekend Partyin&#39;, partyin&#39; (yeah) Partyin&#39;, partyin&#39; (yeah) Fun, fun, fun, fun Lookin&#39; forward to the weekend It&#39;s Friday, Friday Gotta get down on Friday Everybody&#39;s lookin&#39; forward to the weekend, weekend Friday, Friday Gettin&#39; down on Friday Everybody&#39;s lookin&#39; forward to the weekend Partyin&#39;, partyin&#39; (yeah) Partyin&#39;, partyin&#39; (yeah) Fun, fun, fun, fun Lookin&#39; forward to the weekend&quot;) rblack rblack2 &lt;- str_split(rblack, &quot;\\n&quot;, simplify = TRUE) rblack2 str(rblack2) rblack3 &lt;- str_split(rblack2, c(&quot; &quot;)) rblack3 rblack4 &lt;- unlist(rblack3) rblack4 rblack_df &lt;- tibble(words = rblack4) rblack_df %&gt;% group_by(words) %&gt;% summarise(word_count = n()) %&gt;% arrange(desc(word_count)) rblack5 &lt;- str_remove(rblack4, pattern = c(&quot;,&quot;)) rblack5 rblack6 &lt;- str_to_lower(rblack5) rblack6 ##install.packages(&quot;tidytext&quot;) library(tidytext) stop_words rblack_df &lt;- tibble(words = rblack6) rblack_df rblack_small &lt;- anti_join(rblack_df, stop_words, by = c(&quot;words&quot; = &quot;word&quot;)) rblack_small rblack_small %&gt;% group_by(words) %&gt;% summarise(word_count = n()) %&gt;% arrange(desc(word_count)) "],["predictive-modeling-with-knn.html", " 14 Predictive Modeling with knn 14.1 Introduction to Classification 14.2 knn Introduction 14.3 Choosing Predictors and k 14.4 Chapter Exercises 14.5 Exercise Solutions 14.6 Non-Exercise R Code", " 14 Predictive Modeling with knn Goals explain why it’s necessary to use training data and test data when building a predictive model. describe the k-nearest neighbors (knn) procedure. interpret a confusion matrix. use knn to predict a the level of a categorical response variable. 14.1 Introduction to Classification k-nearest neighbors (or knn) is an introductory supervised machine learning algorithm, most commonly used as a classification algorithm. Classification refers to prediction of a categorical response variable with two or more categories. For example, for a data set with SLU students, we might be interested in predicting whether or not each student graduates in four years (so the response has two categories: graduates in 4 years or doesn’t). We might want to classify this response based on various student characteristics like anticipated major, GPA, standardized test scores, etc. knn can also be used to predict a quantitative response, but we’ll focus on categorical responses throughout this section. If you’ve had STAT 213, you might try to draw some parallels to knn and classification using logistic regression. Note, however, that logistic regression required the response to have two levels while knn can classify a response variable that has two or more levels. To introduce this, we will be using pokemon_full.csv data. Pokemon have different Types: we will use Type as a categorical response that we are interested in predicting. For simplicity, we will only use Pokemon’s primary type and we will only use 4 different types: set.seed(1119) library(tidyverse) pokemon &lt;- read_csv(&quot;data/pokemon_full.csv&quot;) %&gt;% filter(Type %in% c(&quot;Steel&quot;, &quot;Dark&quot;, &quot;Fire&quot;, &quot;Ice&quot;)) Our goal is to develop a k-nearest-neighbors model that is able to classify/predict Pokemon Type from a set of predictors, like Pokemon HP, Attack, Defense, etc. 14.1.1 Training and Test Data In order to develop our knn model (note that we still haven’t discussed what knn actually is yet!), we first need to discuss terms that applies to almost all predictive/classification modeling: training and test data. A training data set is a subset of the full data set used to fit various models. For the example below, the training data set will be just 15 observations for pedagogical purposes. More commonly, the training data set will contain 50%-80% of the observations in the full data set. A test data set consists of the remaining 20%-50% of the observations not in the training data set. A test data set is used to assess different the performances of various models that were fit using the training data set. Why do we need to do this division? Using the full data set for both training a model and testing that model is “cheating:” the model will perform better than it should because we are using each observation twice: once for fitting and once for testing. Having a separate test data set that wasn’t used to fit the model gives the model a more “fair” test, as these observations are supposed to be new data that the model hasn’t yet seen. The following code uses that sample_n() function to randomly select 15 observations to be in the training data set. anti_join() then makes a test data set without the 15 pokemon in the training data set. train_sample &lt;- pokemon %&gt;% sample_n(15) test_sample &lt;- anti_join(pokemon, train_sample) train_sample %&gt;% head() #&gt; # A tibble: 6 x 14 #&gt; X1 Name Type HP Attack Defense Speed SpAtk SpDef #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 491 Darkrai Dark 70 90 90 125 135 90 #&gt; 2 136 Flareon Fire 65 130 60 65 95 110 #&gt; 3 571 Zoroark Dark 60 105 60 105 120 60 #&gt; 4 221 Pilosw… Ice 100 100 80 50 60 60 #&gt; 5 668 Pyroar Fire 86 68 72 106 109 66 #&gt; 6 262 Mighty… Dark 70 90 70 70 60 60 #&gt; # … with 5 more variables: Generation &lt;dbl&gt;, #&gt; # Legendary &lt;lgl&gt;, height &lt;dbl&gt;, weight &lt;dbl&gt;, #&gt; # base_experience &lt;dbl&gt; test_sample %&gt;% head() #&gt; # A tibble: 6 x 14 #&gt; X1 Name Type HP Attack Defense Speed SpAtk SpDef #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 4 Charma… Fire 39 52 43 65 60 50 #&gt; 2 5 Charme… Fire 58 64 58 80 80 65 #&gt; 3 37 Vulpix Fire 38 41 40 65 50 65 #&gt; 4 38 Nineta… Fire 73 76 75 100 81 100 #&gt; 5 58 Growli… Fire 55 70 45 60 70 50 #&gt; 6 59 Arcani… Fire 90 110 80 95 100 80 #&gt; # … with 5 more variables: Generation &lt;dbl&gt;, #&gt; # Legendary &lt;lgl&gt;, height &lt;dbl&gt;, weight &lt;dbl&gt;, #&gt; # base_experience &lt;dbl&gt; The ideas of a training data set and test data set are pervasive in predictive and classification models, including models not related to knn. Note that we are going to do this method because it’s the simplest: if you wanted to take this a step further, you’d repeat the training and test process 5 or 10 times, using what’s known as k-fold cross-validation. 14.1.2 Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 14.5. Explain what anti_join() joins on when by isn’t specified and why not specifying a by argument works for this example. 14.2 knn Introduction 14.2.1 knn with k = 1 and 1 Predictor Suppose that we have just those 15 pokemon in our training data set. We want to predict Type from just one predictor, Defense. Below is a plot that shows the defenses of the 15 pokemon in our training data set, and has points coloured by Type and with different shapes for Type. ggplot(data = train_sample, aes(x = Defense, y = 1, colour = Type, shape = Type)) + geom_point(size = 4) + theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank()) We see from the plot that Steel type Pokemon tend to have pretty high defense values. Now suppose that we want to predict the Type for one of the Pokemon in our test data set, Dialga. We know that Dialga has a Defense stat of 120: the plot below shows Dialga marked with a large black X. dialga &lt;- test_sample %&gt;% slice(63) ggplot(data = train_sample, aes(x = Defense, y = 1, colour = Type, shape = Type)) + geom_point(size = 4) + theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank()) + geom_point(data = dialga, colour = &quot;black&quot;, shape = 4, size = 7) What would your prediction for Dialga be? Why? According to knn with k = 1, we would predict Dialga to be Fire type. k = 1 means that we are using the 1st nearest neighbor: in this case the point that is closest to Dialga is a green triangle, corresponding to a Fire type Pokemon. 14.2.2 knn with k &gt; 1 and One Predictor But, we might not necessarily want to predict the response value based on the single nearest neighbor. Dialga is also near many purple plus signs: should those factor in at all? We can extend knn to different values for k. For example, \\(k = 3\\) looks at the 3 nearest neighbors, and assigns a prediction as the category that appears the most among those 3 nearest neighbors. Using k = 3, what would the prediction for Dialga be? Why? 14.2.3 knn with k &gt; 1 and More Than One Predictor We can increase the number of predictors in our knn model as well. We can generally include as many predictors as we would like, but visualizing becomes challenging with more than 2 predictors and nearly impossible with more than 3 predictors. For the case of two predictors, suppose that we want to use Defense and Speed as our predictors for Type. Dialga, the Pokemon we want to predict for, is again marked with a large black X. ggplot(data = train_sample, aes(x = Defense, y = Speed, colour = Type, shape = Type)) + geom_point(size = 3) + geom_point(data = dialga, colour = &quot;black&quot;, shape = 4, size = 5) For \\(k = 1\\), we would predict the Dialga is Steel, as the closest point is the purple + sign in the top-left corner of the graph. For \\(k = 3\\), what Type would you predict for Dialga? For this question, it’s a little hard to tell which three points are closest to Dialga without computing the distances numerically, which is something we will let R do with the knn() function. 14.2.4 Scaling Predictor Variables before Using knn In general, we want to scale any quantitative predictors when using knn because it relies on distances between points in its predictions. This is easiest to see with an example. Suppose, in our Pokemon example, that we want to use height and weight as our predictors in the knn model. We just have 2 observations in our training data set: a Dark Type pokemon with a height of 15 centimeters and a weight of 505 pounds, and a Fire Type Pokemon with a height of 9 centimeters and a weight of 250 pounds. train_tiny &lt;- train_sample %&gt;% slice(1:2) newobs &lt;- tibble(height = 15, weight = 350, Type = &quot;Unknown&quot;) ggplot(data = train_tiny, aes(x = height, y = weight, shape = Type)) + geom_point(size = 5, aes(colour = Type)) + xlim(c(7, 17)) + ylim(c(200, 550)) + geom_point(data = newobs, shape = 4, size = 10) On the plot is also given a Pokemon in our test data set that we wish to predict the Type of, marked with a black X. Upon visual inspection, with k = 1, it looks like we would classify this pokemon as Dark. However, the units of weight and height are on very different scales. We will compute the actual distances in class to see if the conclusion from the calculation matches with the visual conclusion. To get around this issue, it is customary to scale all quantitative predictors before applying knn. One method of doing this is applying \\[ scaled_x = \\frac{x - min(x)}{max(x) - min(x)} \\] For example, scaling weight for the 15 original pokemon: train_sample %&gt;% select(weight) %&gt;% head() #&gt; # A tibble: 6 x 1 #&gt; weight #&gt; &lt;dbl&gt; #&gt; 1 505 #&gt; 2 250 #&gt; 3 811 #&gt; 4 558 #&gt; 5 815 #&gt; 6 370 puts all weights between 0 and 1: train_sample %&gt;% mutate(weight_s = (weight - min(weight)) / (max(weight) - min(weight))) %&gt;% select(weight_s) %&gt;% head() #&gt; # A tibble: 6 x 1 #&gt; weight_s #&gt; &lt;dbl&gt; #&gt; 1 0.187 #&gt; 2 0.0835 #&gt; 3 0.312 #&gt; 4 0.209 #&gt; 5 0.314 #&gt; 6 0.132 If we do the same with height, then the variables will contribute more “equally” to the distance metric used in knn. The code below scales all numeric variables in a data set, using the across() function. across() applies a transformation to every column in a data set that satisfies the condition given in the where argument. ## ?across library(pander) train_sample %&gt;% mutate(across(where(is.numeric), ~ (.x - min(.x)) / (max(.x) - min(.x)))) %&gt;% slice(1:3) #&gt; # A tibble: 3 x 14 #&gt; X1 Name Type HP Attack Defense Speed SpAtk SpDef #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.720 Darkrai Dark 0.417 0.444 0.4 1 1 0.658 #&gt; 2 0.193 Flareon Fire 0.333 0.889 0.1 0.368 0.619 0.921 #&gt; 3 0.838 Zoroark Dark 0.25 0.611 0.1 0.789 0.857 0.263 #&gt; # … with 5 more variables: Generation &lt;dbl&gt;, #&gt; # Legendary &lt;lgl&gt;, height &lt;dbl&gt;, weight &lt;dbl&gt;, #&gt; # base_experience &lt;dbl&gt; 14.2.5 Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 14.5. * Consider again the toy example with just two observations in the training data set and unscaled weight and height as predictors. ggplot(data = train_tiny, aes(x = height, y = weight, shape = Type)) + geom_point(size = 5, aes(colour = Type)) + xlim(c(7, 17)) + ylim(c(200, 550)) + geom_point(data = newobs, shape = 4, size = 10) The actual (height, weight) coordinates of the Fire pokemon are (9, 250), the actual coordinates of the Dark pokemon are (15, 505), and the actual coordinates of the test pokemon are (15, 350). We mentioned that, visually, the pokemon looks “closer” to the Dark type pokemon. Verify that this is not actually the case by computing the actual distances numerically. * After scaling according to the formula in this section, the coordinates (height, weight) of the Fire pokemon are (0, 0) and the coordinates of the Dark pokemon are (1, 1). (Since there are only two observations, the formula doesn’t give any output between 0 and 1 for this tiny example). The scaled coordinates for the test pokemon are (1, 0.39). Verify that, after scaling, the test pokemon is “closer” to the Dark type pokemon by numerically computing distances. Consider again the example with 15 pokemon in the training data set and a single predictor, Defense. ggplot(data = train_sample, aes(x = Defense, y = 1, colour = Type, shape = Type)) + geom_point(size = 4) + theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank()) + geom_point(data = dialga, colour = &quot;black&quot;, shape = 4, size = 7) With k = 2, there is a tie between Fire and Steel. Come up with a way in which you might break ties in a knn algorithm. Explain what knn would use a prediction for all test observations if k equals the number of observations in the training data set. What are some advantages for making k smaller and what are some advantages for making k larger? 14.3 Choosing Predictors and k We now know how knn classifies observations in the test data set, but how do we choose which predictors should be used by the knn algorithm? And how do we choose the number of neighbors, k? We want to measure how “good” models with different predictors and different k’s do, but we first need to define what “good” means. 14.3.1 The Confusion Matrix One definition of “good” in the classification context is a model that has a high proportion of correct predictions in the test data set. This should make some intuitive sense, as we would hope that a “good” model correctly classifies most Dark pokemon as Dark, most Fire pokemon as Fire, etc. In order to examine the performance of a particular model, we’ll create a confusion matrix that shows the results of the model’s classification on observations in the test data set. Note that in STAT 213, we didn’t call this a confusion matrix; we instead called this a classification table. The following video explains confusion matrices in more detail and should also cement the ideas of training and test data. https://www.youtube.com/watch?v=Kdsp6soqA7o. 14.3.2 Using knn in R To make a confusion matrix for a model using the pokemon data set, we first need to obtain predictions from a model. We’ll use the class library to fit a knn model to the pokemon data. Note that, instead of having 15 Pokemon in our training data set, we now have 70 pokemon to give a more reasonable number. The test set has the remaining 50 pokemon. The following code chunk sets a seed so that we all get the same training and test samples, scales all numeric variables in the pokemon data set, and then randomly selects 70 pokemon to be in the training sample. library(tidyverse) set.seed(11232020) ## run this line so that you get the same ## results as I do! ## scale the quantitative predictors pokemon_scaled &lt;- pokemon %&gt;% mutate(across(where(is.numeric), ~ (.x - min(.x)) / (max(.x) - min(.x)))) train_sample_2 &lt;- pokemon_scaled %&gt;% sample_n(70) test_sample_2 &lt;- anti_join(pokemon_scaled, train_sample_2) #&gt; Joining, by = c(&quot;X1&quot;, &quot;Name&quot;, &quot;Type&quot;, &quot;HP&quot;, &quot;Attack&quot;, &quot;Defense&quot;, &quot;Speed&quot;, &quot;SpAtk&quot;, &quot;SpDef&quot;, &quot;Generation&quot;, &quot;Legendary&quot;, &quot;height&quot;, &quot;weight&quot;, &quot;base_experience&quot;) The first knn model we will investigate will have HP, Attack, Defense, and Speed as predictors. The class library can fit knn models with a knn() function but requires the training and test data sets to have only the predictors that we want to use to fit the model. The knn() function also requires the response variable, Type, to be given as a vector. ## install.packages(&quot;class&quot;) library(class) ## create a data frame that only has the predictors ## that we will use train_small &lt;- train_sample_2 %&gt;% select(HP, Attack, Defense, Speed) test_small &lt;- test_sample_2 %&gt;% select(HP, Attack, Defense, Speed) ## put our response variable into a vector train_cat &lt;- train_sample_2$Type test_cat &lt;- test_sample_2$Type Now that the data has been prepared for the knn() function in the class library, we fit the model with 9 nearest neighbors. The arguments to knn() are train, a data set with the training data that contains only the predictors we want to use (and not other predictors or the response). test, a data set with the test data that contains only the predictors we want to use (and not other predictors or the response). cl, a vector of the response variable for the training data. k, the number of nearest neighbors. ## fit the knn model with 9 nearest neighbors knn_mod &lt;- knn(train = train_small, test = test_small, cl = train_cat, k = 9) knn_mod #&gt; [1] Ice Fire Fire Fire Fire Fire Steel Fire Ice #&gt; [10] Fire Fire Fire Fire Ice Ice Steel Ice Dark #&gt; [19] Ice Fire Steel Fire Fire Ice Fire Ice Steel #&gt; [28] Fire Fire Ice Dark Fire Fire Fire Dark Ice #&gt; [37] Ice Fire Ice Fire Fire Fire Fire Fire Fire #&gt; [46] Fire Fire Fire Ice Fire #&gt; Levels: Dark Fire Ice Steel The output of knn_mod gives the predicted categories for the test sample. We can compare the predictions from the knn model with the actual pokemon Types in the test sample with table(), which makes a confusion matrix: table(knn_mod, test_cat) #&gt; test_cat #&gt; knn_mod Dark Fire Ice Steel #&gt; Dark 0 3 0 0 #&gt; Fire 6 13 7 4 #&gt; Ice 5 5 2 1 #&gt; Steel 0 1 0 3 The columns of the confusion matrix give the actual Pokemon types in the test data while the rows give the predicted types from our knn model. The above table tells us that there were 0 pokemon that were Dark type that our knn model correctly classified as Dark. There were 6 pokemon that were Dark type that our knn model incorrectly classified as Fire. There were 5 pokemon that were Dark type and that our knn model incorrectly classified as Ice. In other words, correct predictions appear on the diagonal, while incorrect predictions appear on the off-diagonal. One common metric used to assess overall model performance is the model’s classification rate, which is computed as the number of correct classifications divided by the total number of observations in the test data set. In this case, our classification rate is (0 + 13 + 2 + 3) / 50 #&gt; [1] 0.36 Code to automatically obtain the classification rate from a confusion matrix is tab &lt;- table(knn_mod, test_cat) sum(diag(tab)) / sum(tab) #&gt; [1] 0.36 What does diag() seem to do in the code above? 14.3.3 Exercises Exercises marked with an * indicate that the exercise has a solution at the end of the chapter at 14.5. Change the predictors used or change k to improve the classification rate of the model with k = 9 and Attack, Defense, HP, and Speed as predictors. 14.4 Chapter Exercises There will be no chapter exercises for this section. Instead, we’ll devote some in-class time for you to begin work on your final project. 14.5 Exercise Solutions 14.5.1 Introduction to Classification S 14.5.2 knn Introduction S * Consider again the toy example with just two observations in the training data set and unscaled weight and height as predictors. ggplot(data = train_tiny, aes(x = height, y = weight, shape = Type)) + geom_point(size = 5, aes(colour = Type)) + xlim(c(7, 17)) + ylim(c(200, 550)) + geom_point(data = newobs, shape = 4, size = 10) The actual (height, weight) coordinates of the Fire pokemon are (9, 250), the actual coordinates of the Dark pokemon are (15, 505), and the actual coordinates of the test pokemon are (15, 350). We mentioned that, visually, the pokemon looks “closer” to the Dark type pokemon. Verify that this is not the case by computing the actual distances numerically. * After scaling according to the formula in this section, the coordinates (height, weight) of the Fire pokemon are (0, 0) and the coordinates of the Dark pokemon are (1, 1). (Since there are only two observations, the formula doesn’t give any output between 0 and 1 for this tiny example). The scaled coordinates for the test pokemon are (1, 0.39). Verify that, after scaling, the test pokemon is “closer” to the Dark type pokemon bu numerically computing distances. 14.5.3 Choosing Predictors and k S 14.6 Non-Exercise R Code set.seed(1119) library(tidyverse) pokemon &lt;- read_csv(&quot;data/pokemon_full.csv&quot;) %&gt;% filter(Type %in% c(&quot;Steel&quot;, &quot;Dark&quot;, &quot;Fire&quot;, &quot;Ice&quot;)) train_sample &lt;- pokemon %&gt;% sample_n(15) test_sample &lt;- anti_join(pokemon, train_sample) train_sample %&gt;% head() test_sample %&gt;% head() ggplot(data = train_sample, aes(x = Defense, y = 1, colour = Type, shape = Type)) + geom_point(size = 4) + theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank()) dialga &lt;- test_sample %&gt;% slice(63) ggplot(data = train_sample, aes(x = Defense, y = 1, colour = Type, shape = Type)) + geom_point(size = 4) + theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank()) + geom_point(data = dialga, colour = &quot;black&quot;, shape = 4, size = 7) ggplot(data = train_sample, aes(x = Defense, y = Speed, colour = Type, shape = Type)) + geom_point(size = 3) + geom_point(data = dialga, colour = &quot;black&quot;, shape = 4, size = 5) train_tiny &lt;- train_sample %&gt;% slice(1:2) newobs &lt;- tibble(height = 15, weight = 350, Type = &quot;Unknown&quot;) ggplot(data = train_tiny, aes(x = height, y = weight, shape = Type)) + geom_point(size = 5, aes(colour = Type)) + xlim(c(7, 17)) + ylim(c(200, 550)) + geom_point(data = newobs, shape = 4, size = 10) train_sample %&gt;% select(weight) %&gt;% head() train_sample %&gt;% mutate(weight_s = (weight - min(weight)) / (max(weight) - min(weight))) %&gt;% select(weight_s) %&gt;% head() ## ?across library(pander) train_sample %&gt;% mutate(across(where(is.numeric), ~ (.x - min(.x)) / (max(.x) - min(.x)))) %&gt;% slice(1:3) library(tidyverse) set.seed(11232020) ## run this line so that you get the same ## results as I do! ## scale the quantitative predictors pokemon_scaled &lt;- pokemon %&gt;% mutate(across(where(is.numeric), ~ (.x - min(.x)) / (max(.x) - min(.x)))) train_sample_2 &lt;- pokemon_scaled %&gt;% sample_n(70) test_sample_2 &lt;- anti_join(pokemon_scaled, train_sample_2) ## install.packages(&quot;class&quot;) library(class) ## create a data frame that only has the predictors ## that we will use train_small &lt;- train_sample_2 %&gt;% select(HP, Attack, Defense, Speed) test_small &lt;- test_sample_2 %&gt;% select(HP, Attack, Defense, Speed) ## put our response variable into a vector train_cat &lt;- train_sample_2$Type test_cat &lt;- test_sample_2$Type ## fit the knn model with 9 nearest neighbors knn_mod &lt;- knn(train = train_small, test = test_small, cl = train_cat, k = 9) knn_mod table(knn_mod, test_cat) (0 + 13 + 2 + 3) / 50 tab &lt;- table(knn_mod, test_cat) sum(diag(tab)) / sum(tab) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

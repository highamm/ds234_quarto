# Data Import {#import}

```{r}
#| echo: false
source("_common.R")
```

__Goals:__

* Use `readr` to read in data to `R` from .csv, .txt, and .tsv files.

* Use `rvest` to scrape data from public websites.

* Use `jsonlite` to read in data in JSON (Java Script Object Notation) files.

## `readr` to Read in Data 

Up to now, we have mostly worked with data that was "`R` Ready": meaning that it was in a nice .csv file that could be read into `R` easily with `read_csv()` from the `readr` package. We will begin by looking at some options in the `read_csv()` function and then move into formats other than .csv that data are commonly stored in.

### `read_csv()` Options

The `mtcarsex.csv` has observations on different car models with variables that include things like gas mileage, number of cylinders, etc. Read in the `mtcarsex.csv` data set with the following code. Then, examine the data set with `head()`.

```{r, message = FALSE, warning = FALSE, appendix = TRUE}
library(tidyverse)
library(here)
cars_df <- read_csv(here("data/mtcarsex.csv"))
head(cars_df)
```

What do you notice about the data set that seems odd? Open the .csv file with Excel or some other program to examine the data set outside of `R`.

Type in `?read_csv` in the bottom-left window and look at some of the options in `read_csv()`. In particular, we will use the `na` and the `skip` arguments to fix up our reading.

Let's start with `skip` so that we aren't reading in the first two rows of the data set:

```{r, message = FALSE, warning = FALSE, appendix = TRUE}
cars_df <- read_csv(here("data/mtcarsex.csv"), skip = 2)
## first two lines will be skipped
cars_df
```

That looks better, but there are still a couple of problems. What do you notice?

Go to the help and read about the `na` argument. Let's add that as an option to fix the missing value issue.

```{r, message = FALSE, warning = FALSE, appendix = TRUE}
cars_df <- read_csv(here("data/mtcarsex.csv"), na = c("NA", "-999"), skip = 2)
head(cars_df)
```

Now look at the classes of each variable. Which classes look like they are incorrect?

We've talked about how to re-specify classes of variables using `mutate()` and the `as.factor()` or `as.Date()` or `as.numeric()` functions, but sometimes it's easier just to respecify the class when we are reading in the data. Notice how, when we use `read_csv()`, `R` gives us a message about each of the column types. This is actually an argument in `read_csv()` called `col_types`. We can add a `|> spec()` piping statement after a `read_csv()` statement to tell `R` to print the `col_types` so that it's easy for us to copy and paste it into `read_csv()` and change any classes.

```{r}
read_csv(here("data/mtcarsex.csv"), na = c("NA", "-999"), skip = 2) |>
  spec()
```

For example, notice how `cyl = col_double()` is changed to `cyl = col_factor()` in the code chunk below:

```{r, appendix = TRUE}
cars_df <- read_csv(here("data/mtcarsex.csv"), na = c(NA, "-999"), skip = 2,
  col_types = cols(
  mpg = col_double(),
  cyl = col_factor(),
  disp = col_double(),
  hp = col_double(),
  drat = col_double(),
  wt = col_double(),
  qsec = col_double(),
  vs = col_factor(),
  am = col_double(),
  gear = col_double(),
  carb = col_double()
))
```

Finally, there are two rows with all missing values. These aren't providing anything useful so we can `slice()` them out:

```{r, appendix = TRUE}
cars_df <- read_csv(here("data/mtcarsex.csv"), na = c("NA", "-999"), skip = 2,
  col_types = cols(
  mpg = col_double(),
  cyl = col_factor(),
  disp = col_double(),
  hp = col_double(),
  drat = col_double(),
  wt = col_double(),
  qsec = col_double(),
  vs = col_factor(),
  am = col_double(),
  gear = col_double(),
  carb = col_double()
)) |>
  slice(-(1:2))
head(cars_df)
```

There are __many__ other possible file formats for data storage. For example, there is a data set called `oscars.tsv`, which is a tab-separated file. You can read it in with `read_tsv()` instead of `read_csv()`. 

```{r, message = FALSE, appendix = TRUE}
oscars_df <- read_tsv(here("data/oscars.tsv"))
head(oscars_df)
```

::: {.callout-note}
## Note

We'll be able to work with .txt files and Excel files in the Exercises. Check out <a href="https://rawgit.com/rstudio/cheatsheets/master/data-import.pdf" target="_blank">https://rawgit.com/rstudio/cheatsheets/master/data-import.pdf</a> for a data import cheatsheet. 
:::

The final issue that we will discuss in this section occurs when a data set has units within its cells. Consider the earlier example that we used in the reprex section:

```{r, message = FALSE, appendix = TRUE}
test_df <- read_csv(here("data/parsedf.csv"))
head(test_df)
```

The `parse_number()` function is really useful if we just want the number (no commas, no units, etc.). The function is often paired with `mutate()` since we are creating a new variable:

```{r, appendix = TRUE}
test_df |> mutate(x2 = parse_number(x))
```

__Exercise 1__. Recall the fitness data set.

```{r}
fitness_df <- read_csv(here::here("data/higham_fitness_notclean.csv"))
```

Use the `col_types` argument in `read_csv()` so that `stepgoal` is read in as a logical variable with `col_logical()` and so that `month` and `weekday` are both read in as factors.

## Data Scraping with `rvest`

Sometimes, we might want data from a public website that __isn't__ provided in a file format. To obtain this data, we'll need to use web scraping, a term which just means "getting data from a website." The easiest way to do this in `R` is with the `rvest` package. Note that we could spend an entire semester talking about web scraping, but we will focus only on websites where the scraping of data is "easy" and won't give us any major errors.

Go to the following website and suppose that you wanted the table of gun violence statistics in `R`: <a href="https://en.wikipedia.org/wiki/Gun_violence_in_the_United_States_by_state" target="_blank">https://en.wikipedia.org/wiki/Gun_violence_in_the_United_States_by_state</a>. We could try copy-pasting the table into Excel and reading the data set in with `read_excel()`. Depending on the format of the table, that strategy may work but it may not. Another way is to scrape it directly with `rvest`. Additionally, if the website continually updates (standings for a sports league, enrollment data for a school, best-selling products for a company, etc.), then scraping is much more convenient, as we don't need to continually copy-paste for updated data. 

In the following code chunk, `read_html()` reads in the entire html file from the url provided while `html_nodes()` extracts only the tables on the website.

```{r, results = "hide", message = FALSE, appendix = TRUE}
library(tidyverse)
library(rvest)

## provide the URL and name it something (in this case, url).
url <- "https://en.wikipedia.org/wiki/Gun_violence_in_the_United_States_by_state"

## read_html() convert the html code from the URL into something R can read
tab <- read_html(url) |> 
  html_nodes("table") ## html_nodes can grab only the tables 
```

We see that, for this example, there are 3 tables provided. The tables are stored in a `list` and we can reference the first table using `[[1]]`, the second table using `[[2]]`, etc. 

::: {.callout-important}
## Important

For the purposes of this class, we will figure out which of the 3 tables is the one we actually want using trial and error.
:::

The `html_table()` function converts the table into a `data.frame` object.

```{r, error = TRUE, results = "hide", appendix = TRUE}
test1 <- tab[[1]] |> html_table()
test1
test2 <- tab[[2]] |> html_table()
test2
test3 <- tab[[3]] |> html_table()
test3
```

Which of the 3 tables is the one that we would want to use for an analysis on gun violence in the United States?

As another example, consider scraping data from SLU's athletics page. In particular, suppose we want to do an analysis on SLU's baseball team.

Go to the following website to look at the table of data that we want to scrape: <a href="https://saintsathletics.com/sports/baseball/stats/2023" target="_blank">https://saintsathletics.com/sports/baseball/stats/2023</a>.

After looking at the website, use the following code to scrape the data set.

```{r, results = "hide", appendix = TRUE}
url <- "https://saintsathletics.com/sports/baseball/stats/2023"

tab <- read_html(url) |> html_nodes("table")
tab
obj <- tab[[1]] |> html_table()
obj
obj2 <- tab[[2]] |> html_table()
obj2
```

There's now 72 different tables! See if you can figure out where the first few tables are coming from on the website.

__Exercise 2__. SLU keeps track of diversity of students and makes this data public on the following website: <a href="https://www.stlawu.edu/offices/institutional-research/student-diversity-2021" target="_blank">https://www.stlawu.edu/offices/institutional-research/student-diversity-2021</a>. Use `rvest` to scrape one of the data tables into `R`. 


## JSON Files with `jsonlite`

A final common data format that we will discuss is JSON (JavaScript Object Notation). We will only cover the very basics of JSON data and use the `jsonlite` package in `R` to read in some .json files. JSON files are read in to `R` as a `list` object. 

### Everything Working Well

First, consider data from the mobile game Clash Royale. Install the `jsonlite` package and then use it to read in the `json` file with the function `fromJSON()`:

```{r, message = FALSE, warning = FALSE, appendix = TRUE}
## install.packages("jsonlite")
library(jsonlite)
cr_cards <- fromJSON(here("data/clash_royale_card_info.json"))
```

Next, type `View(cr_cards)` in your console (bottom-left) window to look at the data. See if you can pull out the data set by clicking on some things in the `View()` window.

The following give a couple of ways to grab the data using code. The `as_tibble()` function converts a rectangular object into our familiar `tibble`.

The first option specifies the name of the table that's in the JSON file (in this case, the name is `"cards"`):

```{r, appendix = TRUE}
library(tidyverse)
cr_cards_flat <- cr_cards[["cards"]]
cr_cards_df <- as_tibble(cr_cards_flat)
cr_cards_df
```

The second method uses the `flatten()` function from the `purrr` package, the only package in the core `tidyverse` that we do not talk about in detail in this class. There is also a different `flatten()` function in the `jsonlite` package. In the code below, we specify that we want to use `flatten()` from `purrr` with `purrr::flatten()`. If we wanted to use `flatten()` from `jsonlite`, we'd use `jsonlite::flatten()`

```{r, appendix = TRUE}
cr_cards_flat2 <- purrr::flatten(cr_cards)
cr_cards_df2 <- as_tibble(cr_cards_flat2)
cr_cards_df2
```

Both methods give a `tibble` that we can then use our usual `tidyverse` tools `ggplot2`, `dplyr`, `tidyr`, etc. on.

### Things Aren't Always So Easy

Now let's try to look at some animal crossing data that were obtained from <a href="https://github.com/jefflomacy/villagerdb" target="_blank">https://github.com/jefflomacy/villagerdb</a>. We first just want to look at the data from one individual villager (ace) in the file `ace.json`.

```{r, message = FALSE, warning = FALSE, appendix = TRUE}
acedata <- fromJSON(here("data/ace.json"))
aceflat <- purrr::flatten(acedata)
aceflat
```

Things are now....more complicated. This example is just to show that it's not always easy working with JSON data. Lists can be nested and that creates problems when trying to convert a deeply nested list into our "rectangular" format that's easy to work with. 

There's also the added problem of reading in the `.json` files from all villagers at the same time We could do this with a for loop or a mapping function from `purrr` to download and read in the JSON files for all villagers. We won't delve any more deeply into this, but there's a lot more to all of the file formats that we've discussed this week, particularly web scraping and .json files.

## Practice

### Class Exercises

__Class Exercise 1__. The `birthdays.txt` file has information on the birthdays of various animals on my Animal Crossing island. There are also columns for the Animal's Name, Animal Type, and how long the animal has lived on the island (in weeks). Click on the file to open it to look at the format of the data.

Start with the following code chunk and use the options of `read_delim()` to read in the data (`?read_delim`). The `delim` argument that's already provided specifies that the delimiter (separator) that you'll use is a `-`, as opposed to, for example, a `,` in a .csv file. Arguments that you may need to change include 

* `skip`
* `col_names`
* `na`
* `trim_ws`
* `col_types`

```{r, results = "hide", warning = FALSE, message = FALSE}
library(tidyverse)
df <- read_delim(here("data/birthdays.txt"), delim = " - ")
head(df)
```

```{r, results = "hide", message = FALSE, warning = FALSE, echo = FALSE}
read_delim(here("data/birthdays.txt"), delim = "-", skip = 4,
  col_names = c("Birthday", "Name",
    "Animal", "Island"),
  trim_ws = TRUE,
  col_types = list(
    col_character(), col_character(), col_character(), col_number()
  ), na = c("N/A", "?"))
```

__Class Exercise 2__. Another common format for data to be stored in is an Excel file. Often, it's easiest just to save the Excel file as a .csv file and read it in using `read_csv()`. But, sometimes this route can be difficult (for example, if your Excel file has thousands of sheets). To read in directly from Excel, you'll need to install the `readxl` with `install.packages("readxl")`. Once installed, load the package with `library(readxl)`, and read in the first sheet `evals_prof.xlsx` data set, a similar data set as the one that will be used for Project 2, with the `read_excel()` function.

__Class Exercise 3__. Now, read in the second sheet in the Excel file, using the help file for `?read_excel` to change one of the arguments.

__Class Exercise 4__. A common issue when scraping tables with `rvest` is that there are tables that have two rows of headers, often with duplicate names in one or both of the rows. Examine this issue at <a href="https://en.wikipedia.org/wiki/Josh_Allen_(quarterback)" target="_blank">https://en.wikipedia.org/wiki/Josh_Allen_(quarterback)</a>, scrolling down to the NFL career statistics subsection.

Try to scrape Josh Allen's stats using the "usual" data scraping method with `rvest`:

```{r}
#| output: false
url1 <- "https://en.wikipedia.org/wiki/Josh_Allen_(quarterback)"
tab_allen_stats <- read_html(url1) |> html_nodes("table")
allen_df <- tab_allen_stats[[6]] |> html_table()
allen_df
```

Try to perform __any__ operation on `allen_df` (a `filter()`, `mutate()`, `arrange()`, or anything else). What error do you get?

To fix this issue, we can use the `header = FALSE` argument to `html_table()` and then "manually" construct the column names:

```{r}
#| output: false
allen_stats <- tab_allen_stats[[7]] |> html_table(header = FALSE) 
allen_stats

newnames <- paste(allen_stats[1, ], allen_stats[2, ])
allen_stats |> set_names(newnames) |>
  slice(-1, -2)
```

Explain what the code above is doing to fix the duplicate name issue.

```{r}
#| echo: false
#| output: false
#| warning: false
#| eval: false
library(janitor)
url1 <- "https://en.wikipedia.org/wiki/Josh_Allen_(quarterback)"
tab_allen_stats <- read_html(url1) |> html_nodes("table")
allen_df <- tab_allen_stats[[7]] |> html_table()
allen_df |> janitor::row_to_names(1) |> clean_names()
allen_df |> clean_names()
```

### Your Turn

__Your Turn 1__. Choose a topic/person/place/etc. that interests you that has tables on Wikipedia and scrape the table that is related to that topic.

__Your Turn 2__. Choose a sports team at SLU, and go to that team's website (by simply googling SLU name_of_sport). Scrape the data tables from the "Results" or "Statistics" section of this sport. After you scrape the data, tidy the data set. Then, choose one of the following options (different options might make more/less sense for different sports)

a. Summarise different __team__ statistics, either numerically or graphically. Perhaps make some graphs showing different statistics through time. 

b. Summarise different __individual__ statistics, either numerically or graphically.

::: {.callout-note}
## Note

 A few sports (men's and women's golf, for example), give results in PDF format. PDF format is generally a horrible way to record and share data, as it's very difficult to read in to almost any program. Therefore, avoid sports with PDF results for the purposes of this exercise.
:::

__Your Turn 3__. For either your topic of choice data set or your sports team data set, ask and answer any other questions that make sense for the particular topic or sport that you are looking at!


<!-- ### Exercises {#exercise-8-4} -->

<!-- Exercises marked with an \* indicate that the exercise has a solution at the end of the chapter at \@ref(solutions-8). -->

<!-- 1. \* Read in the `pokedex.json` file, a data set that has information on the 151 original Pokemon. Then, use the `flatten()` function from `purrr` to flatten the list. -->

<!-- 2. \* Use `as_tibble()` to convert your flattened list to a `tibble`. -->

<!-- 3. \* Use `parse_number()` with `mutate()` to tidy two of the variables in the data set. -->

<!-- 4. \* Look at the `type` variable. What looks odd about it? What happens when you try to use it, either in a plot, or using a `dplyr` function? -->

<!-- You can `unnest()` the `Type` variable with the `unnest()` function from `tidyr`. We didn't discuss this function but feel free to read about it with `?unnest` -->

<!-- ```{r, echo = FALSE, message = FALSE, warning = FALSE} -->
<!-- library(jsonlite) -->
<!-- pokedex <- fromJSON(here("data/pokedex.json")) -->
<!-- df <- purrr::flatten(pokedex) -->
<!-- pokemon_df <- as_tibble(df) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- pokemon_unnest <- unnest(pokemon_df, cols = c(type)) -->
<!-- ``` -->

<!-- 5. There are 6 pokemon with a `spawn_chance` of 0. Figure out what these 6 pokemon are. -->

<!-- ```{r, echo = FALSE, results = "hide"} -->
<!-- pokemon_df |> arrange(spawn_chance) |> -->
<!--   filter(spawn_chance == 0) |> -->
<!--   select(spawn_chance, everything())  -->
<!-- ``` -->

<!-- 6. Figure out what the 5 most common Pokemon types are in the first generation (you'll need to use the `unnest()`-ed data set for this: why?).  -->

<!-- ```{r, echo = FALSE, results = "hide"} -->
<!-- pokemon_unnest |> group_by(type) |> -->
<!--   summarise(total_type = n()) |> -->
<!--   arrange(desc(total_type)) -->

<!-- ## 5 most common -->
<!-- pokemon_unnest |> group_by(type) |> -->
<!--   summarise(total_type = n()) |> -->
<!--   arrange(desc(total_type)) |> -->
<!--   slice(1:5) -->
<!-- ``` -->


<!-- 1. \* Read in the `pokedex.json` file, a data set that has information on the 151 original Pokemon. Then, use the `flatten()` function from `purrr` to flatten the list. -->

```{r}
#| eval: false
#| echo: false
library(jsonlite)
pokedex <- fromJSON(here("data/pokedex.json"))
df <- purrr::flatten(pokedex)
```

<!-- 2. \* Use `as_tibble()` to convert your flattened list to a `tibble`. -->

```{r}
#| eval: false
#| echo: false
pokemon_df <- as_tibble(df)
```

<!-- 3. \* Use `parse_number()` with `mutate()` to tidy two of the variables in the data set. -->

```{r}
#| eval: false
#| echo: false
pokemon_df <- pokemon_df |> mutate(height = parse_number(height),
                                    weight = parse_number(weight))
```

<!-- 4. \* Look at the `type` variable. What looks odd about it? What happens when you try to use it, either in a plot, or using a `dplyr` function? -->

```{r}
#| eval: false
#| echo: false
## it's a variable of lists....this is happening because some 
## pokemon have more than one type.

## most ggplot() and dplyr() functions won't work, or
## won't work as you'd expect
```

<!-- You can `unnest()` the `Type` variable with the `unnest()` function from `tidyr`. We didn't discuss this function but feel free to read about it with `?unnest` -->

```{r}
#| eval: false
#| echo: false
pokemon_unnest <- unnest(pokemon_df, cols = c(type))
```

